---
title: 'How do we combine errors? The linear case'
author: Andrea Onofri
date: '2019-04-15'
slug: 'Linear models'
categories:
  - R
  - 'Linear-models'
tags:
  - R
  - 'Linear_models'
  - 'Error_propagation'
archives:
  - 2019
---



<p>In our research work, we usually fit models to experimental data. Our aim is to estimate some biologically relevant parameters, together with their standard errors. Very often, these parameters are interesting in themselves, as they represent means, differences, rates or other important descriptors. In other cases, we use those estimates to derive further indices, by way of some appropriate calculations. For example, think that we have two parameter estimates, say Q and W, with standard errors respectively equal to <span class="math inline">\(\sigma_Q\)</span> and <span class="math inline">\(\sigma_W\)</span>: it might be relevant to calculate the amount:</p>
<p><span class="math display">\[Z = AQ + BW + C\]</span></p>
<p>where A, B and C are three coefficients. The above operation is named ‘linear combination’; it is a sort of a weighted sum of model parameters. The question is: what is the standard error of Z? I would like to show this by way of a simple biological example.</p>
<div id="example" class="section level1">
<h1>Example</h1>
<p>We have measured the germination rate for seeds of <em>Brassica rapa</em> at six levels of water potential in the substrate (-1, -0.9, -0.8, -0.7, -0.6 and -0.5 MPa). We would like to predict the germination rate for a water potential level of -0.75 MPa.</p>
<p>Literature references suggest that the relationship between germination rate and water potential in the substrate is linear. Therefore, as the first step, we fit a linear regression model to our observed data. If we are into R, the code to accomplish this task is shown below.</p>
<pre class="r"><code>GR &lt;- c(0.11, 0.20, 0.34, 0.46, 0.56, 0.68)
Psi &lt;- c(-1, -0.9, -0.8, -0.7, -0.6, -0.5)
lMod &lt;- lm(GR ~ Psi)
summary(lMod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = GR ~ Psi)
## 
## Residuals:
##          1          2          3          4          5          6 
##  0.0076190 -0.0180952  0.0061905  0.0104762 -0.0052381 -0.0009524 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.25952    0.02179   57.79 5.37e-07 ***
## Psi          1.15714    0.02833   40.84 2.15e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01185 on 4 degrees of freedom
## Multiple R-squared:  0.9976, Adjusted R-squared:  0.997 
## F-statistic:  1668 on 1 and 4 DF,  p-value: 2.148e-06</code></pre>
<p>It is clear that we can use the fitted model to calculate the GR-value at -0.75 MPa, as <span class="math inline">\(GR = 1.26 - 1.16 \times 0.75 = 0.39\)</span>. This is indeed a linear combination of model parameters, as we have shown above. Q and W are the estimated model parameters, while <span class="math inline">\(A = 1\)</span>, <span class="math inline">\(B = -0.75\)</span> and <span class="math inline">\(C = 0\)</span>.</p>
<p>Of course, the derived response is also an estimate and we need to give a measure of uncertainty. For both model parameters we have standard errors; the question is: how does the uncertainty in parameter estimates propagates to their linear combination? In simpler words: it is easy to build a weightes sum of model parameters, but, how do we make a weighted sum of their standard errors?</p>
<p>Sokal and Rohlf (1981) at pag. 818 of their book, show that, in general, it is:</p>
<p><span class="math display">\[ \textrm{var}(A \, Q + B \, W) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} \]</span></p>
<p>where <span class="math inline">\(\sigma_{QW}\)</span> is the covariance of Q and W. I attach the proof below; it is pretty simple to understand and it is worth the effort. However, several students in biology are rather reluctant when they have to delve into maths. Therefore, I would also like to give an empirical ‘proof’, by using some simple R code.</p>
<p>Let’s take two samples (Q and W) and combine them by using two coefficients A and B. Let’s calculate the variance for the combination:</p>
<pre class="r"><code>Q &lt;- c(12, 14, 11, 9)
W &lt;- c(2, 4, 7, 8)
A &lt;- 2
B &lt;- 3
C &lt;- 4
var(A * Q + B * W + C)</code></pre>
<pre><code>## [1] 35.58333</code></pre>
<pre class="r"><code>A^2 * var(Q) + B^2 * var(W) + 2 * A * B * cov(Q, W)</code></pre>
<pre><code>## [1] 35.58333</code></pre>
<p>We see that the results match exactly! In our example, the variance and covariance of estimates are retrieved by using the ‘vcov()’ function:</p>
<pre class="r"><code>vcov(lMod)</code></pre>
<pre><code>##              (Intercept)          Psi
## (Intercept) 0.0004749433 0.0006020408
## Psi         0.0006020408 0.0008027211</code></pre>
<pre class="r"><code>sigma2Q &lt;- vcov(lMod)[1,1]
sigma2W &lt;- vcov(lMod)[2,2]
sigmaQW &lt;- vcov(lMod)[1,2]</code></pre>
<p>The standard error for the prediction is simply obtained as:</p>
<pre class="r"><code>sqrt( sigma2Q + 0.75^2 * sigma2W - 2 * 0.75 * sigmaQW )</code></pre>
<pre><code>## [1] 0.004838667</code></pre>
</div>
<div id="the-functions-predict-and-glht" class="section level1">
<h1>The functions ‘predict()’ and ‘glht()’</h1>
<p>Now that we have understood the concept, we can use R to make the calculations. For this example, the ‘predict()’ method represents the most logical approach. We only need to pass the model object and the X value which we have to make a prediction for. This latter value needs to be organised as a data frame, with column name(s) matching the name(s) of the X-variable(s) in the original dataset:</p>
<pre class="r"><code>predict(lMod, newdata = data.frame(Psi = -0.75), 
        se.fit = T)</code></pre>
<pre><code>## $fit
##         1 
## 0.3916667 
## 
## $se.fit
## [1] 0.004838667
## 
## $df
## [1] 4
## 
## $residual.scale
## [1] 0.01185227</code></pre>
<p>Apart from the predict method, there is another function of more general interest, which can be used to build linear combinations of model parameters. It is the ‘glht()’ function in the ‘multcomp’ package. To use this function, we need a model object and we need to organise the coefficients for the transformation in a matrix, with as many rows as there are combinations to calculate. When writing the coefficients, we disregard C: we have seen that every constant value does not affect the variance of the transformation.</p>
<p>For example, just imagine that we want to predict the GR for two levels of water potential, i.e. -0.75 (as above) and -0.55 MPa. The coefficients (A, B) for the combinations are:</p>
<pre class="r"><code>Z1 &lt;- c(1, -0.75)
Z2 &lt;- c(1, -0.55)</code></pre>
<p>We pile up the two vectors in one matrix with two rows:</p>
<pre class="r"><code>M &lt;- matrix(c(Z1, Z2), 2, 2, byrow = T)</code></pre>
<p>And now we pass that matrix to the ‘glht()’ function as an argument:</p>
<pre class="r"><code>library(multcomp)
lcombs &lt;- glht(lMod, linfct = M, adjust = &quot;none&quot;)
summary(lcombs)</code></pre>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = GR ~ Psi)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(&gt;|t|)    
## 1 == 0 0.391667   0.004839   80.94 2.30e-07 ***
## 2 == 0 0.623095   0.007451   83.62 2.02e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<p>The above method can be easily extended to other types of linear models and linear combinations of model parameters. The ‘adjust’ argument is useful whenever we want to obtain familywise confidence intervals, which can account for the multiplicity problem. But this is another story…</p>
<p>Happy work with these functions!</p>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<p>We know that the variance for a random variable is defined as:</p>
<p><span class="math display">\[ var(Z) = \frac{1}{n-1}\sum \left(Z - \hat{Z}\right)^2 = \\ = \frac{1}{n-1}\sum \left(Z - \frac{1}{n} \sum{Z}\right)^2\]</span></p>
<p>If <span class="math inline">\(Z = AQ + BW + C\)</span>, where A, B and C are the coefficients and Q and W are two random variables, we have:</p>
<p><span class="math display">\[ var(Z) = \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ \left(AQ + BW + C\right)}\right]^2 = \\ 
= \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ AQ} - \frac{1}{n} \sum{ BW} - \frac{1}{n} \sum{ C}\right]^2 = \]</span></p>
<p><span class="math display">\[= \frac{1}{n-1}\sum \left[AQ + BW + C - A \hat{Q} - B \hat{W} - C \right]^2 = \\
= \frac{1}{n-1}\sum \left[\left( AQ - A \hat{Q}\right) + \left( BW - B \hat{W}\right) \right]^2 = \]</span></p>
<p><span class="math display">\[ =\frac{1}{n-1}\sum \left[A \left( Q - \hat{Q}\right) + B \left( W - \hat{W}\right) \right]^2 = \\
= \frac{1}{n-1}\sum \left[A^2 \left( Q - \hat{Q}\right)^2 + B^2 \left( W - \hat{W}\right)^2 + 2AB \left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right] =\]</span></p>
<p><span class="math display">\[ = A^2 \frac{1}{n-1} \sum{\left( Q - \hat{Q}\right)^2} + B^2 \frac{1}{n-1}\sum \left( W - \hat{W}\right)^2 + 2AB \frac{1}{n-1}\sum{\left[\left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right]}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[ \textrm{var}(Z) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{Q,W}\]</span></p>
</div>
