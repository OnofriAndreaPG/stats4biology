[
["index.html", "Metodologia di sperimentazione in agricoltura Capitolo 1 Introduzione 1.1 Organizzazione del corso 1.2 Esercitazioni pratiche 1.3 Obiettivi specifici del Corso", " Metodologia di sperimentazione in agricoltura Andrea Onofri 2019-01-20 Capitolo 1 Introduzione 1.1 Organizzazione del corso Questo corso si compone di due parti, che sono trattate in parziale sovrapposizione durante durante il semestre. Nella prima parte, sono fornite le basi teoriche e gli strumenti pratici per pianificare, organizzare e condurre esperimenti scientifici nel settore agrario. Nella seconda parte, dopo un’introduzione agli argomenti generali relativi alla biostatistica, ci occupiamo dell’analisi e dell’interpretazione dei dati ottenuti nelle prove scientifiche, nonché della presentazione dei risultati tramite tesi, report e/o pubblicazione scientifica. In questa seconda parte, daremo ampio spazio all’impiego di modelli matematici interpretativi, utilizzati sia con finalità descrittive, sia con finalità predittive. 1.2 Esercitazioni pratiche Questo corso non ha finalità puramente teoriche, ma fondamentalmente pratiche, improntate al ‘saper fare’. In sostanza, all fine del corso, gli studenti dovranno essere in grado di pianificare esperimenti ed analizzarne i risultati. E’importante quindi lavorare con alcuni casi studio relativi ad esperimenti molto comuni in ambito agrario, come le prove varietali, le prove di concimazione, le prove di diserbo, i dosaggi biologici e i saggi germinativi. Per poter risolvere gli esempi proposti, lo studente dovrà quindi avvalersi di un opportuno software statistiche. In questo caso, abbiamo due proposte: Excel (molto comune) e R (freeware e molto avanzato). A lezione tratteremo prevalentemente R, ma gli studenti, soprattutto quelli che non seguono le lezioni, potranno anche affidarsi ad Excel. In questa dispensa proporremo soluzioni per entrambi i software. 1.3 Obiettivi specifici del Corso Come dicevamo, il Corso ha un contenuto prettamente tecnico e gli obiettivi specifici sono fondamentalmente pratici, con una forte attenzione al ‘saper fare’. Gli studenti dovranno: Rispondere a domande generali sulla metodologia sperimentale e sull’organizzazione degli esperimenti Saper disegnare correttamente l’esperimento proposto (uno), scegliendo adeguatamente il layout sperimentale (disegno, numero di repliche) e disegnando la mappa in modo corretto. Gli esperimenti proposto apparterranno alle seguenti categorie: prove di confronto varietale, prove di confronto tra erbicidi, prove di degradazione dei fitofarmaci, dosaggi biologici con erbicidi, prove di concimazione, prove di epoca di semina; Saper eseguire l’ANOVA per il dataset proposto (uno), scelto tra quelli riportati nell’apposito elenco; Saper adattare il modello opportuno (lineare o nonlineare) ad un dataset assegnato, scelto tra quelli riportati nell’apposito elenco. Saper valutare la bontà di adattamento del modello. Per il punto 3 e il punto 4 gli studenti si dovranno avvalere di un supporto informatico, ad esempio costituito dal software R (citation) o da Excel, con la macro DSAASTAT (per l’ANOVA). "],
["il-metodo-sperimentale-quando-la-scienza-e-scienza.html", "Capitolo 2 Il metodo sperimentale: quando la scienza è scienza 2.1 Introduzione 2.2 Esperimenti buoni e cattivi! 2.3 Scienza = metodo 2.4 Chi valuta se un esperimento è attendibile? 2.5 Il metodo sperimentale 2.6 Metodi sperimentali validi ed invalidi 2.7 Incertezza residua 2.8 Il ruolo della statistica 2.9 Conclusioni", " Capitolo 2 Il metodo sperimentale: quando la scienza è scienza 2.1 Introduzione In una società caratterizzata dal sovraccarico cognitivo immagino sia giusto chiedersi (e chiedere) che cosa sia la scienza, cosa distingua le informazioni scientifiche da tutto quello che invece non è altro che pura opinione, magari autorevole, ma senza il sigillo dell’oggettività. Per quanto affascinante possa sembrare l’idea del ricercatore che con un’improvviso colpo di genio elabora una stupefacente teoria, dovrebbe essere chiaro che l’intuizione è solo un possibile punto di partenza, che non necessariamente prelude al progresso scientifico, per quanto geniale ed innovativa possa essere. In generale, almeno in ambito biologico, nessuna teoria acquisisce automaticamente valenza scientifica, ma rimane solo nell’ambito delle opinioni, indipendentemente dal fatto che nasca da un colpo di genio, oppure grazie ad un paziente e meticoloso lavoro di analisi intellettuale, che magari si concretizza in un modello matematico altamente elegante e complesso. Da un punto di vista puramente intuitivo, è ovvio aspettarsi che una prova scientifica debba uscire dall’ambito delle opinioni legate a divergenze di cultura, percezione e/o credenze individuali, per divenire, al contrario, oggettiva e universalmente valida, distinguendosi quindi da altre verità di natura metafisica, religiosa o pseudoscientifica. Che cosa è che permette alla scienza di divenire tale? A questo proposito, riporto alcuni aforismi significativi: Proof is a justified true belief (Platone, Dialoghi) The interest I have in believing a thing is not a proof of the existence of that thing (Voltaire) A witty saying proves nothing (Voltaire) 2.1.1 Cosa è quindi una prova scientifica? La base di tutta la scienza risiede nel cosiddetto ‘metodo scientifico’, che si fa comunemente risalire a Galileo Galilei (1564-1642) e che è riassunto nella figura seguente. Il metodo scientifico Galileiano Senza andare troppo in profondità, è importante notare due aspetti: il ruolo fondamentale dell’esperimento scientifico, che produce dati a supporto di ipotesi pre-esistenti; lo sviluppo di teorie basate sui dati, che rimangono valide fino a che non si raccolgono altri dati che le confutano, facendo nascere nuove ipotesi che possono portare allo sviluppo di nuove teorie, più affidabili o più semplici. Insomma, l’ingrediente fondamentale di una prova scientifica è che è supportata da un insieme dei dati sperimentali; di fatto, non esiste scienza senza dati! Resta famoso l’aforisma “In God we trust, all the others bring data”, attribuito all’ingegnere e statistico americano W. Edwards Deming (1999-1923), anche se pare che egli in realtà non l’abbia mai pronunciato. 2.2 Esperimenti buoni e cattivi! Non tutti gli esperimenti sono buoni e, di conseguenza, non tutti i dati sono buoni. In particolare, due sono gli elementi che possono portare a dati di diversa affidabilità: Errore sperimentale Campionamento Vediamo qualche dettaglio in più a proposito di questi due elementi. 2.2.1 L’errore sperimentale Alla base della raccolta di dati sperimentali vi è un processo di misurazione, attraverso la quale il fenomeno oggetto di studio viene caratterizzato con appositi strumenti scientifici, più o meno complessi. Il problema è che nessuna misura può essere considerata precisa in senso assoluto, cioè perfettamente coincidente col valore reale della grandezza misurata, che è destinato a rimanere un’entità incognita e indeterminabile. In particolare, in ogni esperimento scientifico esiste un potenziale elemento di confusione che gli scienziati conoscono con il termine di errore sperimentale, con la cui presenza è necessario confrontarsi sempre e comunque. Nel misurare una determinata grandezza fisica, indipendentemente dal metodo scelto per la misura, possiamo sempre commettere due tipi di errore: sistematico ed accidentale (casuale). L’errore sistematico è provocato da difetti intrinseci dello strumento o incapacità peculiari dell’operatore e tende a ripetersi costantemente in misure successive. Un esempio tipico è quello di una bilancia non tarata, che tende ad aggiungere 20 grammi ad ogni misura che effettuiamo. Per queste sue peculiarità, l’errore sistematico non è quantificabile e deve essere contenuto al minimo livello possibile tramite la perfetta taratura degli strumenti e l’adozione di metodi di misura rigidamente standardizzati e accettati dalla comunità scientifica mondiale. L’errore accidentale (o casuale) è invece legato a fattori variabili nel tempo e nello spazio, quali: malfunzionamenti accidentali dello strumento. Si pensi ad esempio al rumore elettrico di uno strumento, che fa fluttuare i risultati delle misure effettuate; imprecisioni o disattenzioni casuali dell’operatore. Si pensi ad esempio ad un banale errore di lettura dello strumento, che può capitare soprattutto ad un operatore che esegua moltissime misure manuali con procedure di routine; irregolarità dell’oggetto da misurare} unite ad una precisione relativamente elevata dello strumento di misura. Si pensi alla misurazione del diametro di un melone con un calibro: è facile che compaiano errori legati all’irregolarità del frutto o al fatto che l’operatore non riesce a misurare lo stesso nel punto in cui il suo diametro è massimo. Oppure, più semplicemente si pensi alla misurazione della produzione di granella di una certa varietà di frumento: anche ipotizzando di avere uno strumenti di misura perfetto e quindi esente da errore, la produzione mostrerebbe comunque una fluttuazione naturale da pianta a pianta, in base al patrimonio genetico e, soprattutto, in base alle condizioni di coltivazione che non possono essere standardizzate oltre ad un certo livello (si pensi alla variabilità del terreno agrario). Dato che queste imprecisioni sono assolutamente casuali è chiaro che le fluttuazioni positive (misura maggiore di quella vera) sono altrettanto probabili di quelle negative e tendono a presentarsi con la stessa frequenza quando si ripetano le misure più volte. Di conseguenza, l’errore sperimentale casuale può essere gestito attraverso la replicazione delle misure: infatti, se ripeto una misura soggetta a questo tipo di errore, nel lungo periodo gli errori positivi e negativi tendono ad annullarsi reciprocamente e la media delle misure effettuate tende quindi a coincidero con il valore reale della grandezza da misurare. 2.2.2 Il campionamento Se è vero, e la pratica sperimentale lo conferma, che ripetere le misure porta ad ottenere molti risultati diversi, nasce il problema di capire quante repliche sono necessarie. Se si ripensa a quanto detto finora, dovrebbe risultare evidente che, per ottenere una misura pari all’effettivo (reale) valore della grandezza da misurare, bisognerebbe effettuarne infinite. Tuttavia è altrettanto evidente che questo procedimento è totalmente improponibile!!! Qual è la strada seguita dagli scenziati, quindi? E’ quella di raccogliere un numero finito di misure, sufficientemente basso da essere compatibile con le umane risorse di tempo e denaro, ma sufficientemente alto da essere giudicato attendibile. Qualunque sia questo valore finito, è evidente che ci troviamo difronte solo ad un campione delle infinite misure che avremmo dovuto fare, ma che non abbiamo fatto. La domanda è: questo campione è rappresentativo o no? E’ in grado di descrivere adeguatamente la realtà? E’ possibile che gli errori sperimentali positivi e negativi non si siano annullati tra loro, confondendosi con l’effetto biologico in studio? In altre parole: possiamo fidarci dei dati che abbiamo raccolto? La possibilità di raccogliere dati sbagliati è tutt’altro che remota. Gli scienziati american Pons e Fleischmann il 23 Marzo del 1989 diffusero pubblicamente la notizia di essere riusciti a riprodurre la fusione nucleare fredda, causando elevatissimo interesse nella comunità scientifica. Purtroppo le loro misure erano vizite da una serie di problemi e il loro risultato fu clamorosamente smentito da esperimenti successivi. Conseguenze di un esperimento sbagliato 2.3 Scienza = metodo Insomma, la scienza deve essere basata sui dati, ma i dati contengono inevitabili fonti di incertezza, legate all’errore sperimentale e al processo di campionamento. Come si può procedere in queste condizioni? Il punto fondamentale è quello di adottare un metodo sperimentale che consenta di ottenere dati il più affidabili possibile. Insomma, questa semplice affermazione significa che bisogna fare eseperimenti ben condotti, precisi, seguendo procedure standardizzate e/o largamente condivise dalla comunità scientifica. Certo è che, per quanto detto in precedenza, il fatto che i dati provengano da un processo di campionamento impedisce, di fatto, di ottenere un’affidabilità totale. Cosa succederebbe se ripetessimo l’esperimento? Insomma, bisogna fare alcune considerazioni, che elenco di seguito: in primo luogo si dovrà accettare il fatto che, contrariamente a quanto si potrebbe o vorrebbe credere, non esistono prove scientifiche totalmente certe, ma l’incertezza è un elemento intrinseco della scienza. In secondo luogo si dovranno utilizzare gli strumenti della statistica necessari per quantificare l’incertezza residua, che dovrà essere sempre riportata a corredo dei risultati di ogni esperimento scientifico. Ogni risultato sarà quindi valutato dalla comunità scientifica sullo sfondo della sua incertezza, seguendo alcune regole di natura probabilistica che consentono di stabilire se la prova scientifica è sufficientemente forte per essere considerata tale. Un elemento fondamentale di valutazione della bontà di un esperimento e dei dati da esso ottenuti sta nella cosiddetta replicabilità, cioè nella probabilità di ottenere risultati molto simili (se non uguali) replicando l’esperimento in condizioni analoghe. Per valutare se un esperimento è replicabile è necessario che questo sia descritto con un grado di dettaglio tale da permettere a chinque di ripeterlo, ottenendo risultati comparabili e non contraddittori. Nessun risultato di cui non sia provata la riproducibilità è da considerarsi valido. E’chiaro comunque che ogni esperimento può essere smentito. Questo non è un problema: la scienza è pronta a considerare una prova scientifica valida fino a che non si raccolgono dati altrettanto affidabili che la confutino. In questo caso, si abbandona la teoria confutata e si abbraccia la nuova. L’abbandono può anche non essere totale: ad esempio la teoria gravitazionale di Newton è ancora oggi valida per molto situazioni pratiche, anche se è stata abbandonoata in favore della teoria della relatività, che spiega meglio il moto dei corpi ad altissime velocità. In effetti, la scienza considere sempre con attenzione il principio del rasoio di Occam, per il quale si accetta sempre la teoria più semplice per interpretare una dato fenomeno, riservando le teorie più complesse alle situazioni più difficili, che giustificano tale livello di complessità. 2.4 Chi valuta se un esperimento è attendibile? Quanto detto finora vorrebbe chiarire come il punto centrale della scienza non è la certezza delle teorie, bensì il metodo che viene utilizzato per definirle. Ognuno di noi è quindi responsabile di verificare che le informazioni in suo possesso siano ‘scientificamente’ attendibili, cioè ottenute con un metodo sperimentale adeguato. Il fatto è che non sempre siamo in grado di compiere questa verifica, perché non abbiamo strumenti ‘culturali’ adeguati, se non nel ristretto ambito delle nostre competenze professionali. Come fare allora? L’unica risposta accettabile è quella di controllare l’attendibilità delle fonti di informazione. In ambito biologico, le riviste autorevoli sono caratterizzate dal procedimento di ‘peer review’, nel quale i manoscritti scientifici, prima della pubblicazione, sono sottoposti ad un comitato editoriale ed assegnati ad un ‘editor’, il quale legge il lavoro e contemporaneamente lo invia a due o tre scienziati anonimi e particolarmente competenti in quello specifico settore scientifico (reviewers o revisori). I revisori, insieme all’editor, compiono un attento lavoro di esame e stabiliscono se l’evidenza scientifica presentata è sufficientemente ‘forte’. Le eventuali critiche vengono presentate all’autore, che è tenuto a rispondere in modo convincente, anche ripetendo gli esperimenti se necessario. Il processo richiede spesso interi mesi ed è abbastanza impegnativo per uno scenziato. E’ piuttosto significativa l’immagine presentata in scienceBlog.com, che allego qui. Il processo di peer review In sostanza il meccanismo di peer review è l’analogo scientifico di un processo, nel quale l’inputato (lavoro scientifico) viene assolto (rilasciato, leggi: rigettato) in presenza di qualunque ragionevole dubbio metodologico. Attenzione: il dubbio che non deve esistere è quello metodologico, dato che il dubbio sul risultato non può essere allontanato completamente e i reviewer controlleranno solo che esso si trovi al disotto della soglia massima, stabilita con metodiche statistiche. Questo procedimento, se effettuato con competenza, dovrebbe aiutare a separare la scienza dalla pseudo-scienza e, comunque, ad eliminare la gran parte degli errori metodologici dai lavori scientifici. 2.5 Il metodo sperimentale Almeno in ambito biologico, la definizione del metodo sperimentale è fondamentalmente attribuita allo scienziato inglese Ronald Fisher (1890-1962), che l’ha esplicitata nel suo famoso testo del 1935 (The design of experiments). Mi sembra opportuno riassumerla nelle tre espressioni ‘chiave’: controllo locale degli errori, replicazione e randomizzazione. Si tratta di: contenere al massimo possibile l’errore sperimentale, con l’adozione di tecniche opportune, in modo da separare le fonti di variabilità, isolando qualla oggetto di studio (controllo locale degli errori); replicare le misure più volte (replicazione) Scegliere le unità sperimentali da misurare in modo totalmente casuale, così da avere un campione rappresentativo ed evitare di confondere gli effetti prodotti dall’errore sperimentale con quelli prodotti dal fenomeno biologico oggetto di studio (randomizzazione) Vediamo ora un’esempio banale di come procedere. 2.6 Metodi sperimentali validi ed invalidi Immaginiamo un ricercatore che abbia un’idea brillante: egli ha inventato un nuovo fertilizzante ‘prodigioso’. E’ evidente che non può presentarsi alla comunità scientifica declamando le doti di questo fertilizzante, in quanto egli verrebbe immediatamente esposto al pubblico ludibrio, perchè sta presentando delle opinioni, non delle evidenze scientifiche (almeno, così dovrebbe essere, in una società sana… purtroppo in un’era di pseudo-scienza siamo sempre pronti a dar credito a chiunque, senza un’adeguata dose di scetticismo… ) 2.6.1 Primo esperimento Come ogni scenziato, egli deve raccogliere dati. E lo fa, organizzando un esperimento, nel quale prende un campo di mais e lo fertilizza con il suo nuovo composto, ritraendo una produzione del 20% superiore a quella usuale. Ovvoamente, se prova a pubblicare questa notizia, il suo lavoro verrà certamente (si spera…) rigettato, in quanto rimane il dubbio su chi sia la causa dell’effetto riscontrato: il fertilizzante? il clima dell’anno di prova? il suolo? la varietà di mais impiegata? E’ chiaro che questo non è un esperimento controllato: il campo trattato e quello non trattato (riferimento) non sono totalmente uguale, eccetto che per il fertilizzante impiegato. 2.6.2 Secondo esperimento A questo punto il ricercatore pianifica un esperimento comparativo controllato: prende due campio di mais, vicini, con lo stesso terreno, semina la stessa varietà di mais e coltiva i due campi esattamente nello stesso modo, con l’unica differenza che in uno di essi somministra il fertilizzante in studio (campo trattato) e nell’altro no (testimone o controllo). Alla fine osserva che il campo trattato produce 130 tonnellate per ettaro, mentre quello non trattato ne produce 115 e conclude che il nuovo fertilizzante è efficace (+ 12% circa). Infatti egli ritiene che, dato che i due campi sono totalmente uguali, l’incremento di produzione non possa che essere attribuito al fertilizzante. Scrive un report, che, purtroppo, viene rigettato. Anche se questo secondo esperimento è meglio del primo, permane tuttavia il dubbio che l’effetto si sia prodotto per caso. Potrebbe infatti esserci stata una qualche situazione non osservata che ha avvantaggiato uno dei due campi. Ad esempio un attacco di insetti, una carenza idrica, o qualsivoglia altra situazione. Questo vi sembra improbabile? Non importa, con una sola osservazione il ricercatore non è in grado di provare che il risultato è replicabile. 2.6.3 Terzo esperimento Avendo imparato la lezione, il ricercatore fa un nuovo esperimento, utilizzando stavolta otto campi: quattro trattati e quattro non trattati. Anche in questo caso osserva un incremento produttivo medio del 12% circa ed è sicuro che l’effetto è replicabile, perché lo ha osservato più volte. Purtroppo, anche questo esperimento non viene considerato affidabile e, di conseguenza, il lavoro non è pubblicabile. Stavolta il problema è che il ricercatore ha scelto i campi trattati con un criterio sistematico (un campo trattato ed uno ‘tradizionale’ contiguo), cosìcchè i campi trattati sono tutti a sinistra di quelli ‘tradizionali’. Ciò crea un ragionevole dubbio: e se vi fosse un gradiente di fertilità da destra verso sinistra? Questo potrebbe dare origine ad una produttività maggiore dei campi a destra, rispetto a quelli a sinistra. In presenza di questo ‘ragionevole’ dubbio, la prova non può avere valenza scientifica. 2.6.4 Quarto esperimento: quello buono Il ricercatore prende allora otto campi ed assegna il trattamento a quattro di essi, scelti in modo totalmente casuale. In questo caso è sicuro che, anche se vi fosse un qualche elemento estraneo di confusione (gradiente di fertilità, attacco di insetti…), esso dovrebbe colpire le unità sperimentali casualmente disposte senza creare vantaggi particolari all’uno o all’altro dei due trattamenti. Ovviamente egli non è certo (e non può esserlo) che l’esperimento sia del tutto attendibile; infatti potrebbe essere stato così sfortunato che un qualche elemento estraneo ignoto si è accanito proprio sulle parcelle non trattate, danneggiandone la produttività. Solo che, grazie alla scelta casuale, questa evenienza diviene altamente improbabile, così da rendere i dubbi irragionevoli. In questo caso l’esperimento è controllato, replicato e randomizzato e il risultato ottenuto, in quanto ragionevolmente attendibile, può essere pubblicato. 2.7 Incertezza residua Insomma, un esperimento valido è controllato, replicato e randomizzato. Tuttavia le misure raccolte sono poche e sono solo un campione di tutte quelle possibili. Infatti il nostro ricercatore ha usato otto campi, ma ne avrebbe potuti usare 16, 32 e così via. Rimane quindi il dubbio, che, se facessimo altre misure (cioè ampliassimo il campione), queste potrebbero invalidare i risultati ottenuti fino a quel momento. Se mi è concesso un paragone calcistico, è un po’ come chiedersi come finirà una partita di calcio dopo aver assistito solo al primo tempo: in alcune circostanze, quando una delle due squadre ha mostrato una chiara superiorità, la previsione è abbastanza facile, mentre in altre circostanze l’equivalenza dei valori in campo la rende alquanto difficile. In tutti i casi, si tratta solo di una previsione, che può essere sempre smentita alla prova dei fatti. Anche la scienza funziona così. Noi osserviamo solo il primo tempo, che, nel caso del nostro ricercatore, consiste di otto misure. Osserviamo che le quattro misure del fertilizzato sono tutte in modo consistente molto più alte di quelle del non trattato e quindi possiamo concludere, con ragionevole certezza, che il fertilizzante è efficace. Altrimenti, se la produzione media del trattato è solo lievemente più alta, il nostro esperimento potrà essere inconclusivo, cioè incapace di fugare i dubbi sull’effettiva efficacia del nostro fertilizzante. Avremo bisogno di fare altre prove di conferma. 2.8 Il ruolo della statistica Abbiamo visto che un esperimento scientifico, anche se ben fatto (controllato, replicato e randomizzato), può portare a evidenze scientifiche più o meno forti. In quest’ottica, la statistica ci fornisce gli strumenti per riassumere le misure effettuate, calcolarne l’incertezza e rappresentare la forza dell’evidenza scientifica, in modo da poter prendere decisioni sull’efficacia dei trattamenti e sull’esigenza di ulteriori verifiche. Imparare a conoscere e comprendere questi strumenti statistici è l’obiettivo di questo corso. 2.9 Conclusioni In conclusione, possiamo ripartire dalla domanda iniziale: “Che cosa è la scienza?”, per rispondere che è scienza tutto ciò che è supportato da dati che abbiano passato il vaglio della peer review, dimostrando di essere stati ottenuti con un procedimento sperimentale privo di vizi metodologici e di essere sufficientemente affidabili in confronto alle fonti di incertezza cui sono associati. Qual è il take-home message di questo articolo? Fidatevi solo delle riviste scientifiche attendibili, cioè quelle che adottano un serio processo di peer review prima della pubblicazione. "],
["introduzione-al-disegno-sperimentale.html", "Capitolo 3 Introduzione al disegno sperimentale 3.1 Definizioni 3.2 Elementi fondamentali del disegno sperimentale 3.3 Progettazione di un esperimento (protocollo) 3.4 Come scrivere un progetto di ricerca o un report di ricerca 3.5 Per approfondimenti", " Capitolo 3 Introduzione al disegno sperimentale 3.1 Definizioni La ricerca scientifica trova la sua unità elementare nell’esperimento, cioè un processo investigativo, con il quale, seguendo un adeguato protocollo, si osserva e si misura la risposta prodotta da uno o più fattori sperimentali nei soggetti coinvolti nello studio. Raramente gli esperimenti sono isolati, più spesso fanno parte di uno sforzo collettivo organizzato, generalmente identificato come progetto di ricerca. Ogni esperimento deve essere attentamente pianificato. Infatti, sappiamo che la variabilità esistente tra soggetti sperimentali, il campionamento, le irregolarità di misura e molti altri fattori perturbativi ci impediscono di osservare la realtà con assoluta precisione. E’ come se osservassimo un fenomeno attraverso una sorta di lente deformante, che ci impone di adottare un metodo sperimentale rigoroso, per evitare di attribuire al fenomeno in studio effetti che sono invece puramente casuali. Schematizzazione del processo sperimentale In particolare, gli esperimenti debbono essere: Precisi Accurati Replicabili/Riproducibili In mancanza di questi requisiti, al termine dell’esperimento possono rimanere dubbi sui risultati, tali da inficiare la validità delle conclusioni raggiunte. Forse vale la pena di chiarire cosa si intende con precisione, accuratezza e replicabilità/riproducibilità. Abbiamo già visto che la presenza dell’errore sperimentale ci impone di ripetere le misure più volte. La precisione di un esperimento non è altro che la variabilità dei risultati tra una replica e l’altra. La precisione, da sola, non garantisce che l’esperimento sia affidabile. Abbiamo menzionato nel capitolo precedente che l’errore sperimentale può essere casuale o sistematico. Quest’ultimo può essere dovuto, per esempio, ad uno strumento non accurato che sovrastima tutte le misure. In questo caso, posso ripetere cento volte la misura, ottenendo sempre lo stesso risultato, molto preciso, ma totalmente inaffidabile, nel senso che non riflette la misura reale del soggetto. E’questa l’accuratezza, cioè la capacità di una misura (o di un esperimento) di restituire il valore reale, anche se dopo come media di numero molto elevato di replicazioni. L’accuratezza è molto più importante della precisione: infatti una misura accurata, anche se imprecisa, riflette bene la realtà, anche se in modo vago. Al contrario, una misura precisa ma inaccurata ci porta completamente fuori strada, perchè non riflette la realtà! Un esperimento/risultato non accurato si dice ‘distorto’ (biased). Oltre a precisione ed accuratezza, siamo anche interessati alla replicabilità di un esperimento, cioè alla possibilità che questo, se ripetuto in condizioni assolutamente analoghe (stessi soggetti, ambiente, strumenti…) restituisca risultati equivalenti. Alcuni biostatistici distinguono la replicabilità dalla riproducibilità, in quanto considerano quest’ultima come la possibilità di ottenere risultati equivalenti ripetendo una misura in condizioni diverse (diversi soggetti, diverso ambiente…). E’evidente che un esperimento può essere totalmente accurato e replicabile, ma non riproducibile con soggetti e condizioni ambientali diverse. Se è così, le conclusioni raggiunte, anche se accurate, non possono essere generalizzate. 3.2 Elementi fondamentali del disegno sperimentale La metodica di organizzazione di un esperimento prende il nome di disegno sperimentale e deve essere sempre adeguatamente formalizzata tramite la redazione di un protocollo sperimentale sufficientemente dettagliato da consentire a chiunque la replicazione dell’esperimento e la verifica dei risultati. Le basi del disegno sperimentale si fanno in genere risalire a Sir Ronald A. Fisher, vissuto in Inghilterra dal 7 Febbraio 1890 al 29 luglio 1962. Laureatosi nel 1912, lavora come statistico per il comune di Londra, fino a quando diviene socio della prestigiosa Eugenics Education Society di Cambridge, fondata nel 1909 da Francis Galton, cugino di Charles Darwin. Dopo la fine della guerra, Karl Pearson gli propone un lavoro presso il rinomato Galton Laboratory, ma egli non accetta a causa della profonda rivalità esistente tra lui e Pearson stesso. Nel 1919 viene assunto presso la Rothamsted Experimental Station, dove si occupa dell’elaborazione dei dati sperimentali e, nel corso dei successivi 7 anni, definisce le basi del disegno sperimentale ed elabora la sua teoria della “analysis of variance”. Il suo libro più importante è “The design of experiment”, del 1935. E’ sua la definizione delle tre componenti fondamentali del disegno sperimentale: controllo degli errori; replicazione; randomizzazione. 3.2.1 Controllo degli errori Controllare gli errori, o, analogamente, eseguire un esperimento controllato signfica fondamentalmente due cose: adottare provvedimenti idonei ad evitare le fonti di errore, mantenendole al livello più basso possibile (alta precisione); agire in modo da isolare l’effetto in studio (accuratezza), evitando che si confonda con effetti casuali e di altra natura. Declinare questi principi richiederebbe una vita di esperienza! Vogliamo solo ricordare alcuni aspetti fondamentali. 3.2.1.1 Campionamento corretto E’evidente che il primo requisito di un esperimento è una corretta scelta delle unità sperimentali, cioè le più piccole unità che ricevono lo ‘stimolo’ rappresentato dal trattamento, in modo indipendente da tutte le altre. E’ bene subito comprendere una fondamentale distinsione tra unità sperimentali e unità osservazionali. Le prime sono appena state definite; le seconde sono quelle che costituiscono l’oggetto della misura e possono anche non coincidere con le prime. Ad esempio: immaginiamo di trattare con un diserbante due vasetti, in modo indipendente l’uno dall’altro. Immaginiamo poi di pesare singolarmente le quattro piante di ciascun vasetto; in questa situazione, il vasetto è l’unità sperimentale, le piante sono invece le unità osservazionali. L’elemento discriminante di questo esempio è l’indipendenza: mentre le unità sperimentali hanno ricevuto il trattamento in modo indipendente l’una dall’altra, le unità osservazionali no. Questa differenza è fondamentale, per motivi che vedremo più avanti. Le unità sperimentali possono essere di varia natura; nel caso degli esperimenti di campo, le unità sperimentali sono dette parcelle e sono un pezzetto di terreno, di varia forma e dimensione. Una prova sperimentale in campo (Foto D. Alberati) Le unità sperimentali sono scelte per campionamento, che è un elemento fondamentale dell’esperimento. Infatti, il censimento, che riguarda tutti i soggetti di un certo ambito, non è, in se’, un esperimento. Ovviamente, il campione deve essere rappresentativo, altrimenti l’esperimento è invalido. Non è possibile dare indicazioni specifiche di campionamento, perché queste dipendono dalla tipologia di esperimento. Illustriamo quindi solo alcuni criteri generali. Prima di campionare, dobbiamo avere una chiara visione della cornice di campionamento, cioè della popolazione da cui io devo campionare. Devo effettuare un esperimento valido per l’Italia centrale, per una località particolare, per tutta Italia. Devo fare un esperimento che riguarda una stalla in particolare o tutte le stalle dove si allevano bovini? Di quale razza? E’ un passaggio fondamentale, in quanto poi le conclusioni non possono che riferirsi alla cornice della popolazione da cui il campione è stato estratto, non altre. Per esperimenti nell’ambito delle scienze sociali, diviene fondamentale che la cornice di campionamento abbia le seguenti caratteristiche: le unità sono tutte identificabili e reperibili le unità sono tutte caratterizzate (es. Id) è aggiornata e organizzata logicamente non mancano soggetti (che potrebbero quindi sfuggire al campionamento) non ci sono soggetti duplicati non ci sono elementi estranei Una volta che la popolazione è nota ed organizzata, dobbiamo trovare un criterio di selezione. Fondamentalmente ci sono tre possibilità: campionamento randomizzato (casuale) campionamento stratificato campionamento sistematico Il campionamento randomizzato è tale che ogni soggetto ha la stessa possibilità di ogni altro di essere incluso nel campione. Tipicamente, questo campionamento è basato su un generatore di numeri casuali, con distribuzione uniforme delle frequenze. Il codice sottostante serve per ottenere cinque elementi casuali da un lotto di 48 identificati con numeri progressivi. sample(1:48, 4) ## [1] 22 37 3 24 Il campionamento casuale può non dare garanzie sufficienti di rappresentatività. Per questo motivo, a volte, si utilizza il campionamento stratificato, con il quale si divide la cornice di campionamento in gruppi omogenei e si prelevano un certo numero di soggetti da ogni gruppo. In questo caso è bene ricordare che potrebbe essere auspicabile mantenere nel campione la stessa relazione tra gruppi che esiste nella popolazione. Ad esempio, se in una popolazione di insetti c’è il 10% di maschi e il 90% di femmine, io devo prelevare \\(n\\) maschi e \\(m\\) femmine, tale che \\(n/m = 0.1\\), altrimenti il campione che ottengo potrebbe non essere rappresentativo. A volte, il campionamento può essere sistematico, nel senso che utilizzo un criterio non casuale, ma in grado di assicurare una certa rappresentatività. Ad esempio, per campionare gli edifici di una via, potrei decidere di prendere il primo a caso e poi procedere prendendone uno si e tre no. Questo campionamento è molto veloce e di facile esecuzione, ma può dare origine a distorsioni. Una forma di campionamento è quella a cluster: in questo caso suddivido gli elementi in gruppi, scelgo a caso un certo numero di gruppi e poi prendo tutti gli elementi di un gruppo. Ad esempio, devo selezionare i bambini delle scuole elementari del comune di Perugia. In questo caso, invece che selezionare i bambini, posso più velocemente selezionare le scuole e prendere tutti i bambini delle scuole selezionate. Evidentemente il metodo si basa sull’ipotesi che la selezione rappresentativa delle scuole crea anche una selezione rappresentativa di bambini. A volte si esegue anche un campionamento a quota, cioè si prendono tutti i soggetti che si incontrano in una certa situazione, fino a che non se ne raccolgono un numero prefissato, per alcune classi specificate (Es. 30 donne, 25 uomini, 15 adolescenti e 20 bambini). Questo tipo di campionamento è talvolta utilizzato negli esperimenti medici. 3.2.1.2 Rigore Direi che questo aspetto è ovvio e non richiede commenti particolari: una ricerca deve essere condotta ‘a regola d’arte’. E’ evidente che, ad esempio, se vogliamo sapere la cinetica di degradazione di un erbicida a 20 °C dovremo realizzare una prova esattamente a quella temperatura, con un erbicida uniformemente distribuito nel terreno, dentro una camera climatica capace di un controllo perfetto della temperatura. Gli strumenti dovranno essere ben tarati e sarà necessario attenersi scrupolosamente a metodi validati e largamente condivisi. Tuttavia, a proporsito di rigore, non bisogna scordare quanto diceva C.F. Gauss a proposito della precisione nei calcoli, e che puà essere anche riferito al rigore nella ricerca : “Manca di mentalità matematica tanto chi non sa riconoscere rapidamente ciò che è evidente, quanto chi si attarda nei calcoli con una precisione superiore alla necessità” 3.2.1.3 Omogeneità Anche in questo caso, l’importanza di scegliere soggetti uniformi e posti in un ambiente uniforme (nello spazio e nel tempo) è evidente. Bisogna comunque tener presente che i risultati di un esperimento si estendono alla popolazione da cui il campione è estratto e della quale esso rappresenta le caratteristiche. Esperimenti nei quali si restringe il campo di variabilità dei soggetti e dell’ambiente sono certamente più precisi, ma forniscono anche risultati meno generalizzabili. L’importante è avere ben chiaro su quale è il campo di validità che si vuole dare ai risultati. Ad esempio, se si vuole ottenere un risultati riferito alla collina umbra, bisognerà scegliere soggetti che rappresentano bene la variabilità pedo-climatica della collina Umbra; né più, né meno. 3.2.1.4 Evitare le ‘intrusioni demoniache’ Secondo Hurlbert (1984), le intrusioni sono eventi totalmente casuali che impattano negativamente con un esperimento in corso. E’evidente che, ad esempio, un’alluvione, l’attacco di insetti o patogeni, la carenza idrica hanno una pesante ricaduta sulla precisione di un esperimento e sulla sua riuscita. Nello stesso lavoro, Hurlbert usa il termine ‘intrusione demoniaca’ per indicare quelle intrusioni che, pur casuali, avrebbero potuto essere previste con un disegno più accurato, sottolineando in questo caso la responsabilità dello sperimentatore. Un esempio è questo: uno sperimentatore vuole studiare l’entità della predazione dovuta alle volpi e quindi usa campi senza steccionate (dove le volpi possono entrare) e campi protetti da steccionate (e quindi liberi da volpi). Se le steccionate, essendo utilizzate dai falchi come punto d’appoggio, finiscono per incrementare l’attività predatoria di questi ultimi, si viene a creare un’intrusione demoniaca, che rende l’esperimento distorto. Il demonio, in questo caso, non è il falco, che danneggia l’esperimento, ma il ricercatore stesso, che non ha saputo prevedere una possibile intrusione. 3.2.2 Replicazione In ogni esperimento, i trattamenti dovrebbe essere replicati su 2 o più unità sperimentali. Ciò permette di: dimostrare che i risultati sono replicabili (ma non è detto che siano riproducibili!) rassicurare che eventuali circostanze aberranti casuali non abbiano provocati risultati distorti misurare l’errore sperimentale, come variabilità di risposta tra repliche trattate nello stesso modo (precisione dell’esperimento) incrementare la precisione dell’esperimento (più sono le repliche più l’esperimento è preciso, perchè si migliora la stima della caratteristica misurata, diminuendo l’incertezza) Per poter essere utili, le repliche debbono essere indipendenti, cioè debbono aver subito tutte le manipolazioni necessarie per l’allocazione del trattamento in modo totalmente indipendente l’una dall’altra. Le manipolazioni comprendono tutte le pratiche necessarie, come ad esempio la preparazione delle soluzioni, la diluizione dei prodotti, ecc.. La manipolazione indipendente è fondamentale, perchè in ogni parte del processo di trattamento possono nascondersi errori più o meno grandi, che possono essere riconosciuti solo se colpiscono in modo casuale le unità sperimentali. Se la manipolazione è, anche solo in parte, comune, questi errori colpiscono tutte le repliche allo stesso modo, diventano sistematici e quindi non più riconoscibili. Di conseguenza, si inficia l’accuratezza dell’esperimento. Quando le repliche non sono indipendenti, si parla di pseudorepliche, contrapposte alle repliche vere. Il numero di repliche dipende dal tipo di esperimento: più sono e meglio è, anche se è necessario trovare un equilibrio accettabile tra precisione e costo dell’esperimento. Nella sperimentazione di campo, 2 repliche sono poche, 3 appena sufficienti, 4 costituiscono la situazione più comune, mentre un numero maggiore di repliche è abbastanza raro, non solo per la difficoltà di seguire l’esperimento, ma anche perché aumentano la dimensione della prova e, di conseguenza, la variabilità del terreno. 3.2.3 Randomizzazione L’indipendenza di manipolazione non garantisce da sola un esperimento corretto. Infatti potrebbe accadere che le caratteristiche innate dei soggetti, o una qualche ‘intrusione’ influenzino in modo sistematico tutte le unità sperimentali trattate nello stesso modo, così da confondersi con l’effetto del trattamento. Un esempio banale è che potremmo somministrare un farmaco a quattro soggetti in modo totalmente indipendente, ma se i quattro soggetti fossero sistematicamente più alti di quelli non trattati finiremmo per confondere una caratteristica innata con l’effetto del farmaco. Oppure, se le repliche di un certo trattamento si trovassero tutte vicine alla scolina, potrebbero essere più danneggiate delle altre unità sperimentali dal ristagno idrico, il cui effetto si confonderebbe con quello del trattamento stesso. Questi problemi sono particolarmente insidiosi e si nascondono anche dietro ai particolari apparentemente più insignificanti. La randomizzazione è l’unico sistema per evitare, o almeno rendere molto improbabile, la confusione dell’effetto del trattamento con fattori casuali e/o comunque diversi dal trattamento stesso. La randomizzazione si declina in vari modi: allocazione casuale del trattamento alle unità sperimentali. Gli esperimenti che prevedono l’allocazione del trattamento sono detti ‘manipolativi’ o ‘disegnati’. A volte l’allocazione del trattamento non è possibile o non è etica. Se volessimo studiare l’effetto delle cinture di sicurezza nell’evitare infortuni gravi, non potremmo certamente provocare incidenti deliberati. In questo caso la randomizzazione è legata alla scelta casuale di soggetti che sono ‘naturalmente’ trattati. Esperimenti di questi tipo, si dicono osservazionali. Un esempio è la valutazione dell’effetto dell’inquinamento con metalli pesanti nella salute degli animali: ovviamente non è possibile, se non su piccola scala, realizzare il livello di inquinamento desiderato e, pertanto, dovremo scegliere soggetti che sono naturalmente sottoposti a questo genere di inquinamento, magari perché vivono vicino a zone industriali. Se i soggetti sono immobili, la randomizzazione ha anche una connotazione legata alla disposizione spaziale e/o temporale casuale. L’assegnazione casuale del trattamento, o la selezione casuale dei soggetti trattati, fanno si che tutti i soggetti abbiano la stessa probabilità di ricevere qualunque trattamento oppure qualunque intrusione casuale. In questo modo, la probabilità che tutte le repliche di un trattamento abbiano qualche caratteristica innata o qualche intrusione comune che li penalizzi/avvantaggi viene minimizzata. Di conseguenza, confondere l’effetto del trattamento con variabilità casuale (‘confounding’), anche se teoricamente possibile, diviene altamente improbabile. 3.2.3.1 Gradienti e blocking Un esperimento in cui l’allocazione del trattamento, o la scelta dei soggetti trattati, o la disposizione spaziale dei soggetti sono totalmente casuali si dice ‘completamente randomizzato’. E’ perfettamente valido, perchè non pone dubbi fondati di inaccuratezza. Tuttavia, in alcune circostanze è possibile porre restrizioni (vincoli) alla randomizzazione, perchè ciò porta ad un esperimento più preciso. In particolare, le unità sperimentali possono presentare delle differenze, ad esempio di fertilità, oppure di sesso. Ad esempio, randomizzare completamente l’allocazione dei trattamenti potrebbe far si che tra le repliche di un trattamento vi siano più maschi che femmine, il che crea un certo livello di ‘confounding’. Pertanto potrebbe essere utile divider i soggetti in due gruppi (maschi e femmine), oppure in più gruppi (molto fertile, mediamente fertile, poco fertile…) e randomizzare i trattamenti all’interno di ogni gruppo. In generale, il blocking consiste nel suddividere i soggetti in gruppi uniformi e ripetere lo stesso esperimento (o parte di esso) all’interno di ciascun gruppo, cioè in una situazione di maggiore omogeneità. Il raggruppamento delle unità sperimentali può tener conto di: vicinanza spaziale (campi, parcelle, stalle …) caratteristiche fisiche (età, peso, sesso … ) vicinanza temporale gestione dei compiti (tecnico, valutatore, giudice …) Chiaramente, randomizzare all’interno del gruppo invece che randomizzare completamente crea un vincolo. A volte i vincoli sono più di uno. Vediamo un esempio. Una certa operazione industriale richiede un solo operatore per essere portata a termine, ma può essere eseguita in quattro modi diversi. Pianificate un esperimento per stabilire qual è il metodo più veloce, avendo a disposizione solo quattro operatori. L’unità sperimentale è il lavoratore. I metodi sono quattro e, volendo lavorare con quattro repliche, avremmo bisogno di sedici operatori per disegnare un esperimento completamente randomizzato. Possiamo tuttavia considerare che un operatore, in quattro turni successivi, può operare con tutti e quattro i metodi. Quindi possiamo disegnare un esperimento in cui il turno fa da unità sperimentale e l’operatore fa da blocco (blocchi randomizzati). Tuttavia, in ogni blocco (operatore) vi è un gradiente, nel senso che i turni successivi al primo sono via via meno efficienti, perché l’operatore accumula stanchezza. Per tener conto di questo potremmo allora introdurre un vincolo ulteriore, per ogni operatore, randomizzando i quattro metodi tra i turni, in modo che ogni metodo, in operatori diversi, capiti in tutti i turni. In sostanza, l’operatore fa da blocco, perché in esso sono contenuti tutti i metodi. Ma anche il turno (per tutti gli operatori) fa da blocco, in quanto in esso sono ancora contenuti tutti i metodi. Se non vi è chiaro, ci torneremo sopra più tardi. Posto che non si deve violare l’indipendenza delle repliche, l’inclusione di vincoli alla randomizzazione è consentita, ma questa deve sempre essere tenuta presente in fase di analisi dei dati. Ronald Fisher diceva “Analyse them as you have randomised them”. Meglio seguire il consiglio. 3.2.3.2 E se ricercatori/soggetti sono influenzabili? Per concludere questa parte, è opportuno menzionare il fatto che, in un esperimento scientifico, il fatto che lo sperimentatore e il soggetto siano coscienti del trattamento somministrato può portare a risultati distorti. Per esempio, nell’eseguire un rilievo, lo sperimentatore può essere influenzato dal sapere con quale diserbante è stata trattata una parcella, cercando inconsciamente conferme alle sue conoscenze pregresse. D’altro canto, nei soggetti sperimentali dotati di coscienza (uomo) sapere di essere stati trattati può influenzare l’esito del trattamento (effetto placebo). Per evitare questi problemi, soprattutto in ambito medico, un esperimento può essere pianificato come: cieco: l’unità sperimentale o lo sperimentatore non sono coscienti dei dettagli del trattamento; doppio cieco: né l’unità sperimentale né lo sperimentatore sono a coscienza dei dettagli del trattamento Un esperimento cieco e/o doppio cieco possono non essere eticamente corretti oppure inutili, nel qual caso si torna ad un esperimento tradizionale ‘aperto’ (open experiment: Tutti sanno tutto’) 3.2.4 Esperimenti non validi A questo punto dovrebbero essere chiare le caratteristiche di un esperimento valido. A completamento, cerchiamo di elencare le caratteristiche di un esperimento non valido. Cattivo controllo degli errori Fondati sospetti di confounding Mancanza di repliche vere Confusione tra repliche vere e pseudo-repliche Mancanza di randomizzazione Presenza di vincoli alla randomizzazione, trascurati in fase di analisi. Le conseguenze di queste problematiche sono abbastanza diverse. 3.2.4.1 Cattivo controllo degli errori Bisogna verificare se il problema è relativo a questioni come la mancanza di scrupolosità, l’uso di soggetti poco omogenei o di un ambiente poco omogeneo, o altri aspetti che inficiano solo la precisione, ma non l’accuratezza dell’esperimento. In questo caso, l’esperimento è ancora valido (accurato), ma la bassa precisione probabilmente impedirà di trarre conclusioni forti. Quindi, un esperimento impreciso si ‘elimina’ da solo, perché sarà inconclusivo. Di questi esperimenti bisogna comunque diffidare, soprattutto quando siano pianificati per mostrare l’assenza di differenze tra due trattamenti alternativi. Mostrare l’assenza di differenze è facile: basta fare male un esperimento, in modo che vi sia un alto livello di incertezza e quindi l’evidenza scientifica sia molto debole. Diversa è la situazione in cui un cattivo controllo degli errori, ad esempio l’adozione di metodi sbagliati, porta a mancanza di accuratezza, cioè a risultati che non riflettono la realtà (campionamento sbagliato, ad esempio; oppure strumenti non tarati; impiego di metodi non validati e/o non accettabili). In questo caso venendo a mancare l’accuratezza, l’esperimento deve essere rigettato, in quanto non fornisce informazioni realistiche. 3.2.4.2 Confounding e correlazione spuria Abbiamo appena menzionato il problema fondamentale della ricerca, cioè il confounding, vale a dire la confusione tra l’effetto del trattamento e un qualche altro effetto casuale, legato alle caratteristiche innate del soggetto o a qualche intrusione più o meno ‘demoniaca’. Abbiamo detto che non possiamo mai avere la certezza dell’assenza di confounding, ma abbiamo anche detto che l’adozione di una pratica sperimentale corretta ne minimizza la probabilità. Chiaramente, rimangono dei rischi che sono tipici di situazioni nelle quali il controllo adottato non è perfetto, come capita, ad esempio, negli esperimenti osservazionali. In questo ambito è piuttosto temuta la cosiddetta ‘correlazione spuria’, una forma di confounding casuale per cui due variabili variano congiuntamente (sono direttamente o inversamente proporzionali), ma in modo del tutto casuale. Esistono, ad esempio, dati che mostrano una chiara correlazione tra le vendite di panna acida e le morti per incidenti in motocicletta. Chiaramente, non esistono spiegazioni scientifiche per questo effetto, che è, ovviamente, del tutto casuale. Il problema è che questa correlazione spuria non è sempre così semplice da rintracciare. Esempio di correlazione spuria A volte il confounding non è casuale, ma è legato ad una variabile esterna che si agisce all’insaputa dello sperimentatore. Ad esempio, è stato osservato che il tasso di crimini è più alto nelle città che hanno più chiese. La spiegazione di questo paradosso sta nel fatto che esiste un ‘confounder’, cioè l’ampiezza della popolazione. Nelle grandi città si riscontrano sia una maggiore incidenza criminale, sia un grande numero di chiese. In sostanza, la popolazione determina sia l’elevato numero di chiese che l’elevato numero di crimini, ma queste ultime due variabili non sono legate tra loro da una relazione causa-effetto (A implica B e A implica C, ma B non implica C). Il confounding non casuale è spesso difficile da evidenziare, soprattutto se le correlazioni misurate sono spiegabili. Inoltre, non è eliminabile con un’accurata randomizzazione, ma solo con l’esecuzione di un esperimento totalmente controllato, nel quale ci si preoccupa di rilevare tutte le variabili necessarie per spiegare gli effetti riscontrati. Di questo è importante tener conto soprattutto negli esperimenti osservazionali, dove il controllo è sempre più difficile e meno completo. 3.2.4.3 Pseudo-repliche e randomizzazione poco attenta Per evidenziare questi problemi e comprendere meglio la differenza tra un esperimento corretto e uno non corretto, è utilissima la classificazione fatta da Hurlbert (1984), che riportiamo di seguito. Indicazioni per una corretta randomizzazione (Hurlbert, 1984) Vengono mostrati 8 soggetti, sottoposti a due trattamenti (bianco e nero), con 8 disegni sperimentali diversi. Il disegno A1 è corretto, in quanto si tratta di un esperimento completamente randomizzato. Ugualmente, è valido il disegno A2, nel quale le unità sperimentali sono state divise in quattro gruppi omogenei e sono state trattate in modo randomizzato all’interno di ogni gruppo. Il disegno A3 è quantomeno ‘sospetto’: vi sono repliche vere, ma l’allocazione dei trattamenti non è randomizzata ed avviene con un processo sistematico per il quale ‘nero’ e ‘bianco’ si alternano. Cosa succederebbe se vi fosse un gradiente di fertilità decrescente da destra verso sinistra? Le unità nere sarebbero avvantaggiate rispetto alle bianche! Insomma, rimangono sospetti di confounding, a meno che non si sia assolutamente certi dell’assenza di gradienti, come capita ad esempio se all’interno dei blocchi, dobbiamo creare una sequenza spazio-temporale. Vediamo tre esempi: ho quattro piante e, per ogni pianta, voglio confrontare un ramo basso con uno alto: è evidente che i due trattamenti sono sempre ordinati in modo sistematico (basso prima di alto). Dobbiamo valutare l’effetto di fitofarmaci somministrati in due epoche diverse (accestimento e inizio-levata); anche qui non possiamo randomizzare, giacchè un’epoca precede sempre l’altra. Dobbiamo confrontare la presenza di residui di un fitofarmaco a due profondità e non possiamo randomizzare, perché una profondità precede sempre l’altra nello spazio. In queste situazioni l’esperimento rimane valido, anche se la randomizzazione segue un processo sistematico e non casuale. Il disegno B1 è usualmente invalido: non vi è randomizzazione e ciò massimizza i problemi del disegno A3: la separazione delle unità sperimentali ‘bianche’ e ‘nere’ non consente una valutazione adeguata dell’effetto del trattamento, che è confuso con ogni potenziale differenza tra la parte destra e la sinistra dell’ambiente in cui la sperimentazione viene eseguita. Ovviamente, la separazione può essere non solo spaziale, ma anche temporale. Anche in questo caso diamo alcuni esempi in cui una situazione come quella descritta in B1 è valida: Vogliamo confrontare la produzione in pianura e in collina. Ovviamente dobbiamo scegliere campioni in due situazioni fisicamente separate Vogliamo confrontare la pescosità di due laghetti Vogliamo confrontare la produttività di due campi contigui. Queste situazioni sono valide, anche se con una restrizione: non siamo in grado di stabilire a chi debba essere attribuito l’effetto. Ad esempio, per la prima situazione, pianura e collina possono dare produzioni diverse per il suolo diverso, il clima diverso, la precessione colturale diversa o un qualunque altro elemento che differenzi le due località. Il disegno B2 è analogo al disegno B1, ma il problema è più grave, perché la separazione fisica è più evidente. Questo disegno è totalmente sbagliato, a meno che non siamo specificatamente interessati all’effetto località (vedi sopra). Il disegno B3 è analogo al disegno B2, ma costituisce una situazione molto frequente nella pratica scientifica. Immaginiamo infatti di voler confrontare la germinazione dei semi a due temperature diverse, utilizzando due camere climatiche e mettendo, in ognuna di esse, quattro capsule Petri identiche. In questa situazione, l’effetto temperatura è totalmente confuso con l’effetto ‘camera climatica (località)’ e risente di ogni malfunzionamente relativo ad una sola delle due camere. Inoltre, le unità sperimentali con lo stesso trattamento di temperature non sono manipolate in modo indipendente, dato che condividono la stessa camera climatica. Di conseguenza, non si può parlare di repliche vere, bensì di pseudorepliche. Altri esempi di pseudorepliche sono schematizzati con il codice B4. Ad esempio: trattare piante in vaso ed analizzare in modo indipendente i singoli individui invece che tutto il vaso; trattare una parcella di terreno e prelevare da essa più campioni, analizzandoli separatamente; trattare una capsula Petri ed analizzare separatamente i semi germinati al suo interno. Questi disegni, in assenza di repliche vere aggiuntive non sono da considerarsi validi. Ad esempio, se io ho due vasetti trattati in modo totalmente indipendente e da ciascuno di essi prelevo due piante e le analizzo separatamente, il disegno è caratterizzato da due repliche vere e due pseudorepliche per ogni replica ed è, pertanto, valido. Il disegno B5 è invece evidentemente invalido, per totale mancanza di repliche. 3.3 Progettazione di un esperimento (protocollo) Qualunque sia l’ambito scientifico, in ogni esperimento possiamo individuare alcune fasi fondamentali, che proviamo ad elencare: Individuazione del background (ricerca bibliografica) ipotesi scientifica; definizione dell’obiettivo; identificazione dei fattore/i sperimentale/i; identificazione dei soggetti sperimentali e delle repliche; identificazione delle variabili da rilevare; allocazione randomizzata dei trattamenti (mappa dell’esperimento) Esecuzione dell’esperimento Nell’analizzare questi aspetti, faremo riferimento ad alcuni esempi pratici, che verranno indicati tra breve. 3.3.1 Ipotesi scientifica \\(\\rightarrow\\) obiettivo dell’esperimento Trascurando la parte di ricerca bibliografica, che è pur fondamentale, nel metodo scientifico galileiano, il punto di partenza di un esperimento è l’ipotesi scientifica, che determina l’obiettivo dell’esperimento. Si tratta del passaggio fondamentale dal quale dipende in modo logico tutto il lavoro successivo. Gli obiettivi debbono essere: rilevanti chiaramente definiti; specifici; misurabili; raggiungibili/realistici; temporalmente organizzati. Il rischio che si corre con obiettivi mal posti è quello di eseguire una ricerca dispersiva, con raccolta di dati non necessari e/o mancanza di dati fondamentali, con costi più elevati del necessario e un uso poco efficiente delle risorse. In genere, prima si definisce un obiettivo generale, seguito da uno o più obiettivi specifici, in genere proiettati su un più breve spazio temporale e che possono essere visti anche come le fasi necessarie per raggiungere l’obiettivo generale. Poniamo quattro esempi pratici. 3.3.2 Casi di studio - 1 3.3.2.1 Esempio 1 - Diserbo chimico Si suppone che gli erbicidi A, B e C siano più efficaci di D, E ed F verso Solanum nigrum, una comune pianta infestante delle colture di pomodoro. L’obiettivo generale della ricerca sarà quello di trovare un’efficace soluzione per l’eliminazione di Solanum nigrum dal pomodoro. Gli obiettivi specifici saranno: valutare l’efficacia erbicida di A, B e C, confrontandola con quella di D, E ed F; valutare la selettività degli anzidetti erbicidi verso il pomodoro; 3.3.2.2 Esempio 2 - Valutazione varietale L’ipotesi è che le varietà di girasole A, B e C non hanno la stessa base genetica e quindi non sono tutte ugualmente produttive. L’obiettivo generale è quello di capire quale tra A, B e C sia più adatta alle condizioni pedoclimatiche della collina Umbra. Gli obiettivi specifici sono quelli di valutare: produttività di A, B e C stabilità produttiva di A, B e C 3.3.2.3 Esempio 3 - Diserbo parziale Nella barbabietola da zucchero, il diserbo localizzato lungo la fila consente di diminuire l’impiego di erbicidi. Tuttavia, se la coltura precedente ha prodotto semi e se non abbiamo effttuato una lavorazione profonda per interrarli, la coltura sarà più infestata e quindi sarà più difficile ottenere una buona produttività con il diserbo parziale. Su questa ipotesi costruiamo un esperimento volto a valutare l’interazione tra lavorazione del terreno e diserbo chimico. Per raggiungere questo obiettivo generale, proveremo a valutare se: il diserbo parziale consente di ottenere produzioni comparabili a quelle del diserbo totale; 2.l’effetto è indipendente dalla lavorazione effettuata. 3.3.2.4 Esempio 4 - Colture poliennali L’ipotesi scientifica è affine a quella dell’esempio 2, ma, in questo caso, vogliamo porre a confronto tre varietà di erba medica (A, B e C). La differenza sta nel fatto che l’erba medica è una coltura poliennale e quindi vogliamo capire se il giudizio di merito è indipendente dall’anno di coltivazione. I nostri obiettivi specifici saranno quindi: valutare la produttività media delle varietà in prova valutare le oscillazione nei quattro anni di durata del cotico erboso 3.3.2.5 Esempio 5 - Inquinamento da micotossine Secondo le notizie in bibliografia, i datteri confezionati in vendita nei supermercati contengono elevate quantità di micotossine. L’obiettivo generale è quello di verificare il livello di infestazione e vedere se questo cambia con il metodo di confezionamento. 3.3.3 Identificazione dei fattori sperimentali Dopo aver definito l’obiettivo di un esperimento, è necessario chiarire esattamente gli stimoli a cui saranno sottoposte le unità sperimentali. Uno ‘stimolo’ sperimentale prende il nome di fattore sperimentale, che può avere più livelli. I livelli del fattore sperimentale prendono il nome di trattamenti (o tesi) sperimentali. 3.3.4 Esperimenti (multi)fattoriali In alcuni casi è necessario inserire in prova più di un fattore sperimentale. In questo caso si parla di esperimenti fattoriali, che possono essere incrociati (crossed) quando sono presenti in prova tutte le possibili combinazioni dei livelli di ogni fattore, oppure di esperimenti innestati (nested) quando i livelli di un fattore cambiano al cambiare dei livelli dell’altro. Ad esempio: Immaginiamo di voler studiare due fattori sperimentali: la varietà di girasole (tre livelli: A, B e C) e la concimazione (2 livelli: pollino e urea). Abbiamo quindi 6 possibili trattamenti (combinazioni): A-pollina, A-urea, B-pollina, B-urea, C-pollina e C-urea. Il disegno è completamente incrociato. Immaginiamo di voler confrontare due specie in agricoltura biologica (orzo e triticale), con tre varietà ciascuna (A, B e C per orzo, D, E e F per triticale). Anche in questo caso abbiamo sei trattamenti: orzo-A, orzo-B, orzo-C, triticale-D, triticale-E e triticale-F, ma il disegno è innestato, perché per il fattore sperimentale ‘varietà’ i livelli cambiano a seconda dei livelli del fattore ‘specie’. 3.3.5 Aggiungere un controllo? In alcuni casi si pone il problema di inserire in prova un trattamento che funga da riferimento per tutti gli altri. In questi casi si parla comunemente di controllo o testimone, che può essere non sottosposto a trattamento trattato con placebo trattato secondo le modalità usuali di riferimento 3.3.6 Fattori sperimentali di trattamento e di blocco Finora abbiamo menzionato quelli che, in lingua inglese, vengono definiti treatment factor (trattamenti sperimentali). Tuttavia, possono esserci altri fattori sperimentali non allocati, ma ‘innati’ e legati alla collocazione spazio-temporale o alle caratteristiche dei soggetti. Questi fattori vengono definiti, sempre in inglese, blocking factors. Di questi fanno parte, ad esempio, il blocco, la località ed ogni altro elemento che permette di raggruppare i soggetti. Anche questi blocking factors devono essere chiaramente identificati ed elencati. Su questa base identifichiamo i fattori sperimentali negli esempi precedenti. 3.3.7 Casi di studio - 2 3.3.7.1 Esempio 1 Il fattore sperimentale oggetto di studio sarà il diserbo del pomodoro, con 5 livelli inseriti in prova (6 trattamenti sperimentali): A, B, C, D, E ed F. Inoltre, si ritiene opportuno inserire in prova un testimone non trattato (NT), che ci permetterà di quantificare la percentuale di malerbe controllate. Inoltre, sarà anche necessario inserire in prova un testimone scerbato manualmente (ST), che ci permetterà di quantificare eventuali perdite produttive dovute alla competizione residua o alla fitotossicità del trattamento. In totale, avremo quindi 8 tesi sperimentali. Come usuale in pieno campo, l’esperimento verrà disegnato a blocchi randomizzati e sarà pertanto necessario inserire un fattore di blocco. 3.3.7.2 Esempio 2 Il fattore sperimentale in studio sarà la varietà di girasole con 3 livelli inclusi in prova (varietà A, B e C). Come testimone, inseriremo la varietà di riferimento per la zona (D). Dato che eseguiremo questa prova su un terreno nel quale vi sono due chiari gradienti di fertilità, disegneremo l’esperimento considerando due fattori di blocco: trasversale e longitudinale (spiego meglio tra poco…). Poichè dobbiamo valutare la stabilità produttiva, dovremo ripetere l’esperimento più volte (es. in tre anni diversi) e quindi avremo un secondo fattore sperimentale, incrociato con il primo. 3.3.7.3 Esempio 3 In questo caso avremo due fattori sperimentali incrociati: il diserbo con due livelli (totale o parziale, localizzato sulla fila) e la lavorazione con tre livelli (aratura profonda, aratura superficiale e minimum tillage). Non vi è la necessità di un testimone, ma avremo la necessità di un fattore di blocco. In totale, avremo sei tesi sperimentali. 3.3.7.4 Esempio 4 Il fattore sperimentale in studio sarà la varietà di erba medica con 3 livelli inclusi in prova (varietà A, B e C) ai quali aggiungiamo il riferimento di zona (D) come testimone. Come nel caso del girasole, dovremo valutare la stabilità produttiva negli anni, ma, dato che abbiamo una coltura poliennale, non avremo bisogno di ripetere la prova, ma potremo ripetere le osservazioni per quattro anni sulla stessa prova. 3.3.7.5 Esempio 5 Per questo esperimento vengono considerate tre diverse modalità di confezionamento (carta, busta di plastica, scatola di plastica perforata). Non vi è necessità di un testimone, ma, dato che le diverse confezioni verranno acquistate in diversi supermercati e dato che sospettiamo differenze nella conservazione tra un supermercato e l’altro, utilizzeremo il supermercato come fattore di raggruppamento. 3.3.8 Identificazione delle unità sperimentali e delle repliche 3.3.8.1 Cornice di campionamento e numero di repliche Per i primi quattro esempi verranno eseguite prove di pieno campo, nella Media Valle del Tevere, che rappresenta la cornice di campionamento adeguata per l’obiettivo previsto. Sappiamo di dover selezionare appezzamenti di terreno rappresentativi della Media Valle del Tevere, omogenei. L’omogeneità dell’ambiente è fondamentale per aumentare la precisione dell’esperimento. La scelta dell’appezzamento è chiaramente fondamentale ed è guidata dall’esperienza, tenendo conto anche di aspetti come la facilità di accesso e la vicinanza di strutture (laboratori, capannoni…) che consentano un’accurata esecuzione degli eventuali prelievi. Oltre alla scelta dell’appezzamento, si possono anche utilizzare alcune strategie per favorire una buona omogeneità delle parcelle. Spesso si usa far precedere la prova da una coltura di ‘omogeneizzazione’, ad esempio avena, che è molto avida di azoto e lascia nel terreno poca fertilità residua. Oppure un prato di erba medica, che, grazie agli sfalci periodici, lascia il terreno libero da piante infestanti. Trattandosi di esperimenti di campo, il numero di repliche sarà di quattro, per ogni trattamento e l’unità sperimentale sarà una parcella, della quale dovremo valutare forma e dimensioni. Per il quinto esempio, la cornice di campionamento sarà data dal territorio del comune di Perugia. L’unità sperimentale sarà la confezione e la scelta del numero di repliche dovrà essere compatibile con la capacità di analisi per la determinazione dell’inquinamento da micotossine. E’ ragionevole pensare che 30 repliche (90 confezioni totali) possano essere adeguate per rappresentatività e facilità di gestione. 3.3.8.2 Campionamento delle unità sperimentali Per le quattro prove di pieno campo, una volta scelto l’appezzamento, dovremo campionare le parcelle di terreno. Questa operazione viene usualmente eseguita su carta, redigendo la mappa dell’esperimento. In primo luogo, si decide la dimensione e la forma della parcella. L’aspetto fondamentale è che ogni parcella deve contenere un numero di piante sufficientemente alto da essere rappresentativo. Per questo motivo le colture a bassa fittezza hanno sempre bisogno di parcelle più grandi che non quelle ad alta fittezza. La dimensione non deve tuttavia eccedere una certa soglia, in quanto con essa aumenta anche la variabilità del terreno e, di conseguenza, diminuisce l’omogeneità dell’esperimento. Per questo motivo, talvolta si preferisce diminuire la dimensione delle parcelle ed, avendo lo spazio sufficiente, aumentare il numero delle repliche. Nello stabilire la dimensione delle parcelle, dovremo tener conto del fatto che la parte più delicata è il bordo, in quanto le piante che si trovano lungo il bordo esterno risentono di condizioni diverse dalle altre piante situate al centro della parcella (effetto bordo). Questo determina variabilità all’interno della parcella, che possiamo minimizzare raccogliendo solo la parte centrale. Si viene così a distinguere la superficie totale della parcella dalla superficie di raccolta (superficie utile), che può essere anche molto minore di quella totale. In generale si ritiene che le colture ad elevata fittezza (frumento, cereali, erba medica…) dovrebbero avere parcelle di almeno 10-20 \\(m^2\\), mentre a bassa fittezza (mis, girasole…) dovrebbero avere parcelle di almeno 20-40 \\(m^2\\). Queste dimensioni sono riferite alla superficie utile di raccolta, non alla dimensione totale. Per quanto riguarda la forma, le parcelle quadrate minimizzano l’effetto bordo, perché, a parità di superficie, hanno un perimetro più basso. Tuttavia esse sono di più difficile gestione, in quanto, considerando il fronte di lavoro di una seminatrice o una mietitrebbiatrice parcellare, possono richiedere la semina o la raccolta in più passate, il che finisce per essere una fonte di errore. Per questo motivo le parcelle sono usualmente rettangolari, con una larghezza pari a quella della macchina impiegata per la semina. Per i quattro esempi in studio potremmo utilizzare una dimensione delle parcelle di 20 \\(m^2\\) per l’erba medica (2 m di larghezza per 10 m di lunghezza) e di 22.5 \\(m^2\\) per pomodoro, mais e barbabietola da zucchero (2.25 m di larghezza per 10 metri di lunghezza). A questo punto possiamo redigere la mappa per il primo esempio. Dato che il campo di prova è largo 30 metri e lungo 400 metri, potremmo immaginare di disegnare otto file di percelle in senso trasversale (8 x 2.25m = 18m), di modo che l’esperimento non sia troppo lungo (il che ne aumenterebbe la variabilità), ma rimanga spazio sufficiente ai lati, per evitare di avvicinarsi troppo alle scoline, dove possono manifestarsi ristagni idrici. La mappa dell’esperimento è un elemento fondamentale e deve riportare tutte le informazioni relative al disegno sperimentale. E’anche importante indicare la direzione del Nord, in modo da facilitare l’orientamento della mappa stessa. Notare inoltre che, intorno alla prova, abbiamo sistemato altre parcelle fuori esperimento con funzione di ‘bordi’. In questo modo si evita che i bordi esterni delle parcelle esterne siano esposti a condizioni molto diverse dagli altri, cosa che potrebbe accentuare l’effetto ‘bordo’, di cui abbiamo parlato in precedenza. Queste parcelle verranno trattate in modo ordinario (semina e diserbo tradizionale del pomodoro). Mappa dell’esperimento relativo all esempio 1 Per l’esperimento relativo all’esempio 5, l’unità sperimentale è una singola confezione di datteri, con le tipologie previste dal piano. Tipologie delle confezioni di datteri Siccome è abbastanza scomodo campionare confezioni di datteri casualmente all’interno del Comune di Perugia, si preferisce un campionamento stratificato, selezionando 10 supermercati rappresentativi, nelle zone più densamente popolate della città. All’interno di ogni supermercato, si selezioneranno casualmente tre repliche per ogni tipo di confezione. 3.3.9 Scelta delle variabili da rilevare Durate e al termine dell’esperimento, sarà necessario rilevare le più importanti caratteristiche dei soggetti sperimentali, sia quelle innate, sia quelle indotte dai trattamenti sperimentali. Per ogni singolo carattere, l’insieme delle modalità/valori che ognuno dei soggetti presenta prende il nome di variabile (proprio perché varia, cioè assume diversi valori, a seconda del soggetto). Ad esempio, quando stiamo studiando l’effetto di due diserbanti su piante infestanti appartenenti ad una certa specie, posto che l’unità sperimentale è costituita da una singola pianta, possiamo avere le seguenti variabili: il prodotto diserbante con cui ogni pianta è stata trattata, il peso di ogni pianta prima del trattamento, il peso di ogni pianta dopo il trattamento. Le variabili sperimentali possono essere molto diverse tra di loro ed è piuttosto importante saperle riconoscere, perché questo condiziona il tipo di analisi statistica da eseguire. 3.3.9.1 Variabili nominali (categoriche) Le variabili nominali esprimono, per ciascun soggetto, l’appartenenza ad una determinata categoria o raggruppamento. L’unica caratteristica delle categorie è l’esclusività, cioè un soggetto che appartiene ad una di esse non puà appartenere a nessuna delle altre. Variabili nominali sono, ad esempio, il sesso, la varietà, il tipo di diserbante impiegato, la modalità di lavorazione e così via. Le variabili categoriche permettono di raggruppare i soggetti, ma non possono essere utilizzate per fare calcoli, se non per definire le proporzioni dei soggetti in ciascun gruppo. 3.3.9.2 Variabili ordinali Anche le variabili ordinali esprimono, per ciascun soggetto, l’appartenenza ad una determinata categoria o raggruppamento. Tuttavia, le diverse categorie sono caratterizzate, oltre che dall’esclusività, anche da una relazione di ordine, nel senso che è possibile stabilire una naturale graduatoria tra esse. Ad esempio, la risposta degli agricoltori a domande relative alla loro percezione sull’utilità di una pratica agronomicapuò essere espressa utilizzando una scala con sei categorie (0, 1, 2, 3, 4 e 5), in ordine crescente da 0 a 5. Di conseguenza possiamo confrontare categorie diverse ed esprimere un giudizio di ordine (2 è maggiore di 1, 3 è minore di 5), ma non possiamo eseguire operazioni matematiche, tipo sottrarre dalla categoria 3 la categoria 2 e così via, dato che la distanza tra le categorie non è necessariamente la stessa. 3.3.9.3 Variabili quantitative discrete Le variabili discrete sono caratterizzate dal fatto che possiedono, oltre alle proprietà dell’esclusività e dell’ordine, anche quella dell’equidistanza tra gli attributi (es., in una scala a 5 punti, la distanza – o la differenza – fra 1 e 3 è uguale a quella fra 2 e 4 e doppia di quella tra 1 e 2). Le variabili discrete consentono la gran parte delle operazioni matematiche e, spesso, possono essere analizzate con metodiche parametriche, facendo riferimento alla distribuzione normale, che, pur essendo continua, in alcune condizioni può essere assunta come buona approssimazione di molte distribuzioni discrete. 3.3.9.4 Variabili quantitative continue Le variabili quantitative continue possiedono tutte le proprietà precedentemente esposte (esclusività delle categorie, ordine, distanza) oltre alla continuità, almeno in un certo intervallo. Tipiche variabili continue sono l’altezza, la produzione, il tempo, la fittezza… Dato che gli strumenti di misura nella realtà sono caratterizzati da una certa risoluzione, si potrebbe arguire che misure su scala continua effettivamente non esistono. Tuttavia questo argomento è più teorico che pratico e, nella ricerca biologica, consideriamo continue tutte le misure nelle quali la risoluzione dello strumento è sufficientemente piccola rispetto alla grandezza da misurare. Viceversa, le variabili continue sono piuttosto rare nelle scienze economiche e sociali in genere. La quantità di informazione fornita dagli strumenti di valutazione cresce passando dalle scale nominali, di più basso livello, a quelle quantitative continue, di livello più elevato. Variabili esprimibili con scale quantitative continue o discrete possono essere espresse anche con scale qualitative, adottando un’opportuna operazione di classamento. Il contrario, cioè traformare in quantitativa una variabile qualitativa, non è invece possibile. 3.3.9.5 Rilievi visivi e sensoriali Nella pratica sperimentale è molto frequente l’adozione di metodi di rilievo basati sull’osservazione di un fenomeno attraverso uno dei sensi (più spesso, la vista, ma anche gusto e olfatto) e l’assegnazione di una valutazione su scala categorica, ordinale o, con un po’ di prudenza, quantitativa discreta o continua. Ad esempio, il ricoprimento delle piante infestanti, la percentuale di controllo di un erbicida e la sua fitotossicità vengono spesso rilevati visivamente, su scale da 0 a 100 o simili. I vantaggi di questa tecnica sono molteplici: Basso costo ed elevata velocità Possibilità di tener conto di alcuni fattori perturbativi esterni, che sono esclusi dalla valutazione, contrariamente a quello che succede con metodi oggettivi di misura non richiede strumentazione costosa A questi vantaggi fanno da contraltare alcuni svantaggi, cioè: Minor precisione (in generale) Soggettività L’osservatore può essere prevenuto Difficoltà di mantenere uniformità di giudizio Richiede esperienza specifica e allenamento I rilievi sensoriali sono ben accettati nella pratica scientifica in alcuni ambiti ben definiti, anche se richiedono attenzione nell’analisi dei dati non potendo essere assimilati tout court con le misure oggettive su scala continua. 3.3.9.6 Variabili di confondimento Quando si pianificano i rilievi da eseguire, oppure anche nel corso dell’esecuzione di un esperimento, bisogna tener presente non soltanto la variabile che esprime l’effetto del trattamento, ma anche tutte le variabili che misurano possibili fattori di confondimento. Ad esempio, immaginiamo di voler valutare la produttività di una specie arborea in funzione della varietà. Immaginiamo anche di sapere che, per questa specie, la produttività dipende anche dall’età. Se facciamo un esperimento possiamo utilizzare alberi della stessa età per minimizzare la variabilità dei soggetti. Tuttavia, se questo non fosse possibile, per ogni albero dobbiamo rilevare non solo la produttività, ma anche l’età, in modo da poter valutare anche l’effetto di questo fattore aggiuntivo e separarlo dall’effetto della varietà. In questo modo l’esperimento diviene molto più preciso. 3.3.10 Casi di studio - 3 Per gli esempi in studio, immaginiamo per semplicità di dover rilevare la produzione per gli esempi da 1 a 4 e il contenuto di micotossine per l’esempio 5. Inoltre, per l’esempio 2, immaginiamo di dover rilevare anche il peso di mille semi. Per questo, prenderemo dalla produzione di granella di ogni parcella, quattro subcampioni da mille semi, da sottoporre a successive misure. 3.3.11 Allocazione dei trattamenti Il problema dell’allocazione dei trattamenti non si pone con l’esempio 5, in quanto, trattandosi di un esperimento osservazionale, le confezione sono già ‘naturalmente’ trattate, cioè appartengono già, all’atto del campionamento, alla tipologia di confezionamento prescelta. Per quanto riguarda gli altri esempi, abbiamo già redatto la mappa secondo le necessità. A questo punto si pone il problema di decidere quali parcelle trattare con cosa, nel rispetto dei trattamenti e delle repliche prescelte. Per questo fine, semplici esperimentipossono anche essere disegnati a mano; per esperimenti più complessi potremo utilizzare il package agricolae in R (de Mendiburu, 2017). 3.3.12 Casi di studio - 4 3.3.12.1 Esempio 1 Questo esempio va disegnato a blocchi randomizzati; tuttavia, a titolo di esempio, esamineremo anche la possibilità che venga disegnato a randomizzazione completa. Quest’ultimo disegno è il più semplice e consiste nell’assegnare ogni trattamento a quattro parcelle casualmente scelte. Con R bisognerà prima creare il vettore dei nomi delle tesi e il numero di repliche per tesi library(agricolae) trt &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;NT&quot;, &quot;TS&quot;) reps &lt;- rep(4, 8) design &lt;- design.crd(trt, r=reps, seed=777, serie=0) design$book ## plots r trt ## 1 1 1 E ## 2 2 1 C ## 3 3 1 B ## 4 4 2 C ## 5 5 1 F ## 6 6 1 TS ## 7 7 1 NT ## 8 8 1 D ## 9 9 2 NT ## 10 10 1 A ## 11 11 2 TS ## 12 12 2 F ## 13 13 3 NT ## 14 14 3 C ## 15 15 3 TS ## 16 16 2 D ## 17 17 3 D ## 18 18 2 E ## 19 19 3 E ## 20 20 4 NT ## 21 21 2 A ## 22 22 4 D ## 23 23 4 E ## 24 24 3 A ## 25 25 4 C ## 26 26 2 B ## 27 27 4 A ## 28 28 3 F ## 29 29 3 B ## 30 30 4 TS ## 31 31 4 F ## 32 32 4 B Possiamo ora riportare la randomizzazione sulla mappa disegnata in precedenza. Schema sperimentale a randomizzazione completa per l’Esempio 1 Questo schema è eccellente se l’ambiente è molto uniforme. Tuttavia, nel caso di un esperimento di camp è lecito aspettarsi un gradiente trasversale, dato che il campo sarà certamente meno fertile vicino alle scoline. Per questo motivo disegneremo l’esperimento a blocchi randomizzati, dividendo prima l’appezzamento in quattro blocchi perpendicolari al gradiente di fertilità. Ad esempio il blocco 1 conterrà le parcelle 1, 9, 17, 25 2, 10, 18 e 26, cioè le prime due colonne della mappa, con un numero di parcellè esattamente uguali al numero delle tesi. Il blocco 2 conterrà le colonne 3 e 4 e così via. Dato che il gradiente è trasversale, le parcelle di un stesso blocco saranno più omogenee che non parcelle su blocchi diversi. Dopo aver diviso la mappa in quattro blocchi di otto parcelle, possiamo allocare gli otto trattamenti a random all’interno di ogni blocco. Con R è possibile utilizzare il codice seguente (notare che la numerazione assegnata da R è diversa dalla nostra, anche se possiamo far riferimento ai valori crescenti all’interno di ogni blocco). reps &lt;- 4 designRCBD &lt;- design.rcbd(trt, r=reps, seed=777, serie=2) book2 &lt;- designRCBD$book book2 ## plots block trt ## 1 101 1 E ## 2 102 1 B ## 3 103 1 NT ## 4 104 1 F ## 5 105 1 D ## 6 106 1 TS ## 7 107 1 C ## 8 108 1 A ## 9 201 2 F ## 10 202 2 A ## 11 203 2 C ## 12 204 2 E ## 13 205 2 TS ## 14 206 2 B ## 15 207 2 NT ## 16 208 2 D ## 17 301 3 TS ## 18 302 3 NT ## 19 303 3 F ## 20 304 3 A ## 21 305 3 B ## 22 306 3 E ## 23 307 3 C ## 24 308 3 D ## 25 401 4 D ## 26 402 4 TS ## 27 403 4 A ## 28 404 4 F ## 29 405 4 E ## 30 406 4 C ## 31 407 4 B ## 32 408 4 NT zigzag(designRCBD) # zigzag numeration ## plots block trt ## 1 101 1 E ## 2 102 1 B ## 3 103 1 NT ## 4 104 1 F ## 5 105 1 D ## 6 106 1 TS ## 7 107 1 C ## 8 108 1 A ## 9 208 2 F ## 10 207 2 A ## 11 206 2 C ## 12 205 2 E ## 13 204 2 TS ## 14 203 2 B ## 15 202 2 NT ## 16 201 2 D ## 17 301 3 TS ## 18 302 3 NT ## 19 303 3 F ## 20 304 3 A ## 21 305 3 B ## 22 306 3 E ## 23 307 3 C ## 24 308 3 D ## 25 408 4 D ## 26 407 4 TS ## 27 406 4 A ## 28 405 4 F ## 29 404 4 E ## 30 403 4 C ## 31 402 4 B ## 32 401 4 NT print(designRCBD$sketch) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] &quot;E&quot; &quot;B&quot; &quot;NT&quot; &quot;F&quot; &quot;D&quot; &quot;TS&quot; &quot;C&quot; &quot;A&quot; ## [2,] &quot;F&quot; &quot;A&quot; &quot;C&quot; &quot;E&quot; &quot;TS&quot; &quot;B&quot; &quot;NT&quot; &quot;D&quot; ## [3,] &quot;TS&quot; &quot;NT&quot; &quot;F&quot; &quot;A&quot; &quot;B&quot; &quot;E&quot; &quot;C&quot; &quot;D&quot; ## [4,] &quot;D&quot; &quot;TS&quot; &quot;A&quot; &quot;F&quot; &quot;E&quot; &quot;C&quot; &quot;B&quot; &quot;NT&quot; Anche in questo caso, riportiamo tutto sulla mappa. Schema sperimentale a blocchi randomizzati per l’Esempio 1 3.3.12.2 Esempio 2 In questo caso, per ognuno dei tre anni di prova, la mappa contiene una griglia 4 x 4, analoga a quella dell’esperimento precedente, ma più piccola. Nella mappa potremo quindi identificare, esclusi i bordi, quattro colonne e quattro righe. Dato che abbiamo presupposto l’esistenza di un gradiente trasversale e lungitudinale (tra righe e tra colonne), l’allocazione dei trattamenti dovrà esser fatta in modo che ognuno di essi si trovi su ogni riga e ogni colonna. Questo tipo di disegno prende il nome di Quadrato latino. Anche in questo caso potremo chiedere ad R di aiutarci a trovare la combinazione corretta (anche se questo potrebbe essere comodamente fatto a mano). trt &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) designLS &lt;- design.lsd(trt, seed=543, serie=2) designLS$book ## plots row col trt ## 1 101 1 1 C ## 2 102 1 2 A ## 3 103 1 3 B ## 4 104 1 4 D ## 5 201 2 1 D ## 6 202 2 2 B ## 7 203 2 3 C ## 8 204 2 4 A ## 9 301 3 1 B ## 10 302 3 2 D ## 11 303 3 3 A ## 12 304 3 4 C ## 13 401 4 1 A ## 14 402 4 2 C ## 15 403 4 3 D ## 16 404 4 4 B Schema sperimentale a quadrato latino per l’Esempio 2 (un anno) A questo punto dobbiamo considerare che questa prova deve essere ripetuta in tre anni. La ripetizione di una prova è sempre fondamentale, in quanto consente di verificare non solo la replicabilità dell’esperimento (che è dimostrata dalle repliche), ma anche la sua riproducibilità (riguardare le definizioni di replicabilità e riproducibilità). In questo caso poi la ripetizione dell’esperimento è indispensabile per misurare la stabilità produttiva, cioè l’oscillazione delle produzioni da un anno all’altro. Ovviamente è anche importante verificare la stabilità produttiva da una località all’altra, che consente di valutare l’esistenza di macro-areali, nei quali è possibile consigliare le stesse varietà. Un’aspetto fondamentale è comunque quello di definire una diversa randomizzazione in ogni anno/località, per evitare che le stesse varietà siano sempre nelle stesse posizioni, che potrebbe dare origine a dubbi di confounding. La definizione delle randomizzazioni per il secondo e terzo anno è lasciata per esercizio. Un’altro aspetto da considerare è la metodica impiegata per la determinazione del peso di 1000 semi. Abbiamo già visto che, per aumentare la precisione e la rappresentatività, da tutta la granella raccolta da una parcella preleviamo quattro lotti da 1000 semi, di cui determinare il peso. In questo modo, per ogni trattamento avremo 16 valori (quattro repliche x quattro lotti per replica). Ovviamente non possiamo affermare di avere 16 repliche, in quanto solo le parcelle sono da considerare repliche, in quanto ricevono il trattamento (varietà) in modo indipendente. I quattro lotti raccolti da ogni parcella sono unità osservazionali (perché ne viene rilevato il peso), ma non unità sperimentali, perché appartengono alla stessa parcella e non sono indipendenti. I quattro lotti si dicono sub-repliche, quindi il disegno ha quattro repliche e quattro sub-repliche per replica (disegno a quadrato latino con sottocampionamento). I due strati di errore (variabilità tra repliche e variabilità tra sub-repliche entro replica), devono essere mantenuti separati in fase di analisi, altrimenti l’analisi è invalida, perché è condotta come se avessimo un più alto grado di precisione (16 repliche) rispetto a quello che abbiamo effettivamente (una sorta di millantato credito!). 3.3.12.3 Esempio 3 In questo caso abbiamo un disegno fattoriale con due livelli a blocchi randomizzati. Nel principio, questo disegno non ha nulla di diverso da quello relativo all’esempio 1, fatto salvo un minor numero di trattamenti (solo 6). Anche in questo caso, ci facciamo aiutare da R. trt &lt;- c(3,2) # factorial 3x2 design2way &lt;-design.ab(trt, r=4, serie=2, design=&quot;rcbd&quot;, seed=777) book &lt;- design2way$book levels(book$A) &lt;- c(&quot;PROF&quot;, &quot;SUP&quot;, &quot;MIN&quot;) levels(book$B) &lt;- c(&quot;TOT&quot;, &quot;PARZ&quot;) book ## plots block A B ## 1 101 1 SUP PARZ ## 2 102 1 PROF PARZ ## 3 103 1 PROF TOT ## 4 104 1 MIN TOT ## 5 105 1 SUP TOT ## 6 106 1 MIN PARZ ## 7 107 2 MIN TOT ## 8 108 2 SUP TOT ## 9 109 2 MIN PARZ ## 10 110 2 PROF TOT ## 11 111 2 SUP PARZ ## 12 112 2 PROF PARZ ## 13 113 3 MIN TOT ## 14 114 3 SUP TOT ## 15 115 3 PROF PARZ ## 16 116 3 MIN PARZ ## 17 117 3 SUP PARZ ## 18 118 3 PROF TOT ## 19 119 4 MIN PARZ ## 20 120 4 PROF TOT ## 21 121 4 PROF PARZ ## 22 122 4 MIN TOT ## 23 123 4 SUP TOT ## 24 124 4 SUP PARZ La mappa risultante è visibile più sotto. Schema sperimentale fattoriale a blocchi randomizzati per l’Esempio 3 Questo disegno è totalmente appropriato, ma ci costringe a lasciare parecchio spazio tra una parcella e l’altra, per poter manovrare con la macchina per la lavorazione del terreno. Sarebbe utile raggruppare le parcelle caratterizzate dalla stessa lavorazione, in modo da poter lavorare su superfici più ampie. Ne guadagnerebbe l’uniformità dell’esperimento e l’accuratezza dei risultati. Possiamo quindi immaginare un disegno a un fattore, con parcelle di dimensione doppia (main-plots), sulle quali eseguire, in modo randomizzato le lavorazioni del terreno. Succesivamente, ogni main-plot può essere suddivisa in due e, su ognuna delle due metà, possono essere allocati in modo random i due trattamenti di diserbo. In questo modo ci troviamo ad operare con parcelle di due dimensioni diverse: le main-plots per le lavorazioni e le sub-plots per il diserbo. Questo tipo di schema prende il nome di parcella suddivisa (split-plot), ed è piuttosto comune nella sperimentazione di pieno campo. Proviamo ad utilizzare R per redigere il piano sperimentale. lavorazione &lt;- c(&quot;PROF&quot;, &quot;SUP&quot;, &quot;MIN&quot;) diserbo &lt;- c(&quot;TOT&quot;, &quot;PARZ&quot;) designSPLIT &lt;- design.split(lavorazione, diserbo, r=4, serie=2, seed=777) book &lt;- designSPLIT$book book ## plots splots block lavorazione diserbo ## 1 101 1 1 SUP PARZ ## 2 101 2 1 SUP TOT ## 3 102 1 1 PROF TOT ## 4 102 2 1 PROF PARZ ## 5 103 1 1 MIN PARZ ## 6 103 2 1 MIN TOT ## 7 104 1 2 SUP PARZ ## 8 104 2 2 SUP TOT ## 9 105 1 2 MIN TOT ## 10 105 2 2 MIN PARZ ## 11 106 1 2 PROF TOT ## 12 106 2 2 PROF PARZ ## 13 107 1 3 MIN TOT ## 14 107 2 3 MIN PARZ ## 15 108 1 3 SUP TOT ## 16 108 2 3 SUP PARZ ## 17 109 1 3 PROF TOT ## 18 109 2 3 PROF PARZ ## 19 110 1 4 PROF PARZ ## 20 110 2 4 PROF TOT ## 21 111 1 4 MIN TOT ## 22 111 2 4 MIN PARZ ## 23 112 1 4 SUP PARZ ## 24 112 2 4 SUP TOT Schema sperimentale split-plot a blocchi randomizzati per l’Esempio 3 In alcune circostanze, soprattutto nelle prove di diserbo chimico, potrebbe trovare applicazione un altro tipo di schema sperimentale, nel quale, in ogni blocco, un trattamento viene applicato a tutte le parcelle di una riga e l’altro trattamento a tutte le parcelle di una colonna. Ad esempio, il disegno sottostante mostra una prova nella quale il terreno è stato diserbato in una striscia nel senso della lunghezza e, dopo il diserbo, le colture sono state seminate in striscia, nel senso della larghezza. Questo disegno è detto strip-plot ed è molto comodo perché consente di lavorare velocemente. Schema sperimentale strip-plot 3.3.12.4 Esempio 4 La prova di erba medica è fondamentalmente un esperimento a blocchi randomizzati, il cui piano è riportato più sotto. Tuttavia, si tratta di una coltura poiliennale nella quale ripeteremo le misurazioni ogni anno sulle stesse parcelle. le misure ripetute non sono randomizzate (non possono esserlo), ma seguono una metrica temporale. Proprio per questo sviluppo lungo la scala del tempo, i dati che si raccolgono in questi esperimenti a misure ripetute sono detti dati longitudinali. Guardando bene il disegno si capisce anche per si parla di split-plot nel tempo. Esempi affini sono relativi all’analisi di accrescimento con misure non distruttive (esempio l’altezza) oppure i prelievi di terreno a profondità diverse, anche se, in quest’ultimo caso, la metrica delle misure ripetute è spaziale, non temporale. Si può notare una certa analogia con il sottocampionamento illsutrato più sopra, nel senso che vengono prese più misure per parcella. Tuttavia, bisogna tener presente che nel sottocampionamento le diverse misure sono solo repliche e non vi è nessuna esigenza di distinguere tra quelle prese nella stessa parcella. Invece, nel caso delle misure ripetute ognuna di esse ha interesse individuale, in quanto espressione di un’anno particolare. Schema sperimentale a blocchi randomizzati con misure ripetute 3.3.12.5 Esempio 5 Per questo disegno osservazionale, la mappa non è necessaria. Tuttavia, si può notare che, in ogni supermercato, abbiamo un disegno a randomizzazione completa, con tre tipi di confezioni e tre repliche, cioè nove confezioni scelte a random da un lotto più grande. Insomma, si tratta di un esperimento ripetuto 9 volte che, pertanto, ha una certa affinità con l’esperimento ripetuto dell’Esempio 2. 3.3.13 Impianto delle prove Da questo punto in poi, subentrano le competenze agronomiche e fitopatologiche necessarie per codurre gli esperimenti, Mi piace solo ricordare alcune pratiche usuali nella sperimentazione di pieno campo, destinate a migliorare l’efficienza della prova. Seminare a densità più alte e poi diradare, per assicurare una migliore uniformità di impianto Prelevare da ogni parcella più campioni ed, eventualmente, omogeneizzarli o mediare i risultati ottenuti (vedi il caso dei 1000 semi) Considerare le caratteristiche naturalmente meno variabili (es. la produzione areica e non la produzione per pianta) Voglio inoltre ricordare che gli esperimenti parcellari configurano una situazione nella quale, per l’elevata cura che si pone nelle tecniche agronomiche, si riesce ad ottenere una produttività almeno del 20% superiore rispetto a quanto avviene nella normale pratica agricola. 3.4 Come scrivere un progetto di ricerca o un report di ricerca Quanto abbiamo finora esposto costituisce uno schema generale che può essere adottato per redigere un progetto di ricerca o un report sui risultati ottenuti (tesi, pubblicazione). Bisogna provare che la ricerca che si è eseguita è precisa, accurata e replicabile/riproducibile e, di conseguenza, i risultati sono validi. Nella redazione di un progetto di ricerca o di un report, è fondamentale tratteggiare bene i seguenti elementi: Titolo della ricerca Descrizione del problema e background scientifico Ipotesi scientifica, motivazioni e obiettivi Tipo di esperimento e durata Disegno sperimentale: trattamenti sperimentali (tesi) a confronto con dettagli relativi all’applicazione Unità sperimentali e criteri per la loro selezione. Dettagli su repliche e randomizzazione Dettagli su eventuali tecniche di ‘blocking’ Variabili da rilevare/rilevate Dettagli su come le variabili saranno/sono state rilevate Esposizione dei risultati (solo report) Discussione (solo report) Conclusioni (solo report) Alcuni aspetti che divengono elemento di valutazione del progetto e/o del report sono i seguenti: La selezione dei metodi deve essere coerente con gli obiettivi Descrizione dettagliata dei materiali e metodi (bisogna che chiunque sia in grado di replicare l’esperimento) Esposizione dei risultati chiara e convincente Discussione approfondita e con molti riferimenti alla letteratura. 3.5 Per approfondimenti Hurlbert, S., 1984. Pseudoreplication and the design of ecological experiments. Ecological Monographs, 54, 187-211 Kuehl, R. O., 2000. Design of experiments: statistical principles of research design and analysis. Duxbury Press (CHAPTER 1) LeClerg, E.; Leonard, W. &amp; Clark, A., 1962. Field Plot Technique. Burgess Publishing Company, (CHAPTER 3) Felipe de Mendiburu (2017). agricolae: Statistical Procedures for Agricultural Research. R package, version 1.2-8. https://CRAN.R-project.org/package=agricolae "],
["per-iniziare-introduzione-ad-r.html", "Capitolo 4 Per iniziare: introduzione ad R 4.1 Cosa è R? 4.2 Oggetti e assegnazioni 4.3 Costanti e vettori 4.4 Matrici 4.5 Operazioni ed operatori 4.6 Funzioni ed argomenti 4.7 Dataframe 4.8 Quale oggetto sto utilizzando? 4.9 Consigli per l’immissione di dati sperimentali 4.10 Alcune operazioni comuni sul dataset 4.11 Workspace 4.12 Script o programmi 4.13 Interrogazione di oggetti 4.14 Altre funzioni matriciali 4.15 Cenni sulle funzionalità grafiche in R 4.16 Per approfondimenti", " Capitolo 4 Per iniziare: introduzione ad R 4.1 Cosa è R? R è un software cugino di S-PLUS, con il quale condivide la gran parte delle procedure ed una perfetta compatibilità. Rispetto al cugino più famoso, è completamente freeware (sotto la licenza GNU General Public Licence della Free Software Foundation) ed è nato proprio per mettere a disposizione degli utenti un software gratuito, potente, mantenendo comunque la capacità di lavorare in proprio senza usare software di frodo. E’uno strumento molto potente, anche da un punto di vista grafico, ma necessita di una certa pratica, in quanto manca di una vera e propria interfaccia grafica (Graphical User Interface: GUI) e, di conseguenza, è spesso necessario scrivere codice. Inoltre, si tratta di un programma Open Source, cioè ognuno può avere accesso al suo codice interno ed, eventualmente, proporne modifiche. Altro vantaggio è che, oltre che un programma, R è anche un linguaggio object oriented, che può essere utilizzato dall’utente per creare funzioni personalizzate. Per evitare noiosi errori che possono essere molto comuni per chi è abituato a lavorare in ambiente WINDOWS, è bene precisare subito che R, come tutti i linguaggi di derivazione UNIX, è case sensitive, cioè distingue tra lettere maiuscole e lettere minuscole. 4.2 Oggetti e assegnazioni 4.3 Costanti e vettori R lavora con valori, stringhe di caratteri, vettori e matrici, che vengono assegnati alle variabili con opportuni comandi. Ad esempio, il comando: y &lt;- 3 y ## [1] 3 assegna il valore 3 alla variabile y. Invece il comando: x &lt;- c(1, 2, 3) x ## [1] 1 2 3 crea un vettore x contenente i numeri 1,2 e 3. Bisogna precisare che con il termine ‘vettore’ in R non ci si riferisce al vettore algebrico, ma più semplicemente ad una serie di numeri (o strighe) consecutivi, rappresentati convenzionalmente da R in una riga. 4.4 Matrici Oltre ai vettori, in R possiamo definire le matrici. Ad esempio il comando: z &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), 2, 4, byrow=TRUE) crea una matrice z a 2 righe e 4 colonne, contenente i numeri da 1 a 8. La matrice viene riempita per riga. Come già mostrato, per visualizzare il contenuto di una variabile basta digitare il nome della variabile. Ad esempio: z ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 Gli elementi di una matrice possono essere richiamati con un opportuno utilizzo delle parentesi quadre: z[1,3] ## [1] 3 4.5 Operazioni ed operatori Le variabili possono essere create anche con opportune operazioni algebriche, che si eseguono utilizzando i normali operatori (+, -, *, /). Ad esempio: f &lt;- 2 * y f ## [1] 6 4.6 Funzioni ed argomenti Per eseguire operazioni particolari si utilizzano, in genere, le funzioni. Una funzione è richiamata con un nome ed uno o più argomenti. Ad esempio, il comando: log(5) ## [1] 1.609438 Calcola il logaritmo naturale di 5 e richiede un solo argomento, cioè il numero di cui calcolare il logaritmo. Al contrario, il comando: log(100, 2) ## [1] 6.643856 Calcola il logaritmo in base 2 di 100 e richiede due argomenti, cioè il numero di cui calcolare il logaritmo e la base del logaritmo. Quando sono necessari due o più argomenti essi debbono essere messi nell’ordine esatto (in questo caso prima il numero poi la base) oppure debbono essere utilizzati i riferimenti corretti. Ad esempio, i due comandi: log(100, base=2) ## [1] 6.643856 log(base=2, 100) ## [1] 6.643856 restituiscono lo stesso risultato, al contrario dei due comandi seguenti: log(100, 2) ## [1] 6.643856 log(2, 100) ## [1] 0.150515 4.7 Dataframe Oltre a vettori e matrici, in R esiste un altro importante oggetto, cioè il dataframe, costituito da una tabella di dati con una o più colonne di variabili e una o più righe di dati. A differenza della matrice, il dataframe può essere utilizzato per memorizzare variabili di diverso tipo (numeri e caratteri). Un dataframe può essere creato unendo più vettori, come nell’esempio seguente. parcelle &lt;- c(1, 2, 3, 4, 5, 6) tesi &lt;- factor(c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;)) dati &lt;- c(12, 15, 16, 13, 11, 19) tabella &lt;- data.frame(&quot;Parc&quot;=parcelle,&quot;Tesi&quot;=tesi,&quot;Produzioni&quot;=dati) tabella ## Parc Tesi Produzioni ## 1 1 A 12 ## 2 2 A 15 ## 3 3 B 16 ## 4 4 B 13 ## 5 5 C 11 ## 6 6 C 19 Per utilizzare i dati in un dataframe, bisognerà accedere ai singoli vettori colonna che lo costituiscono. Per far questo possiamo utilizzare l’estrattore $: tabella$Parc ## [1] 1 2 3 4 5 6 oppure possiamo utilizzare gli indici, che nel caso del dataframe, cioè una struttura dati bidimensionale, sono due, uno per le righe e uno per le colonne, separati da virgole: tabella[,1] ## [1] 1 2 3 4 5 6 oppure si può usare il comando attach(), che crea immediatamente tre vettori (Pianta, Varietà e Altezza), disponibili per le successive elaborazioni.Possiamo osservare infatti che, dopo aver creato la matrice ‘tabella’, digitando quanto segue R ci mette a disposizione il vettore ‘Produzioni’. attach(tabella) ## The following objects are masked from tabella (pos = 5): ## ## Parc, Produzioni, Tesi Produzioni ## [1] 12 15 16 13 11 19 I dataframe possono essere editati velocemente utilizzando il comando fix, che fa apparire una finestra di editing tipo ‘foglio elettronico’. 4.8 Quale oggetto sto utilizzando? Per avere informazioni sulla natura di un oggetto creato in R, posso usare la funzione str(), come nell’esempio seguente: str(tabella) ## &#39;data.frame&#39;: 6 obs. of 3 variables: ## $ Parc : num 1 2 3 4 5 6 ## $ Tesi : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 2 2 3 3 ## $ Produzioni: num 12 15 16 13 11 19 Vediamo infatti che R ci informa che l’oggetto ‘tabella’ è in realtà un dataframe composto da tre colonne, di cui la prima e la terza sono numeriche, mentre la seconda è una variabile qualitativa (fattore). 4.9 Consigli per l’immissione di dati sperimentali I dati delle prove sperimentali si possono o importare in R da altri software (ad esempio Excel) oppue si possono digitare direttamente in R. In quest’ultimo caso, in genere, si crea un vettore per ogni colonna di dati e, successivamente, si riuniscono i vettori in un dataframe, che viene poi salvato nel workspace, come vedremo in seguito. 4.9.1 Immissione manuale di dati L’immissione dei dati in R (e quindi la creazione di vettori) può essere velocizzata utilizzando la funzione scan(), separando i dati con INVIO (questo è comodo perchè ci permette di lavorare senza abbandonare il tastierino numerico!). L’immissione termina quando si digita un INVIO a vuoto. dati &lt;- scan() 1: 12 2: 14 3: 16 4: 18 5: 20 6: Read 5 items dati [1] 12 14 16 18 20 La stessa funzione può essere anche utilizzata per immettere comodamente stringhe di caratteri, con un opportuno impiego dell’argomento what. In questo caso è possibile omettere le virgolette. tesi &lt;- scan(what = &quot;character&quot;) 1: aurelio 2: aurelio 3: aurelio 4: claudio 5: claudio 6: claudio 7: latino 8: latino 9: latino 10: Read 9 items tesi [1] &quot;aurelio&quot; &quot;aurelio&quot; &quot;aurelio&quot; &quot;claudio&quot; &quot;claudio&quot; &quot;claudio&quot; &quot;latino&quot; &quot;latino&quot; &quot;latino&quot; &gt; 4.9.2 Immissione di numeri progressivi Per creare una serie progressiva, si può utilizzare il comando seq(n,m,by=step) che genera una sequenza da \\(n\\) a \\(m\\) con passo pari a \\(step\\). parcelle &lt;- seq(1,50,1) parcelle ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 ## [47] 47 48 49 50 4.9.3 Immissione dei codici delle tesi e dei blocchi A volte i codici delle tesi sono sequenze ripetute di stringhe. Ad esempio, i primi quattro dati potrebbero essere riferiti alla varietà BAIO, i secondi quattro alla varietà DUILIO, i successivi quattro alla varietà PLINIO. Per creare velocemente questo vettore, possiamo utilizzare la funzione rep(), in questo modo. tesi &lt;- factor(c(&quot;BAIO&quot;, &quot;DUILIO&quot;, &quot;PLINIO&quot;)) tesi ## [1] BAIO DUILIO PLINIO ## Levels: BAIO DUILIO PLINIO tesi &lt;- rep(tesi,each=4) tesi ## [1] BAIO BAIO BAIO BAIO DUILIO DUILIO DUILIO DUILIO PLINIO PLINIO ## [11] PLINIO PLINIO ## Levels: BAIO DUILIO PLINIO Notare l’uso della funzione factor() per creare un vettore di dati qualitativi (fattore). Allo stesso modo, per immettere i codici dei blocchi possiamo utilizzare la stessa funzione in un modo diverso. Ammettiamo infatti che i quattro valori di ogni tesi appartengano rispettivamente ai quattro blocchi; si opera quindi in questo modo. tesi &lt;- (c (1, 2, 3, 4)) tesi &lt;- rep(tesi, times=3) tesi ## [1] 1 2 3 4 1 2 3 4 1 2 3 4 4.9.4 Leggere e salvare dati esterni Oltre che immessi da tastiera, i dati possono essere importati in R da files esterni, Inoltre, gli oggetti di R creati nel corso di una sessione possono essere memorizzati su files esterni. Partiamo dal presupposto di aver creato (come frequentemente avviene) il nostro database con EXCEL e di volerlo importare in R nel DATAFRAME dati. Creiamo in EXCEL la tabella riportata di seguito, che si riferisce a 20 piante di mais. Pianta Var Altezza 1 N 172 2 S 154 3 V 150 4 V 188 5 C 162 6 N 145 7 C 157 8 C 178 9 V 175 10 N 158 11 N 153 12 N 191 13 S 174 14 C 141 15 N 165 16 C 163 17 V 148 18 S 152 19 C 169 20 C 185 La procedura è la seguente: salviamo questa tabella nel file di testo: comma delineated ‘import.csv’. Per far questo scegliere ‘Menù - File - Salva con nome’. Scegliere un nome per il file ed indicare: ’Tipo file = CSV (delimitato dal separatore di elenco) (*.csv). Salvare quindi il file in una directory prescelta. Avviare una sessione R, cambiare la directory predefinita del sistema, scegliendo con il menu File, Change Directory, la cartella nella quale abbiamo memorizzato il file di importazione. Leggere il file di testo in un dataframe, con il seguente comando: setwd(&quot;myWorkingDir&quot;) dati &lt;- read.csv(&quot;import.csv&quot;, header=TRUE) Il comando appena descritto ha successo per file CSV creati con la versione inglese di Windows, caratterizzati dal punto come separatore decimale e dalla virgola come separatore di elenco. Se invece il computer fosse settato all’italiana, con la virgola come separatore decimale e il punto e virgola come separatore di elenco, allora si potrebbe utilizzare la funzione read.csv2() (stessa sintassi). Con questi due comandi, in R viene creato un dataframe di nome dati, contenente le tre colonne della tabella ‘import.csv’ appena creata, comprese le intestazioni di colonna. I dati contenuti in un dataframe o in qualunque altro oggetto possono essere salvati in un file esterno (in formato R binario): save(file=&quot;dati1.rda&quot;, dati) ed eventualmente ricaricati: load(&quot;dati1.rda&quot;) Per scrivere in un file di testo (in questo caso comma delineated, ma il separatore di elenco può essere modificato secondo le nostre esigenze con l’argomento sep) si utilizza il seguente comando: write.table(dati, &quot;residui.csv&quot;, row.names=FALSE, col.names=TRUE, sep=&quot;,&quot;) 4.10 Alcune operazioni comuni sul dataset 4.10.1 Selezionare un subset di dati E’ possibile estrarre da un dataframe un subset di dati utilizzando la funzione: subset(dataframe, condizione) Ad esempio, se consideriamo il dataframe tabella creato in precedenza, è possibile selezionare tutte le righe relative alle Tesi A e C come segue: tabella2 &lt;- subset(tabella, Tesi == &quot;A&quot; | Tesi == &quot;C&quot;) tabella2 ## Parc Tesi Produzioni ## 1 1 A 12 ## 2 2 A 15 ## 5 5 C 11 ## 6 6 C 19 Notare il carattere “|” che esprime la condizione logica OR. La condizione logica AND si esprime con il carattere “&amp;”. L’esempio seguente isola i record in cui le varietà sono A o C e, contemporaneamente, la produzione è minore di 19. tabella3 &lt;- subset(tabella, Tesi == &quot;A&quot; | Tesi == &quot;C&quot; &amp; Produzioni &lt; 19) tabella3 ## Parc Tesi Produzioni ## 1 1 A 12 ## 2 2 A 15 ## 5 5 C 11 4.10.2 Ordinare un vettore o un dataframe Un vettore (numerico o carattere) può essere ordinato con il comando sort: y &lt;- c(12, 15, 11, 17, 12, 8, 7, 15) sort(y, decreasing = FALSE) ## [1] 7 8 11 12 12 15 15 17 z &lt;- c(&quot;A&quot;, &quot;C&quot;, &quot;D&quot;, &quot;B&quot;, &quot;F&quot;, &quot;L&quot;, &quot;M&quot;, &quot;E&quot;) sort(z, decreasing = TRUE) ## [1] &quot;M&quot; &quot;L&quot; &quot;F&quot; &quot;E&quot; &quot;D&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; Un dataframe può essere invece ordinato con il comando order(), facendo attenzione al segno meno utilizzabile per l’ordinamento decrescente. dataset[order(dataset$z, dataset$y), ] dataset[order(dataset$z, -dataset$y), ] 4.11 Workspace Gli oggetti creati durante una sessione di lavoro vengono memorizzati nel cosiddetto workspace. Per il salvataggio del workspace nella directory corrente si usa il menu (File/Save Workspace) oppure il comando: save.image(&#39;nomefile.RData&#39;) Il contenuto del workspace viene visualizzato con: ls() Il workspace viene richiamato da menu (File/Open Workspace) oppure con il comando: load(&#39;nomefile.RData&#39;) Per un lavoro efficiente in R è bene tenere il workspace molto pulito, eliminando gli oggetti non necessari. La completa eliminazione degli oggetti nel workspace si esegue con: rm(list=ls()) Uno o più oggetti specifici possono essere eliminati con: rm(oggetto1, oggetto2, .....) Gli oggetti possono anche essere richiamati in base alla loro posizione; ad esempio il comando: rm(list=ls()[3:4]) elimina il terzo e il quarto oggetto dal workspace. Un comando particolarmente utile è il seguente: rm(list=ls()[ls()!=&quot;oggetto1&quot;]) che permette di eliminare dal workspace ogni oggetto meno “oggetto1”. Si possono utilizzare anche clausole logiche più articolate come la seguente: rm(list=ls()[ls()!=&quot;oggetto1&quot; &amp; ls()!=&quot;oggetto2&quot;]) che elimina tutto meno “oggetto1” e “oggetto2”. 4.12 Script o programmi Come è possibile memorizzare dati e workspace, è anche possibile creare uno script (procedura, funzione…) da memorizzare e richiamare in seguito. Nel caso più semplice è possibile scrivere comandi in un semplice editor di testo e salvarli in un file con estensione ‘.r’. I comandi possono poi essere riutilizzati per semplice copia ed incolla sulla console, opppure, nel caso in cui si utilizzi Rstudio (FILE -\\(&gt;\\) APRI SCRIPT o NUOVO SCRIPT) selezionando il comando (o i comandi) da inviare alla console e premendo la combinazione CTRL + INVIO. Lavorare con scripts è molto comodo e consigliabile perchè non si deve partire da zero ad ogni sessione, ma è sufficiente correggere i comandi digitati in sessioni precedenti. Oltre agli script, è possibile creare funzioni personalizzate fino ad arrivare a veri e propri programmi (packages). Immaginiamo ad esempio di voler scrivere una funzione che, dato il valore della produzione rilevata in una parcella di orzo di 20 $ m^2 $ (in kg) e la sua umidità percentuale, calcoli automaticamente il valore della produzione secca in kg/ha. La funzione che dobbiamo implementare è: \\[ PS = PU \\cdot \\frac{100 - U}{100} \\cdot \\frac{10000}{20} \\] ove PS è la produzione secca in kg/ha e PU è la produzione all’umidità U in kg per 20 $ m^2 $. Scriveremo un file di testo (ad esempio con il Block notes o con l’editor interno ad R): PS &lt;- function(PU, U) { PU*((100-U)/100)*(10000/20) } Notare l’uso delle parentesi graffe. Salveremo il file di testo con il nome (ad esempio) “prova.r”. Aprendo una nuova sessione in R, possiamo ricaricare in memoria il file di programma (FILE - SORGENTE CODICE R, oppure da console, con il comando: source(&#39;prova.r&#39;) A differenza di quanto avviene con uno script, i comandi memorizzati nella funzione non vengono eseguiti, ma la funzione ‘PS’ diviene disponibile nel workspace e può essere utilizzata nel modo seguente: PS(20,85) 4.13 Interrogazione di oggetti A differenza di altri linguaggi statistici come SAS o SPSS, R immagazzina i risultati delle analisi negli oggetti, mostrando un output video piuttosto minimale. Per ottenere informazioni è necessario interrogare opportunamente gli oggetti che al loro interno possono contenere altri oggetti da cui recuperare le informazioni interessanti. Gli oggetti che contengono altri oggetti sono detti liste. Ad esempio, se vogliamo calcolare autovettori ed autovalori di una matrice, utilizziamo la funzione ‘eigen’. Questa funzione restituisce una lista di oggetti, che al suo interno contiene i due oggetti values (autovalori) e vectors (autovettori). Per recuperare l’uno o l’altro dei due risultati (autovettori o autovalori) si usa l’operatore di concatenamento (detto anche estrattore) $. matrice &lt;- matrix(c(2,1,3,4),2,2) matrice ## [,1] [,2] ## [1,] 2 3 ## [2,] 1 4 ev &lt;- eigen(matrice) ev ## eigen() decomposition ## $values ## [1] 5 1 ## ## $vectors ## [,1] [,2] ## [1,] -0.7071068 -0.9486833 ## [2,] -0.7071068 0.3162278 ev$values ## [1] 5 1 ev$vectors ## [,1] [,2] ## [1,] -0.7071068 -0.9486833 ## [2,] -0.7071068 0.3162278 4.14 Altre funzioni matriciali Oltre che autovettori ed autovalori di una matrice, R ci permette di gestire altre funzioni di matrice. Se ad esempio abbiamo le matrici: \\[ Z = \\left( {\\begin{array}{*{20}c} 1 &amp; 2 2 &amp; 3 \\end{array}} \\right)\\,\\,\\,\\,\\,\\,\\,Y = \\left( {\\begin{array}{*{20}c} 3 &amp; 2 \\end{array}} \\right) \\] queste possono essere caricate in R con i seguenti comandi: Z &lt;- matrix(c(1,2,2,3),2,2) Y &lt;- matrix(c(3,2),1,2) Possiamo poi ottenere la trasposta di Z con il comando: t(Z) ## [,1] [,2] ## [1,] 1 2 ## [2,] 2 3 Possiamo moltiplicare Y e Z utilizzando l’operatore %*%: Y%*%Z ## [,1] [,2] ## [1,] 7 12 Possiamo calcolare l’inversa di Z con: solve(Z) ## [,1] [,2] ## [1,] -3 2 ## [2,] 2 -1 4.15 Cenni sulle funzionalità grafiche in R R è un linguaggio abbastanza potente e permette di creare grafici interessanti. Ovviamente un trattazione esauriente esula dagli scopi di questo testo, anche se è opportuno dare alcune indicazioni che potrebbero essere utili in seguito. La funzione più utilizzata per produrre grafici è: plot(x,y, type, xlab, ylab, col, lwd, lty...) ovex ed y sono i vettori con le coordinate dei punti da disegnare. Type rappresenta il tipo di grafico (‘’p’‘produce un grafico a punti,’‘l’‘un grafico a linee,’‘b’‘disegna punti uniti da linee,’‘h’’ disegna istogrammi), ’Title disegna il titolo del grafico, sub il sottotitolo,xlab e ylab le etichette degli assi, col è il colore dell’oggetto, lwd il suo spessore, lty il tipo di linea e cos'i via. Per una descrizione più dettagliata si consiglia di consultare la documentazione on line. A titolo di esempio mostriamo l’output dei comandi: x &lt;- c(1,2,3,4) y &lt;- c(10,11,13,17) plot(x, y, &quot;p&quot;, col=&quot;red&quot;, lwd=5,xlab=&quot;Ascissa&quot;, ylab=&quot;Ordinata&quot;) Per sovrapporre un’altra serie di punti alla precedente possiamo utilizzare il comando ‘points’: y2 &lt;- c(17,13,11,10) plot(x, y, &quot;p&quot;, col=&quot;red&quot;, lwd=5,xlab=&quot;Ascissa&quot;, ylab=&quot;Ordinata&quot;) points(x, y2, col=&quot;blue&quot;, lwd=5) Se avessimo voluto sovrapporre una grafico a linee avremmo invece utilizzato la funzione ‘lines’: plot(x, y, &quot;p&quot;, col=&quot;red&quot;, lwd=5,xlab=&quot;Ascissa&quot;, ylab=&quot;Ordinata&quot;) points(x, y2, col=&quot;blue&quot;, lwd=5) lines(x, y2, col=&quot;blue&quot;, lwd=2) Per disegnare una curva si può utilizzare la funzione: curve(funzione, Xiniziale, Xfinale, add=FALSE/TRUE) dove l’argomento ‘add’ serve per specificare se la funzione deve essere aggiunta ad un grafico preesistente. Per aggiungere un titolo ad un grafico possiamo utilizzare la funzione: title(main=&quot;Titolo&quot;) mentre per aggiungere una legenda utilizziamo la funzione: legend(Xcoord, YCoord , legend=c(&quot;Punti&quot;,&quot;X+10&quot;), pch=c(19,-1), col=c(&quot;Red&quot;,&quot;Blue&quot;), lwd=c(3,3), lty=c(0,3)) ove i vettori indicano, per ogni elemento della legenda, il testo che deve essere riportato (legend), il tipo di simbolo (pch, con -1 che indica nessun simbolo), il colore (col), la larghezza (lwd) e il tipo di linea (lty, con 0 che indica nessuna linea). Ad esempio: plot(x, y, &quot;p&quot;, col=&quot;red&quot;, lwd=5, xlab=&quot;Ascissa&quot;, ylab = &quot;Ordinata&quot;) curve(10+x, add=TRUE, lty=1, lwd=2, col=&quot;blue&quot;) title(main=&quot;Grafico di prova&quot;) legend(1,17, legend=c(&quot;Punti&quot;, &quot;X+10&quot;), pch=c(19,-1), col=c(&quot;Red&quot;, &quot;Blue&quot;), lwd=c(3,3), lty=c(0,1)) L’ultima cosa che desideriamo menzionare è la possibilità di disegnare grafici a torta, utilizzando il comando: pie(vettoreNumeri, vettoreEtichette, vettoreColori) Ad esempio il comando: pie(c(20,30,50),label=c(&quot;B&quot;, &quot;C&quot;), col=c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;)) 4.16 Per approfondimenti Per approfondimenti si consiglia la consultazione di: Maindonald J. Using R for Data Analysis and Graphics - Introduction, Examples and Commentary. (PDF, data sets and scripts are available at JM’s homepage. Per conoscere più a fondo l’ambiente di svilupp RStudio, consiglio la lettura di: Oscar Torres Reina, 2013. Introductio to RStudio (v. 1.3). This homepage "],
["primo-passo-la-descrizione-dei-dati-raccolti.html", "Capitolo 5 Primo passo: la descrizione dei dati raccolti 5.1 Le variabili quantitative: analisi chimiche e altre misurazioni fondamentali 5.2 Descrizione dei sottogruppi 5.3 Distribuzioni di frequenza e classamento 5.4 Statistiche descrittive per le distribuzioni di frequenza 5.5 Distribuzioni di frequenza bivariate: le tabelle di contingenza 5.6 Connessione 5.7 Correlazione", " Capitolo 5 Primo passo: la descrizione dei dati raccolti 5.1 Le variabili quantitative: analisi chimiche e altre misurazioni fondamentali In un precedente capitolo abbiamo visto che la replicazione è uno degli elementi fondamentali di un sperimento. Abbiamo anche visto che le \\(n\\) repliche effettuate sono solo un campione delle infinite misure che avremmo potuto fare, ma che non abbiamo fatto per insufficienza di risorse (usualmente tempo, spazio e lavoro). Di conseguenza, dopo aver terminato un esperimento ci troviamo con un collettivo di misure, che debbono essere riassunte e descritte. Se i dati sono quantitativi (su scala continua o discreta), è possibile e necessario descrive almeno due caratteristiche del dataset, vale a dire: tendenza centrale (location) dispersione (shape) Vediamo ora quali sono le statistiche più utilizzate per descrivere un campione. 5.1.1 Indicatori di tendenza centrale La media aritmetica è un concetto molto intuitivo che non necessita di particolari spiegazioni: si calcola con R mediante la funzione mean(vettore). In Excel, si calcola con la funzione “=MEDIA(intervallo)”. Per esempio, carichiamo il dataset ‘heights’ contenuto nel package ‘aomisc’ e calcoliamo la media delle altezze. library(aomisc) data(heights) mean(heights$height) ## [1] 164 Un altro indicatore di tendenza centrale è la mediana, data dal valore che bipartisce la distribuzione di frequenza in modo da lasciare lo stesso numero di termini a sinistra e a destra. Se abbiamo una serie di individui ordinati in graduatoria, la mediana è data dall’ individuo che occupa il posto (n + 1)/2 o, se gli individui sono in numero pari, dalla media delle due osservazioni centrali. Il comando per calcolare la mediana in R è median(vettore). In Excel, si utilizza la funzione “=MEDIANA(intervallo)”. La mediana è un indicatore più robusto della media: infatti, supponiamo di avere cinque valori: 1 - 4 - 7 - 9 - 10 La media è pari a 6.2, mentre la mediana è pari a 7 (valore centrale). Se cambiano il numero più alto in questo modo: 1 - 4 - 7 - 9 - 100 la media di questi cinque valori sarà 24.2, mentre la mediana sarà sempre pari a 7. Insomma, la mediana non è influenzata da valori estremi (outliers), in senso positivo o negativo. median(heights$height) ## [1] 162.5 5.1.2 Indicatori di variabilità Gli indicatori di tendenza centrale, da soli, non ci informano su come le unità sperimentali tendono a differire l’una dall’altra: ad esempio una media pari a 100 può essere ottenuta con tre individui che misurano 99, 100 e 101 rispettivamente o con tre individui che misurano 1, 100 e 199. E’ evidente che in questo secondo gruppo gli individui sono molto più differenti tra loro (dispersi) che nel primo gruppo. Pertanto, i risultati di un processo di misurazione non possono essere descritti solo con la media, ma è necessario anche calcolare un indice di variabilità. Tra essi, il più semplice è il campo di variazione, che è la differenza tra la misura più bassa e la misura più alta. In realtà, non si tratta di un vero e proprio indice di variabilità, in quanto dipende solo dai termini estremi della distribuzione e non necessariamente cresce al crescere della variabilità degli individui. Invece del campo di variazione, possiamo utilizzare i cosiddetti percentili, che bipartiscono la popolazione di partenza in modo da lasciare una certa quantità di termini alla sua sinistra e la restante quantità alla sua destra. Ad esempio, il primo percentile bipartisce la popolazione in modo da lasciare a sinistra l’ 1% dei termini e alla destra il restante 99%. Allo stesso modo l’ ottantesimo percentile bipartisce la popolazione in modo da lasciare a sinistra l’80% dei termini e alla destra il restante 20%. percentile &lt;- 0.8 curve(dnorm(x),from=-3,to=3,axes=FALSE, ylab=&quot;&quot;, xlab=&quot;&quot;) axis(1) lines(c(-3,3),c(0,0)) valori.rosso&lt;-seq(-3,qnorm(percentile),length=100) x.rosso&lt;-c(-3,valori.rosso,qnorm(percentile),-3) y.rosso&lt;-c(0,dnorm(valori.rosso),0,0) polygon(x.rosso,y.rosso,density=20,angle=45, col=&quot;red&quot;) valori.grigio&lt;-seq(qnorm(percentile),3,length=100) x.grigio&lt;-c(qnorm(percentile),valori.grigio,3,qnorm(percentile)) y.grigio&lt;-c(0,dnorm(valori.grigio),0,0) polygon(x.grigio,y.grigio,density=20,angle=45, col=&quot;grey&quot;) text(x=2,y=0.1,&quot;80° percentile&quot;, pos=4) arrows(2,0.1,qnorm(percentile),0) Per descrivere la variabilità di un collettivo è possibile utilizzare, ad esempio, il 25esimo e il 75esimo percentile: se questi sono molto vicini, significa che il 50 % dei soggetti è compreso in un intervallo piccolo e quindi la variabilità della popolazione è bassa. Per calcolare questi due valori con Excel possiamo utilizzare le funzioni “=PERCENTILE.INC(intervallo, 0.25)” e “=PERCENTILE.INC(intervallo, 0.75)”. Per quanto riguarda R, i comandi sono dati più sotto. quantile(heights$height, probs = c(0.25, 0.75)) ## 25% 75% ## 152.75 174.25 A questo proposito, possiamo introdurre il concetto di boxplot (grafico Box-Whisker). Si tratta di una scatola che ha per estremi il 25esimo e il 75esimo percentile ed è tagliata da una linea centrale in corrispondenza della mediana. Dalla scatola partono due linee verticali che identificano il valore massimo e il minimo. Se il massimo (o il minimo) distano dalla mediana più di 1.5 volte la differenza tra la mediana stessa e il 75esimo (o 25esimo) percentile, allora le linee verticali si fermano ad un valore pari ad 1.5 volte il 75esimo (o il 25esimo) percentile rispettivamente ed i dati esterni vengono raffigurati come outliers. I boxplot sono solitamente usati per descrivere campioni numerosi nei quali esista un qualche criterio di raggruppamento. In basso abbiamo create tre gruppi con una funzione di estrazione di numeri casuali. set.seed(1234) A &lt;- runif(20) B &lt;- runif(20) C &lt;- runif(20) series &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 20) values &lt;- c(A, B, C) boxplot(values ~ series) Con Excel, non esiste una funzione in grado di disegnare un boxplot automaticamente e si possono seguire le indicazioni a questo link. Oltre ad esprimere la variabilità di una popolazione con un intervallo (campo di variazione o coppia di percentili) è possibile utilizzare diversi indici sintetici di variabilità, tra cui i più diffusi sono la devianza, la varianza, la deviazione standard ed il coefficiente di variabilità. La devianza (generalmente nota come SS, cioè somma dei quadrati) è data da: \\[SS = \\sum\\limits_{i = 1}^n {(x_i - \\bar x)^2 }\\] Si tratta di un indicatore caratterizzato da significato geometrico molto preciso, collegabile alla somma dei quadrati delle distanze euclidee di ogni osservazione rispetto alla media. In R, non vi è una funzione per il calcolo della devianza (o meglio, esiste una possibilità nell’ambito dei modelli lineari, ma è troppo presto per introdurla…). Possiamo allora un’espressione del tipo: sum( (heights$height - mean(heights$height))^2 ) ## [1] 4050 Come misura di ’distanza’, la devianza ha alcune importanti proprietà (che vedremo meglio in seguito), ma essendo una somma, il valore finale dipende dal numero di scarti da sommare e quindi non è possibile operare confronti tra collettivi formati da un diverso numero di individui. Si può quindi definire un altro indice, detto varianza (nei software di uso più corrente si parla di varianza campionaria, e definito come segue: \\[\\sigma^2 = \\frac{SS}{n - 1}\\] La varianza permette di confrontare la variabilità di collettivi formati da un numero diverso di individui, anche se permane il problema che questo indicatore è espresso in un’unità di misura al quadrato, rispetto a quella delle osservazioni originali: ad esempio se le osservazioni sono espresse in metri, la varianza è espressa in metri quadrati. Per eliminare questo problema si ricorre alla radice quadrata della varianza, cioè la deviazione standard, che si indica con s. La deviazione standard è espressa nella stessa unità di misura dei dati originari ed è quindi molto informativa sulla banda di oscillazione dei dati rispetto alla media. Spesso la variabilità dei dati è in qualche modo proporzionale alla media: collettivi con una media alta hanno anche una variabilità alta e viceversa. Per questo motivo viene utilizzato spesso il coefficiente di variabilità: \\[CV = \\frac{\\sigma }{\\mu } \\times 100\\] che è un numero puro e non dipende dall’unità di misura e dall’ampiezza del collettivo, sicché è molto adatto ad esprimere ad esempio l’errore degli strumenti di misura e delle apparecchiature di analisi. Varianza e deviazione standard sono molto facili da calcolare in R, grazie alle funzioni var(), sd(). In Excel, abbiamo le funzioni “=DEV.Q(intervallo)”, “=VAR(intervallo)” e “=DEV.ST(intervallo)”, rispettivamentep er devianza, varianza e deviazione standard. In genere, la deviazione standard, per le sue caratteristiche, viene utilizzata come indicatore dell’incertezza assoluta associata ad una determinata misurazione, mentre il coefficiente di variabilità (incertezza relativa percentuale; CV), è molto adatto ad esprimere l’errore degli strumenti di misura e delle apparecchiature di analisi. var(heights$height) ## [1] 213.1579 sd(heights$height) ## [1] 14.59993 sd(heights$height)/mean(heights$height) * 100 ## [1] 8.902395 5.1.3 Arrotondamenti Il calcolo della media e della deviazione standard (sia a mano che con il computer) porta all’ottenimento di un numero elevato di cifre decimali. E’ quindi lecito chiedersi quante cifre riportare nel riferire i risultati della misura. L’indicazione generale, da prendere con le dovute cautele è che nel caso della media si riportano un numero di cifre decimali pari a quello rilevato nella misura, mentre per gli indicatori di variabilità si dovrebbe utilizzare un decimale in più. 5.2 Descrizione dei sottogruppi In biometria è molto comune che il gruppo di unità sperimentali sia divisibile in più sottogruppi, dei quali vogliamo conoscere alcune statistiche descrittive. Abbiamo già visto il boxplot; ora potremmo voler calcolare le medie per gruppo. Per questo, possiamo utilizzare la funzione ‘tapply()’: with(heights, tapply(height, var, mean) ) ## C N S V ## 165.00 164.00 160.00 165.25 dove height è la variabile che contiene i valori da mediare, var è la variabile che contiene la codifica di gruppo, mean è la funzione che dobbiamo calcolare. Ovviamente mean può essere sostituito da qualunque altra funzione ammissibile in R, come ad esempio la deviazione standard. Spesso vogliamo calcolare più di una funzione (ad esempio, la media e la deviazione standard). Per questo possiamo tilizzare il package ‘plyr’ e la funzione ‘ddplyr()’. library(plyr) descript &lt;- ddply(heights, ~var, summarise, Media = mean(height), SD = sd(height)) descript ## var Media SD ## 1 C 165.00 14.36431 ## 2 N 164.00 16.19877 ## 3 S 160.00 12.16553 ## 4 V 165.25 19.51709 Con la funzione soprastante abbia creato un nuovo dataset (descript), che può essere utilizzato per il plotting, ad esempio per creare un grafico a dispersione, con l’indicazione della dispersione dei dati. Per sapere le coordinate relative al centro di ogni barra, dobbiamo creare un oggetto con la funzione ‘barplot()’. Questa funzione, oltre che disegnare il grafico, restituisce appunto le coordinate necessarie. coord &lt;- barplot(descript$Media, names.arg = descript$var, ylim = c(0, 200)) arrows(coord, descript$Media - descript$SD, coord, descript$Media + descript$SD, length = 0.05, angle = 90, code = 3) Il grafico non è bellissimo; per ora ci accontenteremo, ma può essere migliorate con un po’ di esercizio. In Excel, è molto conveniente utilizzare la funzione “tabella pivot”. 5.3 Distribuzioni di frequenza e classamento Avendo a che fare con variabili qualitative, possiamo considerare la frequenza assoluta, cioè il numero degli individui che presentano una certa modalità. Ad esempio, se su 500 insetti 100 sono eterotteri, 200 sono imenotteri e 150 sono ortotteri, possiamo concludere che la frequenza assoluta degli eterotteri è pari a 100. Oltre alle frequenze assolute, possiamo considerare anche le frequenze relative, che si calcolano dividendo le frequenze assolute per il numero totale degli individui del collettivo. Nel caso prima accennato, la frequenza relativa degli eterotteri è pari a 100/500, cioè 0.2. Se abbiamo una variabile nella quale le modalità possono essere logicamente ordinate, oltre alle frequenze assolute e relative possiamo prendere in considerazione le cosiddette frequenze cumulate, che si ottengono cumulando i valori di tutte le classi di frequenza che precedono quella considerata. Le distribuzioni di frequenza possono essere costruite anche per le variabili quantitative, tramite un’operazione di classamento, che consiste nel creare classi con intervalli opportuni. Su queste distribuzioni di frequenza possiamo quindi calcolare frequenze assolute, relative e cumulate. In genere, se abbiamo un collettivo molto numeroso è conveniente aggregare i dati in forma di distribuzioni di frequenza, perché la lettura delle informazioni è molto più facile. Qui facciamo un esempio, anche se il dataset che utilizzeremo (‘heights’) non è così numeroso. Vogliamo: valutare la distribuzione delle frequenze assolute, relative e percentuali degli individui di ciascuna varietà; valutare la distribuzione delle frequenze assolute, relative, percentuali e cumulate dell’ altezza degli individui, considerando classi di ampiezza pari a 5 cm; disegnare la torta delle frequenze relative della varietà e l’istogramma delle frequenze assolute dell’altezza. La soluzione al punto 1 con R è facile, attraverso l’impiego della funzione table(). La funzione length() restituisce il numero di elementi in un vettore. #Frequenze assolute table(heights$var) ## ## C N S V ## 7 6 3 4 #Frequenze relative with(heights, table(var)/length(var) ) ## var ## C N S V ## 0.35 0.30 0.15 0.20 #Frequenze percentuali with(heights, table(var)/length(var) * 100 ) ## var ## C N S V ## 35 30 15 20 Per la variabile altezza, che è di tipo quantitativo, si utilizza lo stesso comando table(vettore), ma occorre specificare l’ampiezza delle classi di frequenza con la funzione cut() e l’argomento breaks(), con il quale vengono specificati gli estremi superiori della classe (inclusi per default nella classe stessa). Per le frequenze cumulate si usa invece la funzione cumsum(). freq &lt;- table(cut (heights$height, breaks = c(140,150,160,170,190,200))) freq ## ## (140,150] (150,160] (160,170] (170,190] (190,200] ## 4 5 4 6 1 Per disegnare i grafici si utilizzano le funzioni pie() e barplot(). pie(table(heights$var)) barplot(freq, col=&quot;blue&quot;) In Excel, l’operazione di classamento può essere effettuata utilizzando la formula “=FREQUENZA(matriceDati, matriceClassi)”, che tuttavia è una formula di matrice e quindi deve essere immessa in un intervallo e consolidata con la combinazione tasti “SHIFT+CTRL+INVIO”. 5.4 Statistiche descrittive per le distribuzioni di frequenza Il più semplice indicatore di tendenza centrale, utilizzabile con qualunque tipo di dati è la moda, cioè il valore della classe che presenta la maggior frequenza. Ovviamente, se la variabile è quantitativa, si assume come moda il punto centrale della classe con maggior frequenza. L’individuazione della moda è banale e non richiede calcoli di sorta. Nel caso di distribuzioni di frequenza per caratteri ordinabili (qualitativi e quantitativi), oltre alla moda possiamo calcolare la mediana e gli altri percentili. Oltre a questi, per le distribuzioni di frequenza dicaratteri quantitativi è anche possibile calcolare la media, come illustrato in precedenza, insiema tutti gli indicatori di variabilità già citati. 5.5 Distribuzioni di frequenza bivariate: le tabelle di contingenza In alcuni casi in ciascuna unità sperimentale del collettivo vengono studiati due (o più) caratteri e, di conseguenza, si ha a che fare con distribuzioni di frequenza bivariate (o multivariate). In questo caso si possono costruire delle tabelle di contingenza, cioè delle tabelle a due entrate nelle quali ogni numero rappresenta la frequenza congiunta (in genere assoluta) per una particolare combinazione delle due variabili. Ad esempio consideriamo le variabili Varietà (con i valori SANREMO e FANO) e ‘Forma delle bacche’ (con i valori LUNGO, TONDO, OVALE), riportati nella tabella di contingenza che creeremo come matrice. tabCon &lt;- matrix(c(37, 45, 32, 74, 61, 59), nrow = 2, ncol = 3, byrow = F) row.names(tabCon) &lt;- c(&quot;SANREMO&quot;, &quot;FANO&quot;) colnames(tabCon) &lt;- c(&quot;LUNGO&quot;, &quot;TONDO&quot;, &quot;OVALE&quot;) tabCon ## LUNGO TONDO OVALE ## SANREMO 37 32 61 ## FANO 45 74 59 Ogni riga della tabella sovrastante costituisce una distribuzione condizionata della forma del frutto, dato un certo valore della Varietà, mentre ogni colonna costituisce una distribuzione condizionata della Varietà, data una certa forma del frutto. 5.6 Connessione Se guardiamo le due distribuzioni condizionate per SANREMO e FANO possiamo notare che esiste una certa differenza. Potremmo chiederci quindi se il presentarsi di una data modalità del carattere Varietà (SANREMO o FANO) influenza il presentarsi di una particolare modalità del carattere Forma del frutto. Se ciò non è vero si parla di indipendenza delle variabili (allora le distribuzioni condizionate sono uguali) altrimenti si parla di dipendenza o connessione. In caso di indipendenza, le distribuzioni condizionate delle due variabili dovrebbero essere uguali tra loro, cioè la frequenza relativa condizionale di X per una data modalità di Y deve essere uguale alla frequenza relativa condizionale di X per l’altra modalità di Y e quindi alla frequenza marginale di X. Ad esempio, per il carattere LUNGO la frequenza relativa marginale è pari ad 82/308=0.266 (82 è la somma dei pomodori di forma allungata, mentre 308 è il numero totale dei pomodori); in caso di indipendenza, questa frequenza dovrebbe essere la stessa, indipendentemente dal fatto che il pomodoro sia di varietà SANREMO oppure Fano. In cifre, la frequenza assoluta condizionata per LUNGO|SANREMO dovrebbe essere pari a 0.266x130=34.6. mentre LUNGO|FANO dovrebbe essere pari a 0.266x178=47.4. Con questi principi, possiamo costruire la tabella delle frequenze assolute attese, in caso di indipendenza completa tra i due caratteri. expF &lt;- matrix(c(34.6, 47.4, 44.7, 61.3, 50.6, 69.4), nrow = 2, ncol = 3, byrow = F) row.names(expF) &lt;- c(&quot;SANREMO&quot;, &quot;FANO&quot;) colnames(expF) &lt;- c(&quot;LUNGO&quot;, &quot;TONDO&quot;, &quot;OVALE&quot;) A questo punto è logico costruire un indice statistico di connessione, detto \\(\\chi^2\\), che misuri lo scostamento tra le frequenze osservate e quelle attese nell’ipotesi di indipendenza perfetta: \\[\\chi ^2 = \\frac{{\\left( {f_o - f_a } \\right)^2 }}{{f_a }}\\] dove \\(f_o\\) sta per frequenza osservata ed \\(f_a\\) sta per frequenza attesa nel caso indipendenza. Questo indice assume valore pari a zero nel caso di indipendenza completa (le frequenze osservate sono uguali a quelle attese) ed assume un valore positivo tanto più alto quanto maggiore è la connessione tra i due caratteri, fino ad un valore massimo dato dal prodotto del numero degli individui per il valore minimo tra il numero di righe - 1 e il numero di colonne - 1: \\[\\max \\chi ^2 = n \\cdot \\min (r - 1,\\,c - 1)\\] Nel nostro caso, potremmo calcolare il chi quadro in questo modo: sum( ((tabCon - expF) ^ 2) /expF ) ## [1] 10.22348 Esiste anche un comando più semplice, che consiste nell’utilizzare la funzione as.table() per forzare la matrice dati in una tabella di contingenza ed applicare la funzione ‘summary()’. summary( as.table (tabCon)) ## Number of cases in table: 308 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 10.223, df = 2, p-value = 0.006027 Il valore massimo di chi quadro è pari a 308 e di conseguenza il valore osservato, espresso in relazione al valore massimo è pari a 10.22/308=0.033. Si può quindi concludere che la connessione tra i due caratteri è piuttosto debole. 5.7 Correlazione Se abbiamo a che fare con variabili quantitative, possiamo calcolare l’indice di connessione previa opportuna divisione in classi di frequenza delle due variabili in studio. Oltre a ciò, con variabili quantitative è possibile esplorare l’esistenza della cosidetta relazione di variazione congiunta, che si ha quando al variare di una variabile cambia anche il valore dell’altra. La variazione congiunta si quantifica tramite il coefficiente di correlazione costituito dal rapporto tra la codevianza (o somma dei prodotti) delle due variabili e il prodotto delle loro devianze. Il coefficiente di correlazione varia tra -1 e +1: un valore pari a +1 indica concordanza perfetta (tanto aumenta una variabile, tanto aumenta l’altra), mentre un valore pari a -1 indica discordanza perfetta (tanto aumenta una variabile tanto diminuisce l’altra). Un valore pari a 0 indica assenza di qualunque grado di variazione congiunta tra le due variabili (assenza di correlazione). Valori intermedi tra quelli anzidetti indicano correlazione positiva (se positivi) e negativa (se negativi). In R, per calcolare la correlazione tra due variabili si usa la funzione cor(). In Excel, abbiamo la funzione “=CORRELAZIONE(intervallo1, intervallo2)” Proviamo a considerare questo esempio: il contenuto di olio di 9 lotti di acheni di girasole è stato misurato con due metodi diversi ed è riportato più sotto. a &lt;- c(45, 47, 49, 51, 44, 37, 48, 44, 53) b &lt;- c(44, 44, 49, 53, 48, 34, 47, 46, 51) Valutare la correlazione tra i risultati dei due metodi di analisi. cor(a, b) ## [1] 0.8960795 Possiamo osservare che il coefficiente di correlazione è abbastanza vicino ad 1 e quindi possiamo concludere che esiste un buon grado di concordanza tra i due metodi di analisi. "],
["dalla-popolazione-al-campione-i-modelli-stocastici.html", "Capitolo 6 Dalla popolazione al campione. I modelli stocastici 6.1 Introduzione 6.2 Popolazione di soggetti e popolazione di misure 6.3 La scienza del caso 6.4 Modelli probabilistici (stocastici) 6.5 La distribuzione normale (curva di Gauss) 6.6 La distribuzione t di Student 6.7 La distribuzione F di Fisher 6.8 Modelli stocastici per eventi discreti: la distribuzione binomiale 6.9 ltri modelli stocastici di interesse per lo sperimentatore 6.10 E allora? 6.11 Le simulazioni Monte Carlo", " Capitolo 6 Dalla popolazione al campione. I modelli stocastici 6.1 Introduzione Dietro ad ogni esperimento vi è una popolazione di soggetti misurabili che non può essere considerata nella sua interezza, in quanto non ne abbiamo le possibilità. Ad esempio, se vogliamo misurare la produzione del frumento nella media Valle del Tevere, potremmo impiegare un numero finito, ma elevatissimo, di parcelle di terreno. Per questo, siamo costretti a scegliere casualmente un numero piccolo di parcelle rappresentative e a misurare quelle, in modo da poter ottenere risultati validi reppresentativi dell’intera popolazione, con un processo detto ‘inferenza statistica’. Proviamo, per un attimo, a porre la nostra attenzione non sul campione, ma sulla popolazione da cui esso deriva. Quali sono le sue caratteristiche? La risposta non è banale, proprio perché non conosciamo (e non potremo mai conoscere) tutti i soggetti che compongono la popolazione stessa. Possiamo però fare alcune considerazioni legate alla nostra esperienza professionale, che risultino condivisibili dall’intera comunità scientifica. Queste considerazioni costituiranno la base dell’inferenza statistica e, dato che stiamo facendo congetture sulle caratteristiche di una popolazione (o meglio sui suoi ‘parametri’), parleremo di ASSUNZIONE PARAMETRICA. Insomma, per fare inferenza abbiamo bisogno di condividere il punto di partenza, cioè le caratteristiche della popolazione da cui abbiamo estratto il campione. 6.2 Popolazione di soggetti e popolazione di misure Abbiamo detto che la popolazione è fatta di soggetti, come, ad esempio, le parcelle sperimentali della Media Valle del Tevere. Chiariamo subito che a noi non interessano i soggetti in quanto tali, ma interessano le misure che su di essi immaginiamo di aver preso. Avremo quindi in mano una popolazione di misure. Come saranno queste misure? Possiamo facilmente immaginare che saranno una diversa dall’altra, perchè gli individui sono uno diverso dall’altro. Le differenze saranno dovute ad elementi deterministici, che possiamo individuare, come sesso, età, specie, metodo di coltivazione o trattamento. Lasciamo per un’attimo da parte tutte le differenze che possiamo spiegare (ne riparleremo tra un po’…). Immaginiamo anche che non vi siano differenze ‘sistematiche’, legate a fattori che non siamo in grado di individuare, ad esempio relative ad un’errata taratura dello strumento di misura. Rimangono quindi solo le differenze che puramente stocastiche. Ci chiediamo: è possibile scrivere un modello matematico (un’equazione) che descriva qualcosa che è puramente stocastico? 6.3 La scienza del caso Possiamo farlo, perché esistono i cosiddetti modelli stocastici, niente altro che funzioni probabilità. E’ evidente che, se parliamo di produzione del frumento, a parità di varietà, concimazione, clima, e ogni altro elemento controllabile e spiegabile, le differenze di produzione sono casuali, ma si può dire che valori pari a 1.5 t/ha o 9.5 t/ha sono altamente improbabili, mentre valori di 5.5 e 6.5 t/ha sono decisamente più ragionevoli. Insomma, noi non siamo in grado di prevedere la produzione del frumento, ma potremmo provare a calcolare la probabilità dei diversi outcomes \\(Y\\). A questo punto sarebbe utile avere qualche nozione di calcolo di probabilità e calcolo combinatorio, poter enumerare l’elenco dei casi possibili ed assegnare le probabilità a quelli favorevoli. Tuttavia, in biostatistica facciamo riferimento a modelli probabilistici semplici e ben consolidati e quindi proviamo ad andare avanti, trascurando lo studio formale del calcolo di probabilità. 6.4 Modelli probabilistici (stocastici) Torniamo alla nostra popolazione di misure, che sono influenzate solo dal caso. Possiamo scrivere: \\[Y_i \\sim \\phi(\\Theta)\\] cioè la misura \\(Y\\) presa sul soggetto \\(i\\) appartenente alla popolazione segue una distribuzione di probabilità \\(\\phi\\) caratterizzata da uno o più parametri \\(\\Theta\\). Per capire quale modello probabilistico segue la nostra popolazione dobbiamo chiederci come è la misura. Esprime una quantità o una qualità? Se si tratta di una qualità, le diverse modalità sono nominali od ordinali? 6.4.1 Funzioni di probabilità La differenza è importante. Se abbiamo rilevato una variabile nominale, come il sesso (M/F), l’efficacia (vivo/morto), la germinabilità (germinato/non germinato), possiamo effettivamente calcolare le probabilità come rapporto tra i casi favorevoli e quelli possibili (probabilità ‘frequentista’). Per questo si parla di funzioni di probabilità o distribuzioni di probabilità. Immaginiamo un mazzo di carte con i quattro assi, tre due, due tre e un quattro (10 carte) e definiamo il seguente modello probabilistico: \\[ P(x) = \\left\\{ \\begin{array}{l} 0.4\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x = 1 \\\\ 0.3\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x = 2 \\\\ 0.2\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x = 3 \\\\ 0.1\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x = 4 \\\\ \\end{array} \\right. \\] Notiamo che un modello probabilistico ha due caratteristiche importanti: P(x) è sempre non-negativo (ovvio! le probabilità sono solo positive o uguali a 0); la somma delle probabilità di tutti gli eventi è sempre pari ad 1 (ovvio anche questo: la probabilità che capiti uno qualunque degli eventi è sempre 1). Se gli eventi possibili sono ordinabili (come nel caso precedente), oltre alla funzione di probabilità, si può definire anche la funzione di probabilità cumulata, detta anche funzione di ripartizione con la quale si assegna ad ogni evento la sua probabilità più quella di tutti gli eventi ‘inferiori’. Nell’esempio precedente: \\[ P(x) = \\left\\{ \\begin{array}{l} 0.4\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x \\leq 1 \\\\ 0.7\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x \\leq 2 \\\\ 0.9\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x \\leq 3 \\\\ 1.0\\,\\,\\,\\,\\,\\,se\\,\\,\\,\\,\\,\\,x \\leq 4 \\\\ \\end{array} \\right. \\] In questo caso abbiamo quattro categorie (asso, due, tre e quattro) che sono rappresentate da valori (discreti, ma comunque caratterizzati da relazioni di ordine e distanza). Per una distribuzione di probabilità come questa possiamo calcolare la media (valore atteso) come: \\[ \\mu = E(X) = \\Sigma x_i \\cdot P(X = x_i ) \\] e la varianza come: \\[\\sigma ^2 = Var(X) = E\\left[ {X - E(X)} \\right]^2 = \\Sigma \\left[ {\\left( {x_i - \\mu } \\right)^2 \\cdot P(X = x_i )} \\right]\\] In questo caso specifico, la media è pari a: 1 * 0.4 + 2 * 0.3 + 3 * 0.2 + 4 * 0.1 ## [1] 2 e la varianza è pari a: (1 - 2)^2 * 0.4 + (2 - 2)^2 * 0.3 + (3 - 2)^2 * 0.2 + (4 - 2)^2 * 0.1 ## [1] 1 6.4.2 Funzioni di densità Il discorso cambia se abbiamo a che fare con fenomeni biologici descrivibili su una scala continua: ad esempio l’altezza, la produzione, il peso della biomassa, ecc. In questo caso gli eventi che possono capitare sono infiniti ed è facile intuire che non ha molto senso chiedersi, ad esempio, qual è la probabilità di trovare un individuo con un altezza esattamente pari a 165.00, in quanto capiamo che questa è infinitesima. Al contrario, possiamo immaginare di calcolare la probabilità di ottenere un intervallo di valori, per esempio da 165 a 166. Ovviamente la probabilità di un intervallo dipende sempre dalla sua ampiezza, il che introduce un elemento di scelta arbitraria. Possiamo tuttavia pensare di calcolare la densità di probabilità, vale a dire il rapporto tra la probabilità di un intervallo e la sua ampiezza (cioè la probabilità per unità di ampiezza dell’intervallo; per questo si parla di densità). E’ evidente che se un intervallo diventa infinitamente piccolo anche la probabilità tende a zero con la stessa ‘velocità’, in modo che la densità di probabilità tende ad un numero finito. Insomma, con i fenomeni continui non possiamo lavorare con la probabilità dei singoli eventi, ma possiamo lavorare con la densità di probabilità e definire quindi apposite funzioni di densità. Analogamente alle funzioni di probabilità, le funzioni di densità debbono avere due caratteristiche: assumere solo valori non-negativi; la somma delle probabilità di tutti gli eventi possibili, calcolabile come integrale della funzione di densità, deve essere unitaria (anche in questo caso la probabilità di ottenere uno qualunque degli eventi possibili è pari ad 1). Data una funzione di densità, possiamo costruire la corrispondente funzione di probabilità cumulata, facendo l’integrale per ogni evento pari o inferiore a quello dato. Più in generale, per variabili continue sia la funzione di ripartizione (probabilità cumulata), che la media o la devianza sono definite ricorrendo agli integrali: \\[\\begin{array}{l} P(X) = f(x) \\\\ P(X \\le x) = \\int\\limits_{ - \\infty }^x {f(x)} dx \\\\ \\mu = E(X) = \\int\\limits_{ - \\infty }^{ + \\infty } {xf(x)} dx \\\\ \\sigma ^2 = Var(X) = \\int\\limits_{ - \\infty }^{ + \\infty } {\\left( {x - \\mu } \\right)^2 f(x)} dx \\\\ \\end{array}\\] In pratica, vedremo che, a seconda della funzione di densità, è possibile adottare formule semplificate per le diverse statistiche descrittive. 6.5 La distribuzione normale (curva di Gauss) Torniamo ancora alla nostra popolazione di misure, relative alle produzioni di frumento nella media Valle del Tevere. E’ ragionevole pensare che, effettuando le misurazioni con uno strumento sufficientemente sensibile e in presenza delle sole variazioni casuali (visto che abbiamo idealmente rimosso ogni differenza sistematica spiegabile), i risultati tendono a differire tra di loro, muovendosi intorno ad un valore medio, rispetto al quale le misure superiori ed inferiori sono equiprobabili e tendono ad essere più rare, via via che ci si allontana dal valore medio. Questo ragionamento ci porta verso una densità di frequenza (parliamo di variabili continue) a forma di campana, che potrebbe essere descritta con una funzione continua detta curva di Gauss. La curva è descritta dalla seguente funzione di densità: \\[P(x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}\\exp \\left( {\\frac{{\\left( {x - \\mu } \\right)^2 }}{{2\\sigma ^2 }}} \\right)\\] ove \\(P(x)\\) è la densità di probabilità di una certa misura \\(x\\), mentre \\(\\mu\\) e \\(\\sigma\\) sono rispettivamente la media e la deviazione standard della popolazione (per la dimostrazione si rimanda a testi specializzati). Le variabili casuali che possono essere descritte con la curva di Gauss, prendono il nome di variabili normali. Studiare le principali proprietà matematiche della curva di Gauss è estremamente utile, perché, se supponiamo che essa possa descrivere la gran parte dei fenomeni biologici naturali, possiamo estendere le caratteristiche della curva e all’andamento del fenomeno in studio. Ad esempio, senza voler entrare troppo in dettaglio, il semplice esame grafico della curva di Gauss consente le seguenti osservazioni: La forma della curva dipende da solo da \\(\\mu\\) e \\(\\sigma\\) (figure e ). Ciò significa che, se prendiamo un gruppo di individui e partiamo dal presupposto (assunzione parametrica) che in relazione ad un determinato carattere quantitativo (es. altezza) la distribuzione di frequenza è normale (e quindi può essere descritta con una curva di GAUSS), allora basta conoscere la media e la deviazione standard degli individui e immediatamente conosciamo l’intera distribuzione di frequenza; la curva ha due asintoti e tende a 0 quando x tende a infinito. Questo ci dice che se assumiamo che un fenomeno è descrivibile con una curva di Gauss, allora assumiamo che tutte le misure sono possibili, anche se la loro frequenza decresce man mano che ci si allontana dalla media; la probabilità che la x assuma valori compresi in un certo intervallo è data dall’integrale della curva di Gauss in quell’intervallo; Se la curva di Gauss è stata costruita utilizzando le frequenze relative, l’integrale della funzione è uguale ad 1. Infatti la somma delle frequenze relative di tutte le varianti possibili non può che essere uguale ad 1; la curva è simmetrica. Questo indica che la frequenza dei valori superiori alla media è esattamente uguale alla frequenza dei valori inferiori alla media. dato \\(\\sigma\\), possiamo dire che la frequenza dei valori superiori a \\(\\mu + \\sigma\\) è pari al 15.87% ed è uguale alla frequenza dei valori inferiori a \\(\\mu - \\sigma\\). A questo punto, sempre in relazione alle nostre parcelle di frumento, possiamo scrivere: \\[ Y_i \\sim N(\\mu, \\sigma) \\] cioè possiamo scrivere che seguono una distribuzione di densità normale, con media \\(\\mu\\) e deviazione standard \\(\\sigma\\). A questo punto possiamo utilizzare la funzione di densità normale di R per fare alcune operazioni importanti. 6.5.1 ESERCIZIO 1 Calcolare la densità di una produzione pari a 5.5 t/ha, proveniente da una distribuzione normale con media 6 e deviazione standard pari a 0.6. dnorm(5.5, mean = 6, sd = 0.6) ## [1] 0.4698531 6.5.2 ESERCIZIO 2 Disegnare due distribuzioni normali, con la stessa media e diversa deviazione standard. Disegnare in un box a fianco due distribuzioni normali con la stessa deviazione standard e diversa media. par(mfrow = c(1,2)) curve(dnorm(x, 3, 1), from=0, to=6, ylab=&quot;Density&quot;) curve(dnorm(x, 3, 2), add=T, col=&quot;red&quot;) curve(dnorm(x, 3, 1), from=0, to=6, ylab=&quot;Density&quot;) curve(dnorm(x, 4, 1), add=T, col=&quot;red&quot;) Il primo esercizio non è molto interessante, perché, come abbiamo detto, ci fornisce la densità e non la probabilità. Possiamo però utilizzare la funzione di probabilità cumulata per calcolare le probabilità. 6.5.3 ESERCIZIO 3 Qual è la probabilità che, da un pozzo con un contenuto medio di cloro pari a 1 meq \\(l^{-1}\\), eseguendo l’analisi con uno strumento caratterizzato da una deviazione standard pari a 0.04, io ottengo un valore pari o superiore a 1.1 \\(meq l^{-1}\\)? pnorm(1.1, mean=1, sd=4*1/100, lower.tail=FALSE) ## [1] 0.006209665 Si utilizza l’argomento lower.tail=FALSE, in quanto stiamo cercando la probabilità di una concentrazione pari o superiore ad 1.1, e non pari od inferiore. Sempre utilizzando metodi numerici è possibile calcolare i quantili per una distribuzione normale, noti che siano \\(\\mu\\) e \\(\\sigma\\). 6.5.4 ESERCIZIO 4 Nello stesso strumento sopra indicato e considerando lo stesso tipo di analisi, calcolare: la probabilità di ottenere una misura inferiore a 0.75 la probabilità di ottenere una misura superiore a 1.5 la probabilità di ottenere una misura compresa tra 0.95 e 1.05 Stabilire inoltre: la misura che è superiore al 90% di quelle possibili la misura che è inferiore al 70% di quelle possibili le misure entro le quali si trova il 95% delle misure possibili La soluzione è banale secondo lo schema seguente. pnorm(0.75, 1,4*1/100) ## [1] 2.052263e-10 pnorm(1.5, 1, 4*1/100, lower.tail=FALSE) ## [1] 3.732564e-36 pnorm(1.05, 1, 4*1/100) - pnorm(0.95, 1, 4*1/100) ## [1] 0.7887005 #Quantile function qnorm(0.9, 1, 0.04) ## [1] 1.051262 qnorm(0.7, 1, 0.04, lower.tail=FALSE) ## [1] 0.979024 qnorm(0.975, 1, 0.04) ## [1] 1.078399 qnorm(0.025, 1, 0.04) ## [1] 0.9216014 6.6 La distribuzione t di Student La distribuzione t di Student è analoga per forma ad una distribuzione normale con media 0 e deviazione standard 1. Rispetto a questa è un po’ più dispersa, nel senso che aumenta leggermente la probabilità di avere valori lontani dalla media. In realtà, non esiste una sola distribuzione t di Student, ma ne esistono molte, caratterizzate da un diverso numero di gradi di libertà (\\(\\nu\\)); più questo è basso, più aumenta la sovradispersione. Se il numero di gradi di libertà è infinito, la distribuzione t di Student è identica alla normale standardizzata. 6.6.1 ESERCIZIO 5 Disegnare su un grafico una curva normale standardizzata ed una serie di curve di t, con 2, 6 e 24 gradi di libertà. par(mfrow = c(1, 1)) curve(dnorm(x),-3, +3, col=&quot;Black&quot;, xlab=&quot;&quot;, ylab=&quot;Densità&quot;) curve(dt(x, 2), add=TRUE, col = &quot;blue&quot;) curve(dt(x,6), add=TRUE, col = &quot;red&quot;) curve(dt(x,24), add=TRUE, col = &quot;green&quot;) 6.7 La distribuzione F di Fisher La distribuzione F di Fisher è definita solo per valori positivi ed ha una forma fortemente asimmetrica. Anche in questo caso si tratta di una famiglia di distribuzioni che differiscono tra di loro per due parametri (gradi di libertà) \\(\\nu_1\\) e \\(\\nu_2\\). 6.7.1 ESERCIZIO 6 Disegnare la curva di F con \\(\\nu_1 = \\nu_2 = 3\\). Calcolare la probabilità di estrarre da questa distribuzione un valore pari o superiore a 5. Calcolare il 95° percentile. curve(df(x, 3, 3), 0, +3,col=&quot;Black&quot;, xlab=&quot;&quot;, ylab=&quot;Densità&quot;) pf(5, 3, 3, lower.tail = F) ## [1] 0.109551 qf(0.95, 3, 3) ## [1] 9.276628 6.8 Modelli stocastici per eventi discreti: la distribuzione binomiale Ogni esperimento che consiste in un insieme di prove indipendenti ripetute, per ognuna delle quali abbiamo solo due esiti possibili (successo ed insuccesso), con una probabilità di successo costante, viene detto esperimento Bernoulliano. Nell’ambito di questi esperimenti, spesso siamo interessati a conoscere la probabilità di ottenere k successi su n prove, che può essere descritta attraverso la variabile casuale binomiale. Poniamo di sapere che in una Facoltà di Agraria con un numero molto elevato di studenti il rapporto tra maschi e femmine sia pari a 0.7 e quindi che la probabilità di incontrare un maschio sia pari a P = 0.7 (evento semplice). Deve essere estratto a sorte un viaggio studio per quattro studenti e, per una questione di pari opportunità, si preferirebbe che fossero premiati in ugual misura maschi e femmine (cioè si vogliono premiare due femmine). Qual è la probabilità che un simile evento si realizzi? La probabilità cercata si può ottenere pensando che abbiamo un evento “estrazione” che può dare due risultati possibili (maschio o femmina) e che deve essere ripetuto quattro volte. Se consideriamo “successo” estrarre una femmina, allora la probabilità di successo in ogni estrazione è p=0.3 mentre quella di insuccesso (evento complementare) è pari a \\(1 - p = q = 0.7\\); ATTENZIONE!!!!!! ciò è vero se la popolazione è sufficientemente numerosa da pensare che la singola estrazione non cambia la probabilità degli eventi nelle successive (eventi indipendenti). La probabilità che su quattro estrazioni si abbiano 2 successi (evento “femmina”) e due insuccessi (evento “maschio”) è data da (teorema della probabilità composta): \\[0.3 \\cdot 0.3 \\cdot 0.7 \\cdot 0.7 = 0.3^2 \\cdot 0.7^2\\] In generale, data una popolazione molto numerosa, nella quale gli individui si presentano con due modalità possibili (in questo caso maschio e femmina) e posto di sapere che la frequenza con cui si presenta la prima modalità è pari a \\(p\\) (in questo caso la frequenza delle femmine è pari a 0.3), mentre la frequenza della seconda modalità è pari a \\(q = 1 - p\\), se vogliamo estrarre da questa popolazione \\(n\\) elementi, la probabilità che \\(k\\) di questi presentino la prima modalità (successo) è data da: \\[p^k \\cdot q^{(n-k)}\\] La formula di cui sopra, tuttavia, non risolve il nostro problema, in quanto noi vogliamo che vengano estratte due femmine, indipendentemente dall’ordine con cui esse vengono estratte (prima, seconda, terza o quarta estrazione), mentre la probabilità che abbiamo appena calcolato è quella relativa all’evento in cui le due femmine sono estratte al primo e secondo posto. Di conseguenza (teorema della probabilità totale) alla probabilità dell’evento indicato in precedenza (estrazione di due femmine in prima e seconda posizione) dobbiamo sommare la probabilità di tutti gli altri eventi utili (due femmine in seconda e terza posizione, oppure in terza e seconda, oppure in terza e quarta e così via). Il numero delle combinazioni possibili per 2 femmine in quattro estrazioni (combinazione di 4 elementi di classe 2) è dato dal coefficiente binomiale: \\[\\left( {\\begin{array}{*{20}c} n \\\\ k \\\\ \\end{array}} \\right) = \\frac{n!}{(n - k)!k!}\\] Moltiplicando le due equazioni date in precedenza otteniamo l’equazione della distribuzione binomiale: \\[P(X = x_i ) = \\frac{{n!}}{{(n - k)!k!}} \\cdot p^k \\cdot q^{(n - k)} \\] Nel caso specifico otteniamo il risultato: \\[P(X = 2) = \\frac{4!}{(4 - 2)!2!} \\cdot 0.3^2 \\cdot 0.7^{(4 - 2)} = 0.2646 \\] che è appunto la probabilità cercata. In R, utilizziamo la funzione per calcolare le probabilità della distribuzione binomiale, ogni volta in cui vogliamo sapere la probabilità di ottenere \\(k\\) successi in \\(n\\) prove: dbinom(2, 4, 0.3) ## [1] 0.2646 La funzione binomiale è un modello stocastico e si può dimostrare che il valore atteso (media) è uguale ad \\(n\\cdot p\\), mentre la varianza è pari a \\(n\\cdot p \\cdot q\\): La funzione di ripartizione (probabilità cumulata) si calcola in R con la funzione . Nell’esempio, se vogliamo sapere la probabilità totale di estrarre meno di tre femmine (&lt;= 2 femmine), possiamo operare in questo modo: pbinom(2,4,0.3) ## [1] 0.9163 Che risulta anche dalla somma della probabilità di estrarre 0, 1, 2 femmine: zero &lt;- dbinom(0,4,0.3) uno &lt;- dbinom(1,4,0.3) due &lt;- dbinom(2,4,0.3) zero+uno+due ## [1] 0.9163 La funzione di ripartizione può anche essere utilizzata al contrario, per determinare i quantili, cioè il numero di successi che corrispondono ad una probabilità cumulata pari ad alfa: qbinom(0.9163,4,0.3) ## [1] 2 6.8.1 ESERCIZIO 7 Da una popolazione di insetti che ha un rapporto tra maschi e femmine pari a 0.5, qual è la probabilità di campionare casualmente 2 maschi e 8 femmine? dbinom(2, 10, 0.5) ## [1] 0.04394531 6.8.2 ESERCIZIO 8 Riportare su un grafico la funzione di ripartizione binomiale, per p=0.5 e n=5. Costruire anche la densità di frequenza, utilizzando le opportune funzioni R. prob &lt;- 0.5 n &lt;- 5 barplot(dbinom(seq(0, n, by=1), size=n, prob=prob), main=&quot;Distribuzione binomiale per p=0.5&quot;, xlab=&quot;Successi&quot;, ylab=&quot;Probabilità&quot;, names.arg=seq(0,5)) barplot(pbinom(seq(0, n, by=1), size=n, prob=prob), main=&quot;Distribuzione binomiale per p=0.5&quot;, xlab=&quot;Successi&quot;, ylab=&quot;Probabilità&quot;, names.arg=seq(0,5)) 6.9 ltri modelli stocastici di interesse per lo sperimentatore Oltre a quelli accennati, esistono molti altri modelli stocastici, sia per eventi continui che discreti. Menzioniamo solamente la distribuzione \\(\\chi^2\\), la distribuzione ipergeometrica e quella esponenziale negativa. Molte di queste funzioni sono disponibili in R e possono essere utilizzate con una sintassi simile a quella sopra esposta, per la distribuzione normale. 6.10 E allora? Cerchiamo di ricapitolare. Le popolazioni di misure che sottendono i nostri rilievi sono un oggetto largamente ignoto. Tuttavia è ragionevole supporre che esse seguano una qualche funzione di probabilità/densità (assunzione parametrica). Se le nostre supposizioni sono corrette, possiamo utilizzare gli integrali di queste funzioni di probabilità/densità per calcolare la probabilità di ottenere una certa misura. Oppure possiamo capire quanto è probabile un certo campione o, viceversa, qual è quella misura che è più alta/basse del 95% delle misure e così via. Vedremo che questo modo di procedere ha un’importanza fondamentale nel processo di inferenza. 6.11 Le simulazioni Monte Carlo Sempre se le nostre supposizioni sono corrette, possiamo immaginare di utilizzare un generatore di numeri casuali per simulare i risultati di un esperimento. Infatti, immaginiamo di avere disegnato un esperimento con otto parcelle (repliche) per determinare la famosa produzione del frumento nella Media Valle del Tevere. Se queste parcelle sono scelte casualmente e non ci sono differenze di nessun altro tipo, si tratta di un campione rappresentativo della popolazione di parcelle di quella zona geografica. Se la popolazione è distribuita normalmente, con media pari a 5 t/ha e deviazione standard pari ad 1.2 t/ha, allora possiamo simulare i risultati del nostro esperimento come segue: set.seed(1234) Y &lt;- rnorm(4, 5, 1.2) Y ## [1] 5.515836 4.473487 3.569976 3.706473 mean(Y) ## [1] 4.316443 sd(Y) ## [1] 0.8930255 Il comando ‘set.seed(1234)’ serve per evitare che, nonostante si tratti di una estrazione casuale, voi possiate ottenere gli stessi risultati che ho ottenuto io. Vediamo che il campione non riflette le caratteristiche della popolazione, nel senso che la sua media e la sua deviazione standard differiscono da quelle della popolazione. E’ esattamente quello che capita durante un esperimento! Allo stesso modo possiamo immaginare di estrarre 20 insetti a caso da una popolazione in cui il rapporto tra i sessi è 1. Questo esperimento può essere simulato con: Y &lt;- rbinom(1, size = 20, prob = 0.5) Y ## [1] 10 Il computer ci restituisce il numero delle ‘vittorie’ che potrebbe essere il numero delle ‘femmine’ (ad esempio). faremo largo uso delle simulazioni di Monte Carlo nel capitolo seguente. "],
["modellizzare-lerrore-sperimentale-introduzione-allinferenza-statistica.html", "Capitolo 7 Modellizzare l’errore sperimentale: introduzione all’inferenza statistica 7.1 Introduzione 7.2 L’analisi dei dati: gli ’ingredienti’ fondamentali 7.3 Modello della realtà e ’sampling space’ 7.4 Esempio 1 7.5 Esempio 2 7.6 Riepilogo 1: Caratterizzare l’incertezza di un esperimento 7.7 Gli intervalli di confidenza 7.8 Gli intervalli di confidenza con Excel 7.9 Qual è il senso dell’intervallo di confidenza? 7.10 Analisi statistica dei dati: riassunto del percorso logico 7.11 Presentazione dei risultati degli esperimenti 7.12 Da ricordare 7.13 Esercizi", " Capitolo 7 Modellizzare l’errore sperimentale: introduzione all’inferenza statistica 7.1 Introduzione L’attività sperimentale in ambito scientifico/sociale e, in generale, nelle scienze della vita è condizionata dalla presenza di una componente stocastica, imprevedibile, che va sotto il nome generico di ’errore sperimentale’. Uno dei più influenziali scienziati che si sono confrontati con questo problema è Ronald Fisher (1890-1962), che nel suo famoso testo “Il disegno degli esperimenti” (1935, con seconda edizione nel 1937) ha posto le basi per una corretta metodica sperimentale, volta a minimizzare l’errore sperimentale e, soprattutto, ad impedire che gli effetti da esso prodotti potessero confondersi con gli effetti dei fenomeni scientifici in studio. Era tuttavia evidente, già in quel testo, come un accurato disegno sperimentale è un presupposto necessario, ma non sufficiente per ottenere prove scientifiche attendibili. Infatti, i fenomeni scientifici sono sempre estremamente complessi e sfaccettati e manifestano i loro effetti su un universo di soggetti e situazioni sperimentali altamente variabili, che possono essere studiate solo attraverso un processo di campionamento. In questo modo, dall’universo di varianti possibili, posso ottenere un campione di dimensione sufficientemente piccola da poter essere studiato agevolmente. Anche se il campione è effettivamente rappresentativo, rimane il fatto che esso rappresenta il risultato di uno solo degli infiniti sforzi di campionamento possibili, con l’importante conseguenza che la ripetizione di un esperimento porta sempre a risultati più o meno diversi, perché diversi sono i soggetti e, spesso, anche le condizioni in cui l’esperimento viene eseguito. Insomma, abbiamo studiato un campione di soggetti sperimentali, ma il nostro interesse è fondamentalmente rivolto verso la popolazione che ha generato il campione. Ci si deve allora chiedere quale sia la relazione tra l’esperimento da noi eseguito e la realtà scientifica o meglio quale sia la relazione tra le caratteristiche del campione e quelle della popolazione da cui esso è estratto. Questo processo logico prende il nome di inferenza statistica e può essere condotto secondo le teorie di Karl Pearson (1857-1936), Egon Pearson (suo figlio: 1895-1980) e Jarzy Neyman (1894-1981), oltre a Ronald Fisher. 7.2 L’analisi dei dati: gli ’ingredienti’ fondamentali Nell’analisi dei dati si segue in genere un percorso logico che può essere così sintetizzato: PRESUPPOSTO: I fenomeni biologici seguono una legge di natura (verità ’vera’), che ne costituisce il meccanismo fondamentale. Quando si organizza un esperimento, i soggetti sperimentali obbediscono a questo meccanismo di fondo, al quale tuttavia si sovrappongono molto altri elementi di ’confusione’, altamente incontrollabili, che vanno sotto il nome di errore sperimentale. L’osservazione sperimentale è quindi un’immagine confusa della verità vera e, soprattutto, l’osservazione sperimentale tende ad essere diversa per ogni sforzo di campionamento. Compito del ricercatore è quello di separare l’informazione (che rappresenta la verità ’vera’) dal ’rumore di fondo’ provocato dall’errore sperimentale. Questo dualismo tra verità ’vera’ (inconoscibile) e verità sperimentale (esplorabile tramite un esperimento opportunamente pianificato) è l’aspetto centrale di tutta la biometria ed è schematizzato nella figura seguente. Osservazioni sperimentali e meccanismi perturbativi Osservazioni sperimentali e meccanismi perturbativi In semplici termini algebrici, la questione può essere illustrata in questo modo: \\[Y_t = f(X)\\] La verità ’vera’ (\\(Y_T\\)) segue un modello deterministico (relazione causa-effetto), per il quale essa è funzione dello stimolo. Quando impostiamo un esperimento per investigare questa relazione causale, l’errore sperimentale non permette di osservare esattamente \\(Y_T\\), ma ci fornisce invece \\(Y_o\\), il cui valore cambia quando ripetiamo l’esperimento. Di conseguenza possiamo utilizzare un modello stocastico \\(\\Phi\\) per assegnare valori di probabilità ad ogni ’outcome’ \\(Y_o\\): \\[Y_o \\sim \\Phi(Y_T)\\] A questo punto \\(Y_T\\) diviene il valore atteso del modello stocastico, cioè l’output che tende a realizzarsi nel lungo periodo. 7.3 Modello della realtà e ’sampling space’ Nel percorso logico precedentemente indicato ci sono due aspetti fondamentali che debbono essere attentamente valutati: modello di generazione dei dati sperimentali; sampling distribution (o sample space). Chiariamo i due concetti con un esempio. 7.4 Esempio 1 7.4.0.1 Background Immaginiamo di avere 4’000’000 di semi ben mischiati (in modo che non ci siano raggruppamenti non casuali di qualche tipo), che costituiscono la nostra popolazione di partenza. 7.4.0.2 Obiettivo Vogliamo appurare la frequenza relativa (p) dei semi dormienti. Questa informazione, nella realtà, esiste (\\(\\pi\\) = 0.25), ma non è nota. 7.4.0.3 Materiali e metodi Dato l’elevato numero di ’soggetti’, non possiamo saggiare la germinabilità di tutti i semi, ma dobbiamo necessariamente prelevare un campione casuale di 40 soggetti; ogni seme viene saggiato e, dato che la popolazione è molto numerosa, l’estrazione di un seme non modifica sensibilmente la proporzione di quelli dormienti nella popolazione (esperimenti indipendenti). 7.4.0.4 Risultati Il risultato dell’esperimento è: nove semi dormienti su 40 (9 successi su 40 estrazioni). 7.4.0.5 Analisi dei dati Dopo aver descritto la popolazione e l’esperimento, ci chiediamo quale sia il modello matematico che genera i nostri dati (numero di successi su 40 semi estratti). Il disegno sperimentale ci assicura che ogni estrazione è totalmente indipendente dalla precedente e dalla successiva ed ha due soli risultati possibili, cioè successo (seme dormiente), o insuccesso (seme germinabile). Di conseguenza, ogni singola estrazione si configura come un esperimento Bernoulliano, con probabilità di successo pari a \\(\\pi\\), il cui valore ’vero’ esiste, è fisso, pre-determinato (esiste ancor prima di organizzare l’esperimento), anche se incognito e inconoscibile, a meno di non voler/poter esaminare tutti i semi disponibili. L’insieme delle 40 estrazioni (40 esperimenti Bernoulliani) può produrre un ventaglio di risultati possibili, da 40 successi a 40 insuccessi, per un totale di 41 possibili ’outcomes’. E’ evidente che i 41 possibili risultati non sono ugualmente probabili e si può dimostrare che la probabilità di ottenere k successi (con k che va da 0 ad n; n è al numero delle estrazioni) dipende da \\(\\pi\\) ed è descrivibile matematicamente con la distribuzione binomiale \\(\\phi\\): \\[\\phi(k, n, p) = \\frac{n!}{(n-k)!k!} p^k (1 - p)^{(n-k)}\\] Abbiamo quindi definito il modello matematico che descrive la probabilità di tutti i possibili risultati del nostro esperimento e quindi può in qualche modo essere considerato il ’meccanismo’ che ’genera’ i dati sperimentali osservati. Si tratta di un meccanismo puramente ’stocastico’ nel quale è solo il caso che, attraverso il campionamento, determina il risultato dell’esperimento. 7.4.0.6 Stima dei parametri Dovendo stimare la quantità \\(\\pi\\), la statistica tradizionale trascura totalmente le nostre aspettative sul fenomeno e utilizza soltanto i risultati dell’esperimento. Chiamiamo p la quantità stimata e, dato che abbiamo contato nove semi dormienti, concludiamo che p = 0.225, in quanto questa è la cosa più verosimile (maximum likelihood). Infatti, la probabilità di ottenere 9 successi su 40 estrazioni partendo da una distribuzione binomiale con p = 0.225 è pari a 0.1496, mentre, ad esempio, le probabilità di ottenere sempre 9 successi a partire da popolazioni binomiali rispettivamente con p=0.235 o p=0.215 sono più basse, come è possibile verificare utilizzando il software statistico R (R Core Team, 2015) e la funzione dbinom(k, n, p). dbinom(9, 40, 0.225) ## [1] 0.1495739 dbinom(9, 40, 0.235) ## [1] 0.1479025 dbinom(9, 40, 0.215) ## [1] 0.1478311 Si osserva una chiara discrasia tra la verità ’vera’ e l’osservazione sperimentale (tra \\(\\pi\\) e \\(p\\)), nel senso che il risultato dell’esperimento non riflette esattamente la realtà. Anche se questa discrasia non è normalmente evidente (perché la verità vera è ignota), non può in nessun modo essere trascurata o meglio, non ci si può mai comportare come se essa non esistesse! INSOMMA: l’incertezza è un componente inerente della scienza! 7.4.1 Sampling distribution Abbiamo individuato il modello che ha generato le osservazioni (9 successi su 40 estrazioni), ma è evidente che la natura stocastica di questo modello fa si che l’eventuale ripetizione dell’esperimento potrebbe portare ad ottenere un valore di p diverso. Infatti, se \\(\\pi\\) è uguale a 0.25, trovare 10 successi su 40 è la situazione più probabile, ma sono possibili anche altri risultati, seppur meno probabili, come indicato nella figura seguente. #Distribuzione binomiale barplot(dbinom(seq(0, 40, by=1), size=40, prob=0.25), main=expression(paste(&quot;Distribuzione binomiale per &quot;, pi, &quot; = 0.25&quot;, sep=&quot;&quot;)), xlab=&quot;Successi&quot;, ylab=&quot;Probabilità&quot;, names.arg=seq(0,40)) La ripetizione dell’esperimento può essere simulata (simulazione Monte Carlo) ricorrendo ad un generatore di numeri casuali da una distribuzione binomiale con n = 40 e \\(\\pi\\) = 0.25 (in R si usa la funzione rbinom(numeroDatiCasuali, n, p)): set.seed(1234) rbinom(1, 40, 0.25) ## [1] 11 In questo caso il generatore casuale ha prodotto 7 successi, ma ripetendo ancora otteniamo risultati diversi. Trattandosi di un esperimento Monte Carlo, non ci sono problemi a ripeterlo un numero molto alto di volte (es. 10’000’000), ottenendo così una ’popolazione’ di stime p: set.seed(1234) result &lt;- rbinom(10000000, 40, 0.25) A questo punto possiamo esplorare i risultati ottenuti. result_p &lt;- result/40 mean(result_p) ## [1] 0.2500129 sd(result_p) ## [1] 0.0684611 Osserviamo subito che, anche se i singoli esperimenti portano a stime diverse da \\(\\pi\\), la media di \\(p\\) tende ad essere uguale a \\(\\pi\\). Abbiamo però una variabilità intorno a questo valore, quantificabile con una deviazione stndard di 0.068. Possiamo utilizzare i 10’000’000 di valori p ottenuti per costruire una distribuzione empirica di frequenze, che viene detta ’sampling distribution’. La deviazione standard della ’sampling distribution’ (0.0685, in questo caso) si chiama errore standard. breaks &lt;- seq(0, 0.7, by=0.025) freqAss &lt;- as.numeric( table(cut(result_p, breaks) ) ) freqRel &lt;- freqAss/length(result_p) density &lt;- freqRel/0.025 p_oss &lt;- breaks[2:length(breaks)] plot(density ~ p_oss, type = &quot;h&quot;, xlab = expression(paste(bar(p))), ylab=&quot;Density&quot;, main=&quot;Sampling distribution per p&quot;, xlim=c(0,0.6) ) curve(dnorm(x, 0.25, 0.0685), add=T, col=&quot;red&quot;) La sampling distribution può essere approssimata con una distribuzione normale, con media pari a 0.025 e deviazione standard pari a 0.0685. Lo percepiamo chiaramente dal grafico soprastante. In effetti vi è una spiegazione scientifica per questo, basata sul TEOREMA DEL LIMITE CENTRALE: La sampling distribution di una statistica ottenuta da campioni casuali e indipendenti è approssimativamente normale, indipendentemente dalla distribuzione della popolazione da cui i campioni sono stati estratti. La media della sampling distribution è uguale al valore della statistica calcolata sulla popolazione originale, la deviazione standard della sampling distribution (errore standard) è pari alla deviazione standard della popolazione originale divisa per la radice quadrata della numerosità di un campione. In questo caso la statistica calcolata sulla popolazione originale è \\(\\pi\\) = 0.25 e quindi la media della sampling distribution è pari a \\(\\pi\\) = 0.25. La deviazione standard della popolazione originale (che è binomiale) è pari a \\(\\sqrt{p \\times (1 - p)}\\) e la deviazione standard della sampling distribution è pari a \\(\\sqrt{p \\times (1 - p) / 40}\\) = \\(\\sqrt{0.25 \\times 0.75 / 40} = 0.0685\\). La bontà di questa approssimazione è evidente in figura, anche se non si tratta di campioni di numerosità molto alta. Per concludere questa parte osserviamo che abbiamo finora lavorato con due oggetti, o meglio due distribuzioni di probabilità: La distribuzione di probabilità che descrive come saranno i risultati del nostro esperimento (campionamento). In questo caso si tratta di una distribuzione binomiale, che costituisce un vero e proprio ’meccanismo generativo’ delle nostre osservazioni sperimentali. La sampling distribution, che descrive la variabilità delle stime tra un esperimento e l’altro. Si tratta di una distribuzione diversa dalla precedente anche se ad essa legata, per il tramite del teorema del limite centrale. La sampling distribution caratterizza la riproducibilità di un esperimento e la variabilità stocastica dei suoi risultati, in quanto rappresenta i risultati di tutti gli infiniti esperimenti che avremmo dovuto fare, ma non abbiamo fatto. Rappresenta quindi una proiezione sul futuro e può quindi essere utilizzata per trarre conclusioni in modo ’oggettivo’, per testare ipotesi e per costruire intervalli di incertezza, come vedremo in seguito. I due ’ingredienti fondamentali’ della biostatistica 7.5 Esempio 2 Immaginiamo a questo punto una situazione diversa e più vicina alla pratica sperimentale reale VERITA’ VERA: abbiamo una soluzione erbicida a concentrazione pari a 120 \\(mg/l\\), che viene misurata tramite un gascromatografo. MECCANISMI DI CONFUSIONE: Lo strumento di misura, unitamente a tutte le altre fonti ignote di errore, produce un coefficiente di variabilità del 10% (corrispondete ad una deviazione standard pari a 12 \\(mg/l\\)). VERITA’ SPERIMENTALE: i risultati di analisi chimiche ripetute saranno diversi tra di loro e probabilmente diversi dal valore vero di 120 \\(mg/l\\). Come abbiamo già fatto in precedenza, simuliamo l’esperimento ricorrendo ad un generatore di numeri casuali, con la seguente logica: Immaginiamo che la natura operi secondo un meccanismo perfettamente gaussiano, dove gli errori positivi e negativi sono equiprobabili, con probabilità decrescente al crescere della distanza dal valore ’vero’ Eseguiamo l’esperimento Monte Carlo, considerando che la concentrazione vera \\(\\mu\\) sia pari a 120 e \\(\\sigma\\) sia pari a 12. Otteniamo i tre valori 109.28, 132.29 e 130.85 (ovviamente, ripetendo l’estrazione i valori cambiano!!!) Questo è il risultato dell’esperimento, per creare il quale ci siamo sostituiti alla natura, riproducendo i suoi meccanismi di confusione. Il valore ’vero’ di \\(\\mu\\) in pratica è ignoto, anche se in questo caso lo conosciamo, trattandosi di una simulazione. Traiamo le seguenti conclusioni: In base alle osservazioni in nostro possesso, concludiamo che la concentrazione erbicida è pari a m = 124.14 \\(mg/l\\), con una deviazione standard pari a s = 12.89 \\(mg/l\\). La verità sperimentale non coincide con la verità ’vera’ (\\(m \\ne \\mu\\), ma non siamo molto distanti, compatibilmente con il 10% di variabilità dello strumento di analisi. Lo scopo dell’esperimento però non è fornire informazioni sulla verità sperimentale, ma su quella ’vera’. E’ quindi giustificato un atteggiamento prudenziale da parte nostra. Che cosa succederebbe se ripetessimo l’esperimento molte altre volte? La statistica ’frequentista’ (così chiamata per distinguerla da quella bayesiana) assume che la verità vera è fissa e la variabilità di osservazione è misurabile attraverso un ipotetico meccanismo di ripetizione degli esperimenti. 7.5.1 Definire la ’sampling distribution’ per l’esempio 2 In questo caso l’esperimento è solo ’elettronico’ e possiamo quindi ripeterlo un numero anche molto elevato di volte, seguendo questa procedura: Ripetiamo l’estrazione precedente per 10000 volte (ripetiamo l’analisi chimica per 10000 volte, sempre con tre repliche) Otteniamo 10000 medie, la cui media è pari a 120.03 e la cui deviazione standard è pari 6.94 #Simulazione MONTE CARLO - Esempio 2 set.seed(1234) result &lt;- rep(0, 100000) for (i in 1:100000){ sample &lt;- rnorm(3, 120, 12) result[i] &lt;- mean(sample) } mean(result) ## [1] 119.9882 sd(result) ## [1] 6.924185 In sostanza, la simulazione MONTE CARLO ci consente di fare quello che dovremmo sempre fare (ripetere l’esperimento un numero di volte molto elevato, anche se non infinito), ma che, nella realtà, non possiamo fare. A questo punto abbiamo in mano una popolazione di medie, che viene detta ’sampling distribution’, un ’oggetto’ abbastanza ’teorico’, ma fondamentale per la statistica frequentista, perché caratterizza la variabilità dei risultati di un esperimento, e quindi la sua riproducibilità. Notiamo empiricamente che: La media delle medie è ora praticamente coincidente con \\(\\mu\\), la verità ’vera’. Ciò conferma che l’unico modo di ottenere risultati totalmente precisi è ripetere infinite volte l’esperimento;simExcel La deviazione standard delle medie è pari a circa \\(12/\\sqrt(3)\\) = 6.928, cioè l’errore standard della media (SEM) 7.5.2 Le simulazioni Monte Carlo con Excel Finora abbiamo eseguito le nostre simulazioni utilizzando R, cioè un software statistico avanzato. Tuttavia, anche in Excel esiste uno strumento analogo per la generazione di numeri casuali. Assicurarsi di avere installato ed abilitato gli strumenti di analisi dei dati (in Excel 2003 Strumenti/Componenti aggiuntivi/Strumenti di analisi; in Excel 2010: File/Opzioni/Componenti aggiuntivi/Strumenti di analisi/Vai..). Scegliere ’analisi dei dati’ dal menù dati e selezionare lo strumento di generazione dei numeri casuali normali. Immettere le informazioni richieste. Impiego dello strumento di generazione dei dati in Excel Ricordare che i numeri generati sono disposti in più righe e in più colonne, che possiamo specificare immettendo informazioni nei primi due campio della finestra di immissione, cioè numero di variabili (che corrisponde alle colonne) e numero di numeri casuali (che corrisponde alle righe. Nel campo generatore possiamo immettere il ’seed’ cioè il valore di partenza per la generazione di numeri casuali. A partire dal valore prefissato, si ottengono serie di numeri casuali uguali (si tratta in realtà di numeri pseudo-casuali), in modo che le simulazioni possono essere replicate utilizzando lo stesso generatore. 7.5.3 La distribuzione delle medie campionarie: l’errore standard Se vogliamo affrontare il tema in un modo un po’ più formale, il problema allora è: esiste una distribuzione di frequenze che descrive la variabilità delle medie di tutti gli infiniti campioni estraibili dalla popolazione anzidetta? 7.5.3.1 La propagazione degli errori Questo problema si può risolvere considerando che quando prelevo un individuo da una popolazione, da un collettivo ottengo un risultato variabile, a seconda di chi prelevo. Ogni soggetto quindi porta con sé una sua componente di incertezza, che egli ‘eredita’ dalla popolazione di cui fa parte. In questo caso, la popolazione ha una varianza pari a \\(12^2 = 144\\) e quindi ogni osservazione ha tale varianza. Quando calcolo la media di tre osservazioni, in prima battuta io le sommo. A questo punto posso chiedermi: nota che sia la varianza di tre osservazioni, qual è la varianza della loro somma? Dato che si tratta di osservazioni indipendenti, lapropagazione degli errori ci dice che la varianza della somma è uguale alla somma delle varianze. Quindi è pari a \\(144 \\times 3 = 432\\). Dopo aver sommato, il calcolo della media richiede che il risultato venga diviso per tre. Ci chiediamo ancora: qual è la varianza del quoziente? Anche questa risposta arriva attraverso il teorema di propagazione degli errori: Se divido per tre, la varianza viene divisa per \\(3^2 = 9\\). Insomma la varianza della media è \\(432/9 = 48\\). Quindi la deviazione standard della media è \\(\\sqrt{48} = 6.933\\). E’ facile vedere che questo valore è pari a \\(12/\\sqrt{3}\\). In generale: \\[\\sigma_m = \\frac{\\sigma }{\\sqrt n }\\] dove n è la dimensione del campione. Questa quantità si dice ERRORE STANDARD della media, e rappresenta la deviazione standard della sampling distribution, che, in questo modo, è totalmente caratterizzata ed è in grado di descrivere perfettamente l’incertezza associata alla stima della media. b &lt;- seq(90, 160, by=2.5) hist(result, breaks = b, freq=F, xlab = expression(paste(m)), ylab=&quot;Density&quot;, main=&quot;&quot;) curve(dnorm(x, 120, 12/sqrt(3)), add=T, col=&quot;red&quot;) 7.6 Riepilogo 1: Caratterizzare l’incertezza di un esperimento A questo punto possiamo aggiornare le nostre conclusioni precedenti, aggiornandole alla realtà dei fatti. Abbiamo fatto un esperimento con tre repliche campionando da una distribuzione normale incognita. Abbiamo ottenuto i tre valori 109.28, 132.29 e 130.85. In base alle osservazioni in nostro possesso, concludiamo che la concentrazione erbicida è pari a m = 124.14 \\(mg/l\\), con una deviazione standard pari a 12.89 \\(mg/l\\). Dobbiamo adottare un atteggiamento prudenziale in relazione alla media, dato che non sappiamo il valore vero di \\(\\mu\\). Immaginiamo di conoscere la sampling distribution, che avrà una deviazione standard pari a 12.89/\\(\\sqrt{3}\\) = 7.44 Concludiamo quindi che \\(\\mu\\) è pari a 124.14 \\(\\pm\\) 7.44 7.7 Gli intervalli di confidenza Finora abbiamo in mano una stima associata alla sua incertezza, ma sarebbe interessante poter rispondere a questa domanda: ”qual è la proporzione di medie (cioè di ipotetici risultati del mio esperimento) che si trova all’interno della fascia di incertezza data?” Un passo in avanti in questo senso è stato fatto da Neyman (1941) che ha proposto di calcolare gli ’intervalli di confidenza’ prendendo due valori dalla sampling distribution tali da comprendere al loro interno il 95% dei valori di m, lasciando all’esterno solo il 5% di quelli meno probabili (2.5% per ognuna delle due code). Questo problema di calcolo di probabilità può essere risolto partendo dalla formula seguente (utilizzando la notazione R, per comodità): \\[P(\\textrm{qnorm}(0.025,\\mu,\\sigma/\\sqrt(n)) \\leq m \\leq \\textrm{qnorm}(0.975,\\mu,\\sigma/\\sqrt(n))) = 0.95\\] dove si formalizza il fatto che esiste una probabilità pari a 0.95 che m è compreso tra qnorm(0.025, \\(\\mu\\), \\(\\sigma/\\sqrt(n)\\)) e qnorm(0.975, \\(\\mu\\), \\(\\sigma/\\sqrt(n)\\)), cioè tra il 2.5-esimo e il 97.5-esimo percentile di una distribuzione normale con media \\(\\mu\\) e deviazione standard \\(\\mu\\), \\(\\sigma/\\sqrt(n)\\). Questa espressione è solo un punto di partenza, ma è inutile, in pratica, per calcolare gli intervalli di confidenza della media, in quanto \\(\\mu\\) e \\(\\sigma\\) non sono normalmente noti. Dobbiamo quindi trovarci una statistica che sia più facilmente gestibile. Neyman ha proposto di operare la standardizzazione della sampling distribution, tramite la seguente statistica: \\[T = \\frac{m - \\mu}{s_m}\\] Considerando quanto detto più sopra, ci aspetteremmo che la sampling distribution di T sia costituita da una distribuzione normale standardizzata, con media 0 e deviazione standard 1. Verifichiamo la nostra aspettativa con una nuova simulazione Monte Carlo. Questa volta facciamo la seguente operazione: campioniamo tre individui Calcoliamo il valore di T con la statistica precedente e lo salviamo Con un po’ di pazienza, ripetiamo il tutto 100’000 volte. #SIMULAZIONE MONTE CARLO - t di Student set.seed(435) result &lt;- c() for (i in 1:100000){ sample3 &lt;- rnorm(3, 120, 12) T &lt;- (mean(sample3) - 120) / (sd(sample3)/sqrt(3)) result[i] &lt;- T } Se riportiamo i valori ottenuti su una distribuzione di frequenze otteniamo il grafico sottostante. #Plot sampling distribution b &lt;- seq(-600, 600, by=0.2) hist(result, breaks = b, freq=F, xlab = expression(paste(m)), ylab=&quot;Density&quot;, xlim=c(-10,10), ylim=c(0,0.4), main=&quot;&quot;) curve(dnorm(x, 0, 1), add=TRUE, col=&quot;blue&quot;) curve(dt(x, 2), add=TRUE, col=&quot;red&quot;) In sostanza, la distribuzione normale (blue) è un’approssimazione alla vera distribuzione di T, che diviene accettabile solo quando \\(n\\) è molto grande (non certamente ora). Il motivo di questo è legato al fatto che, oltre a non conoscere \\(\\mu\\), non conosciamo neanche \\(\\sigma\\), il che crea un ulteriore elemento di incertezza. Infatti, vediamo che la distribuzione reale è più ’dispersa’ di quella normale, con un maggior numero di valori sulle code. In pratica, bisogna quindi fare riferimento ad una nuova distribuzione di probabilità, quella di t di Student (in rosso). In realtà, esiste una famiglia infinita di distribuzioni di t di Student, una per ogni numero di gradi di libertà. La distribuzione t di Student con infiniti gradi di libertà coincide con quella normale. In questo caso avremo 2 gradi di libertà. Possiamo quindi scrivere che il 95% dei valori di T è contenuto nell’intervallo sottostante: \\[P \\left[ qt(0.025,n - 1) \\le \\frac{m - \\mu }{s_m} \\le qt(0.975, n - 1) \\right] = 0.95\\] dove \\(qt(0.025,n - 1)\\) e \\(qt(0.975,n - 1)\\) sono rispettivamente il 2.5-esimo e il 97.5-esimo percentile della distribuzione t di Student, con n-1 gradi di libertà. Da questa espressione possiamo facilmente ottenere la seguente: \\[P \\left( m + qt(0.025,n - 1) \\cdot s_m \\le \\mu \\le m + qt(0.975,n - 1) \\cdot s_m \\right) = 0.95\\] che dimostra come nel 95% dei casi la media \\(\\mu\\) della popolazione è contenuta nell’intervallo che va da \\(m + qt(0.025,n - 1) \\cdot s_m\\) \\(m + qt(0.975,n - 1) \\cdot s_m\\). I valori della distribuzione t di Student che lasciano al loro esterno il 5% delle varianti (2.5% per coda) sono: qt(0.025, 2) ## [1] -4.302653 qt(0.975, 2) ## [1] 4.302653 Gli intervalli di confidenza sono pertanto: \\[124.14 - 4.3027 \\cdot 7.44 \\le \\mu \\le m + 4.3027 \\cdot 7.44\\] 7.8 Gli intervalli di confidenza con Excel Se vogliamo calcolare gli intervalli di confidenza con Excel, dobbiamo utilizzare la funzione inversa della distribuzione t di Student. Sfortunatamente, le versioni di Excel non sono completamente coerenti a questo proposito e funzionano in modo diverso in Windows e Mac Os X. Infatti, nel sistema operativo di Microsoft, le funzioni INV.T (da Excel 2007) e INV.T.2T sono ’a due code’ e quindi è sufficiente inserire un livello di probabilità pari a 0.05 per ottenere il valore che, assieme al suo reciproco, definisce l’intervallo che lascia al suo esterno il 5% delle varianti (2.5% per coda). Ad esempio: =INV.T(0.05, 2) = 4.3027 per cui i valori da inserire nella formula precedente sono -4.3027 e 4.3027. Al contrario, su EXCEL per Mac Os x, la funzione ”INV.T” è a ’una coda’ (quella sinistra) ; pertanto, vanno usate in questo modo: =INV.T(0.025, 2) = -4.3027 e =INV.T(0.975, 2) = 4.3027 7.9 Qual è il senso dell’intervallo di confidenza? E’utile ricordare il nostro punto di partenza e il nostro punto di arrivo: PUNTO DI PARTENZA: una distribuzione normale con \\(\\mu\\) = 120 e \\(\\sigma\\) = 12. Nella realtà assumiamo che la distribuzione di partenza sia normale, mentre i suoi parametri sono totalmente ignoti. PUNTO DI ARRIVO: una stima di 124.14 ed un intervallo da 92.13 a 156.15 Qual è il reale valore di questo intervallo? Esso fornisce: una misura di precisione: più piccolo è l’intervallo, maggiore è la precisione; la confidenza che, se ripetessimo l’esperimento, nel 95% dei casi l’intervallo calcolato conterrebbe \\(\\mu\\). Questa seconda affermazione può essere dimostrata abbastanza facilmente: in R, dobbiamo estrarre un numero elevato di campioni composti da tre soggetti da una distribuzione normale con media 120 e deviazione standard 12 calcolare gli intervalli di confidenza ogni volta e memorizzare per ogni intervallo di confidenza il valore 1 o 0 a seconda che la media vera sia contenuta nell’intervallo oppure no. In questo caso eseguiamo 100000 simulazioni: result &lt;- rep(0, 100000) set.seed(1234) for (i in 1:100000){ sample &lt;- rnorm(3, 120, 12) limInf&lt;- mean(sample) + sd(sample)/sqrt(3) * qt(0.025, 2) limSup&lt;- mean(sample) + sd(sample)/sqrt(3) * qt(0.975, 2) if (limInf&lt;= 120 &amp; limSup&gt;= 120) result[i] = 1 } sum(result)/100000 ## [1] 0.94992 Chiaramente NON E’ VERO che: c’è il 95% di probabilità che la media ’vera’ della popolazione si trovi tra 92.13 e 156.15. La media vera della popolazione è sempre fissa e pari a 120 e non cambia affatto tra un campionamento e l’altro. ripetendo l’esperimento, il 95% delle stime che otteniamo cadono nell’intervallo 92.13 - 156.15. Una semplice simulazione mostra che quasi tutte le medie campionate cadono in quell’intervallo: result &lt;- rep(0, 100000) set.seed(1234) for (i in 1:100000){ sample &lt;- rnorm(3, 120, 12) if (mean(sample) &lt;= 156.15 &amp; mean(sample) &gt;= 92.13) result[i] = 1 } sum(result)/100000 ## [1] 0.99996 c’è il 95% di probabilità che l’affermazione ’la media vera è compresa tra 92.13 e 156.15’ sia vera. Nelle normali condizioni sperimentali la media vera è ignota e non sapremo mai nulla su di essa: il nostro intervallo di confidenza può catturarla o no. In questo caso, lo ha fatto,ed è tutto quello che possiamo dire. Insomma, l’intervallo di confidenza vale per la sampling distribution e non vale per ogni singolo campionamento (esperimento). Pertanto, affermazioni del tipo: ”c’è il 95% di probabilità che \\(\\mu\\) è compreso nell’intervallo di confidenza” oppure ”il valor più probabile di \\(\\mu\\) è…” non sono corrette e anzi non hanno senso nella statistica tradizionale. In altre parole, l’intervallo di confidenza è una sorta di polizza assicurativa che ci garantisce che, se operiamo continuativamente con le procedure indicate, al termine della nostra carriera avremo sbagliato in non più del 5% dei casi. 7.10 Analisi statistica dei dati: riassunto del percorso logico Considerando quanto finora detto, possiamo riassumere la logica dell’inferenza tradizionale nel modo seguente: Un esperimento è solo un campione di un numero infinito di esperimenti simili che avremmo potuto/dovuto eseguire, ma che non abbiamo eseguito, per mancanza di risorse; Assumiamo che i dati del nostro esperimento sono generati da un modello matematico probabilistico, che prende una certa forma algebrica e ne stimiamo i parametri utilizzando i dati osservati; Costruiamo la sampling distribution per i parametri stimati o per altre statistiche rilevanti, in modo da caratterizzare i risultati delle infinite repliche del nostro esperimento, che avremmo dovuto fare, ma che non abbiamo fatto. Utilizziamo la sampling distribution per l’inferenza statistica. Riassunto del percorso logico nell’inferenza statistica 7.11 Presentazione dei risultati degli esperimenti Dovrebbe essere chiaro che la presenza dell’errore sperimentale ci obbliga a riportare sempre per ogni misurazione un indicatore di tendenza centrale ed un indicatore di variabilità. L’assenza di quest’ultimo non è, in linea di principio, accettabile. Possiamo considerare le seguenti possibilità: la media associata alla deviazione standard, per descrivere la variabilità originale del fenomeno in studio; la media associata all’errore standard, per descrivere l’incertezza associata alla stima della media; la mediana, associata al 25th e 75th percentile, per descrivere dati e fenomeni che non sembrano seguire una distribuzione normale. 7.12 Da ricordare La natura genera i dati Noi scegliamo un modello deterministico che simula il meccanismo di generazione dei dati attuato dalla natura. Stimiamo i parametri. Confrontiamo le previsioni con i dati osservati. Determiniamo \\(\\epsilon\\) e la sua deviazione standard (\\(\\sigma\\)) Assumiamo un modello stocastico ragionevole per spiegare \\(\\epsilon\\), quasi sempre di tipo gaussiano, con media 0 e deviazione standard pari a \\(\\sigma\\), indipendente dalla X (omoscedasticità) Qualunque stima sperimentale deve essere associata ad un indicatore di variabilità (errore standard o intervallo di confidenza). 7.13 Esercizi Un’analisi chimica è stata eseguita in triplicato, ottenendo i seguenti risultati: 125, 169 e 142 ng/g. Calcolare media, devianza, varianza, deviazione standard e coefficiente di variabilità. Considerare il campione [140 - 170 - 155] proveniente da una popolazione distribuita normalmente. Calcolare la probabilità di estrarre dalla stessa popolazione un campione di tre individui con media: maggiore di 170 minore di 140 compresa tra 170 e 140 Dati i tre individui dell’esercizio precedente (140 - 170 - 155), stimare i limiti di confidenza della media (p = 0.05). Dati i tre individui dell’esercizio precedente, immaginare che essi siano stati estratti da una distribuzione normale con sigma noto e uguale a 2. stimare i limiti di confidenza della media (p = 0.05). Un campione di 400 insetti a cui è stato somministrato un certo insetticida mostra che 136 di essi sono sopravvissuti. Determinare un intervallo di confidenza con grado di fiducia del 95% per la proporzione della popolazione insensibile al trattamento. "],
["breve-introduzione-al-test-dipotesi.html", "Capitolo 8 Breve introduzione al test d’ipotesi 8.1 Confronto tra una media osservata e una media teorica 8.2 Simulazione Monte Carlo 8.3 Soluzione formale 8.4 Interpretazione del P-level 8.5 Confronto tra due medie: il test t di Student 8.6 Approfondimento: tipologie alternative di test t 8.7 Confronto tra due proporzioni 8.8 Conclusioni 8.9 Riepilogo", " Capitolo 8 Breve introduzione al test d’ipotesi Abbiamo visto che l’approccio classico all’analisi dei dati è fondamentalmente caratterizzato da due elementi: modello di generazione dei dati sperimentali; sampling distribution (o sample space o distribuzione campionaria). La sampling distribution viene utilizzata per l’inferenza statistica (stima per intervallo) e può essere utilizzata analogamente per il test d’ipotesi. A questo proposito, riprendiamo l’esempio utilizzato in precedenza per gli intervalli di confidenza. 8.1 Confronto tra una media osservata e una media teorica 8.1.1 ESEMPIO 1 Abbiamo misurato la concentrazione di una soluzione erbicida tramite un gascromatografo. Facendo l’analisi in triplicato, abbiamo ottenuto i tre valori 109.28, 132.29 e 130.85. La media è 124.14 e la deviazione standard è 12.89. Concludiamo che \\(mu\\) è uguale a 124.14, con un errore standard (\\(s_m\\)) pari a 7.44 e un intervallo di confidenza che va da 92.12 a 156.16. Immaginiamo che esista un livello soglia pari a 200 mg/l, al disopra del quale il prodotto diviene tossico per i mammiferi. Dato che non conosciamo il vero valore di \\(\\mu\\) ci chiediamo: è possibile che le nostre tre repliche, nella realtà, provengano da una popolazione che ha media uguale a 200? In questo caso sappiamo bene che non è possibile, visto che abbiamo generato i dati sperimentali (vedi il capitolo precedente), tramite simulazione Monte Carlo, partendo da una verità vera nota (\\(\\mu\\) = 120 e \\(\\sigma\\) = 12); tuttavia, nella realtà, la domanda è lecita. In particolare, possiamo calcolare una statistica, che abbiamo già utilizzato per l’intervallo di confidenza, in grado di misurare la discrepanza tra quanto abbiamo osservato e l’ipotesi nulla: \\[ T = \\frac{m - 200}{s_m} \\] Il valore da noi osservato è T = (124.14 - 200)/7.44 = -10.196, il che implica un certo grado di discrepanza, altrimenti avremmo dovuto osservare un valore di T più vicino a 0. Possiamo affermare che ciò sia imputabile solo alla variabilità di campionamento e che quindi il nostro esperimento conferma l’ipotesi di partenza (ipotesi nulla)? Definiamo quindi la nostra ipotesi di lavoro come ipotesi nulla (\\(H_0\\)): \\[H_0: \\mu = 200\\] oppure, che è anche meglio: \\[H_0: T = 0\\] Oltre all’ipotesi nulla, dobbiamo anche definire l’ipotesi alternativa semplice (a ‘due code’), che potrebbe essere: \\[H_1: T \\neq 0\\] E’possibile anche definire ipotesi alternative complesse del tipo: \\[H_1: T \\leq 0\\] oppure \\[H_1: T \\geq 0\\] Bisogna ricordare che le ipotesi debbono essere stabilite prima di effettuare l’esperimento. In questo caso abbiamo fatto un campionamento e abbiamo trovato un valore (124.14) inferiore a quello atteso (200). Che cosa ci attendevamo prima di fare l’esperimento? Un valore diverso da 200, senza poter lecitamente immaginare se sarebbe stato maggiore o minore? In questo caso l’ipotesi alternativa dovrebbe essere la prima (quella semplice). Immaginavamo invece che avrebbe potuto essere inferiore? Allora potremmo utilizzare la prima ipotesi alternativa complessa. In questo caso propendiamo per la prima ipotesi alternativa complessa, cioè \\(\\mu \\leq 200\\). Siamo in totale coerenza con la logica Galileiana: abbiamo un ipotesi di partenza e un esperimento, col quale eventualmente rigettare questa ipotesi. Fisher, negli anni 20 del 1900, propose di utilizzare come ‘forza dell’evidenza scientifica’ la probabilità di ottenere un risultato uguale o più estremo di quello osservato, calcolata supponendo vera l’ipotesi nulla. Si tratta quindi di capire, tramite la definizione di un’apposita ‘sampling distribution’ per T, qual è la proporzione di valori pari o inferiori a -10.196. Dato che la ipotesi alternativa Per rispondere utilizzeremo la doppia strada: quella empirica (simulazione Monte Carlo) e quella formale. 8.2 Simulazione Monte Carlo Ci chiediamo: come sarebbe la sampling distribution di T, se \\(\\mu\\) fosse uguale a 200? Possiamo costruirla con una simulazione Monte Carlo, ripetendo molte volte (es. 100’000) l’estrazione di campioni con numerosità pari a 3, da una distribuzione normale con media pari a 200 e deviazione standard pari a 12.89 e calcolando la statistica T. Utilizziamo questo valore di deviazione standard perché è quello osservato nel campione e non abbiamo nessun altra informazione disponibile sulla deviazione standard della popolazione originale. Per eseguire questa operazione utilizziamo il seguente codice R: set.seed(1234) result &lt;- rep(0, 100000) for (i in 1:1000000){ sample &lt;- rnorm(3, 200, 12.888926) result[i] &lt;- (mean(sample) - 200) / (sd(sample)/sqrt(3)) } In questo modo otteniamo 100’000 valori di T e possiamo calcolare la proporzione di questi che è pari o inferiore al valore da noi osservato (-10.196): x &lt;- c(109.28, 132.29, 130.85) Tobs &lt;- (mean(x) - 200)/(sd(x)/sqrt(3)) length(result[result &lt; Tobs])/1000000 ## [1] 0.004697 Eseguendo questa simulazione, otteniamo una proporzione di valori pari a 0.0048. Il risultato si riassume dicendo che il P-level per l’ipotesi nulla è pari a 0.0048. La regola di condotta della statistica tradizionale è quella di rigettare l’ipotesi nulla quando il P-level è inferiore ad una certa soglia prefissata (normalmente P \\(\\leq\\) 0.05). Di conseguenza, concludiamo che vi sono elementi sufficienti per contrastare l’ipotesi che il valore incognito della concentrazione di erbicida sia pari a 200 mg/l. In altre parole, l’evidenza scientifica è sufficiente buona per il rifiuto dell’ipotesi nulla, anche se esiste una certa probabilità d’errore, pari appunto alla probabilità che l’ipotesi nulla sia vera (P = 0.0048). 8.3 Soluzione formale Possiamo definire una distribuzione di frequenze per T? Empiricamente possiamo osservare che, analogamente al caso degli intervalli di confidenza, possiamo far riferimento alla distribuzione T di Student. Distribuzione empirica di T, distribuzione normale standardizzata (blu) e distribuzione t di Student con 2 gradi di libertà Senza ricorrere alla simulazione Monte Carlo, possiamo quindi risolvere il problema utilizzando la distribuzione t di Student: pt(-10.194, df=2, lower.tail=T) ## [1] 0.9952569 dove gli argomenti indicano rispettivamente il valore osservato, i gradi di libertà e la coda della distribuzione di nostro interesse (a noi interessa la coda sinistra, cioè i valori inferiori a quello dato). Il risultato ottenuto è pari a 0.00474, molto simile a quello ottenuto per simulazione. Allo stesso valore, più semplicemente, si giunge utilizzando la funzione “t.test()”: t.test(x, mu=200, alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: x ## t = -10.194, df = 2, p-value = 0.004743 ## alternative hypothesis: true mean is less than 200 ## 95 percent confidence interval: ## -Inf 145.8694 ## sample estimates: ## mean of x ## 124.14 8.4 Interpretazione del P-level Quando il P-level è inferiore a 0.05, rifiutiamo l’ipotesi nulla e concludiamo che vi sono elementi sufficienti (prove scientifiche sufficientemente forti) per rifiutare la nostra ipotesi di partenza. Bisogna sottolineare come il P-level nella statistica tradizionale sia stato inizialmente proposto da Fisher come criterio di comportamento e non come un vero e proprio criterio inferenziale-probabilistico. Successivamente, Jarzy Neyman ed Egon Pearson, intorno al 1930, proposero di utilizzare il P-level come probabilità di errore di I specie, cioè come probabilità di rifiutare erroneamente l’ipotesi nulla. Tuttavia, trattandosi di una probabilità calcolata a partire da una sampling distribution, cioè da un’ipotetica infinita ripetizione dell’esperimento, essa non ha alcun valore in relazione al singolo esperimento effettivamente eseguito, come i due autori menzionati in precedenza hanno esplicitamente chiarito. Di conseguenza, nel caso in esempio, affermare che abbiamo una probabilità di errore pari a 0.00474 nel rifiutare l’ipotesi nulla, rappresenterebbe un abuso: le nostre conclusioni potrebbero essere false o vere, ma non abbiamo alcun elemento per scegliere tra le due opzioni. Possiamo solo affermare che, se ripetessimo infinite volte l’esperimento e se l’ipotesi nulla fosse vera, otterremmo un risultato estremo come il nostro o più estremo solo in 4.74 casi (circa) su 1000. In altre parole, nel lungo periodo, basando le nostre conclusioni sul criterio anzidetto (rifiuto l’ipotesi nulla se il P-value è inferiore a 0.05) commettiamo un errore in non più del 5% dei casi. Insomma, il P-value non può essere guardato come la probabilità di ‘falso-positivo’ ad ogni singolo test, ma solo nel lunghissimo periodo. 8.5 Confronto tra due medie: il test t di Student Analizziamo un primo esempio pratico di test d’ipotesi: nella sperimentazione agraria si ha spesso interesse a considerare due popolazioni per scoprire se queste sono diverse per il carattere o i caratteri considerati. Più in particolare, siccome ognuna delle popolazioni sarà descritta dalla sua media, saremo interessati a rispondere al quesito se l’eventuale differenza rilevata tra le due medie è da ritenersi una differenza reale, effettiva e con un preciso significato biologico. In sostanza, in termini statistici, dovremo stabilire se la differenza tra le medie è significativa oppure da attribuire a fattori casuali e quindi non significativa. Anche se il problema può sembrare banale, esso non lo è; basti ripensare al fatto che ogni media stimata si porta dietro un alone di incertezza, definito appunto dall’intervallo di confidenza. 8.5.1 ESEMPIO 2 Un ricercatore ha scelto casualmente dieci piante da una popolazione; ne ha trattate cinque con l’erbicida A e cinque con un placebo (P). Alla fine dell’esperimento ha determinato il peso di ognuna delle dieci piante. E’ evidente che le piante oggetto dell’esperimento sono solo un campione di quelle possibili, così come è evidente che il peso, come ogni altra variabile biologica è soggetto ad una certa variabilità naturale, legata sia a questioni genotipiche che fenotipiche, oltre che ad eventuali errori casuali di misura. I risultati sono i seguenti: A (peso in g): 65 - 68 - 69 - 71 - 78; la media è pari a 70.2, mentre la deviazione standard è pari a 4.87. L’errore standard che è pari a 2.18 e quindi l’intervallo di confidenza della media è 70.2 \\(\\pm\\) 6.04 P (peso in g): 80 - 81 - 84 - 88 - 94; la media è 85.4, mentre la deviazione standard è pari a 5.72. L’errore standard è pari a 2.56, mentre l’intervallo di confidenza per la media è 85.4 \\(\\pm\\) 7.11 Possiamo affermare che A riduce il peso delle piante trattate, coerentemente con le aspettative riguardo ad una molecola erbicida? Nel rispondere a questa domanda bisogna tener presente che i campioni sono totalmente irrilevanti, dato che il nostro interesse è rivolto alle popolazioni che hanno generato i campioni. Vogliamo cioè che le nostre conclusioni abbiano carattere di universalità e non siano specifiche a quanto abbiamo osservato nel nostro esperimento. Intanto possiamo notare che il limite di confidenza superiore per A (70.2 + 6.04 = 76.24) è inferiore al limite di confidenza inferiore per P (75.4 - 7.11 = 68.29). Questo non è un criterio sul quale basare le nostre considerazioni, ma è comunque un segno che le popolazioni da cui provengono i due campioni potrebbero essere diverse. Per trovare un criterio decisionali più rigoroso, possiamo formulare l’ipotesi nulla in questi termini: \\[H_0: \\mu_1 = \\mu_2 = \\mu\\] In altre parole, la nostra ipotesi di lavoro è che i due campioni siano in realtà estratti da due distribuzioni normali con la stessa media e la stessa deviazione standard, il che equivale a dire che i due campioni provengono da un unica distribuzione normale con media \\(\\mu\\) e deviazione standard \\(\\sigma\\). L’ipotesi alternativa semplice può essere definita: \\[H_1 :\\mu_1 \\ne \\mu_2\\] Se abbiamo elementi sufficienti, possiamo anche adottare ipotesi alternative complesse, del tipo \\[H_1 :\\mu _1 &gt; \\mu _2\\] oppure: \\[H_1 :\\mu _1 &lt; \\mu _2\\] anche se queste ipotesi alternative debbono essere ragionevolmente fatte prima di eseguire l’esperimento, non dopo averne visto i risultati. Quale statistica potrebbe meglio descrivere l’andamento dell’esperimento, in relazione all’ipotesi nulla? E’ evidente che questa statistica dovrebbe essere basata su due indicatori diversi: l’entità della differenza tra le medie: più la differenza tra le due medie è alta e più è probabile che essa sia significativa; l’entità dell’errore standard. Più è elevata la variabilità dei dati (e quindi l’errore di stima) più è bassa la probabilità che le differenze osservate tra le medie siano significative. Su queste basi, si può individuare la seguente statistica: \\[T = \\frac{m_1 - m_2}{SED}\\] Si può osservare che T, in realtà, non è altro che il rapporto tra le quantità indicate in precedenza ai punti 1 e 2: infatti la quantità al numeratore è la differenza tra le medie dei due campioni, mentre la quantità al denominatore è il cosiddetto errore standard della differenza tra due medie (SED). Quest’ultima quantità si può ottenere pensando che i due campioni sono estratti in modo indipendente e, pertanto, la varianza della somma (algebrica) è uguale alla somma delle varianze. La varianza delle due medie è data dal quadrato delle loro deviazioni standard, cioè dal quadrato degli errori standard. Pertanto: \\[SED^2 = SEM_1^2 + SEM_2^2\\] Sappiamo anche che il SEM si ottiene dividendo la deviazione standard di ogni campione per la radice quadrata del numero dei dati, quindi: \\[SED^2 = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\] cioè: \\[SED = \\sqrt{ \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} }\\] Possiamo anche scrivere: \\[SED = \\sqrt{ \\frac{s_1^2 \\, n_2 + s_2^2 \\, n_1}{n_1 \\, n_2} }\\] e, se le varianze sono uguali (\\(s_1^2 = s_2^2 = s^2\\)), segue che: \\[SED = \\sqrt {s^2 \\frac{n_1 + n_2}{n_1 \\, n_2 } }\\] Se fosse anche \\(n_1 = n_2 =n\\), potremmo scrivere: \\[SED = \\sqrt{2 \\, \\frac{s^2}{n} } = \\sqrt{2} \\times SEM\\] Il valore osservato per T è quindi uguale a: \\[T = \\frac{85.4 - 70.2}{3.361547} = 4.5217\\] dove il denominatore è ottenuto come: \\[SED = \\sqrt{ 2.18^2 + 2.56^2 } = 3.361547\\] A questo punto avendo osservato T = 4.5217, possiamo chiederci qual è la ‘sampling distribution’ per T, cioè quali valori potrebbe assumere questa statistica se ripetessimo il campionamento infinite volte, assumendo che l’ipotesi nulla fosse vera. La sampling distribution per T potrebbe essere ottenuta empiricamente, utilizzando una simulazione MONTE CARLO ed immaginando di estrarre numerose coppie di campioni, dalla stessa distribuzione normale, analogamente a quanto abbiamo fatto nell’esempio precedente. Se l’ipotesi nulla è vera, possiamo immaginare che questa distribuzione gaussiana abbia una media pari a (70.2 + 85.4)/2 = 77.8 e una deviazione standard pari alla deviazione standard delle dieci osservazioni (tutte insieme, senza distinzioni di trattamento), cioè 5.71. Il codice da utilizzare in R per le simulazioni è il seguente: victo &lt;- c(65, 68, 69, 71, 78) lucrezia &lt;- c(80, 81, 84, 88, 94) media &lt;- mean(c(victo, lucrezia)) devSt &lt;- sd(c(victo, lucrezia)) set.seed(1234) result &lt;- rep(0, 100000) for (i in 1:100000){ sample1 &lt;- rnorm(5, media, devSt) sample2 &lt;- rnorm(5, media, devSt) SED &lt;- sqrt( (sd(sample1)/sqrt(5))^2 + (sd(sample2)/sqrt(5))^2 ) result[i] &lt;- (mean(sample1) - mean(sample2)) / SED } I risultati delle 100’000 simulazioni sono riportati nel grafico sottostante. Possiamo notare che, dei 100’000 valori di T osservati assumendo vera l’ipotesi nulla, solo l’un per mille sono superiori a quello da noi osservato e altrettanti sono inferiori a -4.5217. In totale, la probabilità di osservare un valore di T così alto in valore assoluto e dello 0.21 %. SED_obs &lt;- sqrt( (sd(victo)/sqrt(5))^2 + (sd(lucrezia)/sqrt(5))^2 ) T_obs &lt;- (mean(victo) - mean(lucrezia))/SED_obs (length(result[result &lt; T_obs]) + length(result[result &gt; - T_obs])) /100000 ## [1] 0.00164 #Codice Grafico b &lt;- seq(-12, 12, by=0.25) hist(result, breaks = b, freq=F, xlab = expression(paste(m)), ylab=&quot;Density&quot;, xlim=c(-10,10), ylim=c(0,0.45), main=&quot;&quot;) curve(dnorm(x), add=T, col=&quot;blue&quot;) curve(dt(x, 8), add=T, col=&quot;red&quot;) abline(v = 4.52, lty = 2) abline(v = -4.52, lty = 2) text(5, 0.4, label=&quot;4.52&quot;, adj=0, col = &quot;red&quot;) text(-5, 0.4, label=&quot;-4.52&quot;, adj=1, col = &quot;red&quot;) In modo più formale, sempre analogamente all’esempio precedente, si può dimostrare che la sampling distribution per T è costituita dalla distribuzione t di Student, con 8 gradi di libertà (quattro per campione). A questo punto siamo in grado di calcolare la probabilità di ottenere valori di T altrettanto estremi o più estremi di quello da noi osservato, tenendo però presente che il test è ‘a due code’, nel senso che dobbiamo considerare T osservato e il suo reciproco, che avremmo ottenuto se avessimo cambiato il senso della differenza (\\(m_2 - m_1\\) invece che \\(m_1 - m_2\\)). In altre parole ci chiediamo qual è la possibilità di campionare valori esterni all’intervallo (-4.5217; 4.5217). Possiamo ottenere questa informazione utilizzando la funzione: 2 * pt(T_obs, 8, lower.tail=T) ## [1] 1.998055 Abbiamo moltiplicato per 2 il risultato, in quanto la funzione ‘dt()’ fornisce la probabilità di trovare individui inferiori a -4.5217 (‘lower.tail = T’), che, essendo la distribuzione simmetrica, è esattamente uguale alla probabilità di trovare soggetti superiori a 4.5217. Vediamo che il P-level è minore di 0.05 e possiamo quindi rifiutare l’ipotesi nulla. Concludiamo che vi è un’evidenza scientifica abbastanza forte per ritenere che l’erbicida A induca una riduzione del peso delle piante trattate. Allo stesso valore, più semplicemente, si perviene utilizzando la funzione: t.test(victo, lucrezia, var.equal=T) ## ## Two Sample t-test ## ## data: victo and lucrezia ## t = -4.5217, df = 8, p-value = 0.001945 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -22.951742 -7.448258 ## sample estimates: ## mean of x mean of y ## 70.2 85.4 Gli argomenti della funzione ‘t.test()’ sono i due vettori è l’argomento ‘var.equal’, che in questo caso è stato settato su TRUE. Per comprendere il significato di quest’ultimo argomento dobbiamo vedere quali sono le tipologie di t test possibili. 8.6 Approfondimento: tipologie alternative di test t Il test t può essere di tre tipi: Appaiato. In questo caso le misure sono prese a coppia sullo stesso soggetto e non sono quindi indipendenti. Omoscedastico. Le misure sono prese su soggetti diversi (indipendenti) e possiamo suppore che i due campioni provengano da due popolazioni con la stessa varianza. Eteroscedastico. Le misure sono prese su soggetti diversi, ma le varianze non sono omogenee. Nel nostro esempio vediamo che le varianze dei campioni sono piuttosto simili e quindi adottiamo un test t omoscedastico (‘var.equal = T’). Se dovessimo supporre che i due campioni provengono da popolazioni con varianze diverse, allora si porrebbe il problema di stabilire il numero di gradi di libertà del SEM. Abbiamo visto che se le varianze dei due campioni sono uguali (o meglio, sono due stime della stessa varianza), la varianza della somma/differenza ha un ha un numero di gradi di libertà pari alla somma dei gradi di libertà delle due varianze. Se le varianze fossero diverse, il numero di gradi di libertà della loro combinazione lineare (somma o differenza) si dovrebbe approssimare con la formula di Satterthwaite: \\[DF_s \\simeq \\frac{ \\left( s^2_1 + s^2_2 \\right)^2 }{ \\frac{(s^2_1)^2}{DF_1} + \\frac{(s^2_2)^2}{DF_2} }\\] Vediamo che se le varianze e i gradi di libertà sono uguali, la formula precedente riduce a: \\[DF_s = 2 \\times DF\\] Nel nostro caso, se fosse \\(s^2_1 \\neq s^2_2\\) avremmo un numero frazionario di gradi di libertà: dfS &lt;- (var(victo) + var(lucrezia))^2 / ((var(victo)^2)/4 + (var(lucrezia)^2)/4) dfS ## [1] 7.79772 Il risultato può essere riscontrato con: t.test(victo, lucrezia, var.equal=F) ## ## Welch Two Sample t-test ## ## data: victo and lucrezia ## t = -4.5217, df = 7.7977, p-value = 0.002076 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -22.986884 -7.413116 ## sample estimates: ## mean of x mean of y ## 70.2 85.4 Se invece avessimo rilevato le misure accoppiate su quattro individui avremmo solo 4 gradi di libertà: t.test(victo, lucrezia, var.equal=T, paired=T) ## ## Paired t-test ## ## data: victo and lucrezia ## t = -22.915, df = 4, p-value = 2.149e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -17.04169 -13.35831 ## sample estimates: ## mean of the differences ## -15.2 8.7 Confronto tra due proporzioni Il test di t è molto utile, ma soltanto nel caso in cui si abbia a che fare con caratteri quantitativi, cioè con variabili misurate su una scala continua, per le quali sia possibile calcolare delle statistiche descrittive, come appunto la media. In molti casi, gli sperimentatori sono interessati a rilevare alcune caratteristiche qualitative, come ad esempio lo stato di una pianta in seguito ad un trattamento (morta o viva), il colore dei semi (si ricordino i piselli verdi e gialli di Mendel) ed altre caratteristiche che non sono misurabili facilmente su una scala continua. Avendo a che fare con variabili qualitative l’unico dato che si può rilevare è la proporzione degli individui che presentano le diverse modalità. Proviamo ad osservare il seguente esempio. 8.7.1 ESEMPIO 3 Immaginiamo di voler verificare se un coadiuvante aumenta l’efficacia di un erbicida organizziamo un esperimento in cui utilizziamo l’erbicida da solo e con il coadiuvante. Nel primo caso (erbicida da solo) otteniamo 56 morti su 75 piante trattate, mentre nel secondo caso otteniamo 48 morti su 50 piante trattate. I risultati del nostro esperimento si riducono ad una tabella di contingenza di questo tipo: counts &lt;- c(56, 19, 48, 2) tab &lt;- matrix(counts, 2, 2, byrow = T) row.names(tab) &lt;- c(&quot;E&quot;, &quot;EC&quot;) colnames(tab) &lt;- c(&quot;M&quot;, &quot;V&quot;) tab ## M V ## E 56 19 ## EC 48 2 Abbiamo già visto nel capitolo sulla statistica descrittiva come è possibile costruire un indice di connessione (detto \\(\\chi^2\\)) che misura la quantità del ‘legame’ tra le due variabili rilevate (trattamento e mortalità). Ci stiamo infatti chiedendo se il numero dei morti è indipendente dal tipo di trattamento oppure no. Sappiamo già come calcolare il \\(\\chi^2\\) con R: summary( as.table(tab) ) ## Number of cases in table: 125 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 9.768, df = 1, p-value = 0.001776 Il valore osservato è pari a 9.768. Siccome siamo in ambito inferenziale, dobbiamo ricordare che non ci interessa il confronto tra proporzioni osservate nel caso in studio, ma ci interessa il confronto delle proporzioni delle popolazione da cui i due campioni (uno di 75 l’altro di 50 individui) sono stati estratti. Di conseguenza, è possibile che, pur essendo vera l’ipotesi nulla, noi abbiamo ottenuto un valore maggiore di zero per effetto del campionamento e dell’errore sperimentale, cioè per il solo effetto del caso. Ma quanto è probabile questa evenienza? Poniamo l’ipotesi nulla in questi termini: \\[H_o :\\pi_1 = \\pi_2 = \\pi\\] Ci chiediamo: se l’ipotesi nulla è vera (\\(\\pi_1 = \\pi_2\\)), qual è la sampling distribution per \\(\\chi^2\\)? E’possibile simulare questo esperimento immaginando di campionare da una distribuzione binomiale con \\(\\pi\\) = 0.832 il numero di successi su 75 esperimenti (che corrisponde alla prima cella in alto a sinistra della tabella di contingenza precedente). Per differenza con i totali marginali possiamo poi dedurre le altre tre frequenze e calcolare il valore di \\(\\chi^2\\). La simulazione di un test di \\(\\chi^2\\) può esser fatta utilizzando la funzione ‘r2dtable()’ che produce il numero voluto di tabelle di contingenza, con righe e colonne indipendenti r rispettando i totali marginali voluti. Le tabelle prodotte (nel nostro caso 10’000) sono restituite come lista, quindi possiamo utilizzare la funzione ‘lapply()’ per applicare ad ogni elemento della lista la funzione che restituisce il \\(\\chi^2\\) (‘chiSim’). chiSim &lt;- function(x) summary(as.table(x))$stat set.seed(1234) tabs &lt;- r2dtable(10000, apply(tab, 1, sum), apply(tab, 2, sum)) chiVals &lt;- as.numeric( lapply( tabs, chiSim) ) length(chiVals[chiVals &gt; 9.768]) ## [1] 22 Vediamo che vi sono 19 valori più alti di quello da noi osservato (p = 0.0019). In modo formale, si può dimostrare che, se \\(n\\) è sufficientemente grande (n &gt; 30), il valore osservato di \\(\\chi^2\\) segue appunto la distribuzione della variabile casuale \\(\\chi^2\\), con un numero di gradi di libertà \\(\\nu\\) pari al numero dei confronti (in questo caso 1). Possiamo quindi utilizzare questa variabile casuale per calcolare la probabilità di ottenere valori pari o superiori a 9.768: pchisq(9.76801, 1, lower.tail=F) ## [1] 0.001775746 Allo stesso risultato, ma in modo più semplice, è possibile pervenire utilizzando la già vista funnzione ‘summary()’ applicata alla tabella di contingenza (vedi sopra), oppure: chisq.test(tab, correct=F) ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 9.768, df = 1, p-value = 0.001776 8.8 Conclusioni Abbiamo visto quale strumento abbiamo a disposizione per tirare conclusioni in presenza di incertezza sperimentale. Dovrebbe essere evidente che anche le nostre conclusioni sono incerte, in quanto soggette all’errore di campionamento. In particolare, abbiamo visto che esiste un rischio di errore di prima specie, cioè rifiutare erronamente l’ipotesi nulla (falso positivo). Allo stesso modo, esiste anche un rischio di errore di II specie, cioè accettare erroneamente l’ipotesi nulla (falso negativo). Dei due errori parleremo in una dispensa specifica. 8.9 Riepilogo Lo schema di lavoro, nel test d’ipotesi, è il seguente: Si formula l’ipotesi nulla; Si individua una statistica che descriva l’andamento dell’esperimento, in relazione all’ipotesi nulla; Si individua la sampling distribution per questa statistica, assumendo vera l’ipotesi nulla; la sampling distribution può essere empirica (ottenuta per simulazione) o teorica, scelta in base a considerazioni probabilistiche Si calcola la probabilità che, essendo vera l’ipotesi nulla, si possa osservare una valore altrettanto estremo o più estremo di quello calcolato, per la statistica di riferimento; Se il livello di probabilità è inferiore ad una certa soglia \\(\\alpha\\) prefissata (generalmente 0.05), si rifiuta l’ipotesi nulla. "],
["modelli-matamatici-descrittivi-breve-introduzione.html", "Capitolo 9 Modelli matamatici descrittivi: breve introduzione 9.1 Che c’entra la matematica? 9.2 Mettiamo alcuni paletti 9.3 Metodo di lavoro", " Capitolo 9 Modelli matamatici descrittivi: breve introduzione 9.1 Che c’entra la matematica? L’eredità galileiana ci porta ad immaginare che il funzionamento della natura sia basato su una serie di relazioni causa-effetto, descrivibili utilizzando il linguaggio universale della matematica. La conoscenza esatta di queste relazioni, nella teoria, ci potrebbe consentire di prevedere l’andamento di ogni fenomeno naturale, almeno quelli osservabili con sufficiente precisione. In effetti era proprio questa l’ambizione più grande degli scienziati dell’inizio ’800: conoscendo lo stato iniziale di un sistema doveva essere possibile prevederne l’evoluzione futura. In realtà si è ben presto scoperto che si trattava di un’ambizione irrealistica, non tanto e non solo per la comparsa della meccanica quantistica un secolo dopo, ma anche per l’aumento di importanza degli studi in ambito psichiatrico e medico/biologico. Infatti questi studi riguardano organismi viventi altamente complessi, capaci di una grandissima variabilità di risposte individuali, che rendono altamente difficili, se non impossibili da trovare, le eventuali relazioni di causa-effetto, che pur dovrebbero esistere in pratica. Insomma, tra i biologi prevale la visione che la natura ha le sue relazioni causa-effetto, anche se i limiti della natura umana e le nostre difficoltà di misura ci impediscono di osservarle ed individuarle con precisione. E’ quindi innegabile che la natura e, soprattutto, l’osservazione dei suoi fenomeni, siano caratterizzate da un’intrinseca componente stocastica, che nasconde le relazioni causa-effetto, distorcendone in modo piò o meno evidente i risultati. Il nostro compito è quello di riuscire a capire se quello che osserviamo, pur non essendo totalmente prevedibile in base alle relazioni causa-effetto note, è compatibile con esse o se invece debba essere considerato come una prova a loro confutazione. In semplice linguaggio algebrico, potremmo immaginare che la natura opera secondo un modello deterministico causa-effetto di questo tipo: \\[Y_T = f(X, \\theta)\\] dove \\(Y_T\\) è l’effetto dello stimolo \\(X\\), che segue la funzione \\(f\\), basati su una collezioni di parametri \\(\\theta\\). Tuttavia, la componente stocastica distorce la nostra osservazione e non ci mostra \\(Y_T\\), ma ci mostra \\(Y_o \\neq Y_T\\). Dobbiamo quindi scrivere: \\[Y_o = f(X,\\theta) + \\varepsilon\\] dove \\(\\varepsilon\\) è la componente stocastica che distorce i risultati, per effetto del campionamento. Questa componente stocastica può essere descritta con una modello stocastico, assumendo che segua una certa distribuzione di probabilità, descritta da un parametro di posizione (es. media) e un parametro di forma (es. deviazione standard). \\[ \\varepsilon \\sim \\Phi \\left[ \\lambda, \\sigma \\right] \\] 9.2 Mettiamo alcuni paletti Abbiamo visto che un modello ha due parti (una deterministica e una stocastica), caratterizzate dua varie componenti: \\(Y\\), \\(X\\), \\(f\\), \\(\\theta\\), \\(\\Phi\\), \\(\\lambda\\) e \\(\\sigma\\), che possono assumere le forme più disparate. Per cominciare, sarà bene restringere un po’ il nostro campo d’azione. Immaginiamo quindi che: \\(Y\\) è numerico e univariato \\(X\\) è numerico, categorico, univariato o multivariato \\(f\\) è lineare \\(\\Phi\\) è normale (gaussiano) \\(\\lambda = \\mu = 0\\) \\(\\sigma\\) è la deviazione standard 9.3 Metodo di lavoro Seguiamo questo metodo di lavoro: definiamo X ed Y, le variabili oggetto di studio qual è il modello deterministico sul quale investighiamo? Scegliamo \\(f\\) organizziamo l’esperimento e raccogliamo i dati parametrizziamo il modello, cioè diamo un valore a \\(\\theta\\) stimiamo \\(\\sigma\\) facciamo inferenza Ovviamente, l’esperimento non sarà reale, bensì simulato. "],
["una-variabile-indipendente-categorica-anova-ad-una-via.html", "Capitolo 10 Una variabile indipendente categorica: ANOVA ad una via 10.1 La situazione sperimentale 10.2 La verità ‘vera’ (la popolazione) 10.3 Esecuzione dell’esperimento 10.4 Analisi dei dati 10.5 Per approfondimenti", " Capitolo 10 Una variabile indipendente categorica: ANOVA ad una via 10.1 La situazione sperimentale Immaginiamo di aver posto a confronto cinque trattamenti sperimentali qualitativi (tipi di concime o varietà di frumento o diserbanti) e di aver registrato il loro effetto su una variabile quantitativa (ad esempio la produzione). Ci troviamo quindi ad avere un collettivo di osservazioni, all’interno delle quali esistono gruppi diversi di soggetti, classificabili in base al trattamento a cui sono stati sottoposti. Al solito, sappiamo che i soggetti osservati sono solo un campione di tutti quelli possibili, quindi dietro ai nostri soggetti vi è almeno una popolazione di riferimento. Più precisamente: se i trattamenti effettuati non avessero avuto effetto, avremmo una sola popolazione di riferimento e cinque campioni diversi provenienti dalla stessa popolazione. Se invece i trattamenti fossero stati efficaci, allora avremmo cinque popolazioni diverse, per la media, per la deviazione standard o per entrambe. Per semplicità, assumiamo che le cinque popolazioni differiscano solo per la media e non per la deviazione standard. La prima ipotesi scientifica può essere tradotta in questo modo: \\[ Y_i = \\mu + \\varepsilon\\] con \\(\\varepsilon \\sim N(0, \\sigma)\\). Insomma, abbiamo una sola popolazione distribuita normalmente, con media \\(\\mu\\) e deviazione standard \\(\\sigma\\). La seconda ipotesi, più interessante da un punto di vista sperimentale, è questa: \\[Y_i = \\mu + \\alpha_j + \\varepsilon\\] Qui abbiamo un’intercetta \\(\\mu\\) (vedremo tra breve cosa rappresenta), mentre \\(\\alpha_j\\) è l’effetto del j-esimo trattamento. Per capire cosa rappresenta \\(\\mu\\) dobbiamo pensare a due trattamenti, la cui media sia rispettivamente pari a 15 e 25. Se poniamo \\(\\mu = 20\\) (la media generale), allora \\(\\alpha_1 = - 5\\) e \\(\\alpha_2 = 5\\). Ma è anche vero che alla stessa soluzione si potrebbe pervenire ponendo \\(\\mu = 19\\) e quindi \\(\\alpha_1 = -4\\) e \\(\\alpha_2 = 6\\). ugualmente potremmo porre \\(\\mu = 18\\) e quindi \\(\\alpha_1 = -3\\) e \\(\\alpha_2 = 7\\). E così via. Insomma, l’equazione lineare sovrastante ha infinite soluzioni. Per uscire da questo impasse, dobbiamo porre dei vincoli. Ad esempio potremmo porre il vincolo che \\(\\alpha_1 = 0\\). In questo caso risulta definito che \\(\\mu\\) deve essere pari alla media del primo trattamento (\\(\\mu = 15\\)) e \\(\\alpha_2 = 10\\) (vincolo sul trattamento). Oppure potremmo porre il vincolo che \\(\\sum{\\alpha} = 0\\), quindi risulta definito che \\(\\mu\\) deve essere pari alla media generale (\\(\\mu = 20\\)) e \\(\\alpha_1 = -5\\) (vincolo sulla somma). Un’altra possibilità è imporre che \\(\\mu\\) sia uguale a 0 (vincolo sull’intercetta). Questi vincoli prendono il nome di PARAMETRIZZAZIONI del modello lineare; la prima parametrizzazione (con vincolo sul trattamento) è quella di default, nella gran parte dei software statistici, incluso R. 10.2 La verità ‘vera’ (la popolazione) Immaginiamo che, nella nostra popolazione, le medie dei cinque trattamenti siano: A = 9 B = 5 C = 16 D = 27 E = 31 Immaginiamo anche che l’errore sperimentale si gaussiano, con media 0 e deviazione standard pari a \\(\\sigma = 1.5\\). Penso sia opportuno ricordare l’elenco delle assunzioni che abbiamo posto: il modello deterministico è lineare, additivo non vi sono altri effetti, se non il trattamento e l’errore (puramente stocastico, con media 0 e valori indipendenti) gli errori sono normalmente distribuiti le varianze sono omogenee (unico valore di \\(\\sigma\\), comune per tutti i gruppi) 10.3 Esecuzione dell’esperimento Immaginiamo di fare un esperimento con 8 repliche e generiamo i dati con una simulazione di Monte Carlo. treat &lt;- rep(LETTERS[1:5], each = 8) mu &lt;- 9; alpha2 &lt;- -4; alpha3 &lt;- 7; alpha4 &lt;- 18 alpha5 &lt;- 22; sigma &lt;- 1.5 alpha &lt;- c(0, alpha2, alpha3, alpha4, alpha5) Ye &lt;- rep(mu + alpha, each = 8) set.seed(1234) Y &lt;- Ye + rnorm(40, 0, sigma) dataset &lt;- data.frame(treat = treat, Y = Y) rm(list=(ls()[ls()!=&quot;dataset&quot;])) head(dataset) ## treat Y ## 1 A 9.644795 ## 2 A 8.341859 ## 3 A 7.212470 ## 4 A 7.383091 ## 5 A 8.824202 ## 6 A 7.903032 10.4 Analisi dei dati 10.4.1 Statistiche descrittive Descriviamo la tendenza centrale e la variabilità dei cinque gruppi. library(plyr) statDesc &lt;- medie2 &lt;- ddply(dataset, ~treat, summarise, media = mean(Y), devSt = sd(Y)) statDesc ## treat media devSt ## 1 A 8.167906 0.8779368 ## 2 B 5.698595 1.5159899 ## 3 C 16.280251 1.3902306 ## 4 D 27.149423 1.7569298 ## 5 E 30.948257 1.3541870 Vediamo che i risultati riflettono abbastanza bene, ma non perfettamente, le caratteristiche della popolazione. 10.4.2 Stima dei parametri In questo caso (disegno bilanciato, con lo stesso numero di repliche per trattamento), la stima dei parametri potrebbe essere fatta a mano, abbastanza facilmente. Più in generale, utilizziamo R. mod &lt;- lm(Y ~ factor(treat), data = dataset) La specifica del modello è chiara, considerando che l’incusione dell’intercetta è opzionale (‘~ 1 + treat’) è \\(\\epsilon\\) viene incluso di default. Il termine ‘factor’ sta a significare che la variabile ‘treat’ è un fattore sperimentale. Questa specifica è opzionale in questo caso in cui la variabile è di tipo ‘carattere’, ma è fondamentale quando abbiamo a che fare con una codifica numerica. Vediamo ora le stime dei parametri. summary(mod) ## ## Call: ## lm(formula = Y ~ factor(treat), data = dataset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4779 -0.8498 -0.0626 1.0956 2.0180 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.1679 0.4981 16.400 &lt; 2e-16 *** ## factor(treat)B -2.4693 0.7044 -3.506 0.00127 ** ## factor(treat)C 8.1123 0.7044 11.517 1.86e-13 *** ## factor(treat)D 18.9815 0.7044 26.949 &lt; 2e-16 *** ## factor(treat)E 22.7804 0.7044 32.342 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.409 on 35 degrees of freedom ## Multiple R-squared: 0.983, Adjusted R-squared: 0.981 ## F-statistic: 505.6 on 4 and 35 DF, p-value: &lt; 2.2e-16 Notiamo immediatamente che viene utilizzata la parametrizzazione con vincolo sul trattamento, visto che l’intercetta coincide con la media del primo trattamento. 10.4.3 Stima della varianza Il residuo \\(\\varepsilon\\) rappresenta la componente casuale: se non vi fosse errore. la prima osservazione dovrebbe avere un valore pari al valore atteso del gruppo di cui fa parte (8.5543). In realtà essa è pari a 9.6447955, con un residuo, rispetto all’atteso’ pari a 1.0904955. La serie completa dei residui può essere ottenuta come segue: epsilon &lt;- residuals(mod) La devianza del residuo (somma dei quadrati degli scarti) è: RSS &lt;- sum( epsilon^2 ) RSS ## [1] 69.45655 Quanri gradi di libertà ha questa somma? Dobbiamo tener presente che i residui sono somme di quadrati di scarti rispetto alla media di ogni trattamento. Di conseguenza, la media dei residui è costretta ad essere zero per ogni trattamento, il che significa che, sempre per ogni trattamento, sette scarti sono liberi (7 gradi di libertà), mentre l’ottavo deve essere tale che sommato con gli altri restituisce zero. Quindi abbiamo $ 7 5 = 35$ gradi di libertà. Possiamo quindi calcolare la deviazione standard del residuo come: sigma &lt;- sqrt( RSS/35 ) sigma ## [1] 1.408713 In realtà, questo valore l’avevamo già visto applicando la funzione ‘summary()’ all’oggetto ‘mod’. Da questo valore possiamo calcolare l’errore standard di una media (SEM) e l’errore standard di una differenza (SED): SEM &lt;- sigma / sqrt(8) SED &lt;- sqrt(2) * sigma / sqrt(8) SEM; SED ## [1] 0.4980553 ## [1] 0.7043566 Anche questi valori erano già presenti nell’output della funzione ‘summary()’. Bastava ricordare che l’intercetta è una media, mentre gli altri valori \\(\\alpha\\) sono differenze tra medie. SEM e SED sono espresse nella stessa unità di misura dei dati (sono deviazioni standard) e possono essere utilizzati per costruire intervalli di confidenza intorno alle medie e alle loro differenze. Nell’ipotesi verificata di omogeneità delle varianze, il SEM può essere utilizzato per quantificare l’incertezza di una media, al posto dell’errore standard calcolato per le singole repliche. 10.4.4 Effetto del trattamento Abbiamo calcolato la devianza del residuo, cioè abbiamo misurato la quota parte di variabilità di natura stocastica. Possiamo ora considerare i valori attesi, cioè le osservazioni deurate dall’errore: dataset$Y - residuals(mod) ## 1 2 3 4 5 6 7 ## 8.167906 8.167906 8.167906 8.167906 8.167906 8.167906 8.167906 ## 8 9 10 11 12 13 14 ## 8.167906 5.698595 5.698595 5.698595 5.698595 5.698595 5.698595 ## 15 16 17 18 19 20 21 ## 5.698595 5.698595 16.280251 16.280251 16.280251 16.280251 16.280251 ## 22 23 24 25 26 27 28 ## 16.280251 16.280251 16.280251 27.149423 27.149423 27.149423 27.149423 ## 29 30 31 32 33 34 35 ## 27.149423 27.149423 27.149423 27.149423 30.948257 30.948257 30.948257 ## 36 37 38 39 40 ## 30.948257 30.948257 30.948257 30.948257 30.948257 che possiamo più semplicemente ottenere con la funzione ‘fitted(mod)’. In questo vettore sono rappresentate le medie dei trattamenti; la differenza tra i dati è solo imputabile al trattamento, visto che l’errore l’abbiamo deliberatamente rimosso. Possiamo calcolare la devianza dei dati: Ye &lt;- fitted(mod) SSt &lt;- sum( ( Ye - mean(Ye) )^2 ) In questa caso abbiamo solo cinque valori liberi, dato che tutti i valori appartenenti allo stesso trattamento debbono essere uguali. La devianza del trattamento ha quindi 4 gradi di libertà. 10.4.5 Test d’ipotesi Abbiamo descritto il nostro esperimento e ne abbiamo individuato le caratteristiche rilevanti, stimando i parametri che meglio le descrivono (effetti dei trattamenti e varianza). Le nostre stime non coincidono con la verità vera, ma la riflettono bene, perché i dati sono stati generati rispettando le assunzioni di base del modello (additività, indipendenza, normalità e omogeneità delle varianze). Normalmente non conosciamo il modello generativo, quindi dobbiamo chiederci se i dati rispettano le assunzioni di base del modello. Di questo parleremo in una lezione a parte. Ora, il nostro scopo è capire se il trattamento sperimentale ha prodotto un effetto rilevante, distinguibile da quel del caso (‘rumore di fondo’). In statistica, come nei tribunali, si assume che l’imputato (in questo caso l’effetto del trattamento) non ha commesso il fatto (non è stato efficace) fino a prova contraria. Di conseguenza, l’ipotesi nulla è che il trattamento non ha avuto effetto, cioè che: \\[H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 = \\mu\\] La stessa cosa può essere declinata come: \\[H_0: \\alpha_1 = \\alpha_2 = \\alpha_3 = \\alpha_4 = \\alpha_5 = 0\\] Una statistica rilevante per testare questa ipotesi è data dal rapporto tra la varianza del trattamento e quella dell’errore: \\[F = \\frac{MS_t}{MS_e} \\] Nel nostro caso: MSt &lt;- SSt / 4 MSe &lt;- RSS / 35 Foss &lt;- MSt / MSe Foss ## [1] 505.6306 E’ evidente che se il trattamento non fosse stato efficace non dovrebbe aver prodotto una variabilità superiore a quella dell’errore (quindi F = 1). In questo caso la variabilità prodotta dal trattamento è stata quasi 526 volte superiore a quella dell’errore. Delle due l’una: o il trattamento è stato efficace oppure io sono stato particolarmente sfortunato e, nell’organizzare questo esperimento, si è verificato un caso particolarmente raro. Ci chiediamo: “se l’ipotesi nulla è vera, qual è la ‘sampling distribution’ per F? Potremmo costruire questa distribuzione empiricamente, attraverso una simulazione MONTE CARLO. Con un modello lineare, R ci aiuta, in quanto ci evita di dover programmare la simulazione. In primo luogo, abbiamo già visto che la situazione in cui il trattamento non ha effetto può anch’essa essere descritta da un modello lineare (vedi sopra: \\(Y_i = \\mu + \\varepsilon\\)). Questo modello lineare può essere adattato ai dati analogamente al modello precedente. modNull &lt;- lm(Y ~ 1, data = dataset) Ora possiamo simulare dataset con questo modello e calcolare i valori di F. utilizzeremo la funzione simulate. Ysim &lt;- simulate(modNull, 10000) simF &lt;- function(x) anova ( lm(x ~ factor(treat), data = dataset))$F[1] Fvals &lt;- apply(Ysim, 2, simF) Fvals[1:10] ## sim_1 sim_2 sim_3 sim_4 sim_5 sim_6 sim_7 ## 0.8548588 0.6712136 0.7070373 0.1392298 1.2238092 0.3931931 0.8924572 ## sim_8 sim_9 sim_10 ## 0.6645767 0.5080306 1.7989212 I valori di F riportati non riflettono differenze tra i trattamenti e dovrebbero quindi essere bassi. Infatti vediamo che il minimo è 0.005809 ed il massimo 8.6521682. Tra tutti i 10’000 valori, non ce ne è neanche uno pari o superiore a quello dato, il che vuol dire che la probabilità che l’ipotesi nulla sia vera con F = 525.8 è minore di 1 su 10’000. La sampling distribution (opportunamente discretizzata) è riportata in figura. Si tratta di una distribuzione chiaramente non normale, con media pari a 1.0575004, mediana pari a 0.8571902. b &lt;- seq(0, 10, by=0.1) counts &lt;- table(cut(Fvals, breaks=b)) counts &lt;- c(as.numeric(counts), 0) plot(counts/1000 ~ I(b + 0.05), type=&quot;h&quot;) curve(df(x, 4, 35), add=T, col=&quot;blue&quot;) Più formalmente, si può dimostrare che la sampling distribution di F è data dalla distribuzione F di Fisher, con 4 gradi di libertà al numeratore e 35 al denominatore (in blue in figura). Di conseguenza, possiamo utilizzare la F di Fisher per calcolare la probabilità di ottenere un valore di F altrettanto estremo o più estremo del nostro. Ad esempio, in R, possiamo utilizzare la funzione: 1 - pf(Foss, 4, 35) ## [1] 0 praticamente pari a 0. Insomma, in assenza di un effetto del trattamento (quindi per il solo effetto del caso), se ripetiamo l’esperimento infinite volte, abbiamo abbiamo una probabilità praticamente nulla che si produca un valore di F altrettanto alto o più alto di quello da noi osservato. Di conseguenza, se rifiutiamo l’ipotesi nulla di assenza di effetto del trattamento e accettiamo l’ipotesi alternativa (il trattamento ha avuto effetto significativo) ci portiamo dietro un rischio di errore estremamente piccolo, comunque molto al disotto della soglia prefissata del 5%. INSOMMA: Con l’ANOVA la variabilità totale dei dati viene decomposta in due quote, una attribuibile al trattamento sperimentale ed una inspiegabile (residuo) L’effetto del trattamento è significativo, se la variabilità che esso provoca è superiore a quella inspiegabile Confronto tra varianze (test F). L’ipotesi nulla è che il trattamento NON E’ significativo P rappresenta la probabilità di errore nel rifiutare l’ipotesi nulla L’ipotesi nulla è rifiutata quando P \\(\\leq\\) 0.05 (livello di protezione arbitrario, ma universalmente accettato) La tabella finale dell’ANOVA può essere ottenuta in R utilizzando la seguente funzione. anova(mod) ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(treat) 4 4013.6 1003.41 505.63 &lt; 2.2e-16 *** ## Residuals 35 69.5 1.98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ovviamente, è necessario ricordare che tutte le considerazioni fatte finora sono valide se il dataset è conforme alle assunzioni di base per l’anova, cioè normalità dei residui e omogeneità delle varianze. In questo caso sappiamo che è vero, in generale bisogna eseguire i necessari controlli, di cui parleremo in un prossimo capitolo. 10.5 Per approfondimenti Kuehl, R. O., 2000. Design of experiments: statistical principles of research design and analysis. Duxbury Press (CHAPTER 2) "],
["la-regressione-lineare-semplice.html", "Capitolo 11 La regressione lineare semplice 11.1 Introduzione 11.2 Esempio 11.3 Stima dei parametri 11.4 Valutazione della bontà del modello 11.5 Previsioni", " Capitolo 11 La regressione lineare semplice 11.1 Introduzione Nella sperimentazione agronomica e biologica in genere è normale organizzare prove sperimentali replicate, anche per studiare l’effetto di una fattore quantitativo su una variabile dipendente, anch’essa quantitativa. In questa situazione, l’impiego di metodiche di confronto multiplo, se non del tutto errato, è comunque da considerare ‘improprio’. Infatti, l’inclusione di alcuni particolari livelli della variabile indipendente è frutto solo delle esigenze organizzative, senza alcune interesse particolare per lo sperimentatore, che è invece interessato a capire come cresce/decresce/varia la Y (variabile dipendente) in funzione della X (variabile indipendente). In sostanza lo sperimentatore è interessato a definire una funzione di risposta per tutto il range delle X, non a confrontare le risposte a due particolari livelli di X. 11.2 Esempio Per meglio comprendere la componente stocastica degli esperimenti, immaginiamo di conoscere la legge deterministica che definisce la risposta del frumento alla concimazione azotata. In particolare, immaginiamo che questa risposta produttiva sia fondamentalmente lineare: \\[Y_E = b_0 + b_1 X\\] ed immaginiamo che, senza concimazione (X = 0), la produzione sia pari a 25 q/ha (\\(b_0\\) = 25). Immaginiamo che l’incremente produttivo per kg di azoto somministrato sia pari a 0.15 q/ha (\\(b_1\\) = 0.15). Per individuare questa legge naturale organizziamo un esperimento, con quattro dosi di azoto e quattro repliche. In questo esperimento, come in tutti gli esperimenti, agirà anche una componente stocastica, che in qualche modo sposterà la risposta osservata dalla risposta attesa: \\[Y_o = b_0 + b_1 X + \\varepsilon \\quad \\textrm{con} \\quad \\varepsilon \\sim N(0, \\sigma)\\] Si assume che la componente stocastica \\(\\varepsilon\\) sia distribuita normalmente, come media 0 e deviazione standard \\(\\sigma\\), che immaginiamo essere pari a 2.5 q/ha. Su questa base, generiamo i dati osservati. set.seed(1234) Dose &lt;- rep(c(0, 60, 120, 180), each=4) Yield_E &lt;- 25 + 0.15 * Dose epsilon &lt;- rnorm(16, 0, 2.5) Yield &lt;- Yield_E + epsilon A questo punto scordiamoci la verità ‘vera’, che non è normalmente nota, e iniziamo l’analisi dei risultati dell’esperimento. Trattandosi di una prova replicata, si inizia, come al solito, con l’ANOVA, che porta al seguente risultato: model &lt;- lm(Yield ~ factor(Dose)) anova(model) ## Analysis of Variance Table ## ## Response: Yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(Dose) 3 2077.43 692.48 151.77 8.315e-10 *** ## Residuals 12 54.75 4.56 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Osserviamo che l’effetto del trattamento è significativo e il SEM è pari a \\(\\sqrt{5.10/4} = 1.129\\). Prima di proseguire, verifichiamo che non ci siano problemi relativi alle assunzioni parametriche di base e che, quindi, la trasformazione dei dati non sia necessaria. Il grafico dei residui, riportato di seguito, mostra che non vi sono patologie rilevanti. par(mfrow=c(2,2)) plot(model) Per maggior sicurezza, possiamo anche eseguire test statistici di verifica dell’omogeneità delle varianze (test di Bartlett e test di Levene) o della normalità dei residui (test di Shapiro-Wilks) bartlett.test(Yield ~ factor(Dose)) ## ## Bartlett test of homogeneity of variances ## ## data: Yield by factor(Dose) ## Bartlett&#39;s K-squared = 2.0084, df = 3, p-value = 0.5707 car::leveneTest(Yield ~ factor(Dose)) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 0.4075 0.7504 ## 12 shapiro.test(residuals(model)) ## ## Shapiro-Wilk normality test ## ## data: residuals(model) ## W = 0.97755, p-value = 0.9411 Ovviamente, nessuno di questi strumenti diagnostici rileva problemi con le assunzioni di base. In questo caso è ovvio, perché abbiamo generato noi i dati sotto queste assunzioni. Normalmente questo è invece un passaggio fondamentale. Da questo momento in avanti, l’analisi non prosegue con un test di confronto multiplo (infatti quale senso avrebbe confrontare tra loro le risposte a N0, N60, N120 e così via?), ma con una analisi di regressione. 11.3 Stima dei parametri Dobbiamo a questo punto individuare i parametri \\(b_0\\) e \\(b_1\\) in modo tale che la retta ottenuta sia la più vicina ai dati, cioè in modo da minimizzare gli scostamenti tra i valori di produzione osservati e quelli stimati dal modello (soluzione dei minimi quadrati). La funzione dei minimi quadrati è: \\[\\begin{array}{l} Q = \\sum\\limits_{i = }^N {\\left( {{Y_i} - \\hat Y} \\right)^2 = \\sum\\limits_{i = }^N {{{\\left( {{Y_i} - {b_0} - {b_1}{X_i}} \\right)}^2}} = } \\\\ = \\sum\\limits_{i = }^N {\\left( {Y_i^2 + b_0^2 + b_1^2X_i^2 - 2{Y_i}{b_0} - 2{Y_i}{b_1}{X_i} + 2{b_0}{b_1}{X_i}} \\right)} = \\\\ = \\sum\\limits_{i = }^N {Y_i^2 + Nb_0^2 + b_1^2\\sum\\limits_{i = }^N {X_i^2 - 2{b_0}\\sum\\limits_{i = }^N {Y_i^2 - 2{b_1}\\sum\\limits_{i = }^N {{X_i}{Y_i} + } } } } 2{b_0}{b_1}\\sum\\limits_{i = }^N {{X_i}} \\end{array}\\] Calcolando le derivate parziali rispetto a \\(b_0\\) e \\(b_1\\) che, al momento, sono le nostre incognite, ed eguagliandole a 0 si ottengono le seguenti formule risolutive: \\[{b_1} = \\frac{{\\sum\\limits_{i = 1}^N {\\left[ {\\left( {{X_i} - {\\mu _X}} \\right)\\left( {{Y_i} - {\\mu _Y}} \\right)} \\right]} }}{{\\sum\\limits_{i = 1}^N {{{\\left( {{X_i} - {\\mu _X}} \\right)}^2}} }}\\] e \\[{b_0} = {\\mu _Y} - {b_1}{\\mu _X}\\] Invece che svolgere i calcoli a mano, possiamo eseguire il fitting ai minimi quadrati con R. modelReg &lt;- lm(Yield ~ Dose) summary(modelReg) ## ## Call: ## lm(formula = Yield ~ Dose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4535 -1.0994 -0.3983 0.8945 3.8486 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.111800 0.848832 27.23 1.59e-13 *** ## Dose 0.169744 0.007562 22.45 2.24e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.029 on 14 degrees of freedom ## Multiple R-squared: 0.973, Adjusted R-squared: 0.971 ## F-statistic: 503.9 on 1 and 14 DF, p-value: 2.237e-12 Vediamo che le stime ottenute, correlate dei loro errori standard, corrispondono bene con la verità ‘vera’ che avevamo costruito. 11.4 Valutazione della bontà del modello 11.4.1 Valutazione grafica Abbiamo già valutato la validità delle assunzioni di base per i modelli lineari, in sede di ANOVA. Ora è necessario valutare se i dati osservati sono funzione della variabile esplicativa attraverso il modello dato più l’eventuale errore casuale, senza nessuna componente sistematica di mancanza d’adattamento. Questo può essere fatto, nel modo più semplice, attraverso un grafico dei valori attesi e dei valori osservati, come quello sottostante. plot(Yield ~ Dose) abline(modelReg, lty=2) 11.4.2 Errori standard dei parametri In secondo luogo, possiamo valutare gli errori standard delle stime dei parametri, che non debbono mai essere superiori alla metà del valore del parametro stimato, cosa che in questo caso è pienamente verificata. Se così non fosse, l’intervallo di confidenza del parametro conterrebbe lo zero, il che equivarebbe a dire che, ad esempio, la pendenza ‘vera’ (cioè quella della popolazione) potrebbe essere nulla. In altre parole, la retta potrebbe quindi essere ‘piatta’, dimostrando l’inesistenza di relazione tra la X e la Y. 11.4.3 Test F per la mancanza d’adattamento In terzo luogo, possiamo analizzare i residui della regressione, cioè gli scostamenti dei punti rispetto alla retta e, in particolare, la somma dei loro quadrati. Possiamo vedere che questo valore è pari a 70.38: anova(modelReg) ## Analysis of Variance Table ## ## Response: Yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Dose 1 2074.54 2074.54 503.87 2.237e-12 *** ## Residuals 14 57.64 4.12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Possiamo notare che l’errore della regressione è più alto di quello dell’analisi della varianza, dato che il residuo dell’ANOVA contiene solo la misura dello scostamento di ogni dato rispetto alla media del suo gruppo, che si può considerare ‘errore puro’, mentre il residuo della regressione, oltre all’errore puro, contiene anche una componente detta ‘mancanza d’adattamento’ (lack of fit), misurabile con lo scostamento di ogni media dalla linea di regressione. In effetti, la regressione lineare è solo un’approssimazione della reale relazione biologica tra la concimazione e la produzione del frumento. La devianza dovuta a mancanza d’adattamento puà essere quantificata per differenza: \\[\\textrm{Lack of fit} = 70.38 - 61.23 = 9.153\\] Se consideriamo i gradi di libertà, la devianza totale ne ha 15 (numero dei dati - 1), la devianza del residuo della regressione ne ha 14 (penultima riga a destra nell’output di REGR.LIN, pari al numero dei dati - il numero dei parametri stimati), l’errore sperimentale puro ne ha 12 (vedi l’ANOVA), il lack of fit ne ha quindi 14 - 12 = 2. Possiamo testare la significanza del lack of fit, utilizzando un test di F: se questo è significativo allora la componente di mancanza d’adattamento non è trascurabile, ed il modello di regressione dovrebbe essere rifiutato. L’espressione è: \\[ F_{lack} = \\frac{\\frac{RSS_r - RSS_a}{DF_r-DF_a} } {\\frac{RSS_a}{DF_a}} = \\frac{MS_{lack}}{MSE_a}\\] In R, cioò equivale a confrontare i due modelli: ANOVA e regressione, con la funzione anova(). anova(modelReg, model) ## Analysis of Variance Table ## ## Model 1: Yield ~ Dose ## Model 2: Yield ~ factor(Dose) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 14 57.641 ## 2 12 54.751 2 2.8899 0.3167 0.7345 Vediamo che il test di F non è significativo. Ciò supporta l’idea che non vi sia mancanza d’adattamento e quindi la regressione fornisca una descrizione adeguata dei dati sperimentali. 11.4.4 Test F per la bontà di adattamento e coefficiente di determinazione Possiamo considerare la varianza spiegata dalla regressione, che può essere confrontata con l’errore puro (appunto dato dalla varianza del residuo dell’ANOVA) tramite test F: F &lt;- anova(modelReg)[1,3]/anova(model)[2,3] df(F, 1, 12) ## [1] 8.493063e-13 Vediamo che in questo caso l’ipotesi nulla deve essere rifiutata: la varianza spiegata dalla regressione è significativamente maggiore di quella del residuo. Più frequentemente, la devianza spiegata dalla regressione viene rapportata alla devianza totale, per individuare quanta parte della variabilità dei dati è spiegata dal modello prescelto. Questa proporzione definisce il cosidetto coefficente di determinazione o \\(R^2\\). La devianza totale, in questo caso è: var(Yield)*15 ## [1] 2132.177 \\[R^2 = \\frac{SS_{reg}}{SS_{tot}} = \\frac{1716.83}{1787.21} = 0.961\\] Questa statistica varia da 0 ad 1 e la regressione è tanto migliore quanto più essa si avvicina ad 1. In realtà il coefficiente di determinazione è visibile nell’output della funzione summary() applicata all’oggetto modelReg (vedi più sopra). 11.5 Previsioni Dato che il modello ha mostrato di funzionare bene, con prudenza possiamo utilizzarlo per prevedere le produzioni a dosi intermedie, che non sono state incluse in prova. Ovviamente ci si deve mantenere entro il valore massimo e quello minimo inclusi in prova. In R, ciò è possibile utilizzando la funzione predict(). pred &lt;- predict(modelReg, newdata=data.frame(Dose=80), se=T) pred ## $fit ## 1 ## 36.69131 ## ## $se.fit ## [1] 0.5128794 ## ## $df ## [1] 14 ## ## $residual.scale ## [1] 2.029096 E’anche possibile effettuare la previsione inversa, cioè chiedere ai dati qual è la dose a cui corrisponde una produzione di 45 q/ha. In questo caso dobbiamo tener presente che l’equazione inversa è: \\[X = \\frac{Y - B_0}{b_1}\\] Per calcolare il risultato possiamo utilizzare la funzione deltaMethod(), nel package car, che ci calcola anche gli errori standard con il metodo della propagazione degli errori: car::deltaMethod(modelReg, &quot;(45 - b0)/b1&quot;, parameterNames=c(&quot;b0&quot;, &quot;b1&quot;)) ## Estimate SE 2.5 % 97.5 % ## (45 - b0)/b1 128.9484 3.455662 122.1754 135.7213 "]
]
