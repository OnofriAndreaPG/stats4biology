[["index.html", "Experimental methods in agriculture Introduction Aims How this book is organised Statistical software The authors", " Experimental methods in agriculture Andrea Onofri and Dario Sacco Update: v. 0.9 (2021-10-06), compil. 2021-10-20 Introduction This is the website for the book “Experimental methods in agriculture” and deals with the organisation of experiments and data analyses in agriculture and, more generally, in biology. Experiments are the key element to scientific progress and they need to be designed in a way that reliable data is produced. Once this fundamental requirement has been fullfilled, statistics can be used to summarise and explore the results, separating ‘signal’ from ‘noise’ and reaching appropriate conclusions. In this book, we will try to give some essential information to support the adoption of good research practices, with particular reference to field experiments, which are used to compare, e.g., innovative genotypes, agronomic practices, herbicides and other weed control methods. We firmly believe that the advancement of cropping techniques should always be based on the evidence produced by scientifically sound experiments. We will follow a ‘learn-by-doing’ approach, making use of several examples and case studies, while keeping theory and maths at a minimum level; indeed, we are talking to agronomists and biologists and not to statisticians! This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. It is written in RMarkdown with bookdown and it is rebuilt every now and then to incorporate corrections and updates. This is necessary, as R is a rapidly evolving language. Aims This book is not written aiming at completeness, but it is finely tuned for a 6 ECTS introductory course in biometry, for master or PhD students. It is mainly aimed at building solid foundations for starting a job in the research field and, eventually, to be able to tackle more advanced statistical material. How this book is organised The first two Chapters deal with the experimental design and explain how to distinguish good from bad experiments. One key aspect is that we can never be sure that data are totally reliable and, thus, we assume that they are reliable whenever we can be reasonably sure that they were obtained by using reliable methods. In Chapter 3 we learn how to describe the results, based on some simple stats, such as the mean, median, chi square value and Pearson correlation coefficient. In this chapter, we stick to the observed data, as if we were not interested in anything else. In chapter 4 we learn to see those observed data as the result of deterministic and stochastic processes, which we can describe by using statistical models. In Chapters 5 and 6 we start recognising that the observed data is only one random sample from a wider universe of data and that we are mainly interested on that universe, as we want to use our experiment to draw general conclusions. Going from a sample to a population introduces a certain amount of uncertainty which we have to incorporate into our conclusions. From Chapter 7 to Chapter 12 we deal with ANOVA, that is one of the most widely used techniques of data analysis, while the last two chapters deal with regression models. Within each chapter, we usually start with some motivating examples so that you can see the bigger picture, and then dive into the details. In the final chapter, we provide exercises for each book section, which should help you you practice what you’ve learned. Statistical software In this book, we will work through the examples using the R statistical software, together with the RStudio environment. We selected for a number of reasons: first of all we like it very much and we think that it is a pleasure to use it, once the initial difficulties have been overcame! Second, it is freeware, which is fundamental for the students. Third, in recent years the software skills of students in master programmes have notably increased and writing small chunks of code is no longer a problem for most of them. Last, but not least, we have seen that some experience with R is a very often required skill when applying for a job. Perhaps, we should say that we are very much indebted for the availability of those two wonderful pieces of free software. R is characterised by a modular structure and its basic functionalities can be widely extended by a set of add-in packages. As this is mainly an introductory course, we decided, as long as possible, to stick to the main packages, which come with the basic R installation. However, we could not avoid the use of a few very important packages, which we will indicate later on. We should also mention that this book was built by using the ‘bookdown’ package and it is hosted on the website blog ‘www.statforbiology.com,’ which is built by using the ‘blogdown’ package. We will not use these two packages during the course but we should mention that they were really useful. We recognise that R has a steep learning curve and we will start from the very beginning, without assuming that the students have any preliminary knowledge, either about statistics, or about R. The authors Andrea is Associate Professor at the Department of Agricultural, Food and Environmental Science, University of Perugia and he has taught ‘Experimental methods in Agriculture’ since 2000. Dario was Associate Professors at the Department of Agricultural, Forest and Food Sciences, University of Torino; he used to teach ‘Experimental Methods in Agriculture’ until 2020, when he suddenly died, far too early. Unfortunately, he could not see this book completed. "],["science-and-pseudoscience.html", "Chapter 1 Science and pseudoscience 1.1 Science needs data 1.2 Not all data support science 1.3 Good data is based on good ‘methods’ 1.4 The ‘falsification’ principle 1.5 Trying to falsify a result 1.6 The basic principles of experimental design 1.7 Invalid experiments 1.8 How can we assess whether the data is valid? 1.9 Conclusions 1.10 Further readings", " Chapter 1 Science and pseudoscience In the age of ‘information overload,’ we have plenty of knowledge at our fingertips. We can ‘google’ for a topic and our computer screen is filled with thousands of links, where we can find every piece of information we are looking for. However, one important question remains unanswered: which information is reliable and scientifically sound? We know by experience that the web is full of personal views, opinions, beliefs or, even worse, fake-news; we have nothing against opinions (although we would rather stay away from fake-news), but we need to be able to distinguish between subjective opinions and objective facts. Let’s refer to the body of reliable and objective knowledge by using the term ‘science,’ while all the rest is ‘non-science’ or ‘pseudoscience’; the question is: “How can we draw the line between science and pseudoscience?” It is a relevant question in these days, isn’t it? A theory, in itself, is not necessarily science. It may be well-substantiated, it can incorporate good laws and/or equations, it may come either from a brilliant intuition or from a meticulous research work; it may come from a common man or from a very authoritative scientist… it does not matter: theories do not necessarily represent objective facts. A few aphorisms can help us get to the point: Analogy cannot serve as proof (Pasteur) The interest I have in believing a thing is not a proof of the existence of that thing (Voltaire) A witty saying proves nothing (Voltaire) 1.1 Science needs data Theories need to proved. This fundamental innovation is usually attributed to Galileo Galilei (1564-1642), who is usually regarded as the founder of the scientific method, as summarised in Figure 1.1. Figure 1.1: The steps to scientific method Two aspects need attention: the fundamental role of scientific experiments, that produce data in support of pre-existing hypotheses (theories); once a theory has been supported by the data, it is regarded as acceptable until new data disproves it and/or supports an alternative, more reliable or simpler, theory. Indeed, data is the most important ingredient of science; a very famous aphorism says “In God we trust, all the others bring data.” It is usually attributed to the American statistician, engineer and professor W. Edwards Deming (1900-1993), although it is also attributed to Robert W. Hayden. Trevor Hastie, Robert Tibshirani and Jerome Friedman, the authors of the famous book ‘The Elements of Statistical Learning,’ mention that Professor Hayden told them that he cannot claim any credit for the above quote. A search on the web shows that there is no data confirming that W.E. Deming is the author of the above aphorism. Rather funny; I have just reported a sentence stating the importance of data in science, although it would appear that my attribution is just an unsupported theory! 1.2 Not all data support science Science is based on data, but we need to be careful: not all data can be trusted. In agriculture and, more generally, in biology and other quantitative sciences, we usually deal with measurable phenomena and, therefore, our data consists of a set of measurements of several types (we’ll come back to this in Chapter 2). For a number of reasons, each measure may not exactly reflect the true value and such a difference is usually known as experimental error. This final word (‘error’) should not mislead you: it does not necessarily mean that we are doing something wrong. On the contrary, errors are regarded as an unavoidable component of all experiments, injecting uncertainty in all the observed results. We can list three fundamental sources of uncertainty: measurement error subject-to-subject variability sampling error Measurement errors can be due, e.g., to: (i) uncalibrated instruments, (ii) incorrect measurement protocols, (iii) failures of the measuring devices, (iv) reading/writing errors and other inaccuracies relating to the experimenter’s work and (v) irregularities in the object being measured. In this latter respect, taking, e.g., the precise diameter of a melon is very difficult, as this fruit is not characterised by a regular spherical shape and, furthermore, the observed value is highly dependent on the point where the measurement is taken. Apart from measurement errors, there are other, less obvious, sources of uncertainty. In particular, we should keep into account that research studies in agriculture and biology need to consider a population of individuals; for instance, think that we have to measure the effect of a herbicide on a certain weed species, by assessing the weight reduction of treated plants. Clearly, we cannot just measure one plant, but we have to make a number of measurements on a population of weed plants. Even if we managed to avoid all measurement errors (which is nearly impossible), the observed values would always be different from one another, due to a more or less high degree of subject-to-subject variability. Such an uncertainty does not relate to any type of technical error, but it is an inherent component of the biological process under investigation. Subject-to-subject variability would not be, in itself, a big problem, if we could measure all individuals; unfortunately, populations are so big that we are obliged to measure a small sample and we can never be sure that the observed value for the sample matches the real value for the whole population. Of course, we should do our best to select a representative sample, but we already know that the ‘perfect sample’ does not exist and, in the end, we are always left with several dobts. Was our sample representative? Did we left out some important group of individuals? What will it happen, if we take another sample? In other words, uncertainty is an intrinsic and unavoidable component of all data and, therefore, how can we decide when the data are good enough to support science? 1.3 Good data is based on good ‘methods’ Uncertainty is produced by errors (sensu lato as we said above), but not all errors are created equal! In particular we need to distinguish systematic errors from random errors. Systematic errors tend to occur repeatedly with the same sign; for example, think about an uncalibrated scale, producing always a 20% weight overestimation: we can do as many measurements as we want, but the experimental error will be most often positive. Or, think about a technician, who is following a wrong measuring protocol. On the other hand, random errors relate to unknown, unpredictable and episodic events, producing repeated measures that are different from each other and from the true value. Due to such random nature, random errors have random signs; they may be positive or negative and, at least on the long run, they are expected to produce underestimations or overestimations with equal probability. It is easy to grasp that the consequences of those two types of errors are totally different. In this respect, it will be useful to consider two important traits of a certain body of data: precision accuracy The term precision is usually seen as the ability of discriminating small differences; in this sense, a standard ruler can measure lengths to the nearest millimetre and it is less precise than a calliper, that can measure lengths to the nearest 0.01 millimetre. However, in biometry, the term precision is more often used to mean low variability of results when measurements are repeated. The term accuracy has a totally different meaning and it refers to any possible differences between a measure and the corresponding ‘true’ value. The typical example is an uncalibrated instrument: the measures in themselves can be very precise, but they are inaccurate, because they do not correspond to the real value. We clearly understand that precision is important, but accuracy is fundamental; inaccurate data are said to be biased. Random errors result in imprecise data, but they do not necessary lead to biased data, as we can assume that repeating the measures for a reasonable amount of times should bring to a reliable estimate of the true unknown value (we will be back to this issue later). On the contrary, systematic errors lead to inaccurate (biased) data and wrong conclusions; therefore, they need to be avoided by any possible means, which must be the central objective of all experimental studies. In this sense, perfect instrument calibration and rigorous measurement and sampling protocols play a fundamental role, as we will see later. Unfortunately, inaccurate data and wrong conclusions are not uncommon in science; one of the most famous case was when the American scientists Martin Fleischmann and Stanley Pons, on 23 March 1989, published the results of an important experiment, claiming that they had produced a nuclear reaction at room temperature (cold fusion). Fleischmann and Pons’ announcement drew wide media attention (see Figure 1.2), but several scientists failed to reproduce the results in independent experiments. Later on, several flaws and sources of experimental error were discovered in the original experiment and most scientists considered cold fusion claims dead. Subsequently, cold fusion gained a reputation as pathological science and was marginalised by the wider scientific community, even though a minority of scientists is still investigating on that. Figure 1.2: Consequences of a wrong experiment, producing bad data. Apart from that famous example, we need to go back to our original question: how can we be sure that the data are accurate? The answer is simple: we can never be totally sure, but we should strive to apply research methods as rigorous as possible, so that we can be as sure as possible that the experiment is ‘valid’, i.e. that it does not contain any sources of systematic error (bias). In other words, good data come as the consequence of valid methods, which implies that a scientific proof is such not because we are certain that it corresponds to the truth, but because we are reasonably certain that it was obtained by using valid methods! 1.4 The ‘falsification’ principle The above approach has an important consequence: even if we have used a perfectly valid method and we have, therefore, produced a perfectly valid scientific proof, we can never be sure that we are right, because there could always be a future observation that says we are wrong. This is the basis of the ‘falsification theory,’ as defined by Karl Popper (1902 – 1994): we cannot demonstrate that our data are true, but we can only demonstrate that they are false. In practice, going back to the scientific process, we start from our hypothesis, we design a valid experiment and obtain valid data. In case this data does not appear to contradict the hypothesis, we conclude that such a hypothesis is true because it has not been falsified. The hypothesis is held as true until new valid data arise that falsify it: in this instance, the hypothesis is rejected and a new one is defined and submitted to the falsification process. The falsification theory has been very influential in science. I would like to highlight a few key-points. Science has nothing to do with truth or certainty. Science has a lot to do with uncertainty and we can never prove that a hypothesis is totally right. Therefore, we always organise experiments to reject hypothesis (i.e. to prove that they are false)! We need to use valid methods to ensure that random errors have been minimised, while systematic errors have been avoided as much as possible. The remaining uncertainty due to residual random errors need to be quantified by using the appropriate stats and displayed along with the results. Considering the residual uncertainty, we need to evaluate whether our data are good enough to falsify the original hypothesis. Otherwise, the experiment is inconclusive (but not necessarily the original hypothesis is true!) If we have two competing hypothesis and they are equally good, we select the simplest one (Occam’s razor principle) 1.5 Trying to falsify a result One aspect to be highlighted is that if I want to try and falsify a hypothesis which has been validated by a previous experiment, I need to organise a confirmatory experiment. In this frame, we need to distinguish between: replicability reproducibility An experiment is replicable when it gives the same results when repeated in the very same conditions. This explains why an accurate descriptions of materials and methods is fundamental to every scientific report: how could we repeat an experiment without knowing all detail about it? Unfortunately, field experiments in agriculture are very seldom replicable, due to the environmental conditions, which change unpredictably from one season to the other. Therefore, we can only try to demonstrate that an experiment is reproducible, that is to say that it gives similar results when it is repeated in different conditions. Of course, failing to reproduce the results of an experiment in different conditions does not necessarily negate the validity of the original results. This latter aspect is relevant. Think about Newton’s gravitation law, which has always worked very well to predict the motion of planets as well as objects on Earth. This law was falsified by Einstein’s studies, but it was not totally abandoned; indeed, within the limits of the conditions where it was proved, Newton’s laws are still valid and they are good enough to be used for relevant tasks, such as to plan the trajectory of rockets. 1.6 The basic principles of experimental design So far, we have seen that we need good data to express scientific claims and, to have good data, we need a valid experiment. A good methodology for designing experiments has been described by the English scientist Ronald A. Fisher (1890 - 1962). He graduated in 1912 and worked for six years as a statistician for the City of London, until he became a member of the Eugenics Education Society of Cambridge, founded in 1909 by Francis Galton, the cousin of Charles Darwin. After the end of the First World War, in 1919 he was offered a position at the Galton Laboratory at the University College of London, led by Karl Pearson, but he refused, due to profound rivalry with Pearson himself. Therefore, he begun working at the Rothamsted Experimental Station (Harpenden), where he was busy analysing the vast body of data that had accumulated starting from 1842. During this period, he invented the analysis of variance and defined the basis for valid experiments, publishing his results in the famous book “The design of experiments,” dating back to 1935. In summary, Fisher recognised that a valid experiment must adhere to three fundamental principles: control; replication; randomisation. 1.6.1 Control The term ‘control’ is very often mentioned in Fisher’s book, with a number of different meanings. Perhaps, the most convincing definition is given at the beginning of Chapter 6: ‘control’ consists of establishing controlled conditions, in which all factors except one can be held constant. We can slightly widen this definition by saying that there should not be any difference between experimental units, apart from those factors which are under investigation. The above definition sets the basis for what we call a comparative experiment; I will better explain this concept by using an example. Just assume that we have found a revolutionary fertiliser and we want to compare it with a traditional one: clearly, we cannot use the innovative fertiliser in one field and compare the observed yield with that obtained in the previous season with the traditional fertiliser. We all understand that, apart from the fertiliser, several environmental variables changed from one season to the other. A good controlled experiment would consist of using two field plots next to each other, with the same environmental condition, soil, crop genotype and everything else, apart from the fertiliser, which will be different for the two plots. In these conditions, the observed yield difference shall be reasonably attributed to the fertiliser. Apart from isolating the effect under study, a good control is exerted by using the greatest care to minimise the effects of all potential sources of experimental error. This may seem obvious, but putting it into practice may be overwhelming. Indeed, different types of experiments will require different types of techniques and the best to do to master those techniques is ‘learning by doing,’ preferably under the supervision of an expert technician. I will only underline three general aspects: methodological rigour accurate selection of experimental units avoiding intrusions Methodological rigor refers to the soundness or precision of a study in terms of planning, data collection, analysis, and reporting. It is obvious that, if we intend to study the degradation of a herbicide at 20°C we need an oven that is able to keep that temperature constant, the herbicide needs to be thoroughly mixed with the soil at the exact concentration and we need to use a well calibrated instrument as well as a correct protocol of analysis. However, we should never forget that there is a trade-off between methodological rigour/precision and the need for time and money resources, which is not independent from the aims of the experiment and the expected effect size. It is not necessary to attain a precision of 1 mL if we are determining the irrigation volume for maize! In relation to the selection of experimental units, good control practices would suggest that we select very homogeneous individuals; by doing so, error is minimised and precision is maximised. However, we need to be careful: subjects also need to reliably represent the population from where they were selected. For instance, if we want to assess the effect of a particular diet, we could select cows of the same age, same weight and same sex, so that the diet effect is isolated from all other possible confounding effects. If we do so, we will probably obtain a very high precision, but our results will not allow for any sound generalisations to caws of other ages, weights and sex. Again, there is a trade-off between the homogeneity of experimental subjects and the possibility of generalisation. Last, but not least, I would like to spend a few words about ‘intrusions,’ i.e. all the external events that occur and negatively impact on the experiment (e.g., drought, fungi attacks, aphids). Sometimes, these events are simply unpredictable and we will see that replication and randomisation (the other two principles of experimental design) are mainly meant to avoid that such intrusions produce systematic errors in our results. Some other times, these events are not totally unpredictable and they are named ‘demonic intrusions’ by Hurlbert (1984) in a very influential paper (as opposed to the unpredictable non-demonic intrusions). The aforementioned author reports an example relating to a study about fox predation. If fences are used to avoid the entrance of foxes, but hawks use those fences as perches from which to search for a pray, in the end, foxes may be held responsible for the predation exerted by hawks. Therefore, we end up confounding the effect of an intrusion with the effect under investigation. Hurlbert concludes “Whether such non-malevolent entities are regarded as demons or whether one simply attributes the problem to the experimenter’s lack of foresight and the inadequacy of procedural controls is a subjective matter. It will depend on whether we believe that a reasonably thoughtful experimenter should have been able to foresee the intrusion and taken steps to forestall it.” 1.6.2 Replication In the previous paragraph, we have set the basis of a comparative experiment, wherein two plots put totally in the same conditions are treated with two different fertilisers. Of course, this is not enough to guarantee that the experiment is valid. Indeed, no one would now dream of testing the response to a treatment by comparing two plots, one treated and the other one untreated (Fisher and Wishart, 1930; cited in Hurlbert, 1984). In every valid experiment, the measurements should be replicated in more than one experimental unit, while non-replicated experiments are usually invalid. We can list four main reasons for replication: demonstrate that the measure is replicable (that does not mean it is reproducible, though); ensure that any possible intrusions that affected one single experimental unit has not caused any relevant bias. Of course, the situation becomes troublesome if such an intrusion has affected all replicates! However, we will show that we can take care of this by using randomisation; assess the precision of the experiment, by measuring the variability among replicates; increase the precision of the experiment: the higher the number of replicates the higher the precision and the lower the uncertainty. The key issue about replication is that, to be valid, replicates must be truly independent, i.e. the whole manipulation process to allocate the treatment must have been independently applied to the different replicates. This must be clearly distinguished from pseudo-replication, where at least part of the manipulation has been contemporarily applied to all replicates (Figure 1.3). Figure 1.3: Schematic example of and invalid experiment, where pseudo-replication is committed Some typical examples of pseudo-replication are: (1) spraying a pot with five plants and measuring separately the weight of each plant, (2) treating one soil sample with one herbicide and making four measurements of concentration on four subsamples of the same soil, (3) collecting one soil sample from a field plot and repeating four times the same chemical analysis. In all the above cases, the treatments are applied only to one unit (pot or soil sample) and there are no true replicates, no matter how often the unit is sub-sampled. Clearly, if a random error is committed during the manipulation process, it carries over to all replicates and becomes a very dangerous systematic error. In some cases, pseudo-replication is less evident and less dangerous; for example, when we have to spray a number of replicates with the same herbicide solution, we should prepare different lots of the same solution and independently spray them onto the replicated plots. In practise, very often we only prepare one solution and spray it onto all the replicates, one after the other. Strictly speaking, this would not be correct, because the manipulation is not totally independent: if we made a mistake while preparing the solution (e.g. a wrong concentration), this would affect all replicates and would become a source of bias. However, such a practice is usually regarded as acceptable: if we sprayed the replicates independently, the amount of solution would too small to be precisely delivered by, e.g., a knapsack sprayer. As always, experience and common sense can be good guides to designing valid experiments. Apart from some specific circumstances, the general rule is that valid experiments require true-replicates and pseudo-replication should never be mistaken for true replication, even in the case of laboratory experiments (Morrison &amp; Morris, 2000). You will learn by experience the exceptions to this rule, but we prefer to sound rather prescriptive in this respect: it is not nice to have a paper rejected because we did not menage to convince the editor that our lack of true-replication should be regarded as justified! One common question is: how many replicates do we need? In this respect, we need to find a good balance between precision and costs: four replicates are usually employed in field experiments, although also three is a common value, when the effects are expected to be rather big and we have a small budget. A higher number of replicates is not very common, mainly because the size of the experiment becomes rather big and, consequently, soil variability increases as well. 1.6.3 Randomisation Control and true-replication, in themselves, do not guarantee that the experiment is valid. Indeed, some innate characteristics of experimental units or some random intrusion might systematically influence all replicates of one treatment, so that the effect of such disturbance is confounded with the effect of the experimental treatment. For example, think that we have a field with eight plots, along a positive gradient of fertility, as shown in Figure 1.4; if we treat the plots from 1 to 4 with the fertiliser A and the plots from 5 to 8 with the fertiliser B, a possible difference between the means for A and B might be wrongly attributed to the treatment effect, while it might be due to the innate difference in fertility (confounding effect). Figure 1.4: Example of lack of randomisation: the colours identify two different experimental treatments Randomisation is usually performed by way of random allocation of treatments to the experimental units, which is typical of manipulative experiments. In the case of field experiments, randomisation can also relate to random spatial and temporal dispersion, as we will see in the next Chapter. The allocation of treatments is not always possible, as it may sometimes be impractical, unethical or illegal. For example, if we want to assess the efficacy of seat belts, designing an experiment where people are sent out either with or without fastened seat belts is neither ethical nor legal. In this case, we can only record, retrospectively, the outcome of previous accidents. In this type of experiment we do not allocate the treatments, but we observe units that are ‘naturally’ treated (observational experiment); therefore, randomisation is obtained by random selection of individuals. As the result of using proper randomisation, all experimental units are equally likely to receive any type of disturbance/intrusion, so that the probability that all replicates of the same treatment are affected is minimal. Therefore, confounding the effect of a treatment with other types of systematic effects, if not impossible, is made highly unlikely. 1.7 Invalid experiments Let’s go back to the beginning of this chapter: how do we recognise real science from pseudo-science? By now, we should have learnt that reliable scientific information comes from data obtained in valid experiments. And we should have also learnt that experiments are valid if (and only if) they are properly controlled, replicated and randomised: the lack of any of these fundamental traits makes the results more or less unreliable and doubtful. In our experience as reviewers and statistical consultants, we have found several instances of invalid experiments. It is a pity: in such cases, the paper is rejected and there is hardly any remedy to a poorly designed experiment. The most frequent problems are: lack of good control ‘Confounding’ and spurious correlation Lack of true replicates and/or careless randomisation The consequences may be very different. 1.7.1 Lack of good control In some cases, experiments are not perfectly controlled. Perhaps, this statement is difficult to be interpreted, as no experiments can, indeed, be perfectly controlled: even if we have managed to totally avoid measurement errors, subject-to-subject variability and sampling variability can never be erased. Therefore, in terms of control, how do we draw the line between a valid and an invalid experiment? The suggestion is to carefully check whether the control was good enough not to impact on accuracy. If the experiment was only imprecise, the results do not loose their validity, although they may not be strong enough to reject our initial hypothesis. In other words, imprecise experiments are valid, but, very often, inconclusive. We say that they are not powerful. An experiment becomes invalid when there are reasons to suspect that the lack of good control impacted on data accuracy (wrong sampling, systematic errors, invalid or unacceptable methods). In this case, the experiment should be rejected, because it might be reporting measures that do not correspond to reality. 1.7.2 ‘Confounding’ and spurious correlation Reporting wrong measures is dangerous but confounding is even worse. It happens when the effect of some experimental treatments is confounded with other random or non-random systematic effects. The risk is particularly high with observational experiments. For example, if we observe the relative risk of death for individuals who were naturally exposed to a certain risk factor compared to individuals who were not exposed, the experimental outcome can be affected by several uncontrollable traits, such as sex, height, weight, age and so on. Therefore, we have a stimulus (exposure factor), a response (risk of death) and other external variables, which we call the ‘confounders.’ If one of the confounders is correlated both with the response and with the stimulus, a ‘spurious’ correlation may appear between the stimulus and the response, which does not reflect any real causal effects (Figure 1.5). Figure 1.5: Graphical representation of spurious correlation: an external confounder influences both the stimulus and the response, so that these two latter variables are correlated One typical example of spurious correlation has been found between the number of churches and the number of crimes in American cities. Such a correlation, in itself, does not prove that one factor (religiosity) causes the other one (crime); a thorough explanation should consider the existence of a possible confounder, such as the density of population: big cities have more inhabitants and, consequently, more churches and more crimes with respect to small cities. Accordingly, religiosity and crime are related to density and are related to each other, but such a relationship os only spurious. In the web, we can found a lot of other funny examples of spurious correlations, such as between the consumption of sour cream over years and the number of motorcycle riders killed in accidents (Figure 1.6). In this case, the lack of any scientific bases is clear; in other cases, it may be more difficult to spot. A very witty saying is: “correlation does not mean causation”; please, do not forget it! Figure 1.6: Esempio di correlazione spuria 1.7.3 Lack of true-replicates or careless randomisation Some issues that may lead to the immediate rejection of scientific papers are: there are pseudo-replicates, but no true-replicates true-replicates and pseudo-replicates are mistaken there is no randomisation randomisation was constrained, but the constraint has not been accounted for in statistical analyses. It is very useful to take a look at the classification made by Hurlbert (1984), which we present in Figure 1.7. Figure 1.7: Different types of randomisations, although they are not all correct (taken from: Hurlbert, 1984)! See the text for more explanations. It shows eight experimental subjects, to which two treatments (black and white) were allocated, by using eight different experimental designs. Design A1 is perfectly valid, as the four ‘white’ units and the four ‘black’ units were randomly selected. Design A2 is correct, although the randomisation was not complete; indeed, we divided the units in four groups and, within each group, we made a random selection of the ‘white’ and ‘black’ individual. This is an example of constrained randomisation, as the random selection of individuals is forced to take place within each group. We will see that such a constraint is correct, but it must be taken into account during the process of data analysis. Design A3 looks suspicious: there are true replicates, but treatments were not randomly, but systematically allocated to experimental units. Indeed, black units are always to the right of white units; what would happen in case of a latent right-to-left fertility gradient? Black units would be advantaged and the treatment effect and fertility effect would be confounded. Such suspect may lead to an invalid experiment. Systematic allocation of treatments may be permitted in some instances, when a spatial-temporal sequence has to be evaluated. For example: when we have four trees and we want to compare the yield of a low branch with the yield of a high branch. Clearly, low and high branches are systematically ordered and cannot be randomised; when we need to compare herbicide treatments in two timings (e.g., pre-emergence and post-emergence); clearly, one timing is always before the other one; when we need to compare the amount of herbicide residuals at two different depths, which are always ordered along the soil profile. In those conditions, the experiment is valid even when the randomisation follows a systematic pattern. Design B1 is usually invalid: there is no randomisation and the systematic allocation of treatments creates the segregation of units, which are not interspersed. The treatment effect can be easily confounded with any possible location effects (right vs. left). Also for design B1, there are a few exceptions were such a design could be regarded as valid, e.g., when we want to compare two locations, two regions, two fields, two lakes or any other physically parted conditions. Such location effects need to be evaluated with great care, as we are rarely in the condition of clearly attributing the effect to a specific agronomic factor. For example, two locations can give different yields because of different soil, different weather, different sowing times and so on. Design B2 and B3 are analogous to B1, even though the location effects are usually bigger. Isolative segregation is typical of growth chamber experiments; for example, we can use such a design to compare the germination capability at two different temperatures, by using two different ovens. In this case the temperature effect is totally confounded with the oven effect; it may not be problem in case the two ovens are very similar, but it is clear that any malfunctioning of one of the two ovens will be confounded with the treatment effect. Furthermore, the different replicates in one oven are not, indeed, true replicates, because the temperature treatment is not independently allocated (pseudo-replicates). Design B4 is a general example of pseudo-replication, where replicates are inter-dependent; we have already given other examples in the previous paragraphs. Designs lacking true-replicates are generally invalid, unless true-replicates are also included. For example, if we have four ovens that are randomly allocated to two temperatures (two ovens per temperature) and we have four Petri dishes per oven, there are two true-replicates and four pseudo-replicates per replicate. The design is valid, although we should keep true-replicates and pseudo-replicates clearly parted during data analysis, i.e. we should not behave as if we had 4 \\(\\times\\) 2 = 8 true-replicates! Design B5 is clearly invalid, due to total lack of replication. 1.8 How can we assess whether the data is valid? As we said, we have to check the methods. However, in everyday life this is very seldom possible. Indeed, we may not be expert enough to spot possible shortcomings and, above all, the methods may not detailed in newspapers and magazines, which limit themselves to reporting the result. What can we do, then? The answer is simple: we have to carefully check the sources. Authoritative scientific journals publish manuscripts only after a process of ‘peer review.’ In practise, the submitted manuscript is managed by the handling editor, who reads the paper and sends it to one to three widely renowned experts (reviewers). The editor and reviewers carefully inspect the paper and decide whether it can be published either as it is, or after revision, or, on the other hand, it should be rejected. After this process, we can be reasonably sure that the results are reliable and there are no important shortcomings that would make the experiment invalid. The peer review process is rather demanding for authors and it may require months and two-three reviews before the paper is accepted. We have found a nice picture at scienceBlog.com, which summarises rather well the feelings of a scientists during the reviewing process (Figure 1.8). Figure 1.8: The peer review process 1.9 Conclusions In the end, we can go back to our initial question: “How can we draw the line between science and pseudoscience?” The answer is that science is based on reliable data, obtained in valid scientific experiments, wherein every possibile source of systematic errors and confounding has been properly controlled and minimised. In particular, we have seen that every valid experiment should adhere to three fundamental principles, i.e. control, replication and randomisation. In practice, making sure that the methods were appropriate may require a specific expertise in a certain research field. Therefore, our ‘take-home message’ is: unless we are particularly expert in a given subject, we should always make sure that the results were published in authoritative journals and selected by a thorough peer review process. 1.10 Further readings Fisher, Ronald A. (1971) [1935]. The Design of Experiments (9th ed.). Macmillan. ISBN 0-02-844690-9. Hurlbert, S., 1984. Pseudoreplication and the design of ecological experiments. Ecological Monographs, 54, 187-211 Kuehl, R. O., 2000. Design of experiments: statistical principles of research design and analysis. Duxbury Press (CHAPTER 1) Morrison, D.A. and Morris, E.C., 2000. Pseudoreplication in experimental designs for the manipulation of seed germination treatments. Austral Ecology 25, 292–296. Wollaston V., 2014. Does sour cream cause bike accidents? No, but it looks like it does: Hilarious graphs reveal how statistics can create false connections. Published at: https://www.dailymail.co.uk/sciencetech/article-2640550/Does-sour-cream-cause-bike-accidents-No-looks-like-does-Graphs-reveal-statistics-produce-false-connections.html. Date of last access: 03/09/2020. "],["designing-experiments.html", "Chapter 2 Designing experiments 2.1 The elements of research 2.2 Hypothesis and objectives 2.3 The experimental treatments 2.4 The experimental units 2.5 The allocation of treatments 2.6 The variables 2.7 Setting up a field experiment 2.8 Conclusions 2.9 Further readings", " Chapter 2 Designing experiments 2.1 The elements of research In the previous chapter we have seen that every valid experiment should adhere to three fundamental principles, i.e. control, replication and randomisation. You may wonder: how do we put such principles into practice? Of course, there is not an easy and general answer: setting up a good experiment is mainly a matter of experience and the tuition of an experienced colleague is essential, especially while moving the first steps in the research world. In this chapter, we will focus on some common elements that we need to care about for all experiments, of any type. These elements are: the hypothesis and objectives; the experimental treatments; the experimental units; the allocation of treatments to units; the response variables. All detail about those elements need to be clearly given at the beginning of every good research project, report or scientific manuscript. 2.2 Hypothesis and objectives The Galilean process of research starts from a well founded hypothesis, i.e. a predictive statement about the possible outcome of a certain biological system. Such an hypothesis is usually based on an accurate review of literature information and, possibly, on a set of preliminary experiments. It must be: relevant; clearly defined; specific; testable. A well set hypothesis leads naturally to the definition of the objectives of the experiment, which must be: realistic; achievable; measurable; time constrained. Objectives should always be phrased in such a way that it is possible to exactly identify the moment when they have been achieved. For complex research projects, involving more than one experiment, it may be useful to define a general objective and several specific objectives, organised in successive phases, so that it is easy to check the progress of the research study and to revise the time schedule, in case some unexpected problems arise. Unclear objectives may lead to inefficient research, wherein unnecessary data are collected, while relevant observations are left out. 2.3 The experimental treatments Once the objectives are clear, we need to define the experimental ‘stimuli’ that will be allocated to the experimental units. A set of related ‘stimuli’ is called the experimental factor; for example, if we want to compare the genotypes A, B and C, we have the genotype factor with three levels. If we have one factor with a unique level, we usually talk about mensurative experiment, otherwise, we talk about comparative experiment, which is, by far, the most common situation. 2.3.1 Factorial experiments When we have two (or more) experimental factors, we could either make separate experiments, or we could make a factorial experiment, wherein we combine the levels of the two factors. This second solution is much more interesting, because we can assess possible interaction effects between the two factors (we will talk about this in Chapter 12). Factorial experiments may be planned in two different ways, i.e. they can be crossed or nested. In a crossed design, we have all possible combinations between the levels for all factors; for example, if we want to compare three sunflower genotypes (A, B and C) at two different nitrogen rates (N1 and N2), a crossed factorial experiment should include all the six combinations A-N1, A-N2, B-N1, B-N2, C-N1 and B-N2. Otherwise, in a nested design, the levels of one factor are different, depending on the level of the other factor; for example, if we want to compare organic farming and conventional farming by using the most suitable maize genotypes for each agricultural system, we should use a nested design. Recognising crossed and nested factorial designs is important, because the resulting data needs to be analysed in different ways. 2.3.2 The control Very often, comparative experiments need a suitable control or check level, which is used as the reference against which all other treatments are evaluated. We can include either: an untreated control, a control treated with a placebo, or a control treated with ordinary practices. For example, in a genotype experiment we usually include a reference genotype that is very widely grown in all nearby farms. For herbicide experiments, we always include an untreated control, which is fundamental to assess the composition of weed flora and quantify weed control efficacy for all herbicides under investigation. Furthermore, we can also include a weed-free control (usually hand-weeded) as the reference to evaluate possible symptoms of herbicide phytotoxicity to the crop. In toxicology, the untreated control may be replaced by a control treated with a placebo, i.e. a compound containing the same components of the experimental treatment, except the active ingredient. The placebo is usually necessary when: the experimental subject (usually a human) perceives its condition and reacts to the expectation about the efficacy of the chemical under investigation; the commercial formulation, apart from the active ingredient, contains other components, such as adjuvants, surfactants and other substances which may show some sort of biological effects. 2.4 The experimental units The experimental unit is the physical entity to which the treatment is allocated, e.g. a plant, a plot, an animal, a pot. In this respect, we need to be careful to clearly distinguish the experimental units from the observational units; indeed, we can allocate the treatment to a field plot and measure several plants therein or we can allocate the treatment to a tree and measure several leaves on that tree. A clear distinction between experimental units and observational units can help us avoid problems with pseudo-replication (see Chapter 1). The experimental units are always selected from a wider population of interest. For example, we select the plots from a field, the plants from a crop or some animals from a herd. A sample should be representative and homogeneous, although these may be two contrasting characteristics. Indeed, if we select very homogeneous individuals, we run the risk of getting a sample that no longer represents the whole population, but only a subset of it. For example, if our sample was composed by adult male bovines in good health, it may not necessarily represent a population composed also by females, young and diseased animals. Sampling a population of interest in a proper way may be a daunting task, especially in the social sciences. Several sampling protocols have been defined (e.g., random sampling, systematic sampling, stratified sampling, clustered sampling, convenience sampling, quota sampling, …), which are far beyond the scope of this book; you can read Daniel (2011) for a thorough explanation. In some cases, the process of sampling is less obvious, but that does not mean that there is no sampling. For example, in manipulative laboratory experiments, the experimental units are specifically prepared for each assay, such as the pots for a herbicide assay or the Petri dishes for a germination assay. Even if there is no real selection process, these units should be regarded as sampled from the wider population of pots or Petri dishes that we could have possibly prepared. 2.5 The allocation of treatments Unless we select experimental units that are ‘naturally treated’ (observational experiments; see Chapter 1), one central issue of every experiment is the technique we use to allocate the treatments. In general, following Fisher’s principles, we should pursue a completely randomised allocation, although, in some circumstances, it may be advantageous to put some constraints, as long as such constraints are not neglected during the process of data analysis. Constraints are very common in field experiments and we will see that they set the basis for the so-called experimental layout. In some cases, it is appropriate to hide some detail of the allocation process; for example, in medical research, it may be necessary that the subjects are not aware about which treatment they are going to receive (single-blind experiments), in order to avoid possible unconscious effects. In agriculture, it is often necessary that the researcher is not aware about which treatment was allocated to each unit, in order to avoid that the objectivity of visual and sensory assessments is undermined. If neither the subjects nor the researcher are aware about the treatment, we talk about double-blind experiments. 2.6 The variables At the end of an experiment we produce a set of data (dataset), which is composed by a collection of variables. These variables describe the experimental units in relation to some of their characteristics and we usually distinguish (i) response variables, (ii) factor variables and (iii) accessory variables. The response variables, obviously, describe the response of units to the experimental treatments (e.g., the yield, the weight, the height, and so on), while the factor variables describe the experimental stimulus allocated to each unit (e.g., the tillage method, fertilisation rate, genotype and so on). In some cases, we also record other accessory variables (or covariates), which measure potential confounding effects. For example, if we intend to study the yield of a number of trees, depending on how they are fertilised, the effect of tree age can act as a confounder. Therefore, if we cannot use trees of the same age, we can record the age as an accessory variable and use it to obtain a more reliable assessment of the fertilisation effect. It is useful to classify the variables depending on their characteristics, into nominal variables; ordinal variables; count/ratio variables; continuous variables. 2.6.1 Nominal variables Nominal variables are produced by assigning the subjects to a set of categories, such as dead/alive, germinated/ungerminated, red/blue/green, and so on. The categories can be two (binomial response) or more (multinomial response), they should not have any intrinsic ordering and should be mutually exclusive, in the sense that one individual can only belong to one category. With these variables, we can only count the number of individuals in each category (frequency), while other descriptive stats such as the mean and standard deviation are not used, at least not in the usual sense. 2.6.2 Ordinal variables Ordinal variables are similar to nominal variables, but the categories are intrinsically ordered. For example, we could ask a farmer to express his appreciation for a certain agronomic practice, by using five categories, VERY LOW, LOW, AVERAGE, HIGH and VERY HIGH. The categories are mutually exclusive and ordered by increasing appreciation; thanks to such an ordering, we can calculate both the frequency in each category and the cumulative frequency, which is obtained by summing the frequency in each category to the frequencies in all the preceding categories (see next Chapter). With ordinal variables, descriptive statistics such as the mean can, sometimes, be calculated, as long as the distance between the categories is clearly defined. 2.6.3 Count and ratio variables Sometimes the experimental units are characterised by some countable property; therefore, we can obtain a count for each unit and, consequently, a count variable. Please, note that this is different from a nominal/ordinal variable, where we count the units, not a specific trait in each single unit. For example, we obtain a count variable when we count the number of weeds in a plot, or the number of germinated seeds in a Petri dish, or the number of fruits per plant. When those counts have a predefined plateau, we can express them as relative to the plateau and obtain a ratio variable. For example, if we have ten seeds in a Petri dish, the count of germinated seeds may not exceed ten and it can be expressed as the proportion of germinated seeds. Both count and ratio data are discrete, in the sense that they can only take certain values (they are not continuous), but the mean and other descriptive stats can be be easily calculated with the usual method (see next Chapter). 2.6.4 Continuous variables Continuous variables can take any value within a certain interval, such as the weight, height, yield, time and so on. It could be argued that every measurement instrument is characterised by its own resolution, below which all measurements take the same value. Therefore we could say that all continuous variable are, in practice, discrete. However, we can neglect this detail, as long as the resolution is high enough for practical purposes. Continuous variables give a lot of information, although, in some instances, we may be interested in transforming them into ordinal variables, by using a classification procedure: we subdivide the range in classes (e.g. &lt; 50, 50-100, 100-150, &gt; 150) and count the number of individuals in each class. This is often useful for descriptive purposes with big data, as we will see in the following chapter. 2.6.5 Sensory and visual assessments In some instances, instead of measuring a certain trait of interest, we make visual or sensory assessments. For example, weed control ability and selectivity of herbicides can be assessed either by counting or weighing the survived weeds or by visual observations on a scale from 0 to 100% (or similar scales). Sensory and visual assessments are rather common and give several advantages, such as: low cost, high speed, no need for costly instruments, the possibility of disregarding the effect of external confounders. For example, when scoring the effect of an herbicide, an expert technician can easily distinguish phytotoxic effects from water stress damage and, thus, he can only consider the former effects, which would be impossible with objective weight measurements. Of course, there are also several disadvantages, such as: lower precision subjectivity we can be unconsciously influenced by knowing how the experimental unit has been treated it may be difficult to keep a uniform judgment parameter throughout the survey we need experience and training Sensory and visual data are largely acceptable in science, although their analysis may require some extra care and specific methods. Indeed, the resulting variable may resemble an ordinal variable (we assign one of a set of ordered categories), although the underlying scale is more or less continuous. 2.7 Setting up a field experiment Once all the elements of an experiment have been carefully planned, we must be laid down such an experiment in practice. The techniques greatly vary depending on the disciplines, aims, scales (climatic chamber, greenhouse, laboratory, field and so on) and it is very difficult to provide general information, apart from reinforcing the idea that all valid experiments must controlled, replicated and randomised, as detailed in the previous chapter. In this part, we will focus on the peculiar traits of field experiments, although most of the information relating to the experimental lay-out also applies to other types of experiments. 2.7.1 Selecting the field Field selection is perhaps the key aspect for a successful experiment. First of all, a research field must be close enough to roads, laboratories and other facilities, which will permit a timely execution of sampling and measurements. Secondly, we should not forget that there are countless reasons why a field experiment may turn out inconclusive, due to very wide environmental variability, relating to soil, weather, pests and so on. Therefore, at the onset of every experiment, we need to ensure that those sources of variability are kept to a minimum level, by selecting a very homogeneous field. We need to stay away from field parts with water stagnation, ditches, rows of trees and any other elements inducing an increase of variability in the behaviour of field crops. Knowing the history of the field may also be rather important. Some previous crops (e.g., alfalfa or other legumes) may leave excess fertility in soil, which is not good if, e.g., a N-fertilisation experiment is to be set-up. Likewise, herbicide trials may leave non-homogeneous infestation levels, due to the presence of untreated controls and other low efficacy herbicides. The history of a field is also important, for herbicide and pest management experiments, as we may be interested in having/avoiding a certain weed or pest in our field. If we suspect that there might be problems with soil heterogeneity, we should take some appropriate preliminary actions, such as growing a oat catch crop to remove excess soil nitrogen, growing alfalfa or other forage crops to suppress weed growth or perform deep ploughing to reduce the weed seed bank in the uppermost soil layer. In order to enhance crop homogeneity, small plot experiments (see later) should be managed by suitable machinery, that is optimised to work on small surfaces; furthermore, some interventions (such as sowing, weeding and fertilising) can also be performed by hand. A peculiar technique that is often used to obtain a homogeneous crop density is sowing at overly high density and thin out to optimal density a few days after crop emergence. 2.7.2 Selecting the units within the field Once we have selected a suitable field, we need to identify the experimental units. In this respect, we should distinguish: demonstrative on-farm trials small plot research trials Demonstrative trials usually represent the final stage of research and they are usually conducted on commercial farms under realistic environmental and management conditions, considering all the inherent variability of farming systems. The aim is usually to obtain a reliable validation of new products, managements and technologies at the farm level; therefore, the experimental unit is usually the strip, i.e. a long, rectangular piece of land, wherein the usual farming machinery (plough, planters, sprayers, combine harvester and so on) can be used. The number of treatments under comparison is low and, most often, one new management practice (e.g. crop management, crop protection, plant nutrition, and plant growth regulator) is compared to a local/farmer ‘control’ in two contiguous strips. Such pair (the new management practice and the control) is a replicate; normally we should have a minimum of three (more is better) replicates for capturing within-field variability. For the sake of simplicity, considering the size of strips, randomisation may be omitted, so that the design resembles the type A-3 in Figure 1.4 (see previous chapter). One possible lay-out is shown in Figure 2.1, where we have four fields with two strips each; in one field, the first strip is assigned to one treatment and the second strip is assigned to the other. Figure 2.1: The possible lay out of an on-farm trial, with four fields, two strips per field and a different treatment per strip (yellow and white) On-farm experiments are repeated across locations and growing seasons, so that we can have a better confidence in the selection of improved agronomic practices in new environments. On the other hand, small plot experiments are in the middle between on-farm and laboratory experiments: they are set up in the field, but the experimental unit is represented by a plot, i.e. a small piece of land, usually of 10 to 50 m\\(^2\\) surface (Figure 2.2). In small plot experiments we can keep a high degree of control for most confounding factors, while working in close-to-real conditions, which explains why this type of experiments is very widespread in the agricultural sciences. Of course, the observed yields in small plot experiments are usually 10-30% higher than the corresponding yields in on-farm conditions, due to more careful management of all cropping practices. Figure 2.2: A small plot experiment in the field (Ph. D. Alberati) Considering the shape, we usually prefer rectangular plots, where the width is equal to a multiple of the width of the available machinery for sowing and harvesting. Plot size must be big enough to accommodate a sufficiently high number of plants; for low density crops (e.g. maize), 20-40 m2 minimum are usually required, while for high density crops (e.g. wheat or alfalfa) 10-20 m2 may suffice. Smaller plots may not produce representative results, but, unless we are planning on-farm experiments, bigger plots can also be disadvantageous, as the plot-to-plot variability is increased. If we have a big field at our disposal, we might prefer to increase the number of replicates, instead of increasing the size of plots. When selecting plot shape and size we should consider the presence of border effects, that represent an important source of variability. Indeed, plant growing along the plot edges are not in the same conditions as plants in the middle of each plot; for example, they might be more vigorous and productive, because of the lack of competition on one side. Or, they might be affected by, e.g., the carry-over effects of fertilisers and herbicides across neighbouring plots. Border effects need to be minimised by restricting all measurements to the central rows of each plot, while the plants along the edges are omitted. This way, the surface area for harvest is smaller than the total plot surface area, which should be taken into account while designing the experiment. 2.7.3 Number of replicates For field experiments, the number of replicates is usually set to 3 to 5. A lower number of replicates is not to be recommended, because the experiment becomes very inefficient. On the other hand, a higher number of replicates increases the time and cost requirements and may result in increased soil variability and decreased precision. Once we have selected the number of replicates, the total number of plots is obtained as the product of the number of treatment levels and the number of replicates. 2.7.4 The field map The layout of a field experiment is usually planned in a map (field map), showing the lay-out of plots within the field. An example is shown in Figure 2.3, relating to an experiment with eight treatments and four replicates (32 plots, in total). In order to maximise the homogeneity, we have laid down the plots in eight vertical strips with four plots each. The plots are characterised by a rectangular shape and they are 8 m long and 2 m wide, which makes up a surface area of 16 m2. Around the experiment, we added 24 additional plots, in order to minimise border effects along the edges of the experiment. An arrow pointing towards the North is included, so that we can appropriately orient our map, during the field inspections. All plots are clearly identified by a univocal numbering/coding system. Figure 2.3: Example of a field map for an experiment with 32 plots 2.7.5 The experimental lay-out We can use the map to project the allocation of treatments to the units. While the basic principle of randomisation needs to always be followed, the experimental lay-out can be different, according to our organisational needs. The following lay-outs are very common in agriculture, although we will show that they can be used also in experiments of other types. 2.7.5.1 Completely randomised design (CR) With this design, treatments are allocated to plots in a completely randomised fashion, in strict accordance with Fisher’s rule. An example is shown in Figure 2.4, where we have allocated 8 treatments (the letters from A to H) with four replicates to the 32 plots in Figure 2.3. Figure 2.4: Example of an experiment laid down as a completely randomised design Such an approach is very simple and always correct, although it has the disadvantage that every possible systematic source of heterogeneity goes unnoticed. For example, let’s imagine that, for some reasons, the first three plot columns in Figure 2.4 (plots 1, 2, 3, 9, 10, 11, 17, 18, 19, 25, 26 and 27) are more fertile than all the other columns. In this case, the treatment G is favoured, because three out of four replicates are located in the most fertile part, while the treatment H is penalised, because only one replicate is in that most fertile part. Therefore, CRDs are very common in laboratory/greenhouse experiments or in field experiments characterised by a high degree of environmental, soil and crop homogeneity. 2.7.5.2 Randomised complete block design (RCBD) In RCBDs, the experimental units are divided into homogeneous groups with as many subjects as there are treatments to be allocated. The division is made according to some innate characteristic of subjects, such as age, sex, proximity; for field experiments, we usually exploit some expected fertility gradients. For example, should we expect a left-to-right fertility gradient for the plots in Figure 2.3, we could divide the experiment in four blocks with two plot columns each (8 plot per each block; block 1 would, e.g., contain the plots 1, 9, 17, 25, 2, 10, 18 e 26). Subsequently, we could randomly allocate the eight treatments to the plots in each block, so that there is one replicate per block. By doing so, no treatment should be penalised/favoured (Figure 2.5) Figure 2.5: Example of a completely randomised block design RCBD is the most common design for field experiments, although it can be used wherever the experimental units can be divided in groups, according to some innate property. In the following chapters we will see that the RCBD is very efficient when the variability across blocks is very big, as a big part of the subject-to-subject variability can be accounted for and removed from the unexplained variation. 2.7.5.3 Latin square design In some cases, the experimental units can be grouped according to two innate properties, apart from the experimental treatments. Figure 2.6 shows a design with 4 treatments and four replicates (16 plots in all); if we assume that there are a left-to-right and a bottom-to-top fertility gradients, we can look at the rows and columns as different blocking variables. Therefore, we can allocate the treatments to plots, so that there is one replicate in each row and in each column. Figure 2.6: Example of a latin square design with four treatments (A, B, C and D) and four replicates. The different colours help identify the four treatments and their allocation to the plots. Latin square designs are not only useful for field experiments. For example, if we want to test the effect of four different working protocols in the time required to accomplish a certain task, we can use a number of workers as the experimental units. In order to have four replicates, we need 16 workers, to which we allocate the different protocols, according to a CRD or CRBD. We can reduce the number of workers by allowing each worker to use all four protocols, in four subsequent turns. For example, the first worker can use the protocols A, B, C and D, one after the other in a randomised order. By doing so, we only need four workers and the experiment is designed as CRBD, where the worker acts as a blocking factor. The advantage is that possible worker-to-worker differences in proficiency are not confounded with differences between protocols, as all workers use all protocols. However, we should also consider that workers tend to get tired over time and loose proficiency and, therefore, the protocols used at the beginning of the sequence are favoured with respect to the protocols used later on. We can account for this effect by allocating the protocols in a way that each one is used in all turns; as the consequence, the turn acts as the second blocking factor, as shown in Figure 2.7. This is, indeed, a latin square design. Figure 2.7: Example of a latin square design for the comparison of four working protocols, by using four workers and four turns. The latin square takes its name from the fact that the number of replicates is equal to the number of treatments and, therefore, the field map consists of a square grid, where each treatment can be found in all rows and all columns (some of you may recognise the basic principle of the Sudoku game…). It is a useful design, as it can account for possible plot-to-plot differences in relation to two blocking factors (rows and columns, or workers and turns), so that the unexplained plot-to-plot differences are minimised. The disadvantage is that the number of replicates must be equal to the number of treatments and, therefore, the latin square can only be used for experiments with few treatments. 2.7.5.4 Split-plot and strip-plot designs With factorial experiments we can simply use a CRD or RCBD, by allocating the combinations of all factor levels to the different plots. For example, think about an experiment to compare three types of tillage (minimum tillage = MIN; shallow ploughing = SP; deep ploughing = DP) and two types of chemical weed control methods (broadcast = TOT; in-furrow = PART). With four replicates, the six treatment combinations (MIN-TOT, SP-TOT, DP-TOT, MIN-PART, SP-PART and DP-PART) can be allocated to 24 plots, according to a RCBD, as shown in Figure 2.8. Please note that we had to allow a wide space between the plots, in order to permit the circulation of tractors and tillage machinery. Figure 2.8: Field map for a two-factor factorial experiment, laid down as RCBD For those who have some knowledge with field research, it may be obvious that tillage treatments require big plots and a wide space between plots, due to the size of tillage machinery. On the contrary, spraying herbicides may be easily done also on small plots. Therefore, we could think of using big plots to allocate tillage treatments and splitting these big plots into two subplots, to allocate weed control treatments (split-plot design). The example is shown in Figure 2.9: we note that the allocation of tillage treatments to the 12 main-plots is done according to a RCBD, while the two weed control treatments are randomly allocated to the two sub-plots, within each main-plot. Figure 2.9: Same design as in the previous Figure, laid down as split-plot. An important consequence of split-plot designs is that every main-plot represents a replicate for sub-plot factor levels; indeed, if we look at Figure 2.9, we see that there are four replicates for each tillage level, but there are 12 replicated sub-plots for each weed control level. Therefore, subplot effects are estimated with higher precision. As all other designs, split-plot designs are not specific to agriculture experiments and they find their place in many other research topics. In general, they are used whenever: one factor require bigger experimental units, as in the above shown example; the levels for one factor are difficult to allocate and it is preferable to manipulate groups of experimental units, instead of a single independent experimental unit. For example, we might be interested in studying the corrosion resistance of steel bars treated with four coatings at three furnace temperatures. This latter factor is hard to change, as it takes a long time to reach a new equilibrium temperature within the furnace. Therefore, once the equilibrium temperature is reached, it is convenient to put four steel bars with each of the four coatings inside the furnace and record their corrosion. We repeat the process at the three temperatures and repeat the whole experiment twice. This is an example of a split-plot experiment, where temperatures are allocated to a furnace (main-plot) and coatings are allocated the steel bars (sub-plots). A useful variant of the split plot is used when the treatments are allocated in strips (strip-plot designs), as shown in Figure 2.10. This map refers to an experiment where three crops were sown 40 days after a herbicide treatment, in order to assess possible phytotoxicity effects relating to an excessive persistence of herbicide residues. We see that each block is organised with three rows and two columns: the three crops were sown along the rows and the two herbicide treatments (rimsulfuron and the untreated control) were allocated along the columns. The combinations are, consequently, allocated to subplots. In this design, we have three types of plots: the row-plots, the column-plots and the subplots; the advantage is that the allocation of treatments is rather quick. Figure 2.10: Same design as in the previous Figure, laid down as strip-plot. 2.8 Conclusions In this chapter we have seen the fundamental elements of a research and we have also seen how those elements, considering the three fundamental characteristics of control, replication and randomisation, can be joined together to set-up valid experiments in the field. We have also seen that the different types of designs are commonly used also for laboratory experiments or other types of experiments outside agriculture. 2.9 Further readings Cochran, W.G., Cox, G.M., 1950. Experimental design. John Wiley &amp; Sons, Inc., Books. Daniel, J. 2011. Sampling Essentials: Practical Guidelines for Making Sampling Choices. USA: SAGE. LeClerg, E.L., Leonard, W.H., Clark, A.G., 1962. Field Plot Technique. Burgess Publishing Company, Books. Jones, B., Nachtsheim, C.J., 2009. Split plot designs: what, why and how. Journal of Quality Technology 41, 340–361. "],["describing-the-observations.html", "Chapter 3 Describing the observations 3.1 Quantitative data 3.2 Nominal data 3.3 Descriptive stats with R 3.4 Graphical representations 3.5 Further reading", " Chapter 3 Describing the observations The final outcome of every manipulative/comparative experiment is a dataset, consisting of a set of measures/observations taken on several experimental subjects, in relation to one or more properties (e.g., height, weight, concentration, sex, color). We have seen that the list of values for one of those properties is called a variable; our first task is to describe that variable, by using the most appropriate descriptive stats. In this respect, the different types of variables (see Chapter 2) will require different approaches, as we will see in this chapter. 3.1 Quantitative data For a quantitative variable, we need to describe: location spread shape The three statistics respond, respectively, to the following questions: (1) where are the values located, along the measurement scale? (2) how close are the values to one another? (3) are the values symmetrically distributed around the central value, or are they skewed to the right or to the left? In this chapter, we will only consider the statistics of location and spread, as the statistics of shape are not commonly reported in agriculture and biology. 3.1.1 Statistics of location The most widely known statistic of location is the mean, that is obtained as the sum of data, divided by the number of values: \\[\\mu = \\frac{\\sum\\limits_{i = 1}^n x_i}{n}\\] For example, let us consider the following variable, listing the heights of four maize plants: \\(x = [178, 175, 158, 153]\\) The mean is easily calculated as: \\[\\mu = \\frac{178 + 175 + 158 + 153}{4} = 166\\] The mean can be regarded as the central value in terms of Euclidean distances; indeed, by definition, the sum of the Euclidean distances between the values and the group mean is always zero. In other words, the values above the mean and those below the mean, on average, are equally distant from the mean. That does not imply that the number of values above the mean is the same as the number of values below the mean. For example, if we look at the following values: 1 - 4 - 7 - 9 - 10 we see that the mean is 6.2. If we change the highest value into 100, the new mean is moved upwards to 24.2 and it is no longer in central positioning, with respect to the sorted list of data values. Another important statistic of location is the median, i.e. the central value in a sorted variable. The calculation is easy: first of all, we sort the values in increasing order. If the number of values is odd, the median is given by the value in the \\((n + 1)/2\\) position (\\(n\\) is the number of values). Otherwise, if the number of values is even, we take the two values in the \\(n/2\\) and \\(n/2 + 1\\) positions and average them. The median is always the central value in terms of positioning, i.e., the number of values above the median is always equal to the number of values below the median. For example if we take the same values as above (1 - 4 - 7 - 9 - 10), the median is equal to 7 and it is not affected when we change the highest value into 100. Considering that extreme values (very high or very low) are usually known as outliers, we say that the median is more robust than the mean with respect to outliers. 3.1.2 Statistics of spread Knowing the location of a variable is not enough for our purpose as we miss an important information: how close are the values to the mean? The simplest statistic to express the spread is the range, that is the difference between the highest and lowest value. This is a very rough indicator, though, as it is extremely sensitive to outliers. In the presence of a few outliers, the median is used as a statistic of location and, in that case, it can be associated, as a statistic of spread, to the interval defined by the 25th and 75th percentiles. In general, the percentiles, are the values below which a given percentage of observations falls. More specifically, the 25th percentile is the value below which 25% of the observations falls and the 75th percentile is the value below which 75% of the observations falls (you may have understood that the median corresponds to the 50th percentile). The interval between the 25th and 75th percentile, consequently, contains 50% of all the observed values and, therefore, it is a good statistic of spread. If we prefer to use the mean as a statistic of location, we can use several other important statistics od spread; the first one, in order of calculation, is the deviance, that is also known as the sum of squares. It is the sum of squared differences between each value and the mean: \\[SS = \\sum\\limits_{i = 1}^n {(x_i - \\mu)^2 }\\] In the above expression, the amounts \\(x_i - \\mu\\) (differences between each value and the group mean) are known as residuals. For our sample, the deviance is: \\[SS = \\left(178 - 166 \\right)^2 + \\left(175 - 166 \\right)^2 + \\left(158 - 166 \\right)^2 + \\left(153 - 166 \\right)^2= 458\\] A high deviance corresponds to a high spread; however, we can have a high deviance also when we have low spread and a lot of values. Therefore, the deviance should not be used to compare the spread of two groups with different sizes. Another problem with the deviance is that the measurement unit is also squared with respect to the mean: for our example, if the original variable (height) is measured in cm, the deviance is measured in cm2, which is not very logical. A second important measure of spread is the variance, that is usually obtained dividing the deviance by the number of observations minus one: \\[\\sigma^2 = \\frac{SS}{n - 1}\\] For our group: \\[\\sigma^2 = \\frac{458}{3} = 152.67\\] The variance can be used to compare the spread of two groups with different sizes, but the measurement unit is still squared, with respect to the original variable. The most important measure of spread is the standard deviation, that is the square root of the variance: \\[\\sigma = \\sqrt{\\sigma^2} = \\sqrt{152.67} = 12.36\\] The measurement unit is the same as the data and, for this reason, the standard deviation is the most important statistic of spread and it is usually associated to the mean to summarise a set of measurements. In particular, the interval \\(l = \\mu \\pm \\sigma\\) is often used to describe the absolute uncertainty of replicated measurements. Sometimes, the standard deviation is expressed as a percentage of the mean (coefficient of variability), which is often used to describe the relative uncertainty of measurement instruments: \\[CV = \\frac{\\sigma }{\\mu } \\times 100\\] 3.1.3 Summing the uncertainty In some cases, we measure two quantities and sum them to obtain a derived quantity. For example, we might have made replicated measurements to determine the sucrose content in a certain growth substrate, that was equal to \\(22 \\pm 2\\) (mean and standard deviation). Likewise, another independent set of measures showed that the fructose content in the same substrate was \\(14 \\pm 3\\). Total sugar content is equal to the sum of \\(22 + 14 = 36\\). The absolute uncertainty for the sum is given by the square root of the sum of the squared absolute uncertainties, that is \\(36 \\pm \\sqrt{4 + 9}\\). The absolute uncertainty for a difference is calculated in the very same way. 3.1.4 Relationship between quantitative variables Very frequently, we may have recorded, on each subject, two, or more, quantitative traits, so that, in the end, we have two, or more, response variables. We might be interested in assessing whether, for each pair of variables, when one changes, the other one changes, too (joint variation). The Pearson correlation coefficient is a measure of joint variation and it is equal to the codeviance of the two variables divided by the square root of the product of their deviances: \\[r = \\frac{ \\sum_{i=1}^{n}(x_i - \\mu_x)(y_i-\\mu_y) }{\\sqrt{\\sum_{i=1}^{n}(x_i-\\mu_x)^2 \\sum_{i=1}^{n}(y_i-\\mu_y)^2}}\\] We know about the deviance, already. The codeviance is a statistic that consists of the product of the residuals for the two variables: it is positive, when the residuals for the two variables have the same signs, otherwise it is negative. Consequently, the \\(r\\) coefficient ranges from \\(+1\\) to \\(-1\\): a value of \\(+1\\) implies that, when \\(x\\) increases, \\(y\\) increases by a proportional amount, so that the points on a scatterplot lie on a straight line, with positive slope. On the other hand, a value of \\(-1\\) implies that when \\(x\\) increases, \\(y\\) decreases by a proportional amount, so that the points on a scatterplot lie on a straight line, with negative slope. A value of 0 indicates that there is no joint variability, while intermediate values indicate a more or less high degree of joint variability, although the points on a scatterplot do not exactly lie on a straight line (Figure 3.1). Figure 3.1: Example of positive (left) and negative (right) correlation For example, if we have measured the oil content in sunflower seeds by using two different methods, we may be interested in describing the correlation between the results of the two methods. The observed data are shown in the box below. A &lt;- c(45, 47, 49, 51, 44, 37, 48, 42, 53) B &lt;- c(44, 44, 49, 53, 48, 34, 47, 39, 51) In order to calculate the correlation coefficient, we need to organise our calculations as follows: calculate the residuals for A (\\(z_A\\)) calculate the residuals for B (\\(z_B\\)) calculate the deviances and codeviance First of all, we calculate the two means, that are, respectively, 46.22 and 45.44. Secondly, we can calculate the residuals for both variables, as shown in Table 3.1. From the residuals, we can calculate the deviances and the codeviance, by using the equation above. Table 3.1: Example of the hand calculations that are used to calculate the correlation coefficient A B \\(z_A\\) \\(z_B\\) \\(z_A^2\\) \\(z_B^2\\) \\(z_A \\times z_B\\) 45 44 -1.222 -1.444 1.494 2.086 1.765 47 44 0.778 -1.444 0.605 2.086 -1.123 49 49 2.778 3.556 7.716 12.642 9.877 51 53 4.778 7.556 22.827 57.086 36.099 44 48 -2.222 2.556 4.938 6.531 -5.679 37 34 -9.222 -11.444 85.049 130.975 105.543 48 47 1.778 1.556 3.160 2.420 2.765 42 39 -4.222 -6.444 17.827 41.531 27.210 53 51 6.778 5.556 45.938 30.864 37.654 The deviances for \\(A\\) and \\(B\\) are, respectively, 189.55 and 286.22, while the codeviance is 214.11. Accordingly, the correlation coefficient is: \\[r = \\frac{214.11}{\\sqrt{189.55 \\times 286.22}} = 0.919\\] It is close to 1, so we conclude that there was quite a good agreement between the two methods. 3.2 Nominal data 3.2.1 Distributions of frequencies With nominal data, we can only assign the individuals to one of a number of categories. In the end, the only description we can give of such a dataset is based on the counts (absolute frequencies) of individuals in each category, producing the so called distribution of frequencies. As an example of nominal data we can take the ‘mtcars’ dataset, that was extracted from the 1974 Motor Trend US magazine and comprises 32 old automobiles. The dataset is available in R and we show part of it in table 3.2. Table 3.2: Dataset ‘mtcars’ in R, representing the characteristics of 32 old automobiles; ‘cs’ is the type of engine and ‘gear’ is the number of forward gears. More detail is given in the text. vs gear Mazda RX4 0 4 Mazda RX4 Wag 0 4 Datsun 710 1 4 Hornet 4 Drive 1 3 Hornet Sportabout 0 3 Valiant 1 3 Duster 360 0 3 Merc 240D 1 4 Merc 230 1 4 Merc 280 1 4 Merc 280C 1 4 Merc 450SE 0 3 Merc 450SL 0 3 Merc 450SLC 0 3 Cadillac Fleetwood 0 3 Lincoln Continental 0 3 Chrysler Imperial 0 3 Fiat 128 1 4 Honda Civic 1 4 Toyota Corolla 1 4 Toyota Corona 1 3 Dodge Challenger 0 3 AMC Javelin 0 3 Camaro Z28 0 3 Pontiac Firebird 0 3 Fiat X1-9 1 4 Porsche 914-2 0 5 Lotus Europa 1 5 Ford Pantera L 0 5 Ferrari Dino 0 5 Maserati Bora 0 5 Volvo 142E 1 4 The variable ‘vs’ in ‘mtcars’ takes the values 0 for V-shaped engine and 1 for straight engine. Obviously, the two values 0 and 1 are just used to name the two categories and the resulting variable is purely nominal. The absolute frequencies of cars in the two categories are, respectively 18 and 14 and they are easily obtained by a counting process. We can also calculate the relative frequencies, dividing the absolute frequencies by the total number of observations. These frequencies are, respectively, 0.5625 and 0.4375. If we consider a variable where the classes can be logically ordered, we can also calculate the cumulative frequencies, by summing up the frequency for one class with the frequencies for all previous classes. As an example we take the ‘gear’ variable in the ‘mtcars’ dataset, showing the number of forward gears for each car. We can easily see that 15 cars have 3 gears and 27 cars have 4 gears or less. In some circumstances, it may be convenient to ‘bin’ a continuous variable into a set of intervals. For example, if we have recorded the ages of a big group of people, we can divide the scale into intervals of five years (e.g., from 10 to 15, from 15 to 20 and so on) and, eventually, assign each individual to the appropriate age class. Such a technique is called binning or bucketing and we will see an example later on in this chapter. 3.2.2 Descriptive stats for distributions of frequencies For categorical data, we can retrieve the mode, which is the class with the highest frequency. For ordinal data, wherever distances between classes are meaningful, and for discrete data, we can also calculate the median and other percentiles, as well as the mean and other statistics of spread (e.g., variance, standard deviation). The mean is calculated as: \\[ \\mu = \\frac{\\sum\\limits_{i = 1}^n f_i x_i}{\\sum\\limits_{i = 1}^n f_i}\\] where \\(x_i\\) is the value for the i-th class, and \\(f_i\\) is the frequency for the same class. Likewise, the deviance, is calculated as: \\[ SS = \\sum\\limits_{i = 1}^n f_i (x_i - \\mu)^2 \\] For example, considerin the ‘gear’ variable in Table 3.2, the average number of forward gears is: \\[\\frac{ 15 \\times 3 + 12 \\times 4 + 5 \\times 5}{15 + 12 + 5} = 3.6875\\] while the deviance is: \\[SS = 15 \\times (3 - 3.6875)^2 + 12 \\times (4 - 3.6875)^2 + 5 \\times (5 - 3.l875)^2 = 16.875\\] With interval data (binned data), descriptive statistics should be calculated by using the raw data, if they are available. If they are not, we can use the frequency distribution obtained from binning, by assigning to each individual the central value of the interval class to which it belongs. As an example, we can consider the distribution of frequencies in Table 3.3, relating to the time (in minutes) taken to complete a statistic assignment for a group of students in biotechnology. We can see that the mean is equal to: \\[ \\frac{7.5 \\times 1 + 12.5 \\times 4 + 17.5 \\times 3 + 22.5 \\times 2}{10} = 15.5\\] Table 3.3: Distribution of frequency for the time (in minutes) taken to complete a statistic assignment for a group of students in biotechnology Time interval Central value Count 5 - 10 7.5 1 10 - 15 12.5 4 15 - 20 17.5 3 20 - 25 22.5 2 The calculation of the deviance is left as an exercise. 3.2.3 Contingency tables When we have more than one cataegorical variable, we can summarise the distribution of frequency by using two-way tables, usually known as contingency tables or crosstabs. For example, we can consider the ‘HairEyeColor’ dataset, in the ‘datasets’ package, which is part of the base R installation. It shows the contingency tables of hair and eye color in 592 statistics students, depending on sex; both characters are expressed in four classes, i.e. black, brown, red and blond hair and brown, blue, hazel and green eyes. Considering females, the contingency table is reported in Table 3.4 and it is augmented with row and column sums (see later). Table 3.4: Distribution of hair and eye color for 313 female statistics students, augmented with row and column sums. Dataset taken from R package ‘datasets’ Brown eye Blue eye Hazel eye Green eye ROW SUMS Black hair 36 9 5 2 52 Brown hair 66 34 29 14 143 Red hair 16 7 7 7 37 Blond hair 4 64 5 8 81 COLUMN SUMS 122 114 46 31 313 3.2.4 Independence With a contingency table, we may be interested in assessing whether the two variables show some sort of dependency relationship. In the previous example, is there any relationship between the color of the eyes and the color of the hair? If not, we say that the two variables are independent. Independency is assessed by using the \\(\\chi^2\\) statistic. As the first step, we need to calculate the marginal frequencies, i.e. the sums of frequencies by row and by column (please note that the entries of a contingency table are called joint frequencies). These sums are reported in Table 3.4. Let’s consider black hair: in total there are 52 women with black air, that is \\(52/313 \\times 100 = 16.6\\)% of the total. If the two characters were independent, the above proportion should not change, depending on the color of eyes. For example, we have 122 women with brown eyes and 16.6% of those should be black haired, which makes up an expected value of 20.26837 black haired and brown eyed women (much lower than the observed 36). Another example: the expected value of blue eyed and black haired women is \\(114 \\times 0.166 = 18.9\\) (much higher than the observed). A third example may be useful: in total, there is \\(143/313 = 45.7\\)% of brown haired women and, in case of independence, we would expect \\(46 \\times 0.457 = 21.02\\) brown haired and hazel eyed woman. Keeping on with the calculations, we could derive a table of expected frequency, in the case of complete independence between the two characters. All the expected values in case of independency are reported in Table 3.5. Table 3.5: Expected values of hair and eye color for 313 female statistics students, augmented with row and column sums. Expectations assume total lack of dependency between the two variables. Brown eye Blue eye Hazel eye Green eye ROW SUMS Black hair 20.26837 18.93930 7.642173 5.150160 52 Brown hair 55.73802 52.08307 21.015974 14.162939 143 Red hair 14.42173 13.47604 5.437700 3.664537 37 Blond hair 31.57189 29.50160 11.904153 8.022364 81 COLUMN SUMS 122.00000 114.00000 46.000000 31.000000 313 The observed (table 3.4) and expected (Table 3.5) values are different, which might indicate a some sort of relationship between the two variables; for example, having red hair might imply that we are more likely to have eyes of a certain color. In order to quantify the discrepancy between the two tables, we calculate the \\(\\chi^2\\) stat, that is: \\[\\chi ^2 = \\sum \\left[ \\frac{\\left( {f_o - f_e } \\right)^2 }{f_e } \\right]\\] where \\(f_o\\) are the observed frequencies and \\(f_e\\) are the expected frequencies. For example, for the first value we have: \\[\\chi^2_1 = \\left[ \\frac{\\left( {36 - 20.26837 } \\right)^2 }{20.26837 } \\right]\\] In all, we should calculate 16 ratios and sum them to each other. The final \\(\\chi^2\\) value should be equal to 0 in case of independence and it should increase as the relationship between the two variables increases, up to: \\[\\max \\chi ^2 = n \\cdot \\min (r - 1,\\,c - 1)\\] i.e. the product between the number of subjects (\\(n\\)) and the minimum value between the number of rows minus one and the number of columns minus one (in our case, it is \\(313 \\times 3 = 939\\)). The observed value is 106.66 and it suggests that the two variables are not independent. 3.3 Descriptive stats with R Before reading this part, please make sure that you already have some basic knowledge about the R environment. Otherwise, please go and read the Appendix 1 to this book. Relating to quantitative variables, we can use the dataset ‘heights.csv,’ that is available in an online repository and refers to the height of 20 maize plants. In R, the mean is calculated by the function mean(), as shown in the box below. filePath &lt;- &quot;https://www.casaonofri.it/_datasets/heights.csv&quot; dataset &lt;- read.csv(filePath, header = T) mean(dataset$height) ## [1] 164 The median is obtained by using the function median(): median(dataset$height) ## [1] 162.5 The other percentiles are calculated with the function quantile(), passing the selected probabilities as fractions in a vector: quantile(dataset$height, probs = c(0.25, 0.75)) ## 25% 75% ## 152.75 174.25 The deviance function is not immediately available in R and we should resort to using the following expression: sum( (dataset$height - mean(dataset$height))^2 ) ## [1] 4050 The other variability stats are straightforward to obtain, as well as the correlation coefficient: # Variance and standard deviation var(dataset$height) ## [1] 213.1579 sd(dataset$height) ## [1] 14.59993 # Coefficient of variability sd(dataset$height)/mean(dataset$height) * 100 ## [1] 8.902395 # Correlation cor(A, B) ## [1] 0.9192196 We have just listed some of the main stats that can be used to describe the properties of a quaantitative variable. In our research work we usually deal with several groups of observations, each one including the different replicates of one of a series of experimental treatments. Therefore, we need to be able to obtain the descriptive stats for all groups at the same time. The very basic method to do this, is by using the function tapply(), which takes three arguments, i.e. the vector of observations, the vector of groups and the function to be calculated by groups. The vector of groups is the typical accessory variable, which labels the observations according to the group they belong to. options(width = 60) dataset$var ## [1] &quot;N&quot; &quot;S&quot; &quot;V&quot; &quot;V&quot; &quot;C&quot; &quot;N&quot; &quot;C&quot; &quot;C&quot; &quot;V&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;S&quot; &quot;C&quot; ## [15] &quot;N&quot; &quot;C&quot; &quot;V&quot; &quot;S&quot; &quot;C&quot; &quot;C&quot; mu.height &lt;- tapply(dataset$height, dataset$var, FUN = mean) mu.height ## C N S V ## 165.00 164.00 160.00 165.25 Obviously, the argument FUN can be used to pass any other R function, such as median and sd. In particular, we can get the standard deviations by using the following code: sigma.height &lt;- tapply(dataset$height, dataset$var, sd) sigma.height ## C N S V ## 14.36431 16.19877 12.16553 19.51709 Now, we can combine the two newly created vectors into a summary dataframe. In the box below, we use the function data.frame() to combine the vector of group names and the two vectors of stats to create the ‘descStat’ dataframe, which is handy to create a table or a graph, as we will see later. descStat &lt;- data.frame(group = names(mu.height), mu = mu.height, sigma = sigma.height) descStat ## group mu sigma ## C C 165.00 14.36431 ## N N 164.00 16.19877 ## S S 160.00 12.16553 ## V V 165.25 19.51709 With nominal data, the absolute frequencies of individuals in the different classes can be retrieved by using the table() function, as we show below for the ‘vs’ variable in the ‘mtcars’ dataset. data(mtcars) table(mtcars$vs) ## ## 0 1 ## 18 14 We can also calculate the relative frequencies, dividing by the total number of observations. table(mtcars$vs)/length(mtcars$vs) ## ## 0 1 ## 0.5625 0.4375 Cumulative frequencies can be calculated by the cumsum() function, as shown below for the ‘gear’ variable in the ‘mtcars’ dataset. cumsum(table(mtcars$gear)) ## 3 4 5 ## 15 27 32 Ragarding to binning, we can consider the ‘co2’ dataset, that is included in the base R installation. It contains 468 values of CO_2_ atmospheric concentrations, expressed in parts per million, as observed at monthly intervals in the US. With such a big dataset, the mean and standard deviation are not sufficient to get a good feel for the data and it would be important to have an idea of the shape of the dataset. Therefore we can split the continuous scale into a series of intervals, from 310 ppm to 370 ppm, with breaks every 10 ppm and count the observations in each interval. In the box below, the function cut() assigns each value to the corresponding interval (please note the ‘breaks’ argument, which sets the margins of each interval. Intervals are, by default, left open and right-closed), while the function table() calculates the frequencies. data(co2) co2 &lt;- as.vector(co2) mean(co2) ## [1] 337.0535 min(co2) ## [1] 313.18 max(co2) ## [1] 366.84 # discretization classes &lt;- cut(co2, breaks = c(310,320,330,340,350,360,370)) freq &lt;- table(classes) freq ## classes ## (310,320] (320,330] (330,340] (340,350] (350,360] (360,370] ## 70 117 86 76 86 33 The table() function is also used to create contingency tables, with two (or more) classification factors. The resulting table represent a peculiar class, which is different from other tabular classes, such as arrays and dataframes. This class has methods on its own, as we will see below. In the case of the ‘HairEyeColor’ dataset, this is already defined as a contingency table of class ‘table.’ data(HairEyeColor) tab &lt;- HairEyeColor[,,2] class(tab) ## [1] &quot;table&quot; tab ## Eye ## Hair Brown Blue Hazel Green ## Black 36 9 5 2 ## Brown 66 34 29 14 ## Red 16 7 7 7 ## Blond 4 64 5 8 With such a class, we can calculate the \\(\\chi^2\\) value by using the summary() method, as shown in the box below. summary( tab ) ## Number of cases in table: 313 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 106.66, df = 9, p-value = 7.014e-19 ## Chi-squared approximation may be incorrect The above function returns several results, which we will examine in further detail in a following chapter. 3.4 Graphical representations Apart from tables, also graphs can be used to visualise our descriptive stats. Several types of graphs are possible, and we would like to mention a few possibilities. A barplot is very useful to visualise the properties of groups, e.g. their means or absolute frequencies. For example, if we consider the ‘descStat’ dataframe we have created above at 3.1.3, we could draw a barplot, where the height of bars indicate the mean for each group and we could augment such a barplot by adding error bars to represent the standard deviations (\\(\\mu \\pm \\sigma\\)). In the box below, we use the function barplot, which needs two arguments and an optional third one: the first one is the height of bars, the second one is the name of groups, the third one specifies the limits for the y-axis. We see that the function is used to return the object ‘coord,’ a vector including the abscissas for the central point of each bar. We can use this vector inside the function arrows() to superimpose the error bars (Figure 3.2); the first four arguments of the arrows() function are, respectively, the coordinates of points from which (abscissa and ordinate) and to which (abscissa and ordinate) to draw the error bars, while the other arguments permit to fine tune the type of arrow. coord &lt;- barplot(descStat$mu, names.arg = descStat$group, ylim = c(0, 200), ylab = &quot;Height (cm)&quot;) arrows(coord, descStat$mu - descStat$sigma, coord, descStat$mu + descStat$sigma, length = 0.05, angle = 90, code = 3) Figure 3.2: Example of a simple barplot in R The graph is rather basic, but, with little exercise, we can improve it very much. When the number of replicates is high (e.g., &gt; 15), we can jointly use the 25th, 50th (median) and 75th percentiles to draw the so-called boxplot (Box-Whisker plot; Figure 3.3). I will describe it by using an example: let’s assume we have made an experiment with three treatments (A, B and C) and 20 replicates. We can use the code below to draw a boxplot. rm(list = ls()) A &lt;- c(2, 31, 12, 12, 17, 13, 0, 5, 13, 10, 14, 11, 6, 18, 6, 17, 6, 5, 4, 5) B &lt;- c(8, 8, 5, 3, 6, 18, 13, 20, 19, 3, 11, 7, 8, 12, 6, 17, 6, 7, 22, 18) C &lt;- c(12, 12, 9, 7, 10, 22, 17, 24, 23, 7, 15, 11, 12, 16, 10, 21, 10, 11, 26, 22) series &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 20) values &lt;- c(A, B, C) boxplot(values ~ series) Figure 3.3: A boxplot in R In this boxplot, each group is represented by a box, where the uppermost side is the 75th percentile, the lowermost side is the 25th percentile, while a line is drawn to indicate the median (50th percentile). Two vertical arrows (whiskers) start from the 25^th and 75^th percentile and reach the maximum and minimum values for each group. In the case of treatment A, the maximum value is 31, which is 20.5 units above the median. As this difference is higher than 1.5 times the difference from the median and the 75th percentile, this value is excluded, it is regarded as an outlier and it is represented by an empty circle. For the case when we have a pair of quantitative variables, we can draw a scatterplot, by using the two variables as the co-ordinates. The simplest R plotting function is plot() and an example of its usage is given in Figure 3.4), with reference to the correlation data at 3.1.5. plot(A ~ B, xlab = &quot;b&quot;, ylab = &quot;a&quot;, pch = 21, col = &quot;blue&quot;, cex = 2, bg = &quot;blue&quot;) Figure 3.4: Scatterplot showing the correlation between two variables A distribution of frequency can also be represented by using a pie chart, as shown in Figure 3.5), for the ‘gear’ variable in the ‘mtcars’ dataset. pie(table(mtcars$gear)) Figure 3.5: Representation of a distribution of frequencies by using a pie chart 3.5 Further reading Holcomb Z.C. (2017). Fundamentals of descriptive statistics. Routledge (Taylor and Francis Group), USA "],["modeling-the-experimental-data.html", "Chapter 4 Modeling the experimental data", " Chapter 4 Modeling the experimental data To be done … "],["estimation-of-model-parameters.html", "Chapter 5 Estimation of model parameters", " Chapter 5 Estimation of model parameters To be done … "],["making-decisions-under-uncertainty.html", "Chapter 6 Making Decisions under uncertainty", " Chapter 6 Making Decisions under uncertainty To be done … "],["one-way-anova-models.html", "Chapter 7 One-way ANOVA models", " Chapter 7 One-way ANOVA models To be done … "],["checking-for-the-basic-assumptions.html", "Chapter 8 Checking for the basic assumptions", " Chapter 8 Checking for the basic assumptions … "],["contrasts-and-multiple-comparison-testing.html", "Chapter 9 Contrasts and multiple comparison testing", " Chapter 9 Contrasts and multiple comparison testing To be done … "],["multi-way-anova-models.html", "Chapter 10 Multi-way ANOVA models", " Chapter 10 Multi-way ANOVA models To be done … "],["multi-way-anova-models-with-interactions.html", "Chapter 11 Multi-way ANOVA models with interactions", " Chapter 11 Multi-way ANOVA models with interactions To be done … "],["plots-of-different-sizes.html", "Chapter 12 Plots of different sizes", " Chapter 12 Plots of different sizes To be done … "],["simple-linear-regression.html", "Chapter 13 Simple linear regression", " Chapter 13 Simple linear regression To be done … "],["nonlinear-regression.html", "Chapter 14 Nonlinear regression", " Chapter 14 Nonlinear regression To be done … "],["exercises.html", "Chapter 15 Exercises 15.1 Chapter 3 15.2 Chapter 4 15.3 Chapter 5 15.4 Chapter 6 15.5 Chapters 7 to 9 15.6 Chapter 10 15.7 Chapters 11 and 12 15.8 Chapter 13 15.9 Chapter 14", " Chapter 15 Exercises 15.1 Chapter 3 15.1.1 Exercise 1 A chemical analysis was performed in triplicate, with the following results: 125, 169 and 142 ng/g. Calculate mean, sum of squares, mean square, standard deviation and coefficient of variation. 15.1.2 Exercise 2 Download the Excel file ‘rimsulfuron.csv’ from https://www.casaonofri.it/_datasets/rimsulfuron.csv. This is a dataset relating to a field experiment to compare 15 herbicides with 4 replicates. The response variables are maize yield and height. Describe the dataset and show the results on a barplot, including some measure of variability. Check whether yield correlates to height and comment on the results. 15.1.3 Exercise 3 Load the csv file ‘students.csv’ from https://www.casaonofri.it/_datasets/students.csv. This dataset relates to a number of students, their votes in several undergraduate exams and information on high school. Determine whether (i) the average votes depend on the exam subject and (ii) the average votes depend on high school type. 15.2 Chapter 4 15.2.1 Exercise 1 A xenobiotic substance degrades in soil following a first-order kinetic, which is described by the following equation: \\[Y = 100 \\, e^{-0.07 \\, t}\\] where Y is the concentration at time \\(t\\). After spraying this substance in soil, what is the probability that 50 days later we observe a concentration below the toxicity threshold for mammalians (2 ng/g)? Please, consider that all the unknown sources of experimental error can be regarded as gaussian, with a coefficient of variability equal to 20%. 15.2.2 Exercise 2 Crop yield is a function of its density, according to the following function: \\[ Y = 8 + 8 \\, X - 0.07 \\, X^2\\] Draw the graph and find the required density to obtain the highest yield (use a simple graphical method). What is the probability of obtaining a yield level between 2.5 and 3 t/ha, by using the optimal density? Consider that random variability is 12%. 15.2.3 Exercise 3 The toxicity of a compound changes with the dose, according to the following expression: \\[ Y = \\frac{1}{1 + exp\\left\\{ -2 \\, \\left[log(X) - log(15)\\right] \\right\\}}\\] where \\(Y\\) is the proportion of dead animals and \\(X\\) is the dose. If we treat 150 animals with a dose of 35 g, what is the probability of finding more than 120 dead animals? The individual variability can be approximated by using a gaussian distribution, with a standard error equal to 10. 15.2.4 Exercise 4 Consider the sample C = [140 - 170 - 155], which was drawn by a gaussian distribution. Calculate the probability of drawing an individual value from the same pupulation in the following intervals: higher than 170 lower than 140 within the range from 170 and 140 15.2.5 Exercise 5 Consider a gaussian population with \\(\\mu\\) = 23 and \\(\\sigma\\) = 1. Calculate the probability of drawing three individuals with a mean higher than 25 lower than 21 between 21 and 25 15.3 Chapter 5 15.3.1 Exercise 1 A chemical analysis was repeated three times, with the following results: 125, 169 and 142 ng/g. Calculate mean, deviance, variance, standard deviation, standard errors and confidence intervals (P = 0.95). 15.3.2 Exercise 2 A sample of 400 insects was sprayed with an insecticide and 136 individuals survived the treatment. Determine the efficacy of the insecticide, in terms of proportion of dead insects, together with 95% confidence limits. 15.4 Chapter 6 15.4.1 Exercise 1 We have made an experiment to compare two fungicides A and B. The first fungicide was used to treat 200 fungi colonies and the number of surviving colonies was 180. B was used to treat 100 colonies and 50 survived. Is there a significant difference between A and B? 15.4.2 Exercise 2 We have compared two experimental treatments and, with the first one, we observed the following results: 9.3, 10.2, 9.7. With the second treatment, we obtained: 12.6, 12.3 e 12.5. Are the means significantly different? 15.4.3 Exercise 3 A plant pathologist studied the crop performancies with (A) and without (NT) a fungicide treatment. The results are as follows: A NT 65 54 71 51 6.8 59 Was the treatment effect significant (P &lt; 0.05)? 15.4.4 Exercise 4 In this year, an assay showed that 600 olive drupes out of 750 were attacked by Daucus olee. In a close field, under the same environmental conditions, the count of attacked drupes was 120 on 750. Is such a difference significant or is it just a random fluctuation? 15.4.5 Exercise 5 In a hospital, blood cholesterol level was measured for eight patients, before and after a three months terapy. The observed values were: Patient Before After 1 167.3 126.7 2 186.7 154.2 3 105.0 107.9 4 214.5 209.3 5 148.5 138.5 6 171.5 121.3 7 161.5 112.4 8 243.6 190.5 Can we say that this terapy is effective, or not? 15.5 Chapters 7 to 9 15.5.1 Exercise 1 An experiment was conducted with a completely randomised design to compare the yield of 5 wheat genotypes. The results (in bushels per acre) are as follows: Variety 1 2 3 A 32.4 34.3 37.3 B 20.2 27.5 25.9 C 29.2 27.8 30.2 D 12.8 12.3 14.8 E 21.7 24.5 23.4 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Compare the means Present the results and comment on them The example is taken from: Le Clerg et al. (1962) 15.5.2 Exercise 2 Cell cultures of tomato were grown by using three types of media, based on glucose, fructose and sucrose. The experiment was conducted with a completely randomised design with 5 replicates and a control was also added to the design. Cell growths are reported in the table below: Control Glucose Fructose Sucrose 45 25 28 31 39 28 31 37 40 30 24 35 45 29 28 33 42 33 27 34 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Compare the means Present the results and comment on them 15.5.3 Exercise 3 The failure time for a heating system was assessed, to discover the effect of the operating temperature. Four temperatures were tested with 6 replicates, according to a completely randomised design and the number of hours before failure were measured. The results are as follows: Temp. Hours to failure 1520 1953 1520 2135 1520 2471 1520 4727 1520 6134 1520 6314 1620 1190 1620 1286 1620 1550 1620 2125 1620 2557 1620 2845 1660 651 1660 837 1660 848 1660 1038 1660 1361 1660 1543 1708 511 1708 651 1708 651 1708 652 1708 688 1708 729 Please, note that these are the only possible values for temperature. Determine the best operating temperature, in order to delay failure. 15.5.4 Exercise 4 An entomologist counted the number of eggs laid from a lepidopter on three tobacco genotypes. 15 females were tested for each genotype and the results are as follows: Female Field Resistant USDA 1 211 0 448 2 276 9 906 3 415 143 28 4 787 1 277 5 18 26 634 6 118 127 48 7 1 161 369 8 151 294 137 9 0 0 29 10 253 348 522 11 61 0 319 12 0 14 242 13 275 21 261 14 0 0 566 15 153 218 734 Which is the most resistant genotype? 15.6 Chapter 10 15.6.1 Exercise 1 Data were collected about 5 types of irrigation on orange trees in Spain. The experiment was laid down as complete randomised blocks with 5 replicates and the results are as follows: Method 1 2 3 4 5 Localised 438 413 375 127 320 Surface 413 398 348 112 297 Sprinkler 346 334 281 43 231 Sprinkler + localised 335 321 267 33 219 Submersion 403 380 336 101 293 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Compare the means Present the results and comment on them 15.6.2 Exercise 2 A fertilisation trial was conducted according to a RCBD with five replicates. One value is missing for the second treatment in the fifth block. The observed data are percentage contents in P2 O5 in leaf samples: Treatment 1 2 3 4 5 Unfertilised 5.6 6.1 5.3 5.9 9.4 50 lb N 7.3 6.0 7.7 7.7 NA 100 lb N 6.9 6.0 5.6 7.4 8.2 50 lb N + 75 lb P2O5 10.8 11.2 8.8 10.4 12.9 100 lb N + 75 lb P205 9.6 9.3 12 10.6 11.6 Calculate arithmetic means Calculate the ANOVA Check for the basic assumptions Calculate expected marginal means and compare to arithmetic means The addition of P2 O5 is a convenient practice, in terms of agronomic effect? 15.6.3 Exercise 3 A latin square experiment was planned to assess effect of four different fertilisers on lettuce yield. The observed data are as follows: Fertiliser Row Column Yield A 1 1 104 B 1 2 114 C 1 3 90 D 1 4 140 A 2 4 134 B 2 3 130 C 2 1 144 D 2 2 174 A 3 3 146 B 3 4 142 C 3 2 152 D 3 1 156 A 4 2 147 B 4 1 160 C 4 4 160 D 4 3 163 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Compare the means Present the results and comment on them: what is the best fertiliser? 15.7 Chapters 11 and 12 15.7.1 Exercise 1 A pot experiment was planned to evaluate the best timing for herbicide application against rhizome Sorghum halepense. Five timings were compared (2-3, 4-5, 6-7 and 8-9 leaves), including a splitted treatment in two timings (3-4/8-9 leaves) and the untreated control. In order to understand whether the application is effective against plants coming from rhizomes of different sizes, a second factor was included in the experiment, i.e. rhizome size (2, 4, six nodes). The design was a fully crossed two-way factorial, laid down as completely randomised with four replicates. The results (plant weights three weeks after the herbicide application) are as follows: Sizes ↓ / Timing → 2-3 4-5 6-7 8-9 3-4/8-9 Untreated 2-nodes 34.03 0.10 30.91 33.21 2.89 41.63 22.31 6.08 35.34 43.44 19.06 22.96 21.70 3.73 24.23 44.06 0.10 52.14 14.90 9.15 28.27 35.34 0.68 59.81 4-nodes 42.19 14.86 52.34 39.06 8.62 68.15 51.06 36.03 43.17 61.59 0.05 42.75 43.77 21.85 57.28 48.89 0.10 57.77 31.74 8.71 29.71 49.14 9.65 44.85 6-nodes 20.84 11.37 55.00 41.77 9.80 43.20 26.12 2.24 28.46 37.38 0.10 40.68 35.24 14.17 21.81 39.55 1.42 34.11 13.32 23.93 60.72 48.37 6.83 32.21 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Calculate marginal means and cell means Present the results and comment on them: what type of means should you report? 15.7.2 Exercise 2 Six faba bean genotypes were tested in two sowing times, according to a plit-plot design in 4 complete blocks. Sowing times were randomised to main-plots within blocks and genotypes were randomised to sub-plots within main-plots and blocks. Results are: Sowing Time Genotype 1 2 3 4 Autum Chiaro 4.36 4.00 4.23 3.83 Collameno 3.01 3.32 3.27 3.40 Palombino 3.85 3.85 3.68 3.98 Scuro 4.97 3.98 4.39 4.14 Sicania 4.38 4.01 3.94 2.99 Vesuvio 3.94 4.47 3.93 4.21 Spring Chiaro 2.76 2.64 2.25 2.38 Collameno 2.50 1.79 1.57 1.77 Palombino 2.24 2.21 2.50 2.05 Scuro 3.45 2.94 3.12 2.69 Sicania 3.24 3.60 3.16 3.08 Vesuvio 2.34 2.44 1.71 2.00 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Calculate marginal means and cell means Present the results and comment on them: what type of means should you report? 15.7.3 Exercise 3 Four crops were sown in soil 20 days after the application of three herbicide treatments, in order to evaluate possible carry-over effects of residuals. The untreated control was also added for comparison and the weight of plants was assessed four weeks after sowing. The experiment was laid down as strip-plot and, within each block, the herbicide were randomised to rows and crops to columns. The weight of plants is reported below: Herbidicide Block sorghum rape soyabean sunflower Untreated 1 180 157 199 201 2 236 111 257 358 3 287 217 346 435 4 350 170 211 327 Imazethapyr 1 47 10 193 51 2 43 1 113 4 3 0 20 187 13 4 3 21 122 15 primisulfuron 1 271 8 335 379 2 182 0 201 201 3 283 22 206 307 4 147 24 240 337 rimsulfuron 1 403 238 226 290 2 227 169 195 494 3 400 364 257 397 4 171 134 137 180 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Calculate marginal means and cell means Present the results and comment on them: what type of means should you report? 15.7.4 Exercise 4 A field experiment was conducted to evaluate the effect of fertilisation timing (early, medium, late) on two genotypes. The experiment was designed as a randomised complete block design and the data represent the amount of absorbed nitrogen by the plant: Genotype Block Early Med Late A 1 21.4 50.8 53.2 2 11.3 42.7 44.8 3 34.9 61.8 57.8 B 1 54.8 56.9 57.7 2 47.9 46.8 54.0 3 40.1 57.9 62.0 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Calculate marginal means and cell means Present the results and comment on them: what type of means should you report? 15.7.5 Exercise 5 A study was carried out to evaluate the effect of washing temperature on the reduction of length for four types of fabric. Results are expressed as percentage reduction and the experiment was completely randomised, with two replicates: Fabric 210 °F 215 °F 220 °F 225 °F A 1.8 2.0 4.6 7.5 2.1 2.1 5.0 7.9 B 2.2 4.2 5.4 9.8 2.4 4.0 5.6 9.2 C 2.8 4.4 8.7 13.2 3.2 4.8 8.4 13.0 D 3.2 3.3 5.7 10.9 3.6 3.5 5.8 11.1 Consider the temperature as a factor and: Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Calculate marginal means and cell means Present the results and comment on them: what type of means should you report? 15.7.6 Exercise 6 A chemical process requires one alcohol and one base. A study is organised to evaluate the factorial combinations of three alcohols and two bases on the efficiency of the process, expressed as a percentage. The experiment is designd as completely randomised. Base Alcohol 1 Alcohol 2 Alcohol 3 A 91.3 89.9 89.3 88.1 89.5 87.6 90.7 91.4 90.4 91.4 88.3 90.3 B 87.3 89.4 92.3 91.5 93.1 90.7 91.5 88.3 90.6 94.7 91.5 89.8 Write the linear model for this study and explain the model components Compute the ANOVA Check for the basic assumptions Calculate marginal means and cell means Present the results and comment on them: what type of means should you report? 15.8 Chapter 13 15.8.1 Exercise 1 A study was conducted to evaluate the effect of nitrogen fertilisation in lettuce. The experiment is completely randomised with 4 replicates and the yield results are as follows: N level B1 B2 B3 B4 0 124 114 109 124 50 134 120 114 134 100 146 132 122 146 150 157 150 140 163 200 163 156 156 171 Write the linear model for this study and explain the model components Estimate model parameters Check for the basic assumptions What yield might be obtained by using 120 kg N ha-1? Present the results and comment on them? 15.8.2 Exercise 2 A study was conducted to evaluate the effect of increasing densities of a weed (Sinapis arvensis) on sunflower yield. The experiment was completely randomised. Assuming that the yield response is linear, parameterise the model, check the goodness of fit and find the economical threshold level of weed density, considering that the yield worths 150 Euros per ton and the herbicide treatment costs 40 Euros per hectar. The observed results are: density Rep y ield 0 1 36.63 14 1 29.73 19 1 32.12 28 1 30.61 32 1 27.7 38 1 27.43 54 1 24.79 0 2 36.11 14 2 34.72 19 2 30.12 28 2 30.8 32 2 26.53 38 2 27.6 54 2 23.31 0 3 38.35 14 3 32.16 19 3 31.72 28 3 28.69 32 3 25.88 38 3 28.43 54 3 30.26 0 4 36.74 14 4 32.566 19 4 29.57 28 4 33.663 32 4 28.751 38 4 27.114 54 4 24.664 15.9 Chapter 14 15.9.1 Exercise 1 Two soil samples were treated with two herbicides and put in a climatic chamber at 20°C. Sub-samples were collected from both samples in different times and the concentration of herbicide residues was measured. The results are as follows: Time Herbicide A Herbicide B 0 100.00 100.00 10 50.00 60.00 20 25.00 40.00 30 15.00 23.00 40 7.00 19.00 50 3.50 11.00 60 2.00 5.10 70 1.00 3.00 Assuming that the degradation follows an exponential decay trend, determine the half-life for both herbicides. 15.9.2 Exercise 2 A microbial population grows exponentially over time. Considering the following data, determine the relative rate of growth, by fitting the exponential growth model. Time Cells 0 2 10 3 20 5 30 9 40 17 50 39 60 94 70 201 15.9.3 Exercise 3 An experiment was conducted to determine the absorption of nitrogen by roots of Lemna minor in hydroponic colture. Results (N content) are the following: Conc Rate 2.86 14.58 5.00 24.74 7.52 31.34 22.10 72.97 27.77 77.50 39.20 96.09 45.48 96.97 203.78 108.88 Use nonlinear least squares to estimate the parameters for the rectangular hyperbola (Michaelis-Menten model): \\[Y = \\frac{a X} {b + X}\\] and make sure that model fit is good enough. 15.9.4 Exercise 4 An experiment was conducted to determine the yield of sunflower at increasing densities of a weed (Ammi majus). Based on the following results, parameterise a rectangular hyperbola (\\(Y = (a \\, X)/(b + X)\\) and test for possible lack of fit. The results are: Weed density Yield Loss (%) 0 0 23 17.9 31 21.6 39 26.9 61 29.5 15.9.5 Exercise 5 An experiment was conducted in a pasture, to determine the effect of sampling area on the number of plant species (in general, the higher the sampling area and the higher the number of sampled species). The results are as follows:. Area N. of species 1 4 2 5 4 7 8 8 16 10 32 14 64 19 128 22 256 22 By using the above data, parameterise a power curve \\(Y = a \\, X^b\\) and test for lack of fit. 15.9.6 Exercise 6 Crop growth can be often described by using a Gompertz model. The data below refer to an experiment were sugarbeet was grown either weed free, or weed infested; the weight of the crop per unit area was measured after six different numbers of Days After Emergence (DAE). The experiment was conducted by using a completely randomised design with three replicates and the results are reported below: DAE Infested Weed Free 21 0.06 0.07 21 0.06 0.07 21 0.11 0.07 27 0.20 0.34 27 0.20 0.40 27 0.21 0.25 38 2.13 2.32 38 3.03 1.72 38 1.27 1.22 49 6.13 11.78 49 5.76 13.62 49 7.78 12.15 65 17.05 33.11 65 22.48 24.96 65 12.66 34.66 186 21.51 38.83 186 26.26 27.84 186 27.68 37.72 Parameterise two Gompertz growth models (one for the weed-free crop and one for the infested crop) and evalaute which of the parameters are most influenced by the competition. The Gompertz growth model is: \\[Y = d \\cdot exp\\left\\{- exp \\left[ - b (X - e)\\right] \\right\\}\\] 15.9.7 Exercise 7 Plants of Tripleuspermum inodorum in pots were treated with a sulphonylurea herbicide (tribenuron-methyl) at increasing rates. Three weeks after the treatment the weight per pot was recorded, with the following results: Dose (g a.i. ha\\(^{-1}\\)) Fresh weight (g pot \\(^{-1}\\)) 0 115.83 0 102.90 0 114.35 0.25 91.60 0.25 103.23 0.25 133.97 0.5 98.66 0.5 92.51 0.5 124.19 1 93.92 1 49.21 1 49.24 2 21.85 2 23.77 2 22.46 Assuming that the dose-response relationship can be described by using the following log-logistic model: \\[Y = c + \\frac{d - c}{1 + exp \\left\\{ - b \\left[ log (X) - log (e) \\right] \\right\\}}\\] Parameterise the model and evaluate the goodnes of fit. "],["appendix-a-very-gentle-introduction-to-r.html", "Chapter 16 APPENDIX: A very gentle introduction to R 16.1 What is R? 16.2 Installing R and moving the first steps 16.3 Assignments 16.4 Data types and data objects 16.5 Matrices 16.6 Dataframes 16.7 Working with objects 16.8 Expressions, functions and arguments 16.9 A few useful functions 16.10 Extractors 16.11 Reading external data 16.12 Simple R graphics 16.13 Further readings", " Chapter 16 APPENDIX: A very gentle introduction to R 16.1 What is R? R is a statistical software; it is open source and it works under a freeware GNU licence. It is very powerful, but it has no graphical interface and, thus, we need to write a few lines of cod, which is something we may not be used to do. 16.2 Installing R and moving the first steps In order to get started, please, follow these basic steps: Install R from: https://cran.r-project.org. Follow the link to CRAN (uppermost right side), select one of the available mirrors (you can simply select the first link on top), select your Operating System (OS) and download the base version. Install it by using all default options. Install RStudio from: https://rstudio.com/products/rstudio/. Select RStudio Desktop version, open source edition and download. Install it by using all default options. Launch RStudio. You will see that RStudio consists of four panes, even hough, at the beginning, we will only use two of them, named: (1) SOURCE and (2) CONSOLE. The basic principle is to write code in the SOURCE pane and send it to the CONSOLE pane, by hitting ‘ctrl-R’ or ‘ctrl-Return’ (‘cmd-Return’ in Mac OSx). The SOURCE pane is a text editor and we can save script files, by using the ‘.R’ extension. The CONSOLE pane is where the code is processed, to return the results. Before we start, there are a few important suggestions that we should keep into consideration, in order to save a few headackes: unlike most programs in WINDOWS, R is case-sensitive and, e.g., ‘A’ is not the same as ‘a.’ Please, note that most errors in R are due to small typos, which may take very long to be spotted! Code written in the SOURCE pane MUST BE sent to the console pane, otherwise it is not executed. It’s like writing a WhatsApp message: our mate cannot read our message until we send it away to him! Spaces can be used to write clearer code and they are usually allowed, except within variable names, function names and some operators composed by more than one character. It is useful to comment the code, so that, in future times, we can remember what we intended to do, when we wrote that code. Every line preceded by a hash symbol (#) is not executed and it is regarded as a comment. 16.3 Assignments In R, we work with objects, that must be assigned a name, so that they can be stored in memory and easily recalled. The name is a variable and it is assigned by the assignment operator ‘&lt;-’ (Less-then sign + dash sign). For example, the following code assigns the value of 3 to the ‘y’ variable. The content of a variable can be visualised by simply writing its names and sending it to the console. y &lt;- 3 y ## [1] 3 16.4 Data types and data objects In R, as in most programming languages, we have different data types that can be assigned to a variable: numeric (real numbers) integer (natural numbers) character (use quotation marks: “andrea” or “martina”) factor logic (boolean): TRUE or FALSE Depending on their type, data can be stored in specific objects. The most important object is the vector, that is a uni-dimensional array, storing data of the same type (either numeric, or integer, or logic… you can’t mix!). For example, the following box shows a vector of character strings and a vector of numeric values: we see that the vector is created by the c() function and the elements are separated by commas. sentence &lt;- c(&quot;this&quot;, &quot;is&quot;, &quot;an&quot;, &quot;array&quot;, &quot;of&quot;, &quot;characters&quot;) x &lt;- c(12, 13, 14) The factor vector is different from a character vector, as it is used to store character values belonging to a predefined set of levels; the experimental treatment variables (experimental factors, as we called them in Chapter 2) are usually stored as R factors. The code below shows a character vector that is transformed into a factor, by using the factor() function. treat &lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;) treat ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; factor(treat) ## [1] A A B B C C ## Levels: A B C By now, we have already used a couple of functions ad we have noted that they are characterised by a name followed by a pair of round brackets (e.g., c() or factor()). The arguments go inside the brackets, but we will give more detail later on. 16.5 Matrices Vectors are uni-dimensional arrays, while matrices are bi-dimensional arrays, with rows and columns. The matrix object can be used to store only data of the same type (like a vector) and it is created by using the matrix() function. The first argument to this function is a vector of values, the second argument is the number of rows and the third one is the number of columns. The fourth argument is logical and it specifies whether the matrix is to be populated by row (‘byrow = TRUE’) or by column (‘byrow = FALSE’). z &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8), 2, 4, byrow=TRUE) z ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 16.6 Dataframes The dataframe is also a table (like a matrix), but columns can contain data of different types. It is the most common way to store the experimental data and it should be orginised in a ‘tidy’ way: with one experimental unit per row and all the traits of each unit in different columns. In the box below we create three vectors and combine them in a dataframe. plot &lt;- c(1, 2, 3, 4, 5, 6) treat &lt;- factor(c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;)) yield &lt;- c(12, 15, 16, 13, 11, 19) dataset &lt;- data.frame(&quot;Plot&quot; = plot, &quot;Treatment&quot; = treat, &quot;Yield&quot; = yield) dataset ## Plot Treatment Yield ## 1 1 A 12 ## 2 2 A 15 ## 3 3 B 16 ## 4 4 B 13 ## 5 5 C 11 ## 6 6 C 19 16.7 Working with objects If we have created a number of objects and stored them in memory, we might be interested in viewing them or accessing some of their elements. Objects can be simply viewed by using their name, as shown below. z ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 With objects containing more than one value (vectors, matrices or dataframes) we can use indexing to retreive an element in a specific position. Indexing is performed by using square brackets, containing the index or a list of indices, for multi-dimensional objects. x[1] # First element in a vector ## [1] 12 z[1, 3] # Element in first row and third column, in a dataframe or matrix ## [1] 3 dataset[ ,1] # First Column ## [1] 1 2 3 4 5 6 dataset[1, ] # First Row ## Plot Treatment Yield ## 1 1 A 12 Column vectors in dataframes can also be accessed by using their name and the ‘dollar’ sign, as shown below. dataset$Plot ## [1] 1 2 3 4 5 6 It is also useful to ask for infos about objects, which can be done by using two functions: str(): tells us the structure of an object summary() - summarizes the main traits of an object str(dataset) ## &#39;data.frame&#39;: 6 obs. of 3 variables: ## $ Plot : num 1 2 3 4 5 6 ## $ Treatment: Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 2 2 3 3 ## $ Yield : num 12 15 16 13 11 19 summary(dataset) ## Plot Treatment Yield ## Min. :1.00 A:2 Min. :11.00 ## 1st Qu.:2.25 B:2 1st Qu.:12.25 ## Median :3.50 C:2 Median :14.00 ## Mean :3.50 Mean :14.33 ## 3rd Qu.:4.75 3rd Qu.:15.75 ## Max. :6.00 Max. :19.00 16.8 Expressions, functions and arguments Expressions can be used to return results or store them in new variables. 2 * y ## [1] 6 f &lt;- 2 * y f ## [1] 6 As we anticipated above, functions are characterised by a name and a list of arguments in brackets. log(5) ## [1] 1.609438 Very often, there are multiple arguments and we have to pay some attention on how to supply them. We can either: supply them in the exact order with which R expects them use argument names We can see the required list of arguments and their order by using the R help, that is invoked by a question mark followed by the function name, as shown in the example below. ?log #Getting help # The two arguments are the value and the base for logarithm log(100, 2) # Supplied in order ## [1] 6.643856 log(100, base = 2) # Supplied with names ## [1] 6.643856 log(base=2, 100) # Different order, but correct syntax ## [1] 6.643856 log(2, 100) # Wrong!!! ## [1] 0.150515 16.9 A few useful functions A few functions are useful to analyse the experimental data. For example, it is important to be able to create sequences of values, as shown below. plot &lt;- seq(1, 10,1) plot ## [1] 1 2 3 4 5 6 7 8 9 10 Likewise, we need to be able to save time by repeating vectors or vector elements, as shown below. treat &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) rep(treat, 3) #Repeating whole vector ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; rep(treat, each = 3) #Repeating each element ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; Several vectors can be combined in one vector by using the c() function: y &lt;- c(1,2,3) z &lt;- c(4,5,6) c(y, z) ## [1] 1 2 3 4 5 6 During our R session, objects are created and written to the workspace (environment). At the end of a session (or at the beginning of a new one) we might like to clean the workspace, by using the rm() function as shown below. rm(y, z) # remove specific objects rm(list=ls()) # remove all objects 16.10 Extractors In some cases, functions return several objects, which are allocated to different slots. To extract such objects, we use the ‘$’ operator. For example, the eigen() function calculates the eigenvector and eigenvalues of a matrix and these results are saved into the same variable, but in different slots. We can extract the results as shown below. MAT &lt;- matrix(c(2,1,3,4),2,2) MAT ## [,1] [,2] ## [1,] 2 3 ## [2,] 1 4 ev &lt;- eigen(MAT) ev ## eigen() decomposition ## $values ## [1] 5 1 ## ## $vectors ## [,1] [,2] ## [1,] -0.7071068 -0.9486833 ## [2,] -0.7071068 0.3162278 ev$values ## [1] 5 1 ev$vectors ## [,1] [,2] ## [1,] -0.7071068 -0.9486833 ## [2,] -0.7071068 0.3162278 16.11 Reading external data R is not always the right tool to enter the experimental data and, most often, we enter the data by using a spreadsheet, such as EXCEL. This data can be stored as ‘.xls’ or ‘.xlsx’ files, or, as it is often the case in this book, as ‘.csv’ file. While the former file types are specific to EXCEL, CSV files are a type of cross-platform text data, which does not store information about formatting (bold, italic or lines and background colors…), but it can be opened by all programmes and operating systems. To open ‘.csv’ data, we can use the read.csv() function, while, for EXCEL files, we need to dowload, install and load an additional package (‘readxl’), which is accomplished by using the following code: # install.packages(&quot;readxl&quot;) #install the package: only at first instance library(readxl) # Load the library: at the beginning of each session Loading the file is straightforward: if we know where the file is located, we use the commands: dataset &lt;- read.csv(&quot;fileName&quot;, header=TRUE) # Open CSV file dataset &lt;- read_xls(&quot;fileName&quot;, sheet = &quot;nameOfSheet&quot;) # Open XLS file dataset &lt;- read_xlss(&quot;fileName&quot;, sheet = &quot;nameOfSheet&quot;) # Open XLSX file If we know the filename and its path, we can use it in place of ‘fileName,’ or, more easily, we can use the file.choose() function, which shows a selection windows, from where we can select the file to be opened. dataset &lt;- read.csv(file.choose()&quot;, header=TRUE) # Open CSV file 16.12 Simple R graphics This is a huge topic, that we do not intend to develop here. We would just like to show a couple of examples of how simple graphs can be created with R. The code shown below can be used to draw an x-y scatterplot and to superimpose a curve, by using its equation. This will suffice, so far, but we would like to emphasize that, whit some training, R can be used to draw very professional graphs. x &lt;- c(1, 2, 3, 4) y &lt;- c(10, 11, 13, 17) plot(y ~ x) curve(7.77 * exp(0.189 * x), add = T, col = &quot;red&quot;) 16.13 Further readings Maindonald J. Using R for Data Analysis and Graphics - Introduction, Examples and Commentary. (PDF, data sets and scripts are available at JM’s homepage. Oscar Torres Reina, 2013. Introductio to RStudio (v. 1.3). This homepage "]]
