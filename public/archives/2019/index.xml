<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019 on Fixing the bridge between biologists and statisticians</title>
    <link>http://localhost:4321/archives/2019/</link>
    <description>Recent content in 2019 on Fixing the bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2023, @AndreaOnofri</copyright>
    <lastBuildDate>Fri, 22 Nov 2024 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://localhost:4321/archives/2019/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How do we combine errors? The linear case</title>
      <link>http://localhost:4321/2024/stat_general_errorpropagation/</link>
      <pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2024/stat_general_errorpropagation/</guid>
      <description>&lt;p&gt;In our research work, we usually fit models to experimental data. Our aim is to estimate some biologically relevant parameters, together with their standard errors. Very often, these parameters are interesting in themselves, as they represent means, differences, rates or other important descriptors. In other cases, we use those estimates to derive further indices, by way of some appropriate calculations. For example, think that we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;: it might be relevant to calculate the amount:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #2</title>
      <link>http://localhost:4321/2019/stat_lmm_2-wayssplitrepeatedhet/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_lmm_2-wayssplitrepeatedhet/</guid>
      <description>&lt;div id=&#34;a-repeated-split-plot-experiment-with-heteroscedastic-errors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A repeated split-plot experiment with heteroscedastic errors&lt;/h1&gt;
&lt;p&gt;Let’s imagine a field experiment, where different genotypes of khorasan wheat are to be compared under different nitrogen (N) fertilisation systems. Genotypes require bigger plots, with respect to fertilisation treatments and, therefore, the most convenient choice would be to lay-out the experiment as a split-plot, in a randomised complete block design. Genotypes would be randomly allocated to main plots, while fertilisation systems would be randomly allocated to sub-plots. As usual in agricultural research, the experiment should be repeated in different years, in order to explore the environmental variability of results.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Testing for interactions in nonlinear regression</title>
      <link>http://localhost:4321/2019/stat_nlmm_interaction/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_nlmm_interaction/</guid>
      <description>&lt;div id=&#34;fitting-complex-mixed-models-with-nlme-example-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting ‘complex’ mixed models with ‘nlme’: Example #4&lt;/h1&gt;
&lt;p&gt;Factorial experiments are very common in agriculture and they are usually laid down to test for the significance of interactions between experimental factors. For example, genotype assessments may be performed at two different nitrogen fertilisation levels (e.g. high and low) to understand whether the ranking of genotypes depends on nutrient availability. For those of you who are not very much into agriculture, I will only say that such an assessment is relevant, because we need to know whether we can recommend the same genotypes, e.g., both in conventional agriculture (high nitrogen availability) and in organic agriculture (relatively lower nitrogen availability).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;. Example #1</title>
      <link>http://localhost:4321/2019/stat_lmm_environmentalvariance/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_lmm_environmentalvariance/</guid>
      <description>&lt;div id=&#34;the-environmental-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The environmental variance model&lt;/h1&gt;
&lt;p&gt;Fitting mixed models has become very common in biology and recent developments involve the manipulation of the variance-covariance matrix for random effects and residuals. To the best of my knowledge, within the frame of frequentist methods, the only freeware solution in R should be based on the ‘nlme’ package, as the ‘lmer’ package does not easily permit such manipulations. The ‘nlme’ package is fully described in Pinheiro and Bates (2000). Of course, the ‘asreml’ package can be used, but, unfortunately, this is not freeware.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Germination data and time-to-event methods: comparing germination curves</title>
      <link>http://localhost:4321/2019/stat_seedgermination_comparinglots/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_seedgermination_comparinglots/</guid>
      <description>&lt;p&gt;Very often, seed scientists need to compare the germination behaviour of different seed populations, e.g., different plant species, or one single plant species submitted to different temperatures, light conditions, priming treatments and so on. How should such a comparison be performed?&lt;/p&gt;
&lt;p&gt;Let’s take a practical approach and start from an appropriate example: a few years ago, some collegues studied the germination behaviour for seeds of a plant species (&lt;em&gt;Verbascum arcturus&lt;/em&gt;, BTW…), in different conditions. In detail, they considered the factorial combination of two storage periods (LONG and SHORT storage) and two temperature regimes (FIX: constant daily temperature of 20°C; ALT: alternating daily temperature regime, with 25°C during daytime and 15°C during night time, with a 12:12h photoperiod). If you are a seed scientist and are interested in this experiment, you’ll find detail in Catara &lt;em&gt;et al.&lt;/em&gt; (2016).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Survival analysis and germination data: an overlooked connection</title>
      <link>http://localhost:4321/2019/stat_seedgermination_germination/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_seedgermination_germination/</guid>
      <description>&lt;div id=&#34;the-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The background&lt;/h1&gt;
&lt;p&gt;Seed germination data describe the time until an event of interest occurs. In this sense, they are very similar to survival data, apart from the fact that we deal with a different (and less sad) event: germination instead of death. But, seed germination data are also similar to failure-time data, phenological data, time-to-remission data… the first point is: &lt;strong&gt;germination data are time-to-event data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You may wonder: what’s the matter with time-to-event data? Do they have anything special? With few exceptions, all time-to-event data are affected by a certain form of uncertainty, which takes the name of ‘censoring’. It relates to the fact that the exact time of event may not be precisely know. I think it is good to give an example.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Stabilising transformations: how do I present my results?</title>
      <link>http://localhost:4321/2019/stat_general_reportingresults/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_reportingresults/</guid>
      <description>&lt;p&gt;ANOVA is routinely used in applied biology for data analyses, although, in some instances, the basic assumptions of normality and homoscedasticity of residuals do not hold. In those instances, most biologists would be inclined to adopt some sort of stabilising transformations (logarithm, square root, arcsin square root…), prior to ANOVA. Yes, there might be more advanced and elegant solutions, but stabilising transformations are suggested in most traditional biometry books, they are very straightforward to apply and they do not require any specific statistical software. I do not think that this traditional technique should be underrated.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Genotype experiments: fitting a stability variance model with R</title>
      <link>http://localhost:4321/2019/stat_lmm_stabilityvariance/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_lmm_stabilityvariance/</guid>
      <description>&lt;p&gt;Yield stability is a fundamental aspect for the selection of crop genotypes. The definition of stability is rather complex (see, for example, Annichiarico, 2002); in simple terms, the yield is stable when it does not change much from one environment to the other. It is an important trait, that helps farmers to maintain a good income in most years.&lt;/p&gt;
&lt;p&gt;Agronomists and plant breeders are continuosly concerned with the assessment of genotype stability; this is accomplished by planning genotype experiments, where a number of genotypes is compared on randomised complete block designs, with three to five replicates. These experiments are repeated in several years and/or several locations, in order to measure how the environment influences yield level and the ranking of genotypes.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How do we combine errors, in biology? The delta method</title>
      <link>http://localhost:4321/2019/stat_general_thedeltamethod/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_thedeltamethod/</guid>
      <description>&lt;p&gt;In a recent post I have shown that we can build linear combinations of model parameters (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_errorpropagation/&#34;&gt;see here&lt;/a&gt; ). For example, if we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;, we can build a linear combination as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The standard error for this combination can be obtained as:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>http://localhost:4321/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlationindependence2/</guid>
      <description>&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part I</title>
      <link>http://localhost:4321/2019/stat_general_correlationindependence1/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlationindependence1/</guid>
      <description>&lt;div id=&#34;observations-are-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observations are grouped&lt;/h1&gt;
&lt;p&gt;When we have recorded two traits in different subjects, we can be interested in describing their joint variability, by using the Pearson’s correlation coefficient. That’s ok, altough we have to respect some basic assumptions (e.g. linearity) that have been detailed elsewhere (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlation_alookat/&#34;&gt;see here&lt;/a&gt;). Problems may arise when we need to test the hypothesis that the correlation coefficient is equal to 0. In this case, we need to make sure that all the couples of observations are taken on independent subjects.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Some everyday data tasks: a few hints with R</title>
      <link>http://localhost:4321/2019/r_shapingdata/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/r_shapingdata/</guid>
      <description>&lt;p&gt;We all work with data frames and it is important that we know how we can reshape them, as necessary to meet our needs. I think that there are, at least, four routine tasks that we need to be able to accomplish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;subsetting&lt;/li&gt;
&lt;li&gt;sorting&lt;/li&gt;
&lt;li&gt;casting&lt;/li&gt;
&lt;li&gt;melting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Obviously, there is a wide array of possibilities; I’ll just mention a few, which I regularly use.&lt;/p&gt;
&lt;div id=&#34;subsetting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subsetting the data&lt;/h1&gt;
&lt;p&gt;Subsetting means selecting the records (rows) or the variables (columns) which satisfy certain criteria. Let’s take the ‘students.csv’ dataset, which is available on one of my repositories. It is a database of student’s marks in a series of exams for different subjects.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>http://localhost:4321/2019/stat_general_correlation_alookat/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlation_alookat/</guid>
      <description>&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Some useful equations for nonlinear regression in R</title>
      <link>http://localhost:4321/articles/usefulequations/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/articles/usefulequations/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Very rarely, biological processes follow linear trends. Just think about how a crop grows, or responds to increasing doses of fertilisers/xenobiotics. Or think about how an herbicide degrades in the soil, or about the germination pattern of a seed population. It is very easy to realise that curvilinear trends are far more common than linear trends. Furthermore, asymptotes and/or inflection points are very common in nature. We can be sure: linear equations in biology are just a way to approximate a response over a very narrow range for the independent variable.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
