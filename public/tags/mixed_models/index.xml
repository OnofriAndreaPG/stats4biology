<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mixed_models on The broken bridge between biologists and statisticians</title>
    <link>https://www.statforbiology.com/tags/mixed_models/</link>
    <description>Recent content in mixed_models on The broken bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2018, @AndreaOnofri</copyright>
    <lastBuildDate>Mon, 04 Dec 2023 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://www.statforbiology.com/tags/mixed_models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Repeated measures and subsampling with perennial crops</title>
      <link>https://www.statforbiology.com/2023/stat_lmm_perennialcropssubsampling/</link>
      <pubDate>Mon, 04 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_lmm_perennialcropssubsampling/</guid>
      <description>


&lt;p&gt;In a recent post, I have talked about repeated measures, for a case where measurements were taken repeatedly in the same plots across years &lt;a href=&#34;https://www.statforbiology.com/2023/stat_lmm_perennialcrops/&#34;&gt;see here&lt;/a&gt;. Previously, in another post, I had talked about subsampling, for a case where several random samples were taken from the same plot &lt;a href=&#34;https://www.statforbiology.com/2023/stat_lmm_subsampling_tkw/&#34;&gt;see here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Repeated measures and subsampling are vastly different: in the first case I am specifically interested in the ‘evolution’ of the response over time (or space, sometimes). In the second case (subsampling), I only want to improve the precision/accuracy of my measurements, by taking multiple random samples in each plot.&lt;/p&gt;
&lt;p&gt;In this post, I decided to consider a situation where we have both repeated measures and subsampling. I have been prompted to work on this post by a colleague, that is struggling to find the appropriate model for such a situation; I hope that I can lend him/her a hand to get over this (thanks for your request, Gal!).&lt;/p&gt;
&lt;p&gt;Let’s consider the dataset below, that refers to the yield of lucerne genotypes in three consecutive years, taken from the same plots in a single experiment lasting for three years. The situation is similar to that of my previous post, but, in this case, in each year, we took two subsamples of 1 m&lt;sup&gt;2&lt;/sup&gt; from each plot to obtain two separate yield measurements.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(nlme)
library(dplyr)
filePath &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/alfalfa3years_subs.csv&amp;quot;
dataset &amp;lt;- read.csv(filePath)
dataset[,1:4] &amp;lt;- lapply(dataset[,1:4],
                        function(col) factor(col))
head(dataset)
##   Block Genotype Year SubSample     Yield
## 1     1 4cascine 2006         1  5.967228
## 2     1 4cascine 2006         2  5.021836
## 3     1 4cascine 2007         1 11.319587
## 4     1 4cascine 2007         2 11.917985
## 5     1 4cascine 2008         1 10.534574
## 6     1 4cascine 2008         2 10.374413&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this dataset, we need to consider that the observations taken in different years come from the same plots (repeated measures), so that there are three ‘subplots in time’. In each of these subplots (i.e., in each year), we take two subsamples. By considering this, we can follow the usual rules to build the model (Piepho et al., 2004):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Consider one single year and build the treatment model&lt;/li&gt;
&lt;li&gt;Consider one single year and build the block model&lt;/li&gt;
&lt;li&gt;Consider that the ‘plot’ factor in the block model references the randomisation units, i.e. those units which received the the genotypes by a randomisation process. Assign to this plot factor a random effect.&lt;/li&gt;
&lt;li&gt;Include the year factor into the model and combine the year with all the effects in the treatment model, by crossing or nesting as appropriate.&lt;/li&gt;
&lt;li&gt;Excluding the terms for randomisation units, nest the year in all the other terms in the block model.&lt;/li&gt;
&lt;li&gt;Combine random effects for randomisation units with the repeated factor, by using the colon operator, in order to derive the correct error terms to accommodate correlation structures.&lt;/li&gt;
&lt;li&gt;Introduce subsamples, nested within plots and within years. These subsamples must receive a random effect, because they were randomly sampled.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The models at the different steps are as follows (with R notation):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;treatment model: YIELD ~ GENOTYPE&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:PLOT&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + (1|BLOCK:PLOT)&lt;/li&gt;
&lt;li&gt;treatment model: YIELD ~ GENOTYPE * YEAR&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:YEAR + (1|BLOCK:PLOT)&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:YEAR + (1|BLOCK:PLOT) + (1|BLOCK:PLOT:YEAR)&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:YEAR + (1|BLOCK:PLOT) + (1|BLOCK:PLOT:YEAR) + (1|BLOCK:PLOT:YEAR:SUBSAMPLES)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we take the block, year and genotype effects as fixed (but you can change this as you please), so that the final model is:&lt;/p&gt;
&lt;p&gt;YIELD ~ GENOTYPE * YEAR + BLOCK + BLOCK:YEAR + (1|BLOCK:PLOT) + (1|BLOCK:PLOT:YEAR) + (1|BLOCK:PLOT:YEAR:SUBSAMPLE)&lt;/p&gt;
&lt;p&gt;where the last term does not need to be fitted in R, as it is the residual term, that is fitted by default.&lt;/p&gt;
&lt;p&gt;We start the analysis by building a new variable to uniquely identify each plot; it is easy, if we think that yield values taken for the same genotype in the same block must have been taken in the same plot. Although this is not strictly necessary, we build another variable to uniquely identify, in each plot, the three ‘subplots in time’, within which subsamples were taken (this facilitates the fitting process in R).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Reference the plots
dataset$Plot &amp;lt;- dataset$Block:dataset$Genotype
dataset$Subplot &amp;lt;- dataset$Plot:dataset$Year&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can fit the model with the &lt;code&gt;lme()&lt;/code&gt; function (we also show the fit with the &lt;code&gt;lmer()&lt;/code&gt; function, just in case):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Yield ~ Block + Block:Year + Genotype*Year, 
           random = ~1|Plot/Subplot,
           data = dataset)
anova(mod)
##               numDF denDF  F-value p-value
## (Intercept)       1   240 7374.988  &amp;lt;.0001
## Block             3    57    0.241  0.8676
## Genotype         19    57    2.361  0.0066
## Year              2   114 3124.566  &amp;lt;.0001
## Block:Year        6   114    7.192  &amp;lt;.0001
## Year:Genotype    38   114    2.524  0.0001
# library(lme4)
# mod1 &amp;lt;- lmer(Yield ~ Genotype*Year + Block + Block:Year
#              + (1|Plot) + (1|Subplot), 
#              data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said in our previous post, the above mixed model analyses regards the design as a sort of ‘split-plot in time’ with sub-replicates and it is not necessarily correct, as it assumes that the within-plot correlation is the same for all pairs of observations, regardless of their distance in time. Further analyses might be necessary to assess whether serial correlation structures are necessary.&lt;/p&gt;
&lt;p&gt;Please, also note that the above model works equally well with annual crops, if we repeat the experiment in several years by using exactly the same plots.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Richter, C., 2004. A Mixed Modelling Approach for Randomized Experiments with Repeated Measures. Journal of Agronomy and Crop Science 190, 230–247.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Repeated measures with perennial crops</title>
      <link>https://www.statforbiology.com/2023/stat_lmm_perennialcrops/</link>
      <pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_lmm_perennialcrops/</guid>
      <description>


&lt;p&gt;In this post, I want to discuss a concept that is often mistaken by some of my collegues. With all crops, we are used to repeating experiments across years to obtain multi-year data; the structure of the resulting dataset is always the same and it is exemplified in the box below, that refers to a multi-year genotype experiment with winter wheat.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
library(emmeans)
filePath &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat.csv&amp;quot;
dataset &amp;lt;- read.csv(filePath)
dataset &amp;lt;- dataset %&amp;gt;%
  mutate(across(c(1:3, 5), .fns = factor))
head(dataset)
##   Plot Block Genotype Yield Year
## 1    2     1 COLOSSEO  6.73 1996
## 2  110     2 COLOSSEO  6.96 1996
## 3  181     3 COLOSSEO  5.35 1996
## 4    2     1 COLOSSEO  6.26 1997
## 5  110     2 COLOSSEO  7.01 1997
## 6  181     3 COLOSSEO  6.11 1997&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that we have a column for the blocks, a column for the experimental factor (the genotype, in this instance), a column for the year and a column for the response variable (the yield, in this instance).&lt;/p&gt;
&lt;p&gt;If we intend to take the genotype, year and block effects as fixed, we can fit the correct model with the &lt;code&gt;lm()&lt;/code&gt; function and the resulting ANOVA table looks like the following one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Yield ~ Year/Block + Genotype*Year, data = dataset)
anova(mod1)
## Analysis of Variance Table
## 
## Response: Yield
##               Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Year           6 159.279 26.5466 178.3996 &amp;lt; 2.2e-16 ***
## Genotype       7  11.544  1.6491  11.0824 2.978e-10 ***
## Year:Block    14   3.922  0.2801   1.8826   0.03738 *  
## Year:Genotype 42  27.713  0.6598   4.4342 6.779e-10 ***
## Residuals     98  14.583  0.1488                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we have made different experiments in different years (and with a different randomisation in each year, hopefully), the block effect can only be considered as nested within each year (‘Year/Block’ and not ’Year*Block’). As for the rest, the ANOVA table is very close to that for any types of two-factors factorial experiments.&lt;/p&gt;
&lt;p&gt;The things change dramatically if we have a perennial crop, or a crop with a cycle lasting for more than one year, because yield measurements are taken repeatedly in the same plots across years. For this reason, even though the dataset looks very much like the previous one, it must be analysed in a totally different manner. Based on my experience as an editor of International Journals, I can say that several authors, still today, tend to forget this, resulting in several rejections or, at least, delays in publication.&lt;/p&gt;
&lt;p&gt;Let’s consider the dataset below, that refers to the yield of lucerne genotypes in three consecutive years, taken from the same plots in a single experiment lasting for three years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filePath &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/alfalfa3years.csv&amp;quot;
dataset &amp;lt;- read.csv(filePath)
dataset &amp;lt;- dataset %&amp;gt;%
  mutate(across(c(1:3), .fns = factor))
head(dataset)
##   Block Genotype Year     Yield
## 1     1 4cascine 2006  6.631775
## 2     2 4cascine 2006  6.705397
## 3     3 4cascine 2006  6.499588
## 4     4 4cascine 2006  7.087686
## 5     1 4cascine 2007 14.964927
## 6     2 4cascine 2007 13.584865&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we analyse tha data as in the winter wheat example, we neglect that the observations are grouped within the same plots and, therefore, they are correlated. Consequently, the independence assumption is broken and it is no wonder that the manuscript must be rejected. What should we do, then?&lt;/p&gt;
&lt;p&gt;First of all, we should build a new variable to uniquely identify each plot; it is easy, if we think that yield values taken for the same genotype in the same block must have been taken in the same plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Refernce the plots
dataset$Plot &amp;lt;- dataset$Block:dataset$Genotype&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we can follow the usual rules to build the model (Piepho et al., 2004):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Consider one single year and build the treatment model&lt;/li&gt;
&lt;li&gt;Consider one single year and build the block model&lt;/li&gt;
&lt;li&gt;Include the year factor into the model and combine the year with all the effects in the treatment model, by crossing or nesting as appropriate.&lt;/li&gt;
&lt;li&gt;Consider that the ‘plot’ factor in the block model references the randomisation units, i.e. those units which received the the genotypes by a randomisation process. Assign to this plot factor a random effect.&lt;/li&gt;
&lt;li&gt;Excluding the terms for randomisation units, nest the year in all the other terms in the block model.&lt;/li&gt;
&lt;li&gt;Combine random effects for randomisation units with the repeated factor, by using the colon operator, in order to derive the correct error terms to accommodate correlation structures.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The models at the different steps are as follows (with R notation):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;treatment model: YIELD ~ GENOTYPE&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:PLOT&lt;/li&gt;
&lt;li&gt;treatment model: YIELD ~ GENOTYPE * YEAR&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + (1|BLOCK:PLOT)&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:YEAR + (1|BLOCK:PLOT)&lt;/li&gt;
&lt;li&gt;block model: YIELD ~ BLOCK + BLOCK:YEAR + (1|BLOCK:PLOT) + (1|BLOCK:PLOT:YEAR)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For analogy with the winter wheat experiment we take the block, year and genotype effects as fixed (but you can change this), so that the final model is:&lt;/p&gt;
&lt;p&gt;YIELD ~ BLOCK + BLOCK:YEAR + GENOTYPE * YEAR + (1|BLOCK:PLOT) + (1|BLOCK:PLOT:YEAR)&lt;/p&gt;
&lt;p&gt;where the last term does not need to be fitted in R, as it is the residual term, that is fitted by default. The resulting analysis (with ‘lme’) is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lme(Yield ~ Block + Block:Year + Genotype*Year, 
           random = ~1|Plot,
           data = dataset)
anova(mod2)
##               numDF denDF   F-value p-value
## (Intercept)       1   114 27080.051  &amp;lt;.0001
## Block             3    57     2.139  0.1053
## Genotype         19    57     4.602  &amp;lt;.0001
## Year              2   114  2107.322  &amp;lt;.0001
## Block:Year        6   114     3.818  0.0017
## Year:Genotype    38   114     1.356  0.1115
emmeans(mod2, ~Genotype)
##  Genotype          emmean    SE df lower.CL upper.CL
##  4cascine           11.59 0.315 57    10.96     12.2
##  casalina           12.46 0.315 57    11.83     13.1
##  classe             11.59 0.315 57    10.96     12.2
##  costanza            9.89 0.315 57     9.26     10.5
##  dimitra            11.75 0.315 57    11.12     12.4
##  FDL0101            12.22 0.315 57    11.59     12.9
##  garisenda          11.76 0.315 57    11.13     12.4
##  LaBellaCampagnola  12.17 0.315 57    11.54     12.8
##  LaTorre            11.36 0.315 57    10.73     12.0
##  linfa              11.23 0.315 57    10.60     11.9
##  marina             11.76 0.315 57    11.13     12.4
##  Palladiana         10.89 0.315 57    10.26     11.5
##  PicenaGR           12.12 0.315 57    11.49     12.8
##  PR56S82            11.56 0.315 57    10.93     12.2
##  PR57Q53            11.70 0.315 57    11.07     12.3
##  prosementi         11.79 0.315 57    11.15     12.4
##  RivieraVicentina    9.98 0.315 57     9.35     10.6
##  robot              12.11 0.315 57    11.48     12.7
##  Selene             12.11 0.315 57    11.48     12.7
##  Zenith             11.94 0.315 57    11.31     12.6
## 
## Results are averaged over the levels of: Block, Year 
## Degrees-of-freedom method: containment 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we compare the above analyses with a ‘naive’ (wrong) analysis that neglects the repeated measures, we see big differences and, especially, we see that the SE for genotype means is much higher in the correct analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod3 &amp;lt;- lm(Yield ~ Year/Block + Genotype*Year, 
           data = dataset)
anova(mod3)
## Analysis of Variance Table
## 
## Response: Yield
##                Df  Sum Sq Mean Sq   F value    Pr(&amp;gt;F)    
## Year            2 2602.53 1301.27 1608.2407 &amp;lt; 2.2e-16 ***
## Genotype       19  104.27    5.49    6.7824 4.256e-13 ***
## Year:Block      9   21.80    2.42    2.9930  0.002449 ** 
## Year:Genotype  38   31.83    0.84    1.0351  0.424687    
## Residuals     171  138.36    0.81                        
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
emmeans(mod3, ~Genotype)
##  Genotype          emmean   SE  df lower.CL upper.CL
##  4cascine           11.59 0.26 171    11.07     12.1
##  casalina           12.46 0.26 171    11.94     13.0
##  classe             11.59 0.26 171    11.08     12.1
##  costanza            9.89 0.26 171     9.38     10.4
##  dimitra            11.75 0.26 171    11.24     12.3
##  FDL0101            12.22 0.26 171    11.71     12.7
##  garisenda          11.76 0.26 171    11.24     12.3
##  LaBellaCampagnola  12.17 0.26 171    11.66     12.7
##  LaTorre            11.36 0.26 171    10.84     11.9
##  linfa              11.23 0.26 171    10.72     11.7
##  marina             11.76 0.26 171    11.25     12.3
##  Palladiana         10.89 0.26 171    10.38     11.4
##  PicenaGR           12.12 0.26 171    11.61     12.6
##  PR56S82            11.56 0.26 171    11.05     12.1
##  PR57Q53            11.70 0.26 171    11.19     12.2
##  prosementi         11.79 0.26 171    11.27     12.3
##  RivieraVicentina    9.98 0.26 171     9.47     10.5
##  robot              12.11 0.26 171    11.59     12.6
##  Selene             12.11 0.26 171    11.60     12.6
##  Zenith             11.94 0.26 171    11.42     12.5
## 
## Results are averaged over the levels of: Block, Year 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I just want to conclude by saying that the above mixed model analyses regards the design as a sort of ‘split-plot in time’ and it is not necessarily correct, as it assumes that the within-plot correlation is the same for all pairs of observations, regardless of their distance in time. Further analyses might be necessary to assess whether serial correlation structures are necessary. But we’ll consider this in a future post.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Richter, C., 2004. A Mixed Modelling Approach for Randomized Experiments with Repeated Measures. Journal of Agronomy and Crop Science 190, 230–247.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Subsampling in field experiments</title>
      <link>https://www.statforbiology.com/2023/stat_lmm_subsampling_tkw/</link>
      <pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_lmm_subsampling_tkw/</guid>
      <description>


&lt;p&gt;Subsampling is very common in field experiments in agriculture. It happens when we collect several random samples from each plot and we submit them to some sort of measurement process. Some examples? Let’s imagine that we have randomised field experiments with three replicates and, either,:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;we collect the whole grain yield in each plot, select four subsamples and measure, in each subsample, the oil content or some other relevant chemical property, or&lt;/li&gt;
&lt;li&gt;we collect, from each plot, four plants and measure their heights, or&lt;/li&gt;
&lt;li&gt;we collect a representative soil sample from each plot and perform chemical analyses in triplicate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For all the above examples, we end up with 3 by 4 equal 12 data for each treatment level. The question is: do we have 12 replicates? This is exactly the point: &lt;strong&gt;subsamples should never be mistaken for true-replicates, as the experimental treatments were not independently allocated to each one of them&lt;/strong&gt;. In literature, subsamples are usually known as sub-replicates or pseudo-replicates: for the above examples, we have three true-replicates and four pseudo-replicates per true-replicate. Let’s see how to handle pseudo-replicates in data analysis. But, first of all, do not forget that: &lt;strong&gt;experiments with pseudo-replicates are valid only when we also have true-replicates!&lt;/strong&gt; If we only have pseudo-replicates… well, there is nothing we can do in data analysis that transforms our experiment into a valid one…&lt;/p&gt;
&lt;div id=&#34;a-motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A motivating example&lt;/h1&gt;
&lt;p&gt;Let’s consider a dataset from an experiment where we had 30 genotypes in three blocks and recorded the Weight of Thousand Kernels (TKW) in three subsamples per plot, which were labelled by using the ‘Sample’ variable. In the box below, we load the data and all the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(tidyverse)
library(nlme)
library(emmeans)

filePath &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/TKW.csv&amp;quot;
TKW &amp;lt;- read.csv(filePath)
TKW &amp;lt;- TKW %&amp;gt;% 
  mutate(across(1:4, .fns = factor))
head(TKW)
##   Plot Block  Genotype Sample  TKW
## 1    1     1 Meridiano      1 28.6
## 2    2     1     Solex      1 33.3
## 3    3     1  Liberdur      1 22.3
## 4    4     1  Virgilio      1 28.1
## 5    5     1   PR22D40      1 26.7
## 6    6     1    Ciccio      1 34.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-wrong-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The wrong analysis&lt;/h1&gt;
&lt;p&gt;A ‘naive’ analysis would be to perform an ANOVA on all data, without making any distinction between true-replicates and sub-replicates. Let’s do this by using the code shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Naive analysis
mod &amp;lt;- lm(TKW ~ Block + Genotype, data=TKW)
anova(mod)
## Analysis of Variance Table
## 
## Response: TKW
##            Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block       2  110.3  55.169   7.510 0.0006875 ***
## Genotype   29 7224.7 249.129  33.913 &amp;lt; 2.2e-16 ***
## Residuals 238 1748.4   7.346                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
muG &amp;lt;- emmeans(mod, ~Genotype)
muG
##  Genotype   emmean    SE  df lower.CL upper.CL
##  Achille      34.7 0.903 238     32.9     36.4
##  Alemanno     41.6 0.903 238     39.8     43.4
##  AncoMarzio   32.7 0.903 238     31.0     34.5
##  Arnacoris    24.5 0.903 238     22.7     26.3
##  Casanova     38.6 0.903 238     36.9     40.4
##  Chiara       30.0 0.903 238     28.2     31.8
##  Ciccio       33.5 0.903 238     31.7     35.2
##  Ciclope      37.4 0.903 238     35.6     39.1
##  Claudio      40.7 0.903 238     38.9     42.5
##  Creso        37.8 0.903 238     36.0     39.6
##  Dario        30.9 0.903 238     29.1     32.7
##  Duilio       32.1 0.903 238     30.3     33.9
##  Dylan        36.5 0.903 238     34.8     38.3
##  Imhotep      34.8 0.903 238     33.1     36.6
##  Iride        30.8 0.903 238     29.0     32.6
##  Isildur      26.0 0.903 238     24.2     27.8
##  K26          33.5 0.903 238     31.7     35.2
##  Latinur      37.5 0.903 238     35.7     39.3
##  Liberdur     22.8 0.903 238     21.1     24.6
##  Meridiano    30.8 0.903 238     29.0     32.6
##  Neolatino    36.9 0.903 238     35.1     38.7
##  Normanno     36.8 0.903 238     35.0     38.6
##  PR22D40      26.2 0.903 238     24.4     28.0
##  PR22D89      44.0 0.903 238     42.3     45.8
##  Principe     37.2 0.903 238     35.4     39.0
##  Saragolla    35.9 0.903 238     34.1     37.6
##  Sfinge       40.5 0.903 238     38.8     42.3
##  Simeto       41.1 0.903 238     39.3     42.9
##  Solex        34.6 0.903 238     32.9     36.4
##  Virgilio     29.5 0.903 238     27.7     31.2
## 
## Results are averaged over the levels of: Block 
## Confidence level used: 0.95
pairwise &amp;lt;- as.data.frame(pairs(emmeans(mod, ~Genotype)))
sum(pairwise$p.value &amp;lt; 0.05)
## [1] 225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the SE for genotype means is 0.903, the F test for the genotypes is highly significant and there are 225 significant pairwise comparisons among the 30 genotypes.&lt;/p&gt;
&lt;p&gt;As we said, this is simple, but it is also &lt;strong&gt;terribly wrong&lt;/strong&gt;. By putting true-replicates and pseudo-replicates on an equal footing, we have forgotten that the 270 observations are grouped by plot and that the observations in the same plot are more alike than the observations in different plots, because they share the same ‘origin’. We say that the observations in each plot are correlated and, therefore, the basic assumption of independence of residuals is broken. Our analysis is invalid and our manuscript can be very likely rejected.&lt;/p&gt;
&lt;p&gt;But, why are the editors so critical when we mistake pseudo-replicates for true-replicates? We’ll see this in a few minutes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-correct-way-to-go&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The correct way to go&lt;/h1&gt;
&lt;p&gt;A fully correct model for our dataset is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{ijks} = \mu + \alpha_i + \beta_j + \gamma_{k} + \varepsilon_{s}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the thousand kernel weight for the i&lt;sup&gt;th&lt;/sup&gt; genotype, j&lt;sup&gt;th&lt;/sup&gt; block, k&lt;sup&gt;th&lt;/sup&gt; plot and s&lt;sup&gt;th&lt;/sup&gt; subsample, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the effect of the i&lt;sup&gt;th&lt;/sup&gt; genotype, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the effect of the j&lt;sup&gt;th&lt;/sup&gt; block, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of the the k&lt;sup&gt;th&lt;/sup&gt; plot and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the effect of the s&lt;sup&gt;th&lt;/sup&gt; subsample. The presence of the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; element accounts for the effects of plots as a grouping factor and restores the independence of model residuals.&lt;/p&gt;
&lt;p&gt;Obviously, the difference between plots (for a given genotype and block) must be regarded as a random effect, as well as the difference between subsamples, within each plot. Indeed. we have two random effects and, therefore, this is a mixed model. These two random effects are assumed to be normal, independent from each other, with mean equal to 0 and variances respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;. (BTW: I am regarding the block effect as fixed! You may not agree, but this does not change what I am going to say later…).&lt;/p&gt;
&lt;p&gt;We can fit this mixed model by using the &lt;code&gt;lme()&lt;/code&gt; function in the &lt;code&gt;nlme&lt;/code&gt; package, including the ‘Plot’ (i.e. the grouping factor) as a random effect. Obviously, we need to have a variable in the dataset that uniquely identifies each plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Mixed model fit
mod.mix &amp;lt;- lme(TKW ~ Block + Genotype, 
               random = ~1|Plot, data=TKW)
anova(mod.mix)
##             numDF denDF   F-value p-value
## (Intercept)     1   180 11563.536  &amp;lt;.0001
## Block           2    58     2.005  0.1439
## Genotype       29    58     9.052  &amp;lt;.0001
emmeans(mod.mix, ~Genotype)
##  Genotype   emmean   SE df lower.CL upper.CL
##  Achille      34.7 1.75 58     31.2     38.2
##  Alemanno     41.6 1.75 58     38.1     45.1
##  AncoMarzio   32.7 1.75 58     29.2     36.2
##  Arnacoris    24.5 1.75 58     21.0     28.0
##  Casanova     38.6 1.75 58     35.1     42.1
##  Chiara       30.0 1.75 58     26.5     33.5
##  Ciccio       33.5 1.75 58     30.0     37.0
##  Ciclope      37.4 1.75 58     33.9     40.9
##  Claudio      40.7 1.75 58     37.2     44.2
##  Creso        37.8 1.75 58     34.3     41.3
##  Dario        30.9 1.75 58     27.4     34.4
##  Duilio       32.1 1.75 58     28.6     35.6
##  Dylan        36.5 1.75 58     33.0     40.0
##  Imhotep      34.8 1.75 58     31.3     38.3
##  Iride        30.8 1.75 58     27.3     34.3
##  Isildur      26.0 1.75 58     22.5     29.5
##  K26          33.5 1.75 58     30.0     37.0
##  Latinur      37.5 1.75 58     34.0     41.0
##  Liberdur     22.8 1.75 58     19.3     26.3
##  Meridiano    30.8 1.75 58     27.3     34.3
##  Neolatino    36.9 1.75 58     33.4     40.4
##  Normanno     36.8 1.75 58     33.3     40.3
##  PR22D40      26.2 1.75 58     22.7     29.7
##  PR22D89      44.0 1.75 58     40.5     47.5
##  Principe     37.2 1.75 58     33.7     40.7
##  Saragolla    35.9 1.75 58     32.4     39.4
##  Sfinge       40.5 1.75 58     37.0     44.0
##  Simeto       41.1 1.75 58     37.6     44.6
##  Solex        34.6 1.75 58     31.1     38.1
##  Virgilio     29.5 1.75 58     26.0     33.0
## 
## Results are averaged over the levels of: Block 
## Degrees-of-freedom method: containment 
## Confidence level used: 0.95
pairwise &amp;lt;- as.data.frame(pairs(emmeans(mod.mix, ~Genotype)))
sum(pairwise$p.value &amp;lt; 0.05)
## [1] 91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see several differences with respect to the previous fit:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the ‘naive’ model, the error term for the genotype effect (MS&lt;sub&gt;e&lt;/sub&gt;) is 7.346 with 238 DF and it represents the whole subsample-to-subsample variability. In the mixed model, the error term for the genotype effect is not shown, but it has only 58 degrees of freedom.&lt;/li&gt;
&lt;li&gt;The SE for genotype means is 1.75 and it is much higher than that from the ‘naive’ fit.&lt;/li&gt;
&lt;li&gt;The number of significant pairwise comparisons between genotypes has dropped to 91.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s concentrate on the correct error term for the genotype effect; we know that the &lt;em&gt;error term must be obtained by a comparison of plots treated alike&lt;/em&gt; (see Fisher, 1937. The design of experiments). From this, it is immediately clear that the MS&lt;sub&gt;e&lt;/sub&gt; from te ‘naive’ analysis compares the subsamples treated alike (and not the plots). Now, if we take the ‘naive’ analysis and include the ‘plot’ among the experimental factors, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lm(TKW ~ Block + Genotype + Plot, data=TKW)
anova(mod2)
## Analysis of Variance Table
## 
## Response: TKW
##            Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block       2  110.3  55.169  65.269 &amp;lt; 2.2e-16 ***
## Genotype   29 7224.7 249.129 294.736 &amp;lt; 2.2e-16 ***
## Plot       58 1596.2  27.521  32.559 &amp;lt; 2.2e-16 ***
## Residuals 180  152.1   0.845                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘plot’ effect, once the genotype and block effects have been removed, takes 58 DFs and leave us with 180 DFs in the residuals. Indeed, we have decomposed the MS&lt;sub&gt;e&lt;/sub&gt; from the ‘naive’ analysis in two parts, one measuring the plot-to-plot variability and the other one measuring the subsample-to-subsample variability, within each plot. The MS for this latter element is equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_e\)&lt;/span&gt; from mixed model analysis, while the MS for the former element is the correct error term to test for the genotype effect, because it &lt;em&gt;compares the plots treated alike&lt;/em&gt;. It is equal to &lt;span class=&#34;math inline&#34;&gt;\(3 \times \sigma^2_p + \sigma^2_e\)&lt;/span&gt; and it should be used to calculate the SEs for genotype means, as &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{27.521/9} = 1.748689\)&lt;/span&gt; and not as &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{7.346/9} = 0.90345\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can now understand why the editors reject our manuscripts if we do not analyse the data properly: we may strongly overestimate the precision of our experiment and, consequently, commit a lot of false positive errors!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simpler-alternative&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A simpler alternative&lt;/h1&gt;
&lt;p&gt;In presence of subsampling, we strongly recommend the previous method of data analysis. But, a simpler alternative exists, which is feasible whenever the number of subsamples is the same for all plots: we can proceed in two-steps. In the first step, we calculate the means of subsamples for each plot and, in the second step, we submit the plot means to ANOVA, by considering the genotypes and the blocks as fixed factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First step
TKWm &amp;lt;- aggregate(TKW ~ Block + Genotype, data = TKW, mean)

#Second step
mod2step &amp;lt;- lm(TKW ~ Genotype + Block, data = TKWm)
anova(mod2step)
## Analysis of Variance Table
## 
## Response: TKW
##           Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Genotype  29 2408.24  83.043  9.0522 9.943e-13 ***
## Block      2   36.78  18.390  2.0046    0.1439    
## Residuals 58  532.08   9.174                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
emmeans(mod2step, ~Genotype)
##  Genotype   emmean   SE df lower.CL upper.CL
##  Achille      34.7 1.75 58     31.2     38.2
##  Alemanno     41.6 1.75 58     38.1     45.1
##  AncoMarzio   32.7 1.75 58     29.2     36.2
##  Arnacoris    24.5 1.75 58     21.0     28.0
##  Casanova     38.6 1.75 58     35.1     42.1
##  Chiara       30.0 1.75 58     26.5     33.5
##  Ciccio       33.5 1.75 58     30.0     37.0
##  Ciclope      37.4 1.75 58     33.9     40.9
##  Claudio      40.7 1.75 58     37.2     44.2
##  Creso        37.8 1.75 58     34.3     41.3
##  Dario        30.9 1.75 58     27.4     34.4
##  Duilio       32.1 1.75 58     28.6     35.6
##  Dylan        36.5 1.75 58     33.0     40.0
##  Imhotep      34.8 1.75 58     31.3     38.3
##  Iride        30.8 1.75 58     27.3     34.3
##  Isildur      26.0 1.75 58     22.5     29.5
##  K26          33.5 1.75 58     30.0     37.0
##  Latinur      37.5 1.75 58     34.0     41.0
##  Liberdur     22.8 1.75 58     19.3     26.3
##  Meridiano    30.8 1.75 58     27.3     34.3
##  Neolatino    36.9 1.75 58     33.4     40.4
##  Normanno     36.8 1.75 58     33.3     40.3
##  PR22D40      26.2 1.75 58     22.7     29.7
##  PR22D89      44.0 1.75 58     40.5     47.5
##  Principe     37.2 1.75 58     33.7     40.7
##  Saragolla    35.9 1.75 58     32.4     39.4
##  Sfinge       40.5 1.75 58     37.0     44.0
##  Simeto       41.1 1.75 58     37.6     44.6
##  Solex        34.6 1.75 58     31.1     38.1
##  Virgilio     29.5 1.75 58     26.0     33.0
## 
## Results are averaged over the levels of: Block 
## Confidence level used: 0.95
pairwise &amp;lt;- as.data.frame(pairs(emmeans(mod2step, ~Genotype)))
sum(pairwise$p.value &amp;lt; 0.05)
## [1] 91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are totally the same as with a mixed model fit, although all Mean Squares in ANOVA are fractions of those obtained by mixed model analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please, remember that this simple solution is only feasible when we have the same number of subsamples per each plot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fisher, R.A., 1937. The design of experiments, 2nd ed. Oliver and Boyd, Edinburgh, UK.&lt;/li&gt;
&lt;li&gt;Hurlbert, S.H., 1984. Pseudoreplication and the design of ecological experiments. Ecological Monographs 54, 187–211.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multi-environment split-plot experiments</title>
      <link>https://www.statforbiology.com/2022/stat_lmm_2-wayssplitrepeated/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_lmm_2-wayssplitrepeated/</guid>
      <description>


&lt;p&gt;Have you made a split-plot field experiment? Have you repeated such an experiment in two (or more) years/locations? Have you run into troubles, because the reviewer told you that your ANOVA model was invalid? If so, please, stop for awhile and read: this post might help you understand what was wrong with your analyses.&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;Let’s think of a field experiment, where 6 genotypes of faba bean were compared under two different sowing times (autumn and spring). For the ease of organisation, such an experiment was laid down as a &lt;strong&gt;split-plot&lt;/strong&gt;, in a randomised complete block design; sowing times were randomly allocated to main-plots, while genotypes were randomly allocated to sub-plots. Let’s stop for a moment… does this sound strange to you? Do you need further information about split-plot designs? You can get some general information &lt;a href=&#34;https://www.statforbiology.com/_statbookeng/designing-experiments.html#setting-up-a-field-experiment&#34;&gt;at this link&lt;/a&gt; and hints on how to analyse the results at this &lt;a href=&#34;https://www.statforbiology.com/_statbookeng/plots-of-different-sizes.html&#34;&gt;other link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The above experiment was repeated in three years and two locations (six environments in all), in order to explore the environmental variability of results (we will not make any distinction between years and locations, for the sake of this post). In the end, we recorded crop yield and produced a dataset with 288 record (6 environments by 2 sowing times by 6 genotypes by 4 blocks). If you are interested in more detail about this experiment, you can find them in Stagnari et al., (2007).&lt;/p&gt;
&lt;p&gt;The resulting dataset (‘fabaBeanSplitMet.csv’) is available in a public online repository and contains six columns, the ‘Location’, the ‘Year’, the ‘Sowing’ time, the ‘Genotype’, the ‘Block’ and the response variable, i.e. the ‘Yield’. After loading the dataset, we need to recode the independent variables into factors and create the new ‘Environment’ factor, as the combination of ‘Year’ and ‘Location’ levels. In the box below, we use the ‘dplyr’ package to accomplish this preliminary step (Wickham et al., 2022).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
rm(list=ls())
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/fabaBeanSplitMet.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Location, Year, Sowing, Genotype, Block),
                .fns = factor),
         Environment = factor(Location:Year))
head(dataset)
##   Location      Year Sowing  Genotype Block Yield       Environment
## 1  papiano 2002-2003 autumn    Chiaro     1  2.05 papiano:2002-2003
## 2  papiano 2002-2003 autumn    Chiaro     2  2.50 papiano:2002-2003
## 3  papiano 2002-2003 autumn    Chiaro     3  2.64 papiano:2002-2003
## 4  papiano 2002-2003 autumn    Chiaro     4  2.45 papiano:2002-2003
## 5  papiano 2002-2003 autumn Collameno     1  2.01 papiano:2002-2003
## 6  papiano 2002-2003 autumn Collameno     2  2.19 papiano:2002-2003&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-valid-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building a valid model&lt;/h1&gt;
&lt;p&gt;A model is identified by listing all the effects which we need to explain the observed yield. In this case, considering the aims of our experiment, it is pretty easy to grasp the importance of the ‘sowing date’ effect, the ‘genotype’ effect and their interaction. These are the so-called treatment factors and we have no doubt that they should be included in our model. Furthermore, we should also be interested to know whether those treatment effects interact with the environment effect, so we should clearly add to the model the ‘sowing time by environment’, ‘genotype by environment’ and ‘sowing time by genotype by environment’ interactions.&lt;/p&gt;
&lt;p&gt;At this step, it is possible that we have no specific interest in any other effects, apart from those we have just mentioned; however, if we stop now, our model is still incomplete and, therefore, invalid. Indeed, we should also think about possible grouping factors. You may wonder: what are the grouping factors? This aspect needs particular attention.&lt;/p&gt;
&lt;p&gt;In split-plot and other very common types of designs, the experimental units are not completely randomised, but they are organised (‘grouped’, indeed) by way of some innate attribute, such as the environment or block or main-plot, which they belong to. These attributes are known as ‘grouping factors (see Piepho et al., 2003) and they introduce a sort of ’parenthood’, so that some observations are more alike than others, because they belong to the same ‘group’ (e.g., same block or same main-plot). If we neglect the effects of ‘grouping factors’, these ‘parenthood’ effects remain in the residuals, which will be correlated. The correlation of residuals represent an important violation of the basic assumptions for linear model fitting and, therefore, the model will be invalid and our paper will be rejected. One first conclusion: &lt;strong&gt;please, do never forget the grouping factors, if you want your paper to be accepted!&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What are the grouping factors in this case? First of all we have the environments (six levels), then we have the blocks within each environment (24 levels in all) and, finally, we have the main-plots within each block and within each environment (48 levels, in all). In this latter respect, we can see that each main-plot can be uniquely identified by the combination of one environment, one block and one sowing time. Consequently, the final (valid) model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Environment + Sowing + Environment:Sowing + Genotype + 
        Environment:Genotype + Environment:Sowing:Genotype + 
        Environment:Block + Environment:Block:Sowing&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R, we can abbreviate as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Environment * Sowing * Genotype + 
        Environment:Block + Environment:Block:Sowing&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sorry, I know I am running the risk of being regarded as a boring professor; but, please, remember: &lt;strong&gt;failing to include any of the above mentioned effects in the model, unless they are clearly non-significant, leads to totally invalid results!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, we need to take a very important decision: which factors are fixed and which factors are random? The rule is that all factors that reference randomisation units (to which treatments are allocated) NEED TO BE RANDOM, while, for the other factors, we can make our own subjective choice. Here, the main-plot factor, to which we allocated the sowing dates, needs to be taken as random. For the other factors, we make the most traditional choice of taking them as fixed, although we need to consider that, in other instances, it might be appropriate to regard the ‘environment’ and ‘block’ effects as random (relating to block effects, you may read Dixon, 2016, for interesting information).&lt;/p&gt;
&lt;p&gt;If I were to suggest a simple package to fit the above model, I’d say that you should favour the &lt;code&gt;lmer()&lt;/code&gt; function in the &lt;code&gt;lme4&lt;/code&gt; package, where the random effects are coded by using the ‘(1|effect)’ notation, as shown in the box below; before fitting, we load the ‘lme4’ package, together with the ‘lmerTest’ package, which gives us extra-flexibility to produce an ANOVA table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(lmerTest)
modMix &amp;lt;- lmer(Yield ~ Environment * Sowing * Genotype +
               Environment:Block + (1|Environment:Block:Sowing),
               data = dataset)
anova(modMix)
## Type III Analysis of Variance Table with Satterthwaite&amp;#39;s method
##                             Sum Sq Mean Sq NumDF DenDF  F value    Pr(&amp;gt;F)    
## Environment                 71.084  14.217     5    18 125.4463 2.430e-13 ***
## Sowing                      45.947  45.947     1    18 405.4282 8.574e-14 ***
## Genotype                     8.030   1.606     5   180  14.1707 1.091e-11 ***
## Environment:Sowing          11.022   2.204     5    18  19.4520 1.086e-06 ***
## Environment:Genotype         9.468   0.379    25   180   3.3418 1.454e-06 ***
## Sowing:Genotype              5.340   1.068     5   180   9.4231 5.388e-08 ***
## Environment:Block            4.398   0.244    18    18   2.1560    0.0561 .  
## Environment:Sowing:Genotype  7.912   0.316    25   180   2.7925 4.513e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, multiple comparison testing can be done with the ‘emmeans’ package as we have shown elsewhere. Transforming the environment or block effects into random effects is pretty straightforward, by changing the R notation; please remember that, if you regard the environment as random, all its interactions should also be regarded most often regardes as well as random.&lt;/p&gt;
&lt;p&gt;Did I menage to make myself clear? If not, drop me a line to the address below. Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Stagnari, F., Onofri, A., Jemison, J.J., Monotti, M., 2007. Improved multivariate analyses to discriminate the behaviour of faba bean varieties. Agronomy For Sustainable Development 27, 387–397.&lt;/li&gt;
&lt;li&gt;Wickham H, François R, Henry L, Müller K (2022). Dplyr: A Grammar of Data Manipulation. R package version 1.0.9, &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Meta-analysis for a single study. Is it possible?</title>
      <link>https://www.statforbiology.com/2022/stat_met_metanalyses/</link>
      <pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_met_metanalyses/</guid>
      <description>


&lt;p&gt;We all know that the word meta-analysis encompasses a body of statistical techniques to combine quantitative evidence from several independent studies. However, I have recently discovered that meta-analytic methods can also be used to analyse the results of a single research project. That happened a few months ago, when I was reading a paper from Damesa et al. (2017), where the authors describe some interesting methods of data analyses for multi-environment genotype experiments. These authors gave a few nice examples with related SAS code, that is rooted in mixed models. As an R enthusiast, I was willing to reproduce their analyses with R, but I could not succeed, until I realised that I could make use of the package ‘metafor’ and its bunch of meta-analityc methods.&lt;/p&gt;
&lt;p&gt;In this post, I will share my R coding, for those of you who are interested in meta-analytic methods and multi-environment experiments. Let’s start by having a look at the example that motivated my interest (Example 1 in Damesa et al., 2017, p. 849).&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;Twenty-two different genotypes of maize were compared in Ethiopia, in relation to their yield level, in four sites (Dhera, Melkassa, Mieso, and Ziway). At all sites, there were 11 incomplete blocks in each of three replicates. The data are available in Damesa et al. (2017) as supplemental material; I have put this data at your disposal in my web repository, to reproduce this example; let’s load the data first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
library(sommer)
library(emmeans)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/Damesa2017.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(1:5, .fns = factor))
head(dataset)
##   site rep block plot genotype row col yield
## 1    1   1     1    1        6   1   1  9.93
## 2    1   1     1    2       22   1   2  6.51
## 3    1   1     2    3       17   1   3  7.92
## 4    1   1     2    4       14   1   4  9.28
## 5    1   1     3    5       12   1   5  7.56
## 6    1   1     3    6       10   1   6  9.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a typical multi-environment experiment: we have three blocking factors (‘site’, ‘rep’ and ‘block’) and one treatment factor (‘genotype’), as well as the ‘yield’ response variable. Let’s see how this dataset can be analysed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-golden-standard-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The ‘golden standard’ analysis&lt;/h1&gt;
&lt;p&gt;In most situations with multi-environment experiments, we are interested in broad space inference about genotypes, which means that we want to determine the best genotypes across the whole set of environments. Accordingly, the ‘site’ and ‘site x genotype’ effects must be regarded as random, while the ‘genotype’ effect is fixed. Furthermore, we need to consider the ‘design’ effects, that (in this specific case) are the ‘reps within sites’ and the ‘blocks within reps within sites’ random effects. Finally, we have the residual error term (‘plots within blocks within reps within sites’), that is always included by default.&lt;/p&gt;
&lt;p&gt;So far, so good, but we have to go slightly more complex; for this type of studies, the variances for replicates, blocks, and residual error should be site specific, which is usually the most realistic assumption. In the end, we need to estimate:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;22 genotype means with standard errors&lt;/li&gt;
&lt;li&gt;one variance component for the site effect&lt;/li&gt;
&lt;li&gt;one variance component for the ‘genotype x site’ interaction&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the ‘rep’ effect&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the ‘block within rep’ effect&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the residual error&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we work with the &lt;code&gt;lme()&lt;/code&gt; function in the &lt;code&gt;nlme&lt;/code&gt; package, we have to create a couple of ‘dummy’ variables (‘one’ and ‘GE’), in order to reference the crossed random effects (see Galecki and Burzykowski, 2013).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# One stage analysis
dataset$one &amp;lt;- 1L
dataset$GE &amp;lt;- with(dataset, genotype:site)
model.mix &amp;lt;- lme(yield ~ genotype - 1, 
             random = list(one = pdIdent(~ site - 1),
                           one = pdIdent(~ GE - 1),
                           rep = pdDiag(~ site - 1),
                           block = pdDiag(~ site - 1)),
                              data = dataset,
             weights = varIdent(form = ~1|site))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The means for genotypes are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mg &amp;lt;- emmeans(model.mix, ~ genotype)
mg
##  genotype emmean   SE  df lower.CL upper.CL
##  1          5.15 1.65 210    1.900     8.40
##  2          5.54 1.65 210    2.296     8.79
##  3          5.19 1.65 210    1.939     8.44
##  4          4.59 1.65 210    1.341     7.84
##  5          4.82 1.65 210    1.568     8.07
##  6          4.66 1.65 210    1.411     7.91
##  7          4.64 1.65 210    1.388     7.88
##  8          4.36 1.65 210    1.110     7.61
##  9          5.03 1.65 210    1.785     8.28
##  10         4.84 1.65 210    1.592     8.09
##  11         4.54 1.65 210    1.290     7.79
##  12         4.87 1.65 210    1.622     8.12
##  13         4.84 1.65 210    1.593     8.09
##  14         4.29 1.65 210    1.045     7.54
##  15         4.47 1.65 210    1.224     7.72
##  16         4.37 1.65 210    1.123     7.62
##  17         4.07 1.65 210    0.819     7.32
##  18         4.95 1.65 210    1.697     8.19
##  19         4.71 1.65 210    1.466     7.96
##  20         4.86 1.65 210    1.612     8.11
##  21         4.13 1.65 210    0.885     7.38
##  22         4.63 1.65 210    1.380     7.88
## 
## Degrees-of-freedom method: containment 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the variance components are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(model.mix)
##          Variance          StdDev      
## one =    pdIdent(site - 1)             
## site1    1.045428e+01      3.233309e+00
## site2    1.045428e+01      3.233309e+00
## site3    1.045428e+01      3.233309e+00
## site4    1.045428e+01      3.233309e+00
## one =    pdIdent(GE - 1)               
## GE1:1    1.052944e-01      3.244909e-01
## GE1:2    1.052944e-01      3.244909e-01
## GE1:3    1.052944e-01      3.244909e-01
## GE1:4    1.052944e-01      3.244909e-01
## GE2:1    1.052944e-01      3.244909e-01
## GE2:2    1.052944e-01      3.244909e-01
## GE2:3    1.052944e-01      3.244909e-01
## GE2:4    1.052944e-01      3.244909e-01
## GE3:1    1.052944e-01      3.244909e-01
## GE3:2    1.052944e-01      3.244909e-01
## GE3:3    1.052944e-01      3.244909e-01
## GE3:4    1.052944e-01      3.244909e-01
## GE4:1    1.052944e-01      3.244909e-01
## GE4:2    1.052944e-01      3.244909e-01
## GE4:3    1.052944e-01      3.244909e-01
## GE4:4    1.052944e-01      3.244909e-01
## GE5:1    1.052944e-01      3.244909e-01
## GE5:2    1.052944e-01      3.244909e-01
## GE5:3    1.052944e-01      3.244909e-01
## GE5:4    1.052944e-01      3.244909e-01
## GE6:1    1.052944e-01      3.244909e-01
## GE6:2    1.052944e-01      3.244909e-01
## GE6:3    1.052944e-01      3.244909e-01
## GE6:4    1.052944e-01      3.244909e-01
## GE7:1    1.052944e-01      3.244909e-01
## GE7:2    1.052944e-01      3.244909e-01
## GE7:3    1.052944e-01      3.244909e-01
## GE7:4    1.052944e-01      3.244909e-01
## GE8:1    1.052944e-01      3.244909e-01
## GE8:2    1.052944e-01      3.244909e-01
## GE8:3    1.052944e-01      3.244909e-01
## GE8:4    1.052944e-01      3.244909e-01
## GE9:1    1.052944e-01      3.244909e-01
## GE9:2    1.052944e-01      3.244909e-01
## GE9:3    1.052944e-01      3.244909e-01
## GE9:4    1.052944e-01      3.244909e-01
## GE10:1   1.052944e-01      3.244909e-01
## GE10:2   1.052944e-01      3.244909e-01
## GE10:3   1.052944e-01      3.244909e-01
## GE10:4   1.052944e-01      3.244909e-01
## GE11:1   1.052944e-01      3.244909e-01
## GE11:2   1.052944e-01      3.244909e-01
## GE11:3   1.052944e-01      3.244909e-01
## GE11:4   1.052944e-01      3.244909e-01
## GE12:1   1.052944e-01      3.244909e-01
## GE12:2   1.052944e-01      3.244909e-01
## GE12:3   1.052944e-01      3.244909e-01
## GE12:4   1.052944e-01      3.244909e-01
## GE13:1   1.052944e-01      3.244909e-01
## GE13:2   1.052944e-01      3.244909e-01
## GE13:3   1.052944e-01      3.244909e-01
## GE13:4   1.052944e-01      3.244909e-01
## GE14:1   1.052944e-01      3.244909e-01
## GE14:2   1.052944e-01      3.244909e-01
## GE14:3   1.052944e-01      3.244909e-01
## GE14:4   1.052944e-01      3.244909e-01
## GE15:1   1.052944e-01      3.244909e-01
## GE15:2   1.052944e-01      3.244909e-01
## GE15:3   1.052944e-01      3.244909e-01
## GE15:4   1.052944e-01      3.244909e-01
## GE16:1   1.052944e-01      3.244909e-01
## GE16:2   1.052944e-01      3.244909e-01
## GE16:3   1.052944e-01      3.244909e-01
## GE16:4   1.052944e-01      3.244909e-01
## GE17:1   1.052944e-01      3.244909e-01
## GE17:2   1.052944e-01      3.244909e-01
## GE17:3   1.052944e-01      3.244909e-01
## GE17:4   1.052944e-01      3.244909e-01
## GE18:1   1.052944e-01      3.244909e-01
## GE18:2   1.052944e-01      3.244909e-01
## GE18:3   1.052944e-01      3.244909e-01
## GE18:4   1.052944e-01      3.244909e-01
## GE19:1   1.052944e-01      3.244909e-01
## GE19:2   1.052944e-01      3.244909e-01
## GE19:3   1.052944e-01      3.244909e-01
## GE19:4   1.052944e-01      3.244909e-01
## GE20:1   1.052944e-01      3.244909e-01
## GE20:2   1.052944e-01      3.244909e-01
## GE20:3   1.052944e-01      3.244909e-01
## GE20:4   1.052944e-01      3.244909e-01
## GE21:1   1.052944e-01      3.244909e-01
## GE21:2   1.052944e-01      3.244909e-01
## GE21:3   1.052944e-01      3.244909e-01
## GE21:4   1.052944e-01      3.244909e-01
## GE22:1   1.052944e-01      3.244909e-01
## GE22:2   1.052944e-01      3.244909e-01
## GE22:3   1.052944e-01      3.244909e-01
## GE22:4   1.052944e-01      3.244909e-01
## rep =    pdDiag(site - 1)              
## site1    8.817499e-02      2.969427e-01
## site2    1.383338e+00      1.176154e+00
## site3    4.245188e-09      6.515511e-05
## site4    1.442336e-02      1.200973e-01
## block =  pdDiag(site - 1)              
## site1    3.312025e-01      5.755020e-01
## site2    4.746751e-01      6.889667e-01
## site3    5.498857e-09      7.415428e-05
## site4    6.953371e-02      2.636925e-01
## Residual 1.346652e+00      1.160454e+00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, apart from some differences relating to the optimisation method, the results are equal to those reported in Tables 1 and 2 of Damesa et al. (2017).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-stage-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two-stage analyses&lt;/h1&gt;
&lt;p&gt;The above analysis is fully correct, but, in some circumstances may be unfeasible. In particular, we may have problems when:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the number of sites is very high, and&lt;/li&gt;
&lt;li&gt;different experimental designs have been used in different sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In these circumstances, it is advantageous to break the analysis in two stages, as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;first stage: we separately analyse the different experiments and obtain the means for all genotypes in all sites;&lt;/li&gt;
&lt;li&gt;second stage: we jointly analyse the genotype means from all sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This two-stage analysis is far simpler, because the data are only pooled at the second stage, where possible design constraints are no longer important (they are considered only at the first stage). However, this two-stage analysis does not necessarily lead to the same results as the one-stage analysis, unless the whole information obtained at the first stage is carried forward to the second one (fully efficient two-stage analysis).&lt;/p&gt;
&lt;p&gt;In more detail, genotypic variances and correlations, as observed in the first stage, should not be neglected in the second stage. Damesa et al. (2017) demonstrate that the best approach is to take the full variance-covariance matrix of genotypes at the first stage and bring it forward to the second stage. They give the coding with SAS, but, how do we do it, with R?&lt;/p&gt;
&lt;p&gt;First of all, we perform the first stage of analysis, using the &lt;code&gt;by()&lt;/code&gt; function to analyse the data separately for each site. In each site, we fit a mixed model, where the genotype is fixed, while the replicates and the incomplete blocks within replicates are random effects. Of course, this coding works because the experimental design is the same at all sites, while it should be modified in other cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First stage
model.1step &amp;lt;- by(dataset, dataset$site,
                  function(df) lme(yield ~ genotype - 1, 
             random = ~1|rep/block, 
             data = df) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we use the function &lt;code&gt;lapply()&lt;/code&gt; to get the variance components. The results are similar to those obtained in one-stage analysis (see also Damesa et al., 2017, Table 1)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the variance components
lapply(model.1step, VarCorr)
## $`1`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 0.1003720    0.3168153
## block =     pdLogChol(1)          
## (Intercept) 0.2505444    0.5005441
## Residual    1.2361933    1.1118423
## 
## $`2`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 1.4012207    1.1837317
## block =     pdLogChol(1)          
## (Intercept) 0.4645211    0.6815579
## Residual    0.2020162    0.4494621
## 
## $`3`
##             Variance     StdDev      
## rep =       pdLogChol(1)             
## (Intercept) 2.457639e-10 1.567686e-05
## block =     pdLogChol(1)             
## (Intercept) 1.824486e-09 4.271400e-05
## Residual    1.054905e+00 1.027085e+00
## 
## $`4`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 0.01412879   0.1188646
## block =     pdLogChol(1)          
## (Intercept) 0.07196842   0.2682693
## Residual    0.11262234   0.3355925&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can retrieve the genotypic means at all sites:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the means
sitmeans &amp;lt;- lapply(model.1step, 
                function(el) 
                  data.frame(emmeans(el, ~genotype)))
sitmeans &amp;lt;- do.call(rbind, sitmeans)
sitmeans$site &amp;lt;- factor(rep(1:4, each = 22))
head(sitmeans)
##     genotype   emmean        SE df lower.CL  upper.CL site
## 1.1        1 8.253672 0.7208426 12 6.683091  9.824253    1
## 1.2        2 7.731118 0.7208426 12 6.160537  9.301699    1
## 1.3        3 7.249198 0.7208426 12 5.678617  8.819779    1
## 1.4        4 8.565262 0.7208426 12 6.994681 10.135843    1
## 1.5        5 8.560002 0.7208426 12 6.989421 10.130583    1
## 1.6        6 9.510255 0.7208426 12 7.939674 11.080836    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance-covariance matrix for genotype means is obtained, for each site, by using the &lt;code&gt;vcov()&lt;/code&gt; function. Afterwords, we build a block diagonal matrix using the four variance-covariance matrices as the building blocks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the vcov matrices
Omega &amp;lt;- lapply(model.1step, vcov)
Omega &amp;lt;- Matrix::bdiag(Omega)
round(Omega[1:8, 1:8], 3)
## 8 x 8 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                                                     
## [1,] 0.520 0.061 0.037 0.034 0.033 0.035 0.034 0.033
## [2,] 0.061 0.520 0.061 0.037 0.033 0.034 0.033 0.033
## [3,] 0.037 0.061 0.520 0.061 0.033 0.033 0.034 0.033
## [4,] 0.034 0.037 0.061 0.520 0.033 0.033 0.033 0.033
## [5,] 0.033 0.033 0.033 0.033 0.520 0.035 0.034 0.061
## [6,] 0.035 0.034 0.033 0.033 0.035 0.520 0.061 0.034
## [7,] 0.034 0.033 0.034 0.033 0.034 0.061 0.520 0.033
## [8,] 0.033 0.033 0.033 0.033 0.061 0.034 0.033 0.520&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can proceed to the second stage, which can be performed by using the &lt;code&gt;rma.mv()&lt;/code&gt; function in the &lt;code&gt;metafor&lt;/code&gt; package, as shown in the box below. We see that we inject the variance-covariance matrix coming from the first stage into the second. That’s why this is a meta-analytic technique: we are behaving as if we had obtained the data from the first stage from literature!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Second stage (fully efficient)
mod.meta &amp;lt;- metafor::rma.mv(emmean, Omega, 
                            mods = ~ genotype - 1,
                            random = ~ 1|site/genotype,
                    data = sitmeans, method=&amp;quot;REML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this fit we get the remaining variance components (for the ‘sites’ and for the ‘sites x genotypes’ interaction) and the genotypic means, which correspond to those obtained from one-step analysis, apart from small differences relating to the optimisation method (see also Tables 1 and 2 in Damesa et al., 2017). That’s why Damesa and co-authors talk about &lt;strong&gt;fully efficient two-stage analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance components
mod.meta$sigma2
## [1] 10.4538773  0.1271925
head(mod.meta$beta)
##               [,1]
## genotype1 5.134780
## genotype2 5.509773
## genotype3 5.147052
## genotype4 4.593256
## genotype5 4.844761
## genotype6 4.691955&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A possible approximation to this fully-efficient method is also shown in Damesa et al. (2017) and consists of approximating the variance-covariance matrix of genotypic means (‘Omega’) by using a vector of weights, which can be obtained by taking the diagonal elements of the inverse of the ‘Omega’ matrix. To achieve these, we can use the R coding in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;siij &amp;lt;- diag(solve(Omega))
mod.meta2 &amp;lt;- metafor::rma.mv(emmean, 1/siij,
                            mods = ~ genotype - 1,
                            random = ~ 1|site/genotype,
                    data = sitmeans, method=&amp;quot;REML&amp;quot;) 
mod.meta2$sigma2
## [1] 10.422928  0.127908
head(mod.meta2$beta)
##               [,1]
## genotype1 5.112614
## genotype2 5.431455
## genotype3 5.151905
## genotype4 4.583911
## genotype5 4.811698
## genotype6 4.704518&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we have fully reproduced the results relating to the Example 1 in the paper we used as the reference for this post. Hope this was useful.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Damesa, T.M., Möhring, J., Worku, M., Piepho, H.-P., 2017. One Step at a Time: Stage-Wise Analysis of a Series of Experiments. Agronomy Journal 109, 845. &lt;a href=&#34;https://doi.org/10.2134/agronj2016.07.0395&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2134/agronj2016.07.0395&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Lenth R (2022). Emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.7.5-090001, &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pinheiro JC, Bates DM (2000). Mixed-Effects Models in S and S-PLUS.Springer, New York. &lt;a href=&#34;doi:10.1007/b98882&#34; class=&#34;uri&#34;&gt;doi:10.1007/b98882&lt;/a&gt;. &lt;a href=&#34;https://doi.org/10.1007/b98882&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/b98882&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. Journal of Statistical Software, 36(3), 1-48. &lt;a href=&#34;https://doi.org/10.18637/jss.v036.i03&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v036.i03&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Split-plot designs: the transition to mixed models for a dinosaur</title>
      <link>https://www.statforbiology.com/2021/stat_lmm_splitplottransition/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_lmm_splitplottransition/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Those who long ago took courses in ‘analysis of variance’ or ‘experimental design’ … would have learned methods … based on observed and expected mean squares and methods of testing based on ‘error strata’ (if you weren’t forced to learn this, consider yourself lucky). (&lt;a href=&#34;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&#34;&gt;Douglas Bates, 2006&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In a previous post, I already mentioned that, due to my age, I see myself as a dinosaur within the R-users community. I already mentioned how difficult it is, for a dinosaur, to adjust to new concepts and paradigms in data analysis, after having done things differently for a long time ( &lt;a href=&#34;https://www.statforbiology.com/2020/stat_r_tidyverse_columnwise/&#34;&gt;see this post here&lt;/a&gt; ). Today, I decided to sit and write a second post, relating to data analyses for split-plot designs. Some years ago, when switching to R, this topic required some adjustments to my usual workflow, which gave me a few headaches.&lt;/p&gt;
&lt;p&gt;Let’s start from a real-life example.&lt;/p&gt;
&lt;div id=&#34;a-split-plot-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A split-plot experiment&lt;/h1&gt;
&lt;p&gt;The dataset ‘beet.csv’ is available in a web repository. It was obtained from a split-plot experiment with two experimental factors: three tillage methods (shallow ploughing, deep ploughing and minimum tillage) and two weed control methods (total and partial, meaning that the herbicide was sprayed broadcast or only along crop rows). Tillage methods were allocated to main-plots, while weed control methods were allocated to sub-plots and the experiment was designed in four complete blocks. A typical split-plot field experiment, indeed. The code below can be used to load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/beet.csv&amp;quot;
dataset &amp;lt;- read_csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Tillage, WeedControl, Block), .fns = factor))
dataset
## # A tibble: 24 x 4
##    Tillage WeedControl Block Yield
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
##  1 MIN     TOT         1     11.6 
##  2 MIN     TOT         2      9.28
##  3 MIN     TOT         3      7.02
##  4 MIN     TOT         4      8.02
##  5 MIN     PART        1      5.12
##  6 MIN     PART        2      4.31
##  7 MIN     PART        3      8.94
##  8 MIN     PART        4      5.62
##  9 SP      TOT         1     10.0 
## 10 SP      TOT         2      8.69
## # … with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-traditional-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The traditional approach&lt;/h1&gt;
&lt;p&gt;Split-plot designs are very commonly used in field experiments and they have been in fashion for (at least) eighty years, long before that the mixed model platform with REML estimation was largely available. Whoever has taken a course in ‘experimental design’ at the end of the 80s has studied how to perform a split-plot ANOVA by hand-calculations, based on the method of moments. For the youngest readers, it might be useful to give a few hints on what I used to do thirty years ago with the above dataset:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;calculate the overall mean and the means for the levels of blocks, tillage, weed control and for the combined levels of tillage and weed control.&lt;/li&gt;
&lt;li&gt;Calculate the means for the combined levels of blocks and tillage, which would correspond to the means for the twelve main-plots.&lt;/li&gt;
&lt;li&gt;With all those means, calculate the deviances for all effects and interactions, as the sums of squared residuals with respect to the overall mean.&lt;/li&gt;
&lt;li&gt;Derive the related variance, by using the appropriate number of degrees of freedom for each effect.&lt;/li&gt;
&lt;li&gt;Calculate F ratios, based on the appropriate error stratum, i.e. the mean square for the ‘blocks ⨉ tillage’ combinations (so called: error A) and the residual mean square.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most relevant aspect in the approach outlined above is the ‘block by tillage’ interaction; the mean square for this effect was used as the denominator in the F ratio, to test for the significance of the tillage main effect.&lt;/p&gt;
&lt;p&gt;The above process was simple to teach and simple to grasp and I used to see it as a totally correct approach to balanced (orthogonal) split-plot data. Those of you who are experienced with SAS should probably remember that, before the advent of PROC MIXED in 1992, split-plot designs were analysed with PROC GLM, using the very same approach as outlined above.&lt;/p&gt;
&lt;p&gt;Considering the above background, let’s see what I did when I switched to R?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-step-aov&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First step: ‘aov()’&lt;/h1&gt;
&lt;p&gt;Having the method of moments in mind, my first line of attack was to use the &lt;code&gt;aov()&lt;/code&gt; function, as suggested in Venables and Ripley (2002) at pag. 283. Those authors make use of the nesting operator in the expression &lt;code&gt;Error(Block/Tillage)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov &amp;lt;- aov(Yield ~ Tillage*WeedControl +
                 Error(Block/Tillage), data = dataset)
summary(mod.aov)
## 
## Error: Block
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)
## Residuals  3   3.66    1.22               
## 
## Error: Block:Tillage
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)   
## Tillage    2 23.656   11.83    19.4 0.0024 **
## Residuals  6  3.658    0.61                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Error: Within
##                     Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  
## WeedControl          1   3.32   3.320   1.225 0.2972  
## Tillage:WeedControl  2  19.46   9.732   3.589 0.0714 .
## Residuals            9  24.40   2.711                 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above definition, the block effect is regarded as random, while, in the traditional approach, it is often regarded as fixed. Indeed, still today, there is no consensus among agricultural scientists on whether the block effect should be regarded as random or fixed (see Dixon, 2016); for the sake of this exercise, let me regard it as fixed. After a few attempts, I discovered that I could move the effect of blocks from the &lt;code&gt;Error()&lt;/code&gt; definition to the fixed effect formula and use the expression &lt;code&gt;Error(Block:Tillage)&lt;/code&gt; to specify the uppermost error stratum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov2 &amp;lt;- aov(Yield ~ Block + Tillage*WeedControl +
                 Error(Block:Tillage), data = dataset)
## Warning in aov(Yield ~ Block + Tillage * WeedControl + Error(Block:Tillage), :
## Error() model is singular
summary(mod.aov2)
## 
## Error: Block:Tillage
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)   
## Block      3  3.660    1.22   2.001 0.2155   
## Tillage    2 23.656   11.83  19.399 0.0024 **
## Residuals  6  3.658    0.61                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Error: Within
##                     Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  
## WeedControl          1   3.32   3.320   1.225 0.2972  
## Tillage:WeedControl  2  19.46   9.732   3.589 0.0714 .
## Residuals            9  24.40   2.711                 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the above code produces a warning message, the result is totally the same as I would have obtained by hand-calculations.&lt;/p&gt;
&lt;p&gt;For me, the &lt;code&gt;aov()&lt;/code&gt; function represented a safe harbour, mainly because the result was very much like what I would expect, considering my experience with mean squares and error strata. Unfortunately, I had to realise that there were several limitations to this approach and, finally, I had to switch to the mixed model platform.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-step-the-mixed-model-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Second step: the mixed model framework&lt;/h1&gt;
&lt;p&gt;When making this switch to mixed models, I had the expectation that I should be able to reproduce the results obtained with the &lt;code&gt;aov()&lt;/code&gt; function and, formerly, by hand-calculations.&lt;/p&gt;
&lt;p&gt;I started with the &lt;code&gt;lme()&lt;/code&gt; function in the ‘nlme’ package (Pinheiro et al., 2018) and I had the idea that I could simply replace the &lt;code&gt;Error(Block:Tillage)&lt;/code&gt; statement with &lt;code&gt;random = ~1|Block:Tillage&lt;/code&gt;. Unfortunately, using the &lt;code&gt;:&lt;/code&gt; operator in the &lt;code&gt;lme()&lt;/code&gt; function is not possible and I had to resort to using the nesting operator &lt;code&gt;‘Block/Tillage’&lt;/code&gt;. Consequently, I noted that the F test for the block effect was wrong (of course: the specification was wrong…). I could have removed the block from the fixed effect model, but I was so stupidly determined to reproduce my hand-calculations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
## 
## Attaching package: &amp;#39;nlme&amp;#39;
## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     collapse
mod.lme &amp;lt;- lme(Yield ~ Block + Tillage*WeedControl,
               random = ~1|Block/Tillage, data = dataset)
anova(mod.lme)
## Warning in pf(Fval[i], nDF[i], dDF[i]): NaNs produced
##                     numDF denDF   F-value p-value
## (Intercept)             1     9 120.85864  &amp;lt;.0001
## Block                   3     0   0.08045     NaN
## Tillage                 2     6   6.32281  0.0333
## WeedControl             1     9   1.77497  0.2155
## Tillage:WeedControl     2     9   5.20229  0.0315&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, I tried to switch to the &lt;code&gt;lmer()&lt;/code&gt; function in the ‘lme4’ package (Bates et al., 2015). With this platform, it was possible to include the ‘block by tillage’ interaction as a random effect, according to my usual workflow. Still, the results did not match to those obtained with the &lt;code&gt;aov()&lt;/code&gt; function: an error message was raised and F ratios were totally different. Furthermore, p-levels were not even displayed (yes, now I know that we can use the ‘lmerTest’ package, but, please, wait a few seconds).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
mod.lmer.split &amp;lt;- lmer(Yield ~ Block + WeedControl*Tillage +
                     (1|Block:Tillage), 
                     data=dataset)
anova(mod.lmer.split)
## Analysis of Variance Table
##                     npar  Sum Sq Mean Sq F value
## Block                  3  3.6596  1.2199  0.6521
## WeedControl            1  3.3205  3.3205  1.7750
## Tillage                2 23.6565 11.8282  6.3228
## WeedControl:Tillage    2 19.4641  9.7321  5.2023&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What’s wrong with that? Why was I not able to reproduce my hand-calculations with the mixed model platform?&lt;/p&gt;
&lt;p&gt;I investigated this matter and I found a very enlightening post by Douglas Bates (the author of ‘nlme’ and ‘lme4’), which is available at &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&#34;&gt;this link&lt;/a&gt;. From there, it was clear to me that F ratios in mixed models are “&lt;em&gt;not based on expected mean squares and error strata&lt;/em&gt;”; further ahead, it is said that there is “&lt;em&gt;a problem with the assumption that the reference distribution for these F statistics should be an F distribution with a known numerator of degrees of freedom but a variable denominator degrees of freedom&lt;/em&gt;”. In the end, it was clear to me that, according to Douglas Bates, the traditional approach of calculating p-values from F ratios based on expected mean squares and error strata was not necessarily correct.&lt;/p&gt;
&lt;p&gt;I made some further research on this matter. Indeed, looking at the &lt;code&gt;aov()&lt;/code&gt; output above, I noted that the residual mean square was equal to 2.711, while the mean square for the ‘Block by Tillage’ interaction was 0.6097. My beloved method of moments brought me to a negative estimate of the variance component for the ‘block by tillage’ interaction, that is &lt;span class=&#34;math inline&#34;&gt;\((0.6097 - 2.711)/4 = -0.5254\)&lt;/span&gt;. I gasped: this was unreasonable and, at least, it would imply that the variance component for the ‘block by tillage’ random effect was not significantly different from zero. In other words, the mean square for the ‘block by tillage’ interaction and the mean square for the residuals were nothing but two separate estimates of the residual plot-to-plot error. I started being suspicious about my hand-calculations. Why did I use two estimates of the same amount as two different error strata?&lt;/p&gt;
&lt;p&gt;I tried a different line of attack: considering that the ‘block by tillage’ interaction was not significant, I removed it from the model. Afterwards I fitted a linear fixed effect model, where the two error strata had been pooled into the residual error term. I obtained the very same F ratios as those obtained from the ‘lmer’ fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.lm &amp;lt;- lm(Yield ~ Block + WeedControl*Tillage, data=dataset) 
anova(mod.lm)
## Analysis of Variance Table
## 
## Response: Yield
##                     Df  Sum Sq Mean Sq F value  Pr(&amp;gt;F)  
## Block                3  3.6596  1.2199  0.6521 0.59389  
## WeedControl          1  3.3205  3.3205  1.7750 0.20266  
## Tillage              2 23.6565 11.8282  6.3228 0.01020 *
## WeedControl:Tillage  2 19.4641  9.7321  5.2023 0.01922 *
## Residuals           15 28.0609  1.8707                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In that precise moment when I noted such a result, it was clear to me that, even with simple and orthogonal split-plot designs, hand-calculations do not necessarily produce correct results and should never, ever be used as the reference to assess the validity of a mixed model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suggestions-for-dinosaurs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Suggestions for dinosaurs&lt;/h1&gt;
&lt;p&gt;If you are one of those who have never taken a lesson about expected mean squares and error strata, well, believe me, you are lucky! For us dinosaurs, switching to the mixed model platform may be a daunting task. We need to free up our minds and change our workflow; a few suggestions are following.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-1-change-model-building-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rule 1: change model building process&lt;/h1&gt;
&lt;p&gt;In principle, do no insist on including the ‘block by tillage’ interaction in the model. With split-plot experiments, the main-plot is to be regarded as a &lt;em&gt;grouping structure&lt;/em&gt;, wherein we take repeated measures in different sub-plots. These measures are correlated, as they are more alike than measures taken in different sub-plots.&lt;/p&gt;
&lt;p&gt;Therefore, for this grouping structure (and for all grouping structures in general) we need to code a &lt;em&gt;grouping factor&lt;/em&gt;, to uniquely identify the repeated measures in each main-plot. This factor must be included in the model, otherwise we violate the basic assumption of independence of model residuals. Consider that the main-plot represent the randomisation units to which the tillage treatments were allocated; therefore, the main plot factor needs to be included in the model as a random effect. Please refer to the good paper of Piepho et al. (2003) for further information on this model building approach.&lt;/p&gt;
&lt;p&gt;In the box below I created the main-plot factor by using &lt;code&gt;dplyr()&lt;/code&gt; to combine the levels of blocks and tillage methods. The difference with the traditional approach of using the ‘block by tillage’ interaction in the model is subtle, but, in this case, the &lt;code&gt;lme()&lt;/code&gt; function returns no error. Please, note that, having no interest in the estimation of variance components, I have fitted this model by maximum likelihood estimation: it is confirmed that the main-plot random effect is zero (see the output of the &lt;code&gt;VarCorr()&lt;/code&gt; function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(mainPlots = factor(Block:Tillage))
mod.lme2 &amp;lt;- lme(Yield ~ Block + Tillage * WeedControl,
               random = ~1|mainPlots, data = dataset,
               method = &amp;quot;ML&amp;quot;)
VarCorr(mod.lme2)
## mainPlots = pdLogChol(1) 
##             Variance     StdDev      
## (Intercept) 4.462849e-10 2.112546e-05
## Residual    1.169203e+00 1.081297e+00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-2-change-the-approach-to-hypotheses-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rule 2: change the approach to hypotheses testing&lt;/h1&gt;
&lt;p&gt;In the agricultural sciences we have been very much familiar with ANOVA tables, showing all fixed effects along with their significance level. I am very much convinced that we should refrain from such a (possibly bad) habit. Indeed, there is no point in testing the significance of main effects before testing the significance of the ‘tillage by weed control’ interaction, as main effects are marginal to the interaction effect.&lt;/p&gt;
&lt;p&gt;At first, we need to concentrate on the interaction effect. According to maximum likelihood theory, it is very logic to think of a Likelihood Ratio Test (LRT), which consists of comparing the likelihoods of two alternative and nested models. In this case, the model above (‘mod.lme2’) can be compared with a ‘reduced’ model without the ‘tillage by weed control’ interaction term: if the two likelihoods are similar, that would be a sign that the interaction effect is not significant. The reduced model fit is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.lme3 &amp;lt;- lme(Yield ~ Block + Tillage + WeedControl,
               random = ~1|mainPlots, data = dataset,
               method = &amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logarithms of the two likelihoods show that the ‘full model’ (with the interaction term) is more ‘likely’ than the reduced model. The LRT is calculated as twice the difference between the two log-likelihoods (the logarithm of the ratio of two numbers is the difference of the logarithms, remember?).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll2 &amp;lt;- logLik(mod.lme2)
ll3 &amp;lt;- logLik(mod.lme3)
ll2; ll3
## &amp;#39;log Lik.&amp;#39; -35.93039 (df=11)
## &amp;#39;log Lik.&amp;#39; -42.25294 (df=9)
LRT &amp;lt;- - 2 * (as.numeric(ll3) - as.numeric(ll2))
LRT
## [1] 12.6451&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For large samples and under the null hypothesis that the two models are not significantly different, the LRT is distributed according to a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; with two degrees of freedom (i.e. the difference in the number of model parameters used by the two models). We could use such an assumption to obtain a p-level for the null, for example by way of the &lt;code&gt;anova()&lt;/code&gt; function, to which we pass the two model objects as arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.lme2, mod.lme3)
##          Model df       AIC      BIC    logLik   Test L.Ratio p-value
## mod.lme2     1 11  93.86078 106.8194 -35.93039                       
## mod.lme3     2  9 102.50589 113.1084 -42.25294 1 vs 2 12.6451  0.0018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, our experiment consists of only 24 observations and the large sample theory should not hold. Therefore, instead of relying on the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution, we can build an empirical sampling distribution for the LRT with Monte Carlo simulation (parametric bootstrap). The process is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;simulate a new dataset under the reduced model, using the fitted parameter estimates and assuming normality for the errors and random effects;&lt;/li&gt;
&lt;li&gt;fit to this dataset both the full and the reduced model;&lt;/li&gt;
&lt;li&gt;compute the LRT statistic;&lt;/li&gt;
&lt;li&gt;repeat steps 1 to 3 many times (e.g., 10000);&lt;/li&gt;
&lt;li&gt;examine the distribution of the bootstrapped LRT values and compute the proportion of those exceeding 12.6451 (empirical p-value).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To this aim, we can use the &lt;code&gt;simulate()&lt;/code&gt; function in the ‘nlme’ package. We pass the reduced model object as the first argument, the full model as the argument ‘m2’, the number of simulations and the seed (if we intend to obtain reproducible results). The fit may take quite a few minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- simulate(mod.lme3, nsim = 10000, m2 = mod.lme2, method=&amp;quot;ML&amp;quot;,
               set.seed = 1234)
lrtSimT &amp;lt;- as.numeric(2*(y$alt$ML[,2] - y$null$ML[,2]))
length(lrtSimT[lrtSimT &amp;gt; 12.6451])/length(lrtSimT)
## [1] 0.0211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the interaction is significant and we can go ahead with further analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;What is the take-home message for this post? When we have to analyse a dataset coming from a split-plot experiment, R forces us to use the mixed model platform. We should not necessarily expect to reproduce the approach and the results we were used to obtain when we made our hand-calculations based on least squares and the method of moments. On the contrary, we should adapt our model building and hypothesis testing process to such a very powerful platform, wherein the slit-plot is treated on equal footing to all other types of repeated measures designs.&lt;/p&gt;
&lt;p&gt;Hope this was fun! If you have any comments, drop me a line to the email below.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Douglas Bates, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. &lt;a href=&#34;doi:10.18637/jss.v067.i01&#34; class=&#34;uri&#34;&gt;doi:10.18637/jss.v067.i01&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2018). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-137, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Venables, W.N., Ripley, B.D., Venables, W.N., 2002. Modern applied statistics with S, 4th ed. ed, Statistics and computing. Springer, New York.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for the experimental design in linear/nonlinear regression analyses</title>
      <link>https://www.statforbiology.com/2020/stat_nlmm_designconstraints/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_nlmm_designconstraints/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post, I am going to talk about an issue that is often overlooked by agronomists and biologists. The point is that field experiments are very often laid down in blocks, using split-plot designs, strip-plot designs or other types of designs with grouping factors (blocks, main-plots, sub-plots). We know that these grouping factors should be appropriately accounted for in data analyses: ‘analyze them as you have randomized them’ is a common saying attributed to Ronald Fisher. Indeed, observations in the same group are correlated, as they are more alike than observations in different groups. What happens if we neglect the grouping factors? We break the independence assumption and our inferences are invalid (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;To my experience, field scientists are totally aware of this issue when they deal with ANOVA-type models (e.g., see Jensen et al., 2018). However, a brief survey of literature shows that there is not the same awareness, when field scientists deal with linear/nonlinear regression models. Therefore, I decided to sit down and write this post, in the hope that it may be useful to obtain more reliable data analyses.&lt;/p&gt;
&lt;div id=&#34;an-example-with-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with linear regression&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the ‘yieldDensity.csv’ dataset, that is available on gitHub. It represents an experiment where sunflower was tested with increasing weed densities (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 10 blocks. a swift plot shows that yield is linearly related to weed density, which calls for linear regression analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(lattice)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/yieldDensityB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yield
## 1     1       0 29.90
## 2     2       0 34.23
## 3     3       0 37.12
## 4     4       0 26.37
## 5     5       0 34.48
## 6     6       0 33.70
plot(yield ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We might be tempted to neglect the block effect and run a linear regression analysis of yield against density. This is clearly wrong (I am violating the independence assumption) and inefficient, as any block-to-block variability goes into the residual error term, which is, therefore, inflated.&lt;/p&gt;
&lt;p&gt;Some of my collegues would take the means for densities and use those to fit a linear regression model (two-steps analysis). By doing so, block-to-block variability is cancelled out and the analysis becomes more efficient. However, such a solution is not general, as it is not feasible, e.g., when we have unbalanced designs and heteroscedastic data. With the appropriate approach, sound analyses can also be made in two-steps (Damesa et al., 2017). From my point of view, it is reasonable to search for more general solutions to deal with one-step analyses.&lt;/p&gt;
&lt;p&gt;Based on our experience with traditional ANOVA models, we might think of taking the block effect as fixed and fit it as and additive term. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg &amp;lt;- lm(yield ~ block + density, data=dataset)
summary(mod.reg)
## 
## Call:
## lm(formula = yield ~ block + density, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6062 -0.8242 -0.3315  0.7505  4.6244 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 29.10462    0.57750  50.397  &amp;lt; 2e-16 ***
## block2       4.57750    0.74668   6.130 4.81e-08 ***
## block3       7.05875    0.74668   9.453 4.49e-14 ***
## block4      -3.98000    0.74668  -5.330 1.17e-06 ***
## block5       6.17625    0.74668   8.272 6.37e-12 ***
## block6       5.92750    0.74668   7.938 2.59e-11 ***
## block7       1.23750    0.74668   1.657  0.10199    
## block8       1.25500    0.74668   1.681  0.09733 .  
## block9       2.34875    0.74668   3.146  0.00245 ** 
## block10      2.25125    0.74668   3.015  0.00359 ** 
## density     -0.26744    0.00701 -38.149  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.493 on 69 degrees of freedom
## Multiple R-squared:  0.9635, Adjusted R-squared:  0.9582 
## F-statistic: 181.9 on 10 and 69 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With regression, this solution is not convincing. Indeed, the above model assumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected. Is this a reasonable assumption? I vote no.&lt;/p&gt;
&lt;p&gt;Let’s check this by fitting a different regression model per block (ten different slopes + ten different intercepts):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg2 &amp;lt;- lm(yield ~ block/density + block, data=dataset)
anova(mod.reg, mod.reg2)
## Analysis of Variance Table
## 
## Model 1: yield ~ block + density
## Model 2: yield ~ block/density + block
##   Res.Df    RSS Df Sum of Sq      F  Pr(&amp;gt;F)  
## 1     69 153.88                              
## 2     60 115.75  9    38.135 2.1965 0.03465 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-level confirms that the block had a significant effect both on the intercept and on the slope. To describe such an effect we need 20 parameters in the model, which is not very parsimonious. And above all: which regression line do we use for predictions? Taking the block effect as fixed is clearly sub-optimal with regression models.&lt;/p&gt;
&lt;p&gt;The question is: can we fit a simpler and clearer model? The answer is: yes. Why don’t we take the block effect as random? This is perfectly reasonable. Let’s do it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = ~ density|block, data=dataset)
summary(modMix.1)
## Linear mixed-effects model fit by REML
##   Data: dataset 
##        AIC      BIC    logLik
##   340.9166 355.0569 -164.4583
## 
## Random effects:
##  Formula: ~density | block
##  Structure: General positive-definite, Log-Cholesky parametrization
##             StdDev     Corr  
## (Intercept) 3.16871858 (Intr)
## density     0.02255249 0.09  
## Residual    1.38891957       
## 
## Fixed effects:  yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0370844 69  30.65311       0
## density     -0.26744 0.0096629 69 -27.67704       0
##  Correlation: 
##         (Intr)
## density -0.078
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9923722 -0.5657555 -0.1997103  0.4961675  2.6699060 
## 
## Number of Observations: 80
## Number of Groups: 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above fit shows that the random effects (slope and intercept) are sligthly correlated (r = 0.091). We might like to try a simpler model, where random effects are independent. To do so, we need to consider that the above model is equivalent to the following model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s just two different ways to code the very same model. However, this latter coding, based on a ‘pdMat’ structure, can be easily modified to remove the correlation. Indeed, ‘pdSymm’ specifies a totally unstructured variance-covariance matrix for random effects and it can be replaced by ‘pdDiag’, which specifies a diagonal matrix, where covariances (off-diagonal terms) are constrained to 0. The coding is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.2 &amp;lt;- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)
summary(modMix.2)
## Linear mixed-effects model fit by REML
##   Data: dataset 
##       AIC      BIC   logLik
##   338.952 350.7355 -164.476
## 
## Random effects:
##  Formula: ~density | block
##  Structure: Diagonal
##         (Intercept)    density Residual
## StdDev:    3.198267 0.02293222 1.387148
## 
## Fixed effects:  yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0460282 69  30.39102       0
## density     -0.26744 0.0097463 69 -27.44020       0
##  Correlation: 
##         (Intr)
## density -0.139
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9991174 -0.5451478 -0.1970267  0.4925092  2.6700388 
## 
## Number of Observations: 80
## Number of Groups: 10
anova(modMix.1, modMix.2)
##          Model df      AIC      BIC    logLik   Test    L.Ratio p-value
## modMix.1     1  6 340.9166 355.0569 -164.4583                          
## modMix.2     2  5 338.9520 350.7355 -164.4760 1 vs 2 0.03535079  0.8509&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model could be further simplified. For example, the code below shows how we could fit models with either random intercept or random slope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Model with only random intercept
modMix.3 &amp;lt;- lme(yield ~ density, random = list(block = ~1), data=dataset)

#Alternative
#random = ~ 1|block

#Model with only random slope
modMix.4 &amp;lt;- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)

#Alternative
#random = ~density - 1 | block&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with nonlinear regression&lt;/h1&gt;
&lt;p&gt;The problem may become trickier if we have a nonlinear relationship. Let’s have a look at another similar dataset (‘YieldLossB.csv’), that is also available on gitHub. It represents another experiment where sunflower was grown with the same increasing densities of another weed (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 8 blocks. In this case, the yield loss was recorded and analysed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/YieldLossB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yieldLoss
## 1     1       0     1.532
## 2     2       0    -0.661
## 3     3       0    -0.986
## 4     4       0    -0.697
## 5     5       0    -2.264
## 6     6       0    -1.623
plot(yieldLoss ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A swift plot shows that the relationship between density and yield loss is not linear. Literature references (Cousens, 1985) show that this could be modelled by using a rectangular hyperbola:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[YL = \frac{i \, D}{1 + \frac{i \, D}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(YL\)&lt;/span&gt; is the yield loss, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is weed density, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the slope at the origin of axes and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum asymptotic yield loss. This function, together with self-starters, is available in the ‘NLS.YL()’ function in the ‘aomisc’ package, which is the accompanying package for this blog. If you do not have this package, please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this link&lt;/a&gt; to download it.&lt;/p&gt;
&lt;p&gt;The problem is the very same as above: the block effect may produce random fluctuations for both model parameters. The only difference is that we need to use the ‘nlme()’ function instead of ‘lme()’. With nonlinear mixed models, I strongly suggest you use a ‘groupedData’ object, which permits to avoid several problems. The second line below shows how to turn a data frame into a ‘groupedData’ object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
datasetG &amp;lt;- groupedData(yieldLoss ~ 1|block, dataset)
nlin.mix &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                        fixed = list(i ~ 1, A ~ 1),
            random = i + A ~ 1|block)
summary(nlin.mix)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   474.8228 491.5478 -231.4114
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite, Log-Cholesky parametrization
##          StdDev    Corr 
## i        0.1112839 i    
## A        4.0444538 0.195
## Residual 1.4142272      
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23238 0.0382246 104 32.24038       0
## A 68.52305 1.9449745 104 35.23082       0
##  Correlation: 
##   i     
## A -0.408
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4416770 -0.7049388 -0.1805690  0.3385458  2.8788981 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly to linear mixed models, the above coding implies correlated random effects (r = 0.194). Alternatively, the above model can be coded by using a ’pdMat construct, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix2 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdSymm(list(i ~ 1, A ~ 1)))
summary(nlin.mix2)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   474.8225 491.5475 -231.4113
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite
##          StdDev    Corr 
## i        0.1112839 i    
## A        4.0466971 0.194
## Residual 1.4142009      
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23242  0.038225 104 32.24107       0
## A 68.52068  1.945173 104 35.22600       0
##  Correlation: 
##   i     
## A -0.409
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4414051 -0.7049356 -0.1805322  0.3385275  2.8787362 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can try to simplify the model, for example by excluding the correlation between random effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix3 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdDiag(list(i ~ 1, A ~ 1)))
summary(nlin.mix3)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   472.9076 486.8451 -231.4538
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: Diagonal
##                 i        A Residual
## StdDev: 0.1172791 4.389173 1.408963
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23243 0.0393514 104 31.31852       0
## A 68.57655 1.9905549 104 34.45097       0
##  Correlation: 
##   i     
## A -0.459
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.3577291 -0.6849962 -0.1785860  0.3255925  2.8592764 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a little fantasy, we can easily code several alternative models to represent alternative hypotheses about the observed data. Obviously, the very same method can be used (and SHOULD be used) to account for other grouping factors, such as main-plots in split-plot designs or plots in repeated measure designs.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Cousens, R., 1985. A simple model relating yield loss to weed density. Annals of Applied Biology 107, 239–252. &lt;a href=&#34;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jensen, S.M., Schaarschmidt, F., Onofri, A., Ritz, C., 2018. Experimental design matters for statistical analysis: how to handle blocking: Experimental design matters for statistical analysis. Pest Management Science 74, 523–534. &lt;a href=&#34;https://doi.org/10.1002/ps.4773&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1002/ps.4773&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building ANOVA-models for long-term experiments in agriculture</title>
      <link>https://www.statforbiology.com/2020/stat_lte_modelbuilding/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_lte_modelbuilding/</guid>
      <description>


&lt;p&gt;This is the follow-up of a manuscript that we (some colleagues and I) have published in 2016 in the European Journal of Agronomy (Onofri et al., 2016). I thought that it might be a good idea to rework some concepts to make them less formal, simpler to follow and more closely related to the implementation with R. Please, be patient: this lesson may be longer than usual.&lt;/p&gt;
&lt;div id=&#34;what-are-long-term-experiments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are long-term experiments?&lt;/h1&gt;
&lt;p&gt;Agricultural experiments have to deal with long-term effects of cropping practices. Think about fertilisation: certain types of organic fertilisers may give effects on soil fertility, which are only observed after a relatively high number of years (say: 10-15). In order to observe those long-term effects, we need to plan Long Term Experiments (LTEs), wherein each plot is regarded as a small cropping system, with the selected combination of rotation, fertilisation, weed control and other cropping practices. Due to the fact that yield and other relevant variables are repeatedly recorded over time, LTEs represent a particular class of multi-environment experiments with repeated measures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-main-problem-with-ltes-lack-of-independence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The main problem with LTEs: lack of independence&lt;/h1&gt;
&lt;p&gt;We know that, with linear models, once the effects of experimental factors have been accounted for, the residuals must be independent. Otherwise, inferences are invalid.&lt;/p&gt;
&lt;p&gt;With LTEs, observations are repeatedly taken on the same plot and, therefore, the residuals cannot be independent. Indeed, all measurements taken on one specific plot will be affected by the peculiar characteristics of that plot and they will be more alike than measurements taken in different plots. Thus, there will be a ‘plot’ effect, which will induce a within-plot correlation. The problem is: how do we restore the necessary independence of residuals?&lt;/p&gt;
&lt;p&gt;At the basic level, the main way to account for the ‘plot’ effect is by including a random term in the model; in this way, we recognise that there is a plot-to-plot variability, following a gaussian distribution, with mean equal to 0 and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B\)&lt;/span&gt; (‘plot’ error). This plot-to-plot variability is additional to the usual residual variability (within-plot error), that is also gaussian with mean equal to 0 and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As the result, if we take one observation, the variance will be equal to the sum &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B + \sigma^2\)&lt;/span&gt;. If we take two observations in different plots, they will have different random ‘plot’ effects and, thus, they will be independent. Otherwise, if we take two observations in the same plot, they will share the same random plot effect and they will ‘co-vary’, showing a positive covariance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B\)&lt;/span&gt;. The correlation among observations in the same plot will be quantified by the ratio &lt;span class=&#34;math inline&#34;&gt;\(\rho = \sigma^2_B / (\sigma^2_B + \sigma^2)\)&lt;/span&gt; (intra-class correlation).&lt;/p&gt;
&lt;p&gt;In simple words, adding a random plot effect to the model accounts for the fact that observations in the same plot are correlated. This would be similar to a split-plot design (indeed, we talk about split-plot in time) with the important difference that the sub-plot factor (year) is not randomised. The correlation of observations in one plot will always be the same, independent from the year, which is known as Compound Symmetry (CS) correlation structure. Be careful: &lt;strong&gt;it may be more reasonable to assume that observations close in time are more correlated than observations distant in time&lt;/strong&gt;, but we will address this point elsewhere.&lt;/p&gt;
&lt;p&gt;It is necessary to remember that, apart from plots, the experimental design may be characterised by other grouping structures, such as blocks or main plots. All these grouping structures must be appropriately referenced in the model, to account for intra-group correlation. I’ll be back into this in a few moments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-problem-with-ltes-rotation-treatments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Another problem with LTEs: rotation treatments&lt;/h1&gt;
&lt;p&gt;In many instances, the aim of LTEs is to compare different cropping systems, which are allocated to different plots, possibly in different blocks. When the different cropping systems involve rotations, we need to consider a very important rule, that was pointed out by W.G. Cochran in a seminal paper dating back to 1937: “&lt;em&gt;The most important rule about rotation experiments is that each crop in the rotation must be grown every year&lt;/em&gt;”. If we do not follow this rule, “&lt;em&gt;the experiment has to last longer to obtain equal information on the long-term effects of the treatments&lt;/em&gt;” and “&lt;em&gt;the effects of the treatments on the separate crops are obtained under different seasonal conditions, so that a compact summary of the results of the experiment as a whole is made exceedingly difficult&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;The above rule has important consequences: first of all, with, e.g., a three year rotation, we need three plots per treatment and per block in each year, which increases the size of the experiment (that’s why some LTEs are designed without within-year replicates; Patterson, 1964). Secondly, if we want to consider only one crop in the rotation, the experiment becomes unbalanced, as not all plots contribute useful data in each one year. Last, but not least, in long rotations the test crop may return onto the same plot after a relatively long period of time, which may create a totally different correlation structure, compared to short rotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-build-anova-like-models-for-ltes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How do we build ‘ANOVA-like’ models for LTEs?&lt;/h1&gt;
&lt;p&gt;For the reasons explained above, building ANOVA-like models for data analyses may be a daunting task and it is useful to follow a structured procedure. First of all, we need to remember that ANOVA models are based on classification variables, commonly known as factors. There are three types of factors:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;treatment factors, which are randomly allocated to randomisation units (e.g. rotations, fertilisations, management of crop residues);&lt;/li&gt;
&lt;li&gt;block factors, which group the observations according to some ‘innate’ (not randomly allocated) criterion (e.g. by position), such as the blocks, the locations, the main-plots, the sub-plots and so on. Block factors may represent the randomisation units, to which treatments are randomly allocated;&lt;/li&gt;
&lt;li&gt;repeated factors, which relate to time and thus cannot be randomised (e.g. years, cycles …).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to build a model, the starting point is to list all factors (treatment, grouping and repeated) and their relationships. We can follow the general method proposed by Piepho et al. (2003), which we have slightly modified, to make it more ‘R-centric’. The relationships among factors can be specified by using the following operators:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the ‘colon’ operator denotes an interaction of crossed effects (e.g. A:B means that A and B are crossed factors);&lt;/li&gt;
&lt;li&gt;the ‘nesting’ operator denotes nested effects (e.g. A/B means that B is nested within A and it is equal to A + A:B);&lt;/li&gt;
&lt;li&gt;the ‘crossing’ operator denotes the full factorial model for two terms (A*B = A + B + A:B).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When building models we need to pay attention to properly code interactions. Let’s have a look at a simple two-way ANOVA, with the ‘JohnsonGrass.csv’ dataset. In this case we have the two crossed effects Length and Timing and we could build a model as: ‘RIZOMEWEIGHT ~ LENGTH + TIMING + LENGTH:TIMING’ that is shortened as: ’RIZOMEWEIGHT ~ LENGTH*TIMING’. However, if we build our model as: ‘RIZOMEWEIGHT ~ LENGTH:TIMING’, the two main effects are ‘absorbed’ by the term ‘LENGTH:TIMING’, which is no longer an interaction. The code below may clear up what I mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- readr::read_csv(&amp;quot;https://www.casaonofri.it/_datasets/JohnsonGrass.csv&amp;quot;)

mod1 &amp;lt;- lm(RizomeWeight ~ Length + Timing + Length:Timing, data = dataset)
anova(mod1)
## Analysis of Variance Table
## 
## Response: RizomeWeight
##               Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Length         2  1762.2   881.1  9.4795 0.0002961 ***
## Timing         5 16927.9  3385.6 36.4241 3.896e-16 ***
## Length:Timing 10   952.7    95.3  1.0250 0.4354263    
## Residuals     54  5019.2    92.9                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
mod2 &amp;lt;- lm(RizomeWeight ~ Length:Timing, data = dataset)
anova(mod2)
## Analysis of Variance Table
## 
## Response: RizomeWeight
##               Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Length:Timing 17 19642.8 1155.46  12.431 4.766e-13 ***
## Residuals     54  5019.2   92.95                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-to-model-building&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Steps to model building&lt;/h1&gt;
&lt;p&gt;The steps to model building may be summarised as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Select the repeated factor.&lt;/li&gt;
&lt;li&gt;Consider one fixed level of the repeated factor and build a treatment model for the randomized treatment factors.&lt;/li&gt;
&lt;li&gt;Consider one fixed level of the repeated factor and build a block model for block factors.&lt;/li&gt;
&lt;li&gt;Check whether randomised treatment factors might interact with block effects: if such an interaction is to be expected it should be added to the model.&lt;/li&gt;
&lt;li&gt;Include the unrandomised repeated factor into the model.&lt;/li&gt;
&lt;li&gt;Combine treatment model and repeated factor model, by crossing or nesting as appropriate.&lt;/li&gt;
&lt;li&gt;Consider which effects in the block model reference randomisation units, i.e. those units which receive the levels of a factor or factor combination by a randomisation process. It should be clear that randomisation units can be seen as randomly selected from a wider population. Therefore, the corresponding terms should be assigned a separate random effect, as explicitly recommended in Piepho (2004).&lt;/li&gt;
&lt;li&gt;Excluding the terms for randomisation units, nest the repeated factor in all the other terms in the block model.&lt;/li&gt;
&lt;li&gt;Combine random effects for randomisation units with the repeated factor, by using the colon operator, in order to derive the correct error terms to accommodate correlation structures.&lt;/li&gt;
&lt;li&gt;Apart from randomisation units (see #7), decide which factors are random and which are fixed. In our examples, the random model will include all random terms for randomisation units (terms at steps 7 and 9), while the fixed model will include all the other terms. Several extensions/changes to this basic approach are possible.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key idea for the above approach is that for a properly designed experiment, valid analyses should be possible for the data at each single level of the repeated factor. Such a basic requirement should never be taken for granted, but it should be carefully checked before the beginning of the model building process (see later for Example 3).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-further-definitions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some further definitions&lt;/h1&gt;
&lt;p&gt;It is perhaps important to clear up some definitions, which we will use afterwards. Each crop component in a rotation is usually known as a phase; e.g., in the rotation Maize-Wheat-Wheat, Maize is phase 1, Wheat is phase 2 and Wheat is phase 3. The number of phases defines the period (duration) of the rotation. All phases need to be contemporarily present in any one year and, therefore, we can define the so-called sequences: i.e. each of the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; possible arrangements for a rotation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; years, having the same crop ordering, but different initial phases (e.g Maize - Wheat - Wheat, Wheat - Wheat - Maize and Wheat - Maize - Wheat). Each sequences is uniquely identified by its starting phase, which needs to be randomly allocated to each plot at the start of the experiment.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;In order to give a practical demonstration, we have selected five exemplary datasets, relating to LTEs with different designs. If you are in a hurry, you can follow the links below to jump directly to the example that is most relevant for you.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Example 1. &lt;a href=&#34;#example-1-ltes-with-monocultures-or-perennial-crops&#34;&gt;LTEs to compare monocultures or perennial crops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 2. &lt;a href=&#34;#example-2.-ltes-with-different-rotations-of-the-same-length-and-one-test-crop-per-rotation-cycle&#34;&gt;LTEs to compare rotations of the same length and one test crop per rotation cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 3. &lt;a href=&#34;#example-3.-ltes-with-a-fixed-rotation-one-test-crop-per-cycle-and-different-treatments&#34;&gt;LTEs with a fixed rotation (one test crop per cycle) and different treatments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 4. &lt;a href=&#34;#example-4.-lte-with-a-fixed-rotation-different-treatments-and-more-than-one-phase-per-crop-and-cycle&#34;&gt;LTE with a fixed rotation, different treatments and more than one phase per crop and cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 5. &lt;a href=&#34;#example-5-ltes-with-several-rotations-of-different-lengths-and-different-number-of-phases-per-crop-and-rotation-cycle&#34;&gt;LTEs with several rotations of different lengths and different number of phases per crop and rotation cycle&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will analyse all examples, using the ‘tidyverse’ (Wickham, 2019) for data management and the ‘nlme’ package to fit random effect models (Pinheiro et al., 2019). We will also use the ‘asreml-R’ (Butler, 2019) package, for those of you who own a licence. Let’s load those packages in the R environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
# library(asreml)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;example-1-ltes-with-monocultures-or-perennial-crops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: LTEs with monocultures or perennial crops&lt;/h2&gt;
&lt;p&gt;Wheat is grown in continuous cropping from 1983 to 2012, with three fertilisation levels (150, 200 and 250 kg N ha&lt;span class=&#34;math inline&#34;&gt;\(^{-1}\)&lt;/span&gt;), randomly assigned to three plots in each of three blocks. In all, there are nine plots with yearly sampling, with a total of 270 wheat yield observations in 30 years. The following figure shows the design for one block: the spatial split for each plot represents the actual temporal split.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this example, the repeated factor is the year (YEAR). In one year, the treatment factor is nitrogen fertilisation (N) and there are two block factors, i.e. the blocks (BLOCK) and the plots within each block (PLOT). Therefore, the block model is BLOCK + BLOCK:PLOT.&lt;/p&gt;
&lt;p&gt;We now introduce the repeated factor YEAR and combine it with the treatment model, by including N*YEAR = N + YEAR + N:YEAR. The term BLOCK:PLOT references the randomisation units and receives a random effect. As the year might interact with the block, we add the term BLOCK:YEAR. We also combine the year with the random effect for plots (BLOCK:PLOT:YEAR), although this residual term does not need to be explicitly coded when implementing the model. The final model is (the operator ~ means ‘is modelled as’):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ N + BLOCK + YEAR + N:YEAR + BLOCK:YEAR
RANDOM = BLOCK:PLOT + BLOCK:PLOT:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the above notation in R, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE1.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Block, Plot, Year, N), factor))
head(dataset)
## # A tibble: 6 x 5
##   Plot  N     Year  Block Yield
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
## 1 33    fn150 1983  1      4.54
## 2 98    fn150 1983  3      4.18
## 3 162   fn150 1983  2      3.7 
## 4 33    fn150 1984  1      4.57
## 5 98    fn150 1984  3      5.04
## 6 162   fn150 1984  2      5.06
# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Year + N + N:Year + Block:Year,
           random = ~ 1|Plot, data = dataset)
anova(mod)
##             numDF denDF  F-value p-value
## (Intercept)     1   116 5434.427  &amp;lt;.0001
## Block           2     4    0.300  0.7564
## Year           29   116   36.496  &amp;lt;.0001
## N               2     4    1.304  0.3664
## Year:N         58   116    1.663  0.0105
## Block:Year     58   116    2.545  &amp;lt;.0001
# Implementation with asreml (the residual statement is unnecessary, here)
# Need to sort the data according to the residual statement
# datasetS &amp;lt;- dataset %&amp;gt;% 
#  arrange(Plot, Year)
# mod &amp;lt;- asreml(Yield ~ Block + Year + N + N:Year + Block:Year,
#           random = ~ Plot, 
#           residual = ~ Plot:Year, data = datasetS)
# wald(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is worth to notice that the same model may be fitted in an alternative way, i.e. by dropping the BLOCK:PLOT random effects and using the residual term BLOCK:PLOT:YEAR to accommodate the CS structure into the model. This may be done very intuitively with ‘asreml’, by using the following notation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Code not run
# mod &amp;lt;- asreml(Yield ~ Block + Year + N + N:Year + Block:Year,
#           residual = ~ Plot:cor(Year), data = datasetS)
# summary(mod)$varcomp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the final command returns the correlation between observations in the same plot. With ‘lme’ package, the notation is different, as we have to switch from the ‘lme()’ to the ‘gls()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- gls(Yield ~ Block + Year + N + N:Year + Block:Year,
           correlation = corCompSymm(form = ~1|Plot), data = dataset)
mod$modelStruct$corStruct
## Correlation structure of class corCompSymm representing
##       Rho 
## 0.1269864&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This alternative coding can be used to implement different correlation structures for the cases when a simple CS correlation structure is not satisfactory. For example, when the observations close in time are more correlated than those distant in time, we can implement a serial correlation structure by appropriately changing the ‘residual’ argument in ‘asreml()’ or the ‘correlation’ argument in ‘lme()’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2.-ltes-with-different-rotations-of-the-same-length-and-one-test-crop-per-rotation-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2. LTEs with different rotations of the same length and one test crop per rotation cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in five types of two-year rotations, with either pea (&lt;em&gt;Pisum sativum&lt;/em&gt; L), grain sorghum (&lt;em&gt;Sorghum bicolor&lt;/em&gt; (L.) Moench), sugar beet (&lt;em&gt;Beta vulgaris&lt;/em&gt; L. subsp. &lt;em&gt;saccharifera&lt;/em&gt;), sunflower (&lt;em&gt;Helianthus annuus&lt;/em&gt; L.) and faba bean (&lt;em&gt;Vicia faba&lt;/em&gt; L. subsp. &lt;em&gt;minor&lt;/em&gt;). For each rotation, there are two possible sequences (wheat in odd years and wheat in even years) and the ten combinations (five rotations by two sequences) are completely randomised to ten plots per each of three blocks (Figure 2). Therefore, five wheat plots out of the available ten plots are used from each block and year, for a total of 450 observations, from 1983 to 2012. The experimental design for one block is shown in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure2.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example we have two crops in a rotation and both crops are grown in different plots in the same year. Thus, for each rotation, we have two sequences in time (e.g., maize-sunflower and sunflower-maize). If we consider only one of the two crops, the main difference with respect to Example 1 is that the data obtained in two consecutive years for the same treatment and block are independent, in the sense that they are obtained in different plots. Otherwise, data obtained in a two-year interval (on different rotation cycles) on the same block are correlated, as they originate from the same plot.&lt;/p&gt;
&lt;p&gt;Which is the repeated factor? Indeed, if we look only at one phase in the rotation (in this case wheat), we note that observations are repeated every second year on the same plot (Figure above), according to the sequence they belong to. In other words, observations are repeated on each rotation cycle (CYC; two years) in the same plot, while there is neither a within-cycle repetition nor a within-cycle phase difference: we have only one observation per plot per cycle. Therefore, it is natural to take the rotation cycle as the repeated factor (CYC instead of YEAR).&lt;/p&gt;
&lt;p&gt;As the next step, we should look at what happens in one fixed level of CYC: what did we randomize to the ten plots in a two-years time slot? It is clear that, considering only wheat, we randomised each combination of rotation (ROT) and positioning in the sequence (SEQ; i.e. wheat as the first crop of the sequence and wheat as the second crop of the sequence). Therefore, the treatment factors are ROT and SEQ. Now we can cross the repeated factor with the treatment factors. Accordingly, The model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ SEQ*ROT + BLOCK + CYC + ROT:CYC + BLOCK:CYC
RANDOM: BLOCK:PLOT + BLOCK:PLOT:CYC &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code below shows how to fit the model with R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE2.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor))
head(dataset)
## # A tibble: 6 x 8
##   Block Main  Plot  Rot   Year  Sequence Cycle Yield
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
## 1 1     1_1   4     SBW   1983  1        1      5.1 
## 2 3     3_1   70    SBW   1983  1        1      4.5 
## 3 2     2_1   135   SBW   1983  1        1      4.53
## 4 1     1_0   27    SBW   1984  0        1      5.83
## 5 3     3_0   95    SBW   1984  0        1      6.26
## 6 2     2_0   160   SBW   1984  0        1      6.22
# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Rot*Sequence + Block:Sequence +
             Cycle + Cycle:Sequence + Rot:Cycle + 
             Rot:Sequence:Cycle + Cycle:Block +
             Cycle:Block:Sequence,
             random = ~1|Plot,
           data = dataset)
anova(mod)
##                      numDF denDF   F-value p-value
## (Intercept)              1   224 115809.71  &amp;lt;.0001
## Block                    2    16     42.68  &amp;lt;.0001
## Rot                      4    16      4.19  0.0165
## Sequence                 1    16     26.14  0.0001
## Cycle                   14   224    133.43  &amp;lt;.0001
## Rot:Sequence             4    16      3.60  0.0283
## Block:Sequence           2    16      2.24  0.1384
## Sequence:Cycle          14   224    119.86  &amp;lt;.0001
## Rot:Cycle               56   224      1.74  0.0025
## Block:Cycle             28   224      8.77  &amp;lt;.0001
## Rot:Sequence:Cycle      56   224      1.61  0.0081
## Block:Sequence:Cycle    28   224      6.85  &amp;lt;.0001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach is commonly suggested in literature (see Yates, 1954) and it is convenient, mainly because the resulting model is orthogonal and may be fitted by ordinary least squares. Indeed, for Dataset 2 (and similar experiments), there is only one observation for each block, treatment, cycle, sequence and no missing data (in our case: 3 blocks x 5 rotations x 2 sequences x 15 cycles = 450 observations). The phase should not enter into this model, as we are looking only at one of the two crops (only one phase).&lt;/p&gt;
&lt;p&gt;However, the drawback is that such an approach cannot be immediately extended to the other more complex examples (e.g. rotations with different lengths and/or with a different number of test-crops). Furthermore, the effect of years is partitioned into three components, i.e. ‘cycles’, ‘sequences’ and ‘cycle x sequences’, which might make modeling possible ‘fertility’ trends over time less immediate. In this respect, we should note that possible differences between sequences for a given cycle (wheat as the first crop of the sequence and wheat as the second crop of the sequence, i.e. wheat in even years and wheat in odd years) do not carry any meaning that helps understand the behaviour of rotations.&lt;/p&gt;
&lt;p&gt;An alternative and more natural approach is to take the year as the repeated factor; indeed, for Example 2, the year effect is totally confounded with the factorial combination of ‘cycle’ and ‘sequence’ (15 cycles x 2 sequences = 30 years). If we consider the YEAR as the repeated factor, in one year the treatment model is composed only by the rotation (ROT), that is allocated to plots (PLOT), within blocks. The block model for one year is BLOCK/PLOT = BLOCK + BLOCK:PLOT. We can combine the treatment model with the repeated factor (ROT:YEAR) and add the term BLOCK:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ ROT + BLOCK + YEAR + ROT:YEAR + BLOCK:YEAR
RANDOM: BLOCK:PLOT + BLOCK:PLOT:YEAR &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from random effects for randomisation units, this model is totally similar to the one used for multi-environment genotype experiments and, represents a convenient and clear platform for the analyses of LTE data. We remind the reader that the residual term (BLOCK:PLOT:YEAR) does not need to be explicitly coded when implementing the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Rot + Year + Rot:Year + Block:Year,
             random = ~1|Plot,
           data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us a good common modelling platform for all datasets, although it may be argued that the models are no longer orthogonal, as not all plots produce data in all years. It should be recognised, however, that the lack of orthogonality can easily be accommodated within mixed models.&lt;/p&gt;
&lt;p&gt;The situation is totally different if we look at both the phases of the rotation (e.g. wheat and sunflower): in this case, we have a phase difference within each cycle and, considering one level of the repeated factor year, the treatment model should contain both the rotation and the phase, together with their interaction. When we introduce the year (steps 5 and 6 above), we also introduce the interactions ‘year x rotation’, ‘year x phase’ and ‘year x rotation x phase’, which are all meaningful when studying the behaviour of rotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3.-ltes-with-a-fixed-rotation-one-test-crop-per-cycle-and-different-treatments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3. LTEs with a fixed rotation (one test crop per cycle) and different treatments&lt;/h2&gt;
&lt;p&gt;Durum wheat (&lt;em&gt;Triticum durum&lt;/em&gt; L.) is grown in a two-year rotation with a spring crop and nine cropping systems, consisting of the factorial combination of three soil tillage methods (CT: conventional 40 cm deep ploughing; M: scarification at 25 cm; S: sod seeding with chemical desiccation and chopping) and three N-fertilisation levels (N0, N90 and N180, corresponding to 0, 90 and 180 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;). The two possible rotation sequences (wheat-spring crop and spring crop-wheat) are arranged in two adjacent fields, which therefore host the two different crops of the rotation in the same year. Within the two fields, there are two independent randomisations, each with two blocks, tillage levels randomised to main-plots (1500 &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;) and N levels randomised to sub-plots (500 &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), according to a split-plot design with two replicates. The design is taken from Seddaiu et al. (2016), while the data have been simulated by using Monte Carlo methods.&lt;/p&gt;
&lt;p&gt;This type of LTE is very similar to the previous one, though we have a different experimental layout:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;there are two experimental treatments, laid out in a split-plot design;&lt;/li&gt;
&lt;li&gt;the two sequences are accommodated in two fields.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The experimental design for Dataset 3, for each of two fields in one year is reported in the figure below. The position of wheat and spring crop is exchanged in the following year (CT: conventional ploughing; M: scarification; S: sod seeding; 0: no nitrogen fertilisation; 1: 90 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;; 2: 180 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure3.jpg&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before proceeding to model building for Example 3, we need to discuss whether valid analyses are possible at each single level of the repeated factor. Indeed, this is clearly true if we take the year as the repeated factor and consider only one of the two crops in the rotation (wheat, in this case). However, if we intended to consider both crops and compare e.g. their yields, the crop effect would be confounded with the field effect within a single year and, therefore, valid within-year analyses would not be possible. In this case, we should resort to taking the rotation cycle as the repeated factor.&lt;/p&gt;
&lt;p&gt;Dealing only with wheat, we can therefore take the YEAR as the repeated factor and consider that, in one year, the randomised treatment factors are tillage (T) and nitrogen fertilisation (N) and the treatment model is T + N + T:N.&lt;/p&gt;
&lt;p&gt;The block factors are the FIELDS, the BLOCKS within fields, the MAIN plots within blocks and the subplots (SUB) within main plots. The block model (for one year) is FIELD + FIELD:BLOCK + FIELD:BLOCK:MAIN + FIELD:BLOCK:MAIN:SUB.&lt;/p&gt;
&lt;p&gt;The treatment and repeated model can be combined as: (T + N + T:N)*YEAR = T + N + T:N + YEAR + T:YEAR + N:YEAR + T:N:YEAR.&lt;/p&gt;
&lt;p&gt;At this stage, the FIELD main effect needs to be removed, as it is totally confounded with the years. We assign a random effect to the other randomisation units, i.e. FIELD:BLOCK, FIELD:BLOCK:MAIN and FIELD:BLOCK:MAIN:SUB and combine these random terms with the repeated factor YEAR, by using the colon operator, which leads us to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ T + N + T:N + YEAR + T:YEAR + N:YEAR + T:N:YEAR
RANDOM: FIELD:BLOCK + FIELD:BLOCK:MAIN + FIELD:BLOCK:MAIN:SUB + FIELD:BLOCK:YEAR + FIELD:BLOCK:MAIN:YEAR + FIELD:BLOCK:MAIN:SUB:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual, the last term (residual) does not need to be explicitly included when implementing the model, but it can be used, together with the two previous ones (FIELD:BLOCK:YEAR + FIELD:BLOCK:MAIN:YEAR) to accommodate possible serial correlation structures into the model, by allowing year-specifity of all design effects and the residuals. For this types of models with several crossed random effects, the coding of ‘lme()’ is not straightforward and it does not always lead to a flexible implementation of correlation structures.&lt;/p&gt;
&lt;p&gt;In the code below we need to build dummy variables for all random effects (five variables, excluding the residual error term, which is not needed). Afterwards, we have to code the random effects as a list; R expects that the element of such list are nested and, therefore, we need to work around this by coding an additional variable, which takes the value of ‘1’ for all subjects, so that the nesting structure is only artificial. We use the ‘pdIdent’ construct to say that each random effect is homoscedastic and uncorrelated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE3.csv&amp;quot;)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:9), factor)) %&amp;gt;% 
  mutate(FB = factor(Block:Field),
         FBM = factor(FB:Main),
         FBMS = factor(FBM:Sub),
         FBY = factor(FB:Year),
         FBMY = factor(FBM:Year),
         one = 1L)

mod &amp;lt;- lme(Yield ~ T + N + N:T + 
                 Year + Year:T + Year:N + Year:N:T,
                 random = list(one = pdIdent(~FB - 1),
                               one = pdIdent(~FBM - 1),
                               one = pdIdent(~FBMS - 1),
                               one = pdIdent(~FBY - 1),
                               one = pdIdent(~FBMY - 1)),
               data = dataset, na.action = na.omit)

# Fixed effects tested by using LRT
library(car)
Anova(mod)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##             Chisq Df Pr(&amp;gt;Chisq)    
## T          5.1169  2    0.07743 .  
## N        804.1717  2  &amp;lt; 2.2e-16 ***
## Year     446.6794 17  &amp;lt; 2.2e-16 ***
## T:N        6.2105  4    0.18397    
## T:Year   142.5132 34  3.065e-15 ***
## N:Year   246.5584 34  &amp;lt; 2.2e-16 ***
## T:N:Year 159.3230 68  2.705e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fit takes a long time and it is not easy to manipulate the model to introduce correlation structures for the random effects. However, it is not difficult to introduce the correlation of residuals, by using the ‘correlation’ argument (see above).&lt;/p&gt;
&lt;p&gt;Coding the same model with ‘asreml()’ is easier and so is to introduce patterned correlations structures. However, the design has to be balanced and, therefore, we need to introduce NAs for missing observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Asreml fit (not run)
# datasetS &amp;lt;- dataset %&amp;gt;% 
#  arrange(Sub, Year)
# mod2 &amp;lt;- asreml(Yield ~ T + N + N:T + 
#                 Year + Year:T + Year:N + Year:N:T,
#                 random = ~FB + FB:Main + FB:Main:Sub + 
#                 FB:Year + FB:Main:Year,
#               residual = ~ Sub:Year,
#               data = datasetS)
# summary(mod2)$varcomp&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4.-lte-with-a-fixed-rotation-different-treatments-and-more-than-one-phase-per-crop-and-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4. LTE with a fixed rotation, different treatments and more than one phase per crop and cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in a three-year rotation maize-wheat-wheat, under two types of management of crop residues (burial and removal), which are randomised to main plots, while the three possible rotation sequences are randomised to subplots. This experiment has 18 plots (three sequences x two treatment levels x three blocks) and, in every year, 12 of those are cropped with wheat and six with maize.&lt;/p&gt;
&lt;p&gt;Also in this case, the response variable is wheat yield from 1983 to 2012, i.e. twelve observations per year and 360 observations in total. Data obtained in the same plot in different years belong to two different phases (wheat after maize and wheat after wheat; the experimental design for one block is shown in the figure below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure4.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With respect to Example 3, the situation becomes more complex, because we have two distinct phases in each rotation cycle (phase difference: wheat after maize and wheat after wheat). As usual, we start by regarding the YEAR as the repeated factor. In one year, the treatment factors are the management of soil residues (RES, that is randomly allocated to main-plots) and the phases (P; randomly allocated to subplots); the treatment model is indeed: RES*P.&lt;/p&gt;
&lt;p&gt;In one year, the block model is: BLOCK/MAIN/SUB = BLOCK + BLOCK:MAIN + BLOCK:MAIN:SUB. Introducing the YEAR as repeated factor, we can combine the treatment model with the repeated model as: RES + P + &lt;a href=&#34;RES:P&#34; class=&#34;uri&#34;&gt;RES:P&lt;/a&gt; + YEAR + &lt;a href=&#34;RES:YEAR&#34; class=&#34;uri&#34;&gt;RES:YEAR&lt;/a&gt; + P:YEAR + &lt;a href=&#34;RES:P:YEAR&#34; class=&#34;uri&#34;&gt;RES:P:YEAR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The terms BLOCK:MAIN and BLOCK:MAIN:SUB reference randomisation units and should receive random effects. The blocks may interact with the years (BLOCK:YEAR), while the random effects for randomisation units can be made year-specific by adding BLOCK:MAIN:YEAR and the residual term BLOCK:MAIN:SUB:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ RES + P + RES:P + YEAR + RES:YEAR + P:YEAR + RES:P:YEAR + BLOCK + BLOCK:YEAR
RANDOM: BLOCK:MAIN + BLOCK:MAIN:SUB + BLOCK:MAIN:YEAR + BLOCK:MAIN:SUB:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE4.csv&amp;quot;)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor)) %&amp;gt;% 
  mutate(Main = factor(Block:Res),
         Sub = factor(Main:Sub))

mod &amp;lt;- lme(Yield ~ Block + Res*P + 
              Year + Year:Res + Year:P + Year:Res:P +
              Block:Year, 
           random = list(Main = pdIdent(~1),
                         Main = pdIdent(~Sub - 1),
                         Main = pdIdent(~Year - 1)),
           data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-5-ltes-with-several-rotations-of-different-lengths-and-different-number-of-phases-per-crop-and-rotation-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 5: LTEs with several rotations of different lengths and different number of phases per crop and rotation cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in five maize (M) - wheat (W) rotations of different lengths, i.e. M-W, M-W-W, M-W-W-W, M-W-W-W-W, M-W-W-W-W-W. For all rotations, all phases are contemporarily present in each year, for a total of 20 plots (one for each of the possible sequences, i.e. 2 + 3 + 4 + 5 + 6 = 20) in each of three blocks. Considering wheat yield as the response variable, we find that only 15 observations are obtained in each year, for a total of 1350 data, from 1983 to 2012.&lt;/p&gt;
&lt;p&gt;Experiments of this type represent a high degree of complexity. Indeed, in contrast to all other examples, after 30 years there are plots with: (i) a different number of observations for the same test crop; (ii) a different number of cycles (in some cases the last cycle is also incomplete); (iii) a different number of phases for wheat.&lt;/p&gt;
&lt;p&gt;In some cases, it is necessary to compare several rotations with different characteristics (e.g. a different duration and/or a different number of tests crops and /or a different number of phases per crop), which may create a complex design with some degree of non-orthogonality.&lt;/p&gt;
&lt;p&gt;The repeated factor is again the YEAR. In one year, the treatment factors are the rotation system (ROT) and the rotation phase (P), which are randomly allocated to plots. As there is a different number of phases for each rotation, we nest the phase within the rotation, leading to the following treatment model: ROT/P = ROT + P:ROT.&lt;/p&gt;
&lt;p&gt;In one year, the block model is BLOCK/PLOT = BLOCK + BLOCK:PLOT. The repeated factor is included and combined with the treatment model, by introducing YEAR + ROT:YEAR + ROT:P:YEAR.&lt;/p&gt;
&lt;p&gt;The term BLOCK:PLOT references randomisation units and needs to receive a random effect, while the term YEAR:BLOCK can be added to the model, together with the residual term BLOCK:PLOT:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ ROT + P:ROT + YEAR + ROT:YEAR + ROT:P:YEAR
RANDOM: BLOCK:PLOT + BLOCK:PLOT:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the implementation below we did not succeed reaching convergence with ‘lme()’, but we were successful ‘with asreml()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE5.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor))

# mod &amp;lt;- asreml(Yield ~ Block + Rot + Rot:P + 
#                 Year + Year:Rot + Year:Rot:P + Block:Year,
#               random = ~Plot,
#               data = dataset)

# mod &amp;lt;- lme(Yield ~ Block + Rot + Rot:P + 
#                Year + Year:Rot + Year:Rot:P + Block:Year, 
#                random=~1|Plot,
#              data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Warning!&lt;/h1&gt;
&lt;p&gt;Models 1 to 5 are fairly similar and they are very closely related to those used for multi-environment experiments. However, there are some peculiar aspects which needs to be taken under consideration.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Apart from Dataset 1, there is always a certain degree of unbalance, as plots do not produce data every year. Testing for fixed effects requires great care.&lt;/li&gt;
&lt;li&gt;In all cases, the model formulations shown above induce a compound symmetry correlation structure for observations taken in the same plot over time (‘split-plot in time’). This is seldom appropriate and, thus, more complex correlations structures should be considered.&lt;/li&gt;
&lt;li&gt;In the above formulations, only randomisation units have been given a random effect, while all the other effects have been regarded as fixed. Obviously, depending on the aims of the analyses, it might be convenient and appropriate to regard some of these effects (e.g., the year main effect and interactions) as random.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It’s all, thanks for reading this far. If you have any comments, please, drop me a note at &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;. Best wishes,&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Brien, C.J., Demetrio, C.G.B., 2009. Formulating mixed models for experiments, including longitudinal experiments. Journal of Agricultural, Biological and Environmental Statistics 14, 253–280.&lt;/li&gt;
&lt;li&gt;David Butler (2019). asreml: Fits the Linear Mixed Model. R package version 4.1.0.110. www.vsni.co.uk&lt;/li&gt;
&lt;li&gt;Cochran, W.G., 1939. Long-Term Agricultural Experiments. Supplement to the Journal of the Royal Statistical Society 6, 104–148.&lt;/li&gt;
&lt;li&gt;Onofri, A., Seddaiu, G., Piepho, H.-P., 2016. Long-Term Experiments with cropping systems: Case studies on data analysis. European Journal of Agronomy 77, 223–235.&lt;/li&gt;
&lt;li&gt;Patterson, H.D., 1964. Theory of Cyclic Rotation Experiments. Journal of the Royal Statistical Society. Series B (Methodological) 26, 1–45.&lt;/li&gt;
&lt;li&gt;Payne, R.W., 2015. The design and analysis of long-term rotation experiments. Agronomy Journal 107, 772–785.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Richter, C., 2004. A Mixed Modelling Approach for Randomized Experiments with Repeated Measures. Journal of Agronomy and Crop Science 190, 230–247.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2019). &lt;em&gt;nlme: Linear and Nonlinear Mixed Effects Models&lt;/em&gt;. R package version 3.1-142, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Seddaiu, G., Iocola, I., Farina, R., Orsini, R., Iezzi, G., Roggero, P.P., 2016. Long term effects of tillage practices and N fertilization in rainfed Mediterranean cropping systems: durum wheat, sunflower and maize grain yield. European Journal of Agronomy 77, 166–178. &lt;a href=&#34;https://doi.org/10.1016/j.eja.2016.02.008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.eja.2016.02.008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34; class=&#34;uri&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting complex mixed models with nlme. Example #5</title>
      <link>https://www.statforbiology.com/2020/stat_met_jointreg/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_met_jointreg/</guid>
      <description>


&lt;div id=&#34;a-joint-regression-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Joint Regression model&lt;/h1&gt;
&lt;p&gt;Let’s talk about a very old, but, nonetheless, useful technique. It is widely known that the yield of a genotype in different environments depends on environmental covariates, such as the amount of rainfall in some critical periods of time. Apart from rain, also temperature, wind, solar radiation, air humidity and soil characteristics may concur to characterise a certain environment as good or bad and, ultimately, to determine yield potential.&lt;/p&gt;
&lt;p&gt;Early in the 60s, several authors proposed that the yield of genotypes is expressed as a function of an environmental index &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt;, measuring the yield potential of each environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (Finlay and Wilkinson, 1963; Eberhart and Russel, 1966; Perkins and Jinks, 1968). For example, for a genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, we could write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu_i + \beta_i e_j\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the yield &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in a certain environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is expressed as a linear function of the environmental index &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is the slope, which expresses how the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; responds to the environment.&lt;/p&gt;
&lt;p&gt;A graphical example may be useful; in the figure below we have two genotypes tested in 10 environments. The yield of the first genotype (red) increases as the environmental index increases, with slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 0.81\)&lt;/span&gt;. On the other hand, the yield of the second genotype (blue) does not change much with the environment (&lt;span class=&#34;math inline&#34;&gt;\(\beta_2 = -0.08)\)&lt;/span&gt;. Clearly, a high value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; demonstrates that the genotype is responsive to the environment and makes profit of favorable conditions. Otherwise, a low &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; value (close to 0) demonstrates that the genotype is not responsive and tends to give more or less the same yield in all environments (static stability; Wood, 1976).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_met_JointReg_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By now, it should be clear that &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a relevant measure of stability. Now, the problem is: how do we determine such value from a multi-environment genotype experiment? As usual, let’s start from a meaningful example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-multi-environment-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A multi-environment experiment&lt;/h1&gt;
&lt;p&gt;Let’s take the data in Sharma (2006; Statistical And Biometrical Techniques In Plant Breeding, New Age International ltd. New Delhi, India). They refer to a multi-environment experiment with 7 genotypes, 6 environments and 3 blocks; let’s load the data in the dataframe ‘dataFull’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(emmeans)
## Welcome to emmeans.
## NOTE -- Important change from versions &amp;lt;= 1.41:
##     Indicator predictors are now treated as 2-level factors by default.
##     To revert to old behavior, use emm_options(cov.keep = character(0))
Block &amp;lt;- factor(rep(c(1:3), 42))
Var &amp;lt;- factor(rep(LETTERS[1:7],each=18))
Loc &amp;lt;- factor(rep(rep(letters[1:6], each=3), 7))
P1 &amp;lt;- factor(Loc:Block)
Yield &amp;lt;- c(60,65,60,80,65,75,70,75,70,72,82,90,48,45,50,50,40,40,
           80,90,83,70,60,60,85,90,90,70,85,80,40,40,40,38,40,50,
           25,28,30,40,35,35,35,30,30,40,35,35,35,25,20,35,30,30,
           50,65,50,40,40,40,48,50,52,45,45,50,50,50,45,40,48,40,
           52,50,55,55,54,50,40,40,60,48,38,45,38,30,40,35,40,35,
           22,25,25,30,28,32,28,25,30,26,28,28,45,50,45,50,50,50,
           30,30,25,28,34,35,40,45,35,30,32,35,45,35,38,44,45,40)
dataFull &amp;lt;- data.frame(Block, Var, Loc, Yield)
rm(Block, Var, Loc, P1, Yield)
head(dataFull)
##   Block Var Loc Yield
## 1     1   A   a    60
## 2     2   A   a    65
## 3     3   A   a    60
## 4     1   A   b    80
## 5     2   A   b    65
## 6     3   A   b    75&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-an-environmental-index&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is an environmental index?&lt;/h1&gt;
&lt;p&gt;First of all, we need to define an environmental index, which can describe the yield potential in each of the seven environments. Yates and Cochran (1937) proposed that we use the mean of all observations in each environment, expressed as the difference between the environmental mean yield &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and the overall mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (i.e. &lt;span class=&#34;math inline&#34;&gt;\(e_j = \mu_j - \mu\)&lt;/span&gt;). Let’s do it; in the box below we use the package ‘dplyr’ to augment the dataset with a new variable, representing the environmental indices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
dataFull &amp;lt;- dataFull %&amp;gt;%
  group_by(Loc) %&amp;gt;% 
  mutate(ej = mean(Yield) - mean(dataFull$Yield))
head(dataFull)
## # A tibble: 6 x 5
## # Groups:   Loc [2]
##   Block Var   Loc   Yield    ej
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 1     A     a        60 1.45 
## 2 2     A     a        65 1.45 
## 3 3     A     a        60 1.45 
## 4 1     A     b        80 0.786
## 5 2     A     b        65 0.786
## 6 3     A     b        75 0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This step is ok with balanced data and it is clear that a high environmental index identifies the favorable environments, while a low (negative) environmental index identifies unfavorable environments. It is necessary to keep in mind that we have unwillingly put a constraint on &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; values, that have to sum up to zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-model-definition-equation-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Full model definition (Equation 1)&lt;/h1&gt;
&lt;p&gt;Now, it is possible to regress the yield data for each genotype against the environmental indices, according to the following joint regression model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \gamma_{jk} + \mu_i + \beta_i e_j + d_{ij} + \varepsilon_{ijk} \quad\quad\quad \textrm{(Equation 1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where: &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; is the yield for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of blocks within environments and &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the average yield for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. As we have seen in the figure above, the average yield of a genotype in each environment cannot be exactly described by the regression against the environmental indices (in other words: the observed means do not lie along the regression line). As the consequence, we need the random term &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; to represent the deviation from the regression line for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Finally, the random elements &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; represent the deviations between the replicates for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (within-trial errors). As I said, &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; are random, with variances equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;According to Finlay-Wilkinson, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; is assumed to be equal for all genotypes. Otherwise, according to Eberarth-Russel, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d}\)&lt;/span&gt; may assume a different value for each genotype (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d(i)}\)&lt;/span&gt;) and may become a further measure of stability: if this is small, a genotype does not show relevant variability of yield, apart from that due to the regression against the environmental indices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;We can start the analyses by fitting a traditional ANOVA model, to keep as a reference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov &amp;lt;- lm(Yield ~ Loc/Block + Var*Loc, data = dataFull)
anova(mod.aov)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Loc        5  1856.0   371.2  17.9749 1.575e-11 ***
## Var        6 20599.2  3433.2 166.2504 &amp;lt; 2.2e-16 ***
## Loc:Block 12   309.8    25.8   1.2502    0.2673    
## Loc:Var   30 12063.6   402.1  19.4724 &amp;lt; 2.2e-16 ***
## Residuals 72  1486.9    20.7                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said, Equation 1 is a mixed model, which calls for the use of the ‘lme()’ function. For better understanding, it is useful to start by augmenting the previous ANOVA model with the regression term (‘Var/ej’). We use the nesting operator, to have different regression lines for each level of ‘Var’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Augmented ANOVA model
mod.aov2 &amp;lt;- lm(Yield ~ Loc/Block + Var/ej + Loc:Var, data=dataFull)
anova(mod.aov2)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Loc        5  1856.0   371.2  17.9749 1.575e-11 ***
## Var        6 20599.2  3433.2 166.2504 &amp;lt; 2.2e-16 ***
## Loc:Block 12   309.8    25.8   1.2502    0.2673    
## Var:ej     6  9181.2  1530.2  74.0985 &amp;lt; 2.2e-16 ***
## Loc:Var   24  2882.5   120.1   5.8159 2.960e-09 ***
## Residuals 72  1486.9    20.7                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the GE interaction in the ANOVA model has been decomposed into two parts: the regression term (‘Var/ej’) and the deviation from regression (‘Loc:Var’), with 6 and 24 degrees of freedom, respectively. This second term corresponds to &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; in Equation 1 (please, note that the two terms ‘Var/ej’ and ‘Loc:Var’ are partly confounded).&lt;/p&gt;
&lt;p&gt;The above analysis is only useful for teaching purposes, but it is unsatisfactory, because the &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; terms have been regarded as fixed, which is pretty illogical. Therefore, we change the fixed effect model into a mixed model, where we include the random ‘genotype by environment’ interaction. We also change the fixed block effect into a random effect and remove the intercept, to more strictly adhere to the parameterisation of Equation 1. The two random effects ‘Loc:Block’ and ‘Loc:Var’ are not nested into each other and we need to code them by using ‘pdMat’ constructs, which are not straightforward. You can use the code in the box below as a guidance to fit a Finlay-Wilkinson model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finlay-Wilkinson model
modFull1 &amp;lt;- lme(Yield ~ Var/ej - 1, 
                random = list(Loc = pdIdent(~ Var - 1),
                              Loc = pdIdent(~ Block - 1)), 
                data=dataFull)
summary(modFull1)$tTable
##              Value Std.Error  DF    t-value      p-value
## VarA    63.1666667 2.4017164 107 26.3006350 1.624334e-48
## VarB    66.1666667 2.4017164 107 27.5497417 2.135264e-50
## VarC    31.8333333 2.4017164 107 13.2544097 2.599693e-24
## VarD    47.1111111 2.4017164 107 19.6156012 3.170228e-37
## VarE    44.7222222 2.4017164 107 18.6209421 2.378452e-35
## VarF    34.2777778 2.4017164 107 14.2722004 1.614127e-26
## VarG    35.8888889 2.4017164 107 14.9430169 6.028635e-28
## VarA:ej  3.2249875 0.6257787 107  5.1535588 1.176645e-06
## VarB:ej  4.7936139 0.6257787 107  7.6602379 8.827229e-12
## VarC:ej  0.4771074 0.6257787 107  0.7624219 4.474857e-01
## VarD:ej  0.3653064 0.6257787 107  0.5837629 5.606084e-01
## VarE:ej  1.2369950 0.6257787 107  1.9767291 5.064533e-02
## VarF:ej -2.4316943 0.6257787 107 -3.8858692 1.770611e-04
## VarG:ej -0.6663160 0.6257787 107 -1.0647790 2.893729e-01
VarCorr(modFull1)
##          Variance           StdDev   
## Loc =    pdIdent(Var - 1)            
## VarA     27.5007919         5.2441197
## VarB     27.5007919         5.2441197
## VarC     27.5007919         5.2441197
## VarD     27.5007919         5.2441197
## VarE     27.5007919         5.2441197
## VarF     27.5007919         5.2441197
## VarG     27.5007919         5.2441197
## Loc =    pdIdent(Block - 1)          
## Block1    0.4478291         0.6692003
## Block2    0.4478291         0.6692003
## Block3    0.4478291         0.6692003
## Residual 20.8781458         4.5692610&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the variance component &lt;span class=&#34;math inline&#34;&gt;\(\sigma_d\)&lt;/span&gt; (27.50) is the same for all genotypes; if we want to let a different value for each genotype (Eberarth-Russel model), we need to change the ‘pdMat’ construct for the ‘Loc:Var’ effect, turning from ‘pdIdent’ to ‘pdDiag’, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eberhart-Russel model
modFull2 &amp;lt;- lme(Yield ~ Var/ej - 1, 
                random = list(Loc = pdDiag(~ Var - 1),
                              Loc = pdIdent(~ Block - 1)), 
                data=dataFull)
summary(modFull2)$tTable
##              Value Std.Error  DF    t-value      p-value
## VarA    63.1666667 3.0507629 107 20.7052032 3.221930e-39
## VarB    66.1666667 2.7818326 107 23.7852798 1.604422e-44
## VarC    31.8333333 1.7240721 107 18.4640387 4.753742e-35
## VarD    47.1111111 2.3526521 107 20.0246824 5.564350e-38
## VarE    44.7222222 2.4054296 107 18.5921974 2.699536e-35
## VarF    34.2777778 1.9814442 107 17.2993906 8.947485e-33
## VarG    35.8888889 2.2617501 107 15.8677515 7.076551e-30
## VarA:ej  3.2249875 0.7948909 107  4.0571447 9.466174e-05
## VarB:ej  4.7936139 0.7248198 107  6.6135249 1.522848e-09
## VarC:ej  0.4771074 0.4492152 107  1.0620909 2.905857e-01
## VarD:ej  0.3653064 0.6129948 107  0.5959372 5.524757e-01
## VarE:ej  1.2369950 0.6267462 107  1.9736777 5.099652e-02
## VarF:ej -2.4316943 0.5162748 107 -4.7100774 7.473942e-06
## VarG:ej -0.6663160 0.5893098 107 -1.1306718 2.607213e-01
VarCorr(modFull2)
##          Variance           StdDev   
## Loc =    pdDiag(Var - 1)             
## VarA     48.7341240         6.9809830
## VarB     39.3227526         6.2707856
## VarC     10.7257438         3.2750181
## VarD     26.1010286         5.1089166
## VarE     27.6077467         5.2543074
## VarF     16.4479246         4.0556041
## VarG     23.5842788         4.8563648
## Loc =    pdIdent(Block - 1)          
## Block1    0.4520678         0.6723599
## Block2    0.4520678         0.6723599
## Block3    0.4520678         0.6723599
## Residual 20.8743411         4.5688446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the regression slopes we see that the genotypes A and B are the most responsive to the environment (&lt;span class=&#34;math inline&#34;&gt;\(\beta_A = 3.22\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_B = 4.79\)&lt;/span&gt;, respectively), while the genotypes C and D are stable in a static sense, although their average yield is pretty low.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-joint-regression-model-in-two-steps-equation-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a joint regression model in two-steps (Equation 2)&lt;/h1&gt;
&lt;p&gt;In the previous analyses we used the plot data to fit a joint regression model. In order to reduce the computational burden, it may be useful to split the analyses in two-steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;we analyse the plot data, to retrieve the means for the ‘genotype by environment’ combinations;&lt;/li&gt;
&lt;li&gt;we fit the joint regression model to those means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results of the two approaches are not necessarily the same, as some information in the first step is lost in the second. Several weighing schemes have been proposed to make two-steps fitting more reliable (Möhring and Piepho, 2009); in this example, I will show an unweighted two-steps analyses, which is simple, but not necessarily the best way to go.&lt;/p&gt;
&lt;p&gt;A model for the second step is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu_i + \beta_i e_j + f_{ij} \quad\quad\quad \textrm{(Equation 2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the residual random component &lt;span class=&#34;math inline&#34;&gt;\(f_{ij}\)&lt;/span&gt; is assumed as normally distributed, with mean equal to zero and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f\)&lt;/span&gt;. In general, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f &amp;gt; \sigma^2_d\)&lt;/span&gt;, as the residual sum of squares from Model 2 also contains a component for within trial errors. Indeed, for a balanced experiment, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_{f} = \sigma^2_d + \frac{\sigma^2}{k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the within-trial error, which needs to be obtained from the first step. In the previous analyses we have already fitted an anova model to the whole dataset (‘mod.aov’). In the box below, we make use of the ‘emmeans’ package to retrieve the least squares means for the seven genotypes in all locations. Subsequently, the environmental means are calculated and centered, by subtracting the overall mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emmeans)
muGE &amp;lt;- as.data.frame( emmeans(mod.aov, ~Var:Loc) )[,1:3]
names(muGE) &amp;lt;- c(&amp;quot;Var&amp;quot;, &amp;quot;Loc&amp;quot;, &amp;quot;Yield&amp;quot;)
muGE &amp;lt;- muGE %&amp;gt;% 
  group_by(Loc) %&amp;gt;% 
  mutate(ej = mean(Yield) - mean(muGE$Yield))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we fit Equation 2 to the means. In the code below we assume homoscedasticity and, thus, we are fitting the Finlay-Wilkinson model. The variance component &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; is obtained by subtracting a fraction of the residual variance from the first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finlay-Wilkinson model
modFinlay &amp;lt;- lm(Yield ~ Var/ej - 1, data=muGE)
summary(modFinlay)
## 
## Call:
## lm(formula = Yield ~ Var/ej - 1, data = muGE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.3981 -3.5314 -0.8864  3.7791 11.2045 
## 
## Coefficients:
##         Estimate Std. Error t value Pr(&amp;gt;|t|)    
## VarA     63.1667     2.3915  26.413  &amp;lt; 2e-16 ***
## VarB     66.1667     2.3915  27.668  &amp;lt; 2e-16 ***
## VarC     31.8333     2.3915  13.311 1.24e-13 ***
## VarD     47.1111     2.3915  19.699  &amp;lt; 2e-16 ***
## VarE     44.7222     2.3915  18.701  &amp;lt; 2e-16 ***
## VarF     34.2778     2.3915  14.333 2.02e-14 ***
## VarG     35.8889     2.3915  15.007 6.45e-15 ***
## VarA:ej   3.2250     0.6231   5.176 1.72e-05 ***
## VarB:ej   4.7936     0.6231   7.693 2.22e-08 ***
## VarC:ej   0.4771     0.6231   0.766 0.450272    
## VarD:ej   0.3653     0.6231   0.586 0.562398    
## VarE:ej   1.2370     0.6231   1.985 0.056998 .  
## VarF:ej  -2.4317     0.6231  -3.902 0.000545 ***
## VarG:ej  -0.6663     0.6231  -1.069 0.294052    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 5.858 on 28 degrees of freedom
## Multiple R-squared:  0.9905, Adjusted R-squared:  0.9857 
## F-statistic: 208.3 on 14 and 28 DF,  p-value: &amp;lt; 2.2e-16
sigmaf &amp;lt;- summary(modFinlay)$sigma^2 
sigma2 &amp;lt;- summary(mod.aov)$sigma^2 
sigmaf - sigma2/3 #sigma2_d
## [1] 27.43169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the box below, we allow for different variances for each genotype and, therefore, we fit the Eberarth-Russel model. As before, we can retrieve the variance components &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d(i)}\)&lt;/span&gt; from the fitted model object, by subtracting the within-trial error obtained in the first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eberarth-Russel model
modEberarth &amp;lt;- gls(Yield ~ Var/ej - 1, 
              weights=varIdent(form=~1|Var), data=muGE)
coefs &amp;lt;- summary(modEberarth)$tTable
coefs
##              Value Std.Error    t-value      p-value
## VarA    63.1666667 3.0434527 20.7549360 1.531581e-18
## VarB    66.1666667 2.7653537 23.9270177 3.508778e-20
## VarC    31.8333333 1.7165377 18.5450822 2.912238e-17
## VarD    47.1111111 2.3344802 20.1805574 3.204306e-18
## VarE    44.7222222 2.3899219 18.7128381 2.304763e-17
## VarF    34.2777778 1.9783684 17.3262868 1.685683e-16
## VarG    35.8888889 2.2589244 15.8876005 1.537133e-15
## VarA:ej  3.2249875 0.7929862  4.0668898 3.511248e-04
## VarB:ej  4.7936139 0.7205262  6.6529352 3.218756e-07
## VarC:ej  0.4771074 0.4472521  1.0667527 2.951955e-01
## VarD:ej  0.3653064 0.6082600  0.6005761 5.529531e-01
## VarE:ej  1.2369950 0.6227056  1.9864844 5.684599e-02
## VarF:ej -2.4316943 0.5154734 -4.7174004 6.004832e-05
## VarG:ej -0.6663160 0.5885736 -1.1320862 2.672006e-01
sigma &amp;lt;- summary(modEberarth)$sigma
sigma2fi &amp;lt;- (c(1, coef(modEberarth$modelStruct$varStruct, uncons = FALSE)) * sigma)^2
names(sigma2fi)[1] &amp;lt;- &amp;quot;A&amp;quot;
sigma2fi - sigma2/3 #sigma2_di
##        A        B        C        D        E        F        G 
## 48.69203 38.99949 10.79541 25.81519 27.38676 16.60005 23.73284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting in two steps we obtain the very same result as with fitting in one step, but it ain’t necessarily so.&lt;/p&gt;
&lt;p&gt;I would like to conclude by saying that a joint regression model, the way I have fitted it here, is simple and intuitively appealing, although it has been criticized for a number of reasons. In particular, it has been noted that the environmental indices &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; are estimated from the observed data and, therefore, they are not precisely known. On the contrary, linear regression makes the assumption that the levels of explanatory variables are precisely known and not sampled. As the consequence, our estimates of the slopes &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; may be biased. Furthermore, in our construction we have put some arbitrary constraints on the environmental indices (&lt;span class=&#34;math inline&#34;&gt;\(\sum{e_j} = 0\)&lt;/span&gt;) and on the regression slopes (&lt;span class=&#34;math inline&#34;&gt;\(\sum({\beta_i})/G = 1\)&lt;/span&gt;; where G is the number of genotypes), which are not necessarily reasonable.&lt;/p&gt;
&lt;p&gt;Alternative methods of fitting joint regression models have been proposed (see Piepho, 1998), but they are slightly more complex and I will deal with them in a future post.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Eberhart, S.A., Russel, W.A., 1966. Stability parameters for comparing verieties. Crop Science 6, 36–40.&lt;/li&gt;
&lt;li&gt;Finlay, K.W., Wilkinson, G.N., 1963. The analysis of adaptation in a plant-breeding programme. Australian Journal of Agricultural Research 14, 742–754.&lt;/li&gt;
&lt;li&gt;Möhring, J., Piepho, H.-P., 2009. Comparison of Weighting in Two-Stage Analysis of Plant Breeding Trials. Crop Science 49, 1977–1988.&lt;/li&gt;
&lt;li&gt;Perkins, J.M., Jinks, J.L., 1968. Environmental gentype-environmental components of variability. III. Multiple lines and crosses. Heredity 23, 339–356.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1998. Methods for comparing the yield stability of cropping systems - A review. Journal of Agronomy and Crop Science 180, 193–213.&lt;/li&gt;
&lt;li&gt;Wood, J., 1976. The use of environmental variables in the interpretation of genotype-environment interaction. Heredity 37, 1–7.&lt;/li&gt;
&lt;li&gt;Yates, F., and Cochran G., 1938. The analysis of groups of experiments. Journal of Agricultural Sciences, 28, 556—580.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #2</title>
      <link>https://www.statforbiology.com/2019/stat_lmm_2-wayssplitrepeatedhet/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_lmm_2-wayssplitrepeatedhet/</guid>
      <description>


&lt;div id=&#34;a-repeated-split-plot-experiment-with-heteroscedastic-errors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A repeated split-plot experiment with heteroscedastic errors&lt;/h1&gt;
&lt;p&gt;Let’s imagine a field experiment, where different genotypes of khorasan wheat are to be compared under different nitrogen (N) fertilisation systems. Genotypes require bigger plots, with respect to fertilisation treatments and, therefore, the most convenient choice would be to lay-out the experiment as a split-plot, in a randomised complete block design. Genotypes would be randomly allocated to main plots, while fertilisation systems would be randomly allocated to sub-plots. As usual in agricultural research, the experiment should be repeated in different years, in order to explore the environmental variability of results.&lt;/p&gt;
&lt;p&gt;What could we expect from such an experiment?&lt;/p&gt;
&lt;p&gt;Please, look at the dataset ‘kamut.csv’, which is available on github. It provides the results for a split-plot experiment with 15 genotypes and 2 N fertilisation treatments, laid-out in three blocks and repeated in four years (360 observations, in all).&lt;/p&gt;
&lt;p&gt;The dataset has five columns, the ‘Year’, the ‘Genotype’, the fertilisation level (‘N’), the ‘Block’ and the response variable, i.e. ‘Yield’. The fifteen genotypes are coded by using the letters from A to O, while the levels of the other independent variables are coded by using numbers. The following snippets loads the file and recodes the numerical independent variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/kamutHet.csv&amp;quot;)
dataset$Block &amp;lt;- factor(dataset$Block)
dataset$Year &amp;lt;- factor(dataset$Year)
dataset$N &amp;lt;- factor(dataset$N)
dataset$Genotype &amp;lt;- factor(dataset$Genotype)
head(dataset)
##   Year Genotype N Block Yield
## 1 2004        A 1     1 2.235
## 2 2004        A 1     2 2.605
## 3 2004        A 1     3 2.323
## 4 2004        A 2     1 3.766
## 5 2004        A 2     2 4.094
## 6 2004        A 2     3 3.902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, it may be useful to code some ‘helper’ factors, to represent the blocks (within years) and the main-plots. The first factors (‘YearBlock’) has 12 levels (4 years and 3 blocks per year) and the second factor (‘MainPlot’) has 180 levels (4 years, 3 blocks per year and 15 genotypes per block).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$YearBlock &amp;lt;- with(dataset, factor(Year:Block))
dataset$MainPlot &amp;lt;- with(dataset, factor(Year:Block:Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the analyses, we will make use of the ‘plyr’ (Wickham, 2011), ‘car’ (Fox and Weisberg, 2011) and ‘nlme’ (Pinheiro et al., 2018) packages, which we load now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(car)
library(nlme)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is always useful to start by separately considering the results for each year. This gives us a feel for what happened in all experiments. What model do we have to fit to single-year split-plot data? In order to avoid mathematical notation, I will follow the notation proposed by Piepho (2003), by using the names of variables, as reported in the dataset. The treatment model for this split-plot design is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Genotype * N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All treatment effects are fixed. The block model, referencing all grouping structures, is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Block + Block:MainPlot + Block:MainPlot:Subplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first element references the blocks, while the second element references the main-plots, to which the genotypes are randomly allocated (randomisation unit). The third element references the sub-plots, to which N treatments are randomly allocated (another randomisation unit); this latter element corresponds to the residual error and, therefore, it is fitted by default and needs not be explicitly included in the model. Main-plot and sub-plot effects need to be random, as they reference randomisation units (Piepho, 2003). The nature of the block effect is still under debate (Dixon, 2016), but I’ll take it as random (do not worry: I will also show how we can take it as fixed).&lt;/p&gt;
&lt;p&gt;Coding a split-plot model in ‘lme’ is rather simple:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lme(Yield ~ Genotype * N, random = ~1|Block/MainPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the notation ‘Block/MainPlot’ is totally equivalent to ‘Block + Block:MainPlot’. Instead of manually fitting this model four times (one per year), we can ask R to do so by using the ‘ddply()’ function in the ‘plyr’ package. In the code below, I used this technique to retrieve the residual variance for each experiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmmFits &amp;lt;- ddply(dataset, c(&amp;quot;Year&amp;quot;),
      function(df) summary( lme(Yield ~ Genotype * N,
                 random = ~1|Block/MainPlot,
                 data = df))$sigma^2 )
lmmFits
##   Year          V1
## 1 2004 0.052761644
## 2 2005 0.001423833
## 3 2006 0.776028791
## 4 2007 0.817594477&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see great differences! The residual variance in 2005 is more that 500 times smaller than that observed in 2007. Clearly, if we pool the data and make an ANOVA, when we pool the data, we violate the homoscedasticity assumption. In general, this problem has an obvious solution: we can model the variance-covariance matrix of observations, allowing a different variance per year. In R, this is only possible by using the ‘lme()’ function (unless we want to use the ‘asreml-R’ package, which is not freeware, unfortunately). The question is: how do we code such a model?&lt;/p&gt;
&lt;p&gt;First of all, let’s derive a correct mixed model. The treatment model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Genotype * N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have mentioned that the genotype and N effects are likely to be taken as fixed. The block model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year + Year:Block + Year:Block:MainPlot + Year:Block:MainPlot:Subplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second element in the block model references the blocks within years, the second element references the main-plots, while the third element references the sub-plots and, as before, it is not needed. The year effect is likely to interact with both the treatment effects, so we need to add the following effects:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year + Year:Genotype + Year:N + Year:Genotype:N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent to writing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year*Genotype*N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The year effect can be taken as either as random or as fixed. In this post, we will show both approaches&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;year-effect-is-fixed&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Year effect is fixed&lt;/h1&gt;
&lt;p&gt;If we take the year effect as fixed and the block effect as random, we see that the random effects are nested (blocks within years and main-plots within blocks and within years). The function ‘lme()’ is specifically tailored to deal with nested random effects and, therefore, fitting the above model is rather easy. In the first snippet we fit a homoscedastic model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix1 &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = ~1|YearBlock/MainPlot,
                 data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also fit this model with the ‘lme4’ package and the ‘lmer()’; however, we are not happy with this, because we have seen clear signs of heteroscedastic within-year errors. Thus, let’s account for such an heteroscedasticity, by using the ‘weights()’ argument and the ‘varIdent()’ variance structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2 &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = ~1|YearBlock/MainPlot,
                 data = dataset,
               weights = varIdent(form = ~1|Year))
AIC(modMix1, modMix2)
##          df      AIC
## modMix1 123 856.6704
## modMix2 126 575.1967&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the Akaike Information Criterion, we see that the second model is better than the first one, which supports the idea of heteroscedastic residuals. From this moment on, the analyses proceeds as usual, e.g. by testing for fixed effects and comparing means, as necessary. Just a few words about testing for fixed effects: Wald F tests can be obtained by using the ‘anova()’ function, although I usually avoid this with ‘lme’ objects, as there is no reliable approximation to degrees of freedom. With ‘lme’ objects, I suggest using the ‘Anova()’ function in the ‘car’ package, which shows the results of Wald chi square tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Anova(modMix2)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##                    Chisq Df Pr(&amp;gt;Chisq)    
## Year              51.072  3  4.722e-11 ***
## Genotype         543.499 14  &amp;lt; 2.2e-16 ***
## N               2289.523  1  &amp;lt; 2.2e-16 ***
## Year:Genotype    123.847 42  5.281e-10 ***
## Year:N            21.695  3  7.549e-05 ***
## Genotype:N      1356.179 14  &amp;lt; 2.2e-16 ***
## Year:Genotype:N  224.477 42  &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One further aspect: do you prefer fixed blocks? Then you can fit the following model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix4 &amp;lt;- lme(Yield ~ Year * Genotype * N + Year:Block,
                 random = ~1|MainPlot,
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;year-effect-is-random&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Year effect is random&lt;/h1&gt;
&lt;p&gt;If we’d rather take the year effect as random, all the interactions therein are random as well (Year:Genotype, Year:N and Year:Genotype:N). Similarly, the block (within years) effect needs to be random. Therefore, we have several crossed random effects, which are not straightforward to code with ‘lme()’. First, I will show the code, second, I will comment it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix5 &amp;lt;- lme(Yield ~ Genotype * N,
                  random = list(Year = pdIdent(~1),
                                Year = pdIdent(~Block - 1),
                                Year = pdIdent(~MainPlot - 1),
                                Year = pdIdent(~Genotype - 1),
                                Year = pdIdent(~N - 1),
                                Genotype = pdIdent(~N - 1)),
                  data=dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that random effects are coded using a named list; each component of this list is a &lt;em&gt;pdMat&lt;/em&gt; object with name equal to a grouping factor. For example, the component ‘Year = pdIdent(~ 1)’ represents a random year effect, while ‘Year = pdIdent(~ Block - 1)’ represents a random year effect for each level of Block, i.e. a random ‘year x block’ interaction. This latter variance component is the same for all blocks (‘varIdent’), i.e. there is homoscedastic at this level.&lt;/p&gt;
&lt;p&gt;It is important to remember that the grouping factors in the list are treated as nested; however, the grouping factor is only one (‘Year’), so that the nesting is irrelevant. The only exception is the genotype, which is regarded as nested within the year. As the consequence, the component ‘Genotype = pdIdent(~N - 1)’, specifies a random year:genotype effect for each level of N treatment, i.e. a random year:genotype:N interaction.&lt;/p&gt;
&lt;p&gt;I agree, this is not straightforward to understand! If necessary, take a look at the good book of Gałecki and Burzykowski (2013). When fitting the above model, be patient; convergence may take a few seconds. I’d only like to reinforce the idea that, in case you need to test for fixed effects, you should not rely on the ‘anova()’ function, but you should prefer Wald chi square tests in the ‘Anova()’ function in the ‘car’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Anova(modMix5, type = 2)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##              Chisq Df Pr(&amp;gt;Chisq)    
## Genotype   68.6430 14  3.395e-09 ***
## N           2.4682  1     0.1162    
## Genotype:N 14.1153 14     0.4412    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another note: coding random effects as a named list is always possible. For example ‘modMix2’ can also be coded as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2b &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = list(YearBlock = ~ 1, MainPlot = ~ 1),
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, also as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2c &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = list(YearBlock = pdIdent(~ 1), MainPlot = pdIdent(~ 1)),
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this is useful! Have fun with it.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fox J. and Weisberg S. (2011). An {R} Companion to Applied Regression, Second Edition. Thousand Oaks CA: Sage. URL: &lt;a href=&#34;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion&#34; class=&#34;uri&#34;&gt;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2018). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-137, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Hadley Wickham (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1-29. URL: &lt;a href=&#34;http://www.jstatsoft.org/v40/i01/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v40/i01/&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #4</title>
      <link>https://www.statforbiology.com/2019/stat_nlmm_interaction/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_nlmm_interaction/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;testing-for-interactions-in-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing for interactions in nonlinear regression&lt;/h1&gt;
&lt;p&gt;Factorial experiments are very common in agriculture and they are usually laid down to test for the significance of interactions between experimental factors. For example, genotype assessments may be performed at two different nitrogen fertilisation levels (e.g. high and low) to understand whether the ranking of genotypes depends on nutrient availability. For those of you who are not very much into agriculture, I will only say that such an assessment is relevant, because we need to know whether we can recommend the same genotypes, e.g., both in conventional agriculture (high nitrogen availability) and in organic agriculture (relatively lower nitrogen availability).&lt;/p&gt;
&lt;p&gt;Let’s consider an experiment where we have tested three genotypes (let’s call them A, B and C, for brevity), at two nitrogen rates (‘high’ an ‘low’) in a complete block factorial design, with four replicates. Biomass subsamples were taken from each of the 24 plots in eight different times (Days After Sowing: DAS), to evaluate the growth of biomass over time.&lt;/p&gt;
&lt;p&gt;The dataset is available on gitHub and the following code loads it and transforms the ‘Block’ variable into a factor. For this post, we will use several packages, including ‘aomisc’, the accompanying package for this blog. Please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page for downloading and installing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(lattice)
library(nlme)
library(aomisc)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/growthNGEN.csv&amp;quot;,
  header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
dataset$N &amp;lt;- factor(dataset$N)
dataset$GEN &amp;lt;- factor(dataset$GEN)
head(dataset)
##   Id DAS Block Plot GEN   N  Yield
## 1  1   0     1    1   A Low  2.786
## 2  2  15     1    1   A Low  5.871
## 3  3  30     1    1   A Low 13.265
## 4  4  45     1    1   A Low 16.926
## 5  5  60     1    1   A Low 22.812
## 6  6  75     1    1   A Low 25.346&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is composed by the following variables:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;‘Id’: a numerical code for observations&lt;/li&gt;
&lt;li&gt;‘DAS’: i.e., Days After Sowing. It is the moment when the sample was collected&lt;/li&gt;
&lt;li&gt;‘Block’, ‘Plot’, ‘GEN’ and ‘N’ represent respectively the block, plot, genotype and nitrogen level for each observation&lt;/li&gt;
&lt;li&gt;‘Yield’ represents the harvested biomass.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It may be useful to take a look at the observed growth data, as displayed on the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the growth is sygmoidal (presumably logistic) and that the variance of observations increases over time, i.e. the variance is proportional to the expected response.&lt;/p&gt;
&lt;p&gt;The question is: how do we analyse this data? Let’s build a model in a sequential fashion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The model&lt;/h1&gt;
&lt;p&gt;We could empirically assume that the relationship between biomass and time is logistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijkl} = \frac{d_{ijkl}}{1 + exp\left[b \left( X_{ijkl} - e_{ijkl}\right)\right]} + \varepsilon_{ijkl}\quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the observed biomass yield at time &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, for the i-th genotype, j-th nitrogen level, k-th block and l-th plot, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum asymptotic biomass level when time goes to infinity, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at inflection point, while &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the time when the biomass yield is equal to &lt;span class=&#34;math inline&#34;&gt;\(d/2\)&lt;/span&gt;. We are mostly interested in the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;: the first one describes the yield potential of a genotype, while the second one gives a measure of the speed of growth.&lt;/p&gt;
&lt;p&gt;There are repeated measures in each plot and, therefore, model parameters may show some variability, depending on the genotype, nitrogen level, block and plot. In particular, it may be acceptable to assume that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is pretty constant and independent on the above factors, while &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; may change according to the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left\{ {\begin{array}{*{20}{c}}
d_{ijkl} = \mu_d + g_{di} + N_{dj} + gN_{dij} + \theta_{dk} + \zeta_{dl}\\
e_{ijkl} = \mu_e + q_{ei} + N_{ej} + gN_{eij} + \theta_{ek} + \zeta_{el}
\end{array}} \right. \quad \quad (2) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for each parameter, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the fixed effect of the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed effect of j-th nitrogen level, &lt;span class=&#34;math inline&#34;&gt;\(gN\)&lt;/span&gt; is the fixed interaction effect, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the random effect of blocks, while &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; is the random effect of plots within blocks. These two equations are totally equivalent to those commonly used for linear mixed models, in the case of a two-factor factorial block design, wherein &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; would be the residual error term. Indeed, in principle, we could also think about a two-steps fitting procedure, where we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;fit the logistic model to the data for each plot and obtain estimates for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;use these estimates to fit a linear mixed model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will not pursue this two-steps technique here and we will concentrate on one-step fitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-wrong-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A wrong method&lt;/h1&gt;
&lt;p&gt;If the observations were independent (i.e. no blocks and no repeated measures), this model could be fit by using conventional nonlinear regression. My preference goes to the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2015).&lt;/p&gt;
&lt;p&gt;The coding is reported below: ‘Yield’ is a function of (&lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt;) DAS, by way of a three-parameters logistic function (‘fct = L.3()’). Different curves have to be fitted for different combinations of genotype and nitrogen levels (‘curveid = N:GEN’), although these curves should be partly based on common parameter values (‘pmodels = …). The ’pmodels’ argument requires a few additional comments. It must be a vector with as many element as there are parameters in the model (three, in this case: &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;). Each element represents a linear function of variables and refers to the parameters in alphabetical order, i.e. the first element refers to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the second refers to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the third to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not dependent on any variable (‘~ 1’) and thus a constant value is fitted across curves; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; depend on a fully factorial combination of genotype and nitrogen level (~ N*GEN = ~N + GEN + N:GEN). Finally, we used the argument ‘bcVal = 0.5’ to specify that we intend to use a Transform-Both-Sides technique, where a logarithmic transformation is performed for both sides of the equations. This is necessary to account for heteroscedasticity, but it does not affect the scale of parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive1 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = GEN:N,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N*GEN), bcVal = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model may be useful for other circumstances (no blocks and no repeated measures), but it is wrong in our example. Indeed, observations are clustered within blocks and plots; by neglecting this, we violate the assumption of independence of model residuals. A swift plot of residuals against fitted values shows that there are no problems with heteroscedasticity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Considering the above, we have to use a different model, here, although I will show that this naive fit may turn out useful.&lt;/p&gt;
&lt;div id=&#34;nonlinear-mixed-model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nonlinear mixed model fitting&lt;/h2&gt;
&lt;p&gt;In order to account for the clustering of observations, we switch to a Nonlinear Mixed-Effect model (NLME). A good choice is the ‘nlme()’ function in the ‘nlme’ package (Pinheiro and Bates, 2000), although the syntax may be cumbersome, at times. I will try to help, listing and commenting the most important arguments for this function. We need to specify the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A deterministic function. In this case, we use the ‘nlsL.3()’ function in the ‘aomisc’ package, which provides a logistic growth model with the same parameterisation as the ‘L.3()’ function in the ‘drm’ package.&lt;/li&gt;
&lt;li&gt;Linear functions for model parameters. The ‘fixed’ argument in the ‘nlme’ function is very similar to the ‘pmodels’ argument in the ‘drm’ function above, in the sense that it requires a list, wherein each element is a linear function of variables. The only difference is that the parameter name needs to be specified on the left side of the function.&lt;/li&gt;
&lt;li&gt;Random effects for model parameters. These are specified by using the ‘random’ argument. In this case, the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; are expected to show random variability from block to block and from plot to plot, within a block. For the sake of simplicity, as the parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not affected by the genotype and nitrogen level, we also expect that it does not show any random variability across blocks and plots.&lt;/li&gt;
&lt;li&gt;Starting values for model parameters. Self starting routines are not used by ‘nlme()’ and thus we need to specify a named vector, holding the initial values of model parameters. In this case, I decided to use the output from the ‘naive’ nonlinear regression above, which, therefore, turns out useful.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The transformation of both sides of the equation is made explicitely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
modnlme1 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N*GEN),
                    start = coef(modNaive1), 
                 control = list(msMaxIter = 400))
summary(modnlme1)$tTable
##                     Value   Std.Error  DF    t-value      p-value
## b              0.05652837 0.002157629 228 26.1993017 1.044744e-70
## d.(Intercept) 33.91575345 1.222612873 228 27.7403863 5.073988e-75
## d.NLow        -3.18382833 1.592502937 228 -1.9992606 4.676721e-02
## d.GENB        18.90014652 1.864712716 228 10.1356881 3.602004e-20
## d.GENC        -1.15934036 1.686405289 228 -0.6874625 4.924900e-01
## d.NLow:GENB   -5.99216674 2.455759122 228 -2.4400466 1.544863e-02
## d.NLow:GENC   -5.82864839 2.217534817 228 -2.6284360 9.160827e-03
## e.(Intercept) 55.20071251 2.320784619 228 23.7853664 1.087154e-63
## e.NLow        -9.06217439 3.127165895 228 -2.8978873 4.123120e-03
## e.GENB        -4.47038044 2.761533560 228 -1.6188036 1.068720e-01
## e.GENC         4.00746113 3.084383636 228  1.2992745 1.951620e-01
## e.NLow:GENB   -4.71367433 4.055953643 228 -1.1621618 2.463848e-01
## e.NLow:GENC    2.23951083 4.609547667 228  0.4858418 6.275459e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plots above, we see that the overall fit is good. Fixed effects and variance components for random effects are obtained as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(modnlme1)$tTable
##                     Value   Std.Error  DF    t-value      p-value
## b              0.05652837 0.002157629 228 26.1993017 1.044744e-70
## d.(Intercept) 33.91575345 1.222612873 228 27.7403863 5.073988e-75
## d.NLow        -3.18382833 1.592502937 228 -1.9992606 4.676721e-02
## d.GENB        18.90014652 1.864712716 228 10.1356881 3.602004e-20
## d.GENC        -1.15934036 1.686405289 228 -0.6874625 4.924900e-01
## d.NLow:GENB   -5.99216674 2.455759122 228 -2.4400466 1.544863e-02
## d.NLow:GENC   -5.82864839 2.217534817 228 -2.6284360 9.160827e-03
## e.(Intercept) 55.20071251 2.320784619 228 23.7853664 1.087154e-63
## e.NLow        -9.06217439 3.127165895 228 -2.8978873 4.123120e-03
## e.GENB        -4.47038044 2.761533560 228 -1.6188036 1.068720e-01
## e.GENC         4.00746113 3.084383636 228  1.2992745 1.951620e-01
## e.NLow:GENB   -4.71367433 4.055953643 228 -1.1621618 2.463848e-01
## e.NLow:GENC    2.23951083 4.609547667 228  0.4858418 6.275459e-01
VarCorr(modnlme1)
##               Variance                     StdDev       Corr  
## Block =       pdLogChol(list(d ~ 1,e ~ 1))                    
## d.(Intercept) 4.134390e-08                 2.033320e-04 d.(In)
## e.(Intercept) 1.851943e-08                 1.360861e-04 -0.001
## Plot =        pdLogChol(list(d ~ 1,e ~ 1))                    
## d.(Intercept) 3.396536e-09                 5.827981e-05 d.(In)
## e.(Intercept) 1.023108e-09                 3.198605e-05 0     
## Residual      1.750623e-01                 4.184044e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s go back to our initial aim: testing the significance of the ‘genotype x nitrogen’ interaction. Indeed, we have two available tests: on for the parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and one for the parameter &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. First of all, we code two ‘reduced’ models, where the genotype and nitrogen effects are purely addictive. To do so, we change the specification of the fixed effects from ’~ N*GEN’ to ‘~ N + GEN’. Also in this case, we use a ‘naive’ nonlinear regression fit to get starting values for model parameters, to be used in the following NLME model fitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive2 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N + GEN,  ~ N * GEN), bcVal = 0.5)

modnlme2 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N + GEN, e ~ N*GEN),
                    start = coef(modNaive2), control = list(msMaxIter = 200))
modNaive3 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N + GEN), bcVal = 0.5)

modnlme3 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N + GEN),
                    start = coef(modNaive3), control = list(msMaxIter = 200))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s consider the first reduced model ‘modnlme2’. In this model, the ‘genotype x nitrogen’ interaction has been removed for the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter. We can compare this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme2)
##          Model df      AIC      BIC    logLik   Test L.Ratio p-value
## modnlme1     1 20 329.1496 400.6686 -144.5748                       
## modnlme2     2 18 334.2187 398.5857 -149.1093 1 vs 2 9.06907  0.0107&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This test is significant, but the AIC value is very close for the two models. Considering that a LRT in mixed models is usually rather liberal, it should be possible to conclude that the ‘genotype x nitrogen’ interaction is not significant and, therefore, the ranking of genotypes in terms of yield potential, as measured by the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter should be independent on nitrogen level.&lt;/p&gt;
&lt;p&gt;Let’s now consider the second reduced model ‘modnlme3’. In this second model, the ‘genotype x nitrogen’ interaction has been removed for the ‘e’ parameter. We can compare also this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme3)
##          Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## modnlme1     1 20 329.1496 400.6686 -144.5748                        
## modnlme3     2 18 328.2446 392.6117 -146.1223 1 vs 2 3.095066  0.2128&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this second test, the lack of significance for the ‘genotype x nitrogen’ interaction seems to be less questionable than in the first one.&lt;/p&gt;
&lt;p&gt;I would like to conclude by drawing your attention to the ‘medrm’ function in the ‘medrc’ package, which can also be used to fit this type of nonlinear mixed-effects models.&lt;/p&gt;
&lt;p&gt;Happy coding with R!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J.C., Gerhard, D., 2015. Dose-Response Analysis Using R. PLOS ONE 10, e0146021. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0146021&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0146021&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;. Example #1</title>
      <link>https://www.statforbiology.com/2019/stat_lmm_environmentalvariance/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_lmm_environmentalvariance/</guid>
      <description>


&lt;div id=&#34;the-environmental-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The environmental variance model&lt;/h1&gt;
&lt;p&gt;Fitting mixed models has become very common in biology and recent developments involve the manipulation of the variance-covariance matrix for random effects and residuals. To the best of my knowledge, within the frame of frequentist methods, the only freeware solution in R should be based on the ‘nlme’ package, as the ‘lmer’ package does not easily permit such manipulations. The ‘nlme’ package is fully described in Pinheiro and Bates (2000). Of course, the ‘asreml’ package can be used, but, unfortunately, this is not freeware.&lt;/p&gt;
&lt;p&gt;Coding mixed models in ‘nlme’ is not always easy, especially when we have crossed random effects, which is very common with agricultural experiments. I have been struggling with this issue very often in the last years and I thought it might be useful to publish a few examples in this blog, to save collegues from a few headaches. Please, note that I have already published other posts dealing with the use of the ‘lme()’ function in the ‘nlme’ package, for example &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence2/&#34;&gt;this post here&lt;/a&gt; about the correlation in designed experiments and &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/&#34;&gt;this other post here&lt;/a&gt;, about heteroscedastic multienvironment experiments.&lt;/p&gt;
&lt;p&gt;The first example in this series relates to a randomised complete block design with three replicates, comparing winter wheat genotypes. The experiment was repeated in seven years in the same location. The dataset (‘WinterWheat’) is available in the ‘aomisc’ package, which is the companion package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;. Please, note that this dataset shows the data for eight genotypes, but the model that we want to fit requires that the number of environments is higher than the number of genotypes. Therefore, we have to make a subset, at the beginning, removing a couple of genotypes.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset, subsets it and turns the ‘Genotype’, ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
data(WinterWheat)
WinterWheat &amp;lt;- WinterWheat[WinterWheat$Genotype != &amp;quot;SIMETO&amp;quot; &amp;amp; WinterWheat$Genotype != &amp;quot;SOLEX&amp;quot;,]
WinterWheat$Genotype &amp;lt;- factor(WinterWheat$Genotype)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996
## 11   91     2   DUILIO  5.57 1996
## 12  138     2   GRAZIA  6.09 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the so-called ‘environmental variance model’. This model is often used in stability analyses for multi-environment experiments and I will closely follow the coding proposed in Piepho (1999):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + g_i + r_{jk}  +  h_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g_i\)&lt;/span&gt; is the effect for the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; is the effect for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(h_{ij}\)&lt;/span&gt; is a random deviation from the expected yield for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual variability of yield between plots, within each environment and block.&lt;/p&gt;
&lt;p&gt;We usually assume that &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; are independent and normally distributed, with variances equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;, respectively. Such an assumption may be questioned, but we will not do it now, for the sake of simplicity.&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(h_{ij}\)&lt;/span&gt;, which we will assume as normally distributed with variance-covariance matrix equal to &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;. In particular, it is reasonable to expect that the genotypes will have different variances across environments (heteroscedasticity), which can be interpreted as static stability measures (‘environmental variances’; hence the name ‘environmental variance model’). Furthermore, it is reasonable that, if an environment is good for one genotype, it may also be good for other genotypes, so that yields in each environment are correlated, although the correlations can be different for each couple of genotypes. To reflect our expectations, the &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; matrix needs to be totally unstructured, with the only constraint that it is positive definite.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown how the above model can be coded by using SAS and I translated his code into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EnvVarMod &amp;lt;- lme(Yield ~ Genotype, 
  random = list(Year = pdSymm(~Genotype - 1), 
              Year = pdIdent(~Block - 1)),
  control = list(opt = &amp;quot;optim&amp;quot;, maxIter = 100),
  data=WinterWheat)
VarCorr(EnvVarMod)
##                  Variance             StdDev    Corr                
## Year =           pdSymm(Genotype - 1)                               
## GenotypeCOLOSSEO 0.48876512           0.6991174 GCOLOS GCRESO GDUILI
## GenotypeCRESO    0.70949309           0.8423141 0.969               
## GenotypeDUILIO   2.37438440           1.5409038 0.840  0.840        
## GenotypeGRAZIA   1.18078525           1.0866394 0.844  0.763  0.942 
## GenotypeIRIDE    1.23555204           1.1115539 0.857  0.885  0.970 
## GenotypeSANCARLO 0.93335518           0.9661031 0.928  0.941  0.962 
## Year =           pdIdent(Block - 1)                                 
## Block1           0.02748257           0.1657787                     
## Block2           0.02748257           0.1657787                     
## Block3           0.02748257           0.1657787                     
## Residual         0.12990355           0.3604214                     
##                               
## Year =                        
## GenotypeCOLOSSEO GGRAZI GIRIDE
## GenotypeCRESO                 
## GenotypeDUILIO                
## GenotypeGRAZIA                
## GenotypeIRIDE    0.896        
## GenotypeSANCARLO 0.884  0.942 
## Year =                        
## Block1                        
## Block2                        
## Block3                        
## Residual&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I coded the random effects as a list, by using the ‘Year’ as the nesting factor (Galecki and Burzykowski, 2013). In order to specify a totally unstructured variance-covariance matrix for the genotypes within years, I used the ‘pdMat’ construct ‘pdSymm()’. This model is rather complex and may take long to converge.&lt;/p&gt;
&lt;p&gt;The environmental variances are retrieved by the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;envVar &amp;lt;- as.numeric ( VarCorr(EnvVarMod)[2:7,1] )
envVar
## [1] 0.4887651 0.7094931 2.3743844 1.1807853 1.2355520 0.9333552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the correlations are given by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(EnvVarMod)[2:7,3:7]
##                  Corr                                        
## GenotypeCOLOSSEO &amp;quot;GCOLOS&amp;quot; &amp;quot;GCRESO&amp;quot; &amp;quot;GDUILI&amp;quot; &amp;quot;GGRAZI&amp;quot; &amp;quot;GIRIDE&amp;quot;
## GenotypeCRESO    &amp;quot;0.969&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeDUILIO   &amp;quot;0.840&amp;quot;  &amp;quot;0.840&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeGRAZIA   &amp;quot;0.844&amp;quot;  &amp;quot;0.763&amp;quot;  &amp;quot;0.942&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeIRIDE    &amp;quot;0.857&amp;quot;  &amp;quot;0.885&amp;quot;  &amp;quot;0.970&amp;quot;  &amp;quot;0.896&amp;quot;  &amp;quot;&amp;quot;      
## GenotypeSANCARLO &amp;quot;0.928&amp;quot;  &amp;quot;0.941&amp;quot;  &amp;quot;0.962&amp;quot;  &amp;quot;0.884&amp;quot;  &amp;quot;0.942&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unweighted-two-stage-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unweighted two-stage fitting&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit an environmental variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retrieved the means for genotypes in all years, we can fit the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu + g_i + a_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is the mean yield for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment and &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; is the residual term, which includes the genotype x environment random interaction, the block x environment random interaction and the residual error term.&lt;/p&gt;
&lt;p&gt;In this model we have only one random effect (&lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt;) and, therefore, this is a fixed linear model. However, we need to model the variance-covariance matrix of residuals (&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;), by adopting a totally unstructured form. Please, note that, when working with raw data, we have modelled &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;, i.e. the variance-covariance matrix for the random effects. I have used the ‘gls()’ function, together with the ‘weights’ and ‘correlation’ arguments. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
envVarModM &amp;lt;- gls(Yield ~ Genotype, 
  data = WinterWheatM,
  weights = varIdent(form=~1|Genotype),
  correlation = corSymm(form=~1|Year))
summary(envVarModM)
## Generalized least squares fit by REML
##   Model: Yield ~ Genotype 
##   Data: WinterWheatM 
##       AIC      BIC   logLik
##   80.6022 123.3572 -13.3011
## 
## Correlation Structure: General
##  Formula: ~1 | Year 
##  Parameter estimate(s):
##  Correlation: 
##   1     2     3     4     5    
## 2 0.947                        
## 3 0.809 0.815                  
## 4 0.816 0.736 0.921            
## 5 0.817 0.866 0.952 0.869      
## 6 0.888 0.925 0.949 0.856 0.907
## Variance function:
##  Structure: Different standard deviations per stratum
##  Formula: ~1 | Genotype 
##  Parameter estimates:
## COLOSSEO    CRESO   DUILIO   GRAZIA    IRIDE SANCARLO 
## 1.000000 1.189653 2.143713 1.528848 1.560620 1.356423 
## 
## Coefficients:
##                      Value Std.Error   t-value p-value
## (Intercept)       6.413333 0.2742314 23.386574  0.0000
## GenotypeCRESO    -0.439524 0.1107463 -3.968746  0.0003
## GenotypeDUILIO    0.178571 0.3999797  0.446451  0.6579
## GenotypeGRAZIA   -0.330952 0.2518270 -1.314205  0.1971
## GenotypeIRIDE     0.281905 0.2580726  1.092347  0.2819
## GenotypeSANCARLO -0.192857 0.1802547 -1.069915  0.2918
## 
##  Correlation: 
##                  (Intr) GCRESO GDUILI GGRAZI GIRIDE
## GenotypeCRESO     0.312                            
## GenotypeDUILIO    0.503  0.371                     
## GenotypeGRAZIA    0.269 -0.095  0.774              
## GenotypeIRIDE     0.292  0.545  0.857  0.638       
## GenotypeSANCARLO  0.310  0.612  0.856  0.537  0.713
## 
## Standardized residuals:
##        Min         Q1        Med         Q3        Max 
## -2.0949678 -0.5680656  0.1735444  0.7599596  1.3395000 
## 
## Residual standard error: 0.7255481 
## Degrees of freedom: 42 total; 36 residual&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance-covariance matrix for residuals can be obtained using the ‘getVarCov()’ function in the ‘nlme’ package, although I had to discover that there is a small buglet there, which causes problems in some instances (such as here). Please, &lt;a href=&#34;https://www.jepusto.com/bug-in-nlme-getvarcov/&#34;&gt;see this link&lt;/a&gt;; I have included the correct code in the ‘getVarCov.gls()’ function in the ‘aomisc’ package, that is the companion package for this blog.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- getVarCov.gls(envVarModM)
R
## Marginal variance covariance matrix
##         [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
## [1,] 0.52642 0.59280 0.91285 0.65647 0.67116 0.63376
## [2,] 0.59280 0.74503 1.09440 0.70422 0.84652 0.78560
## [3,] 0.91285 1.09440 2.41920 1.58850 1.67700 1.45230
## [4,] 0.65647 0.70422 1.58850 1.23040 1.09160 0.93442
## [5,] 0.67116 0.84652 1.67700 1.09160 1.28210 1.01070
## [6,] 0.63376 0.78560 1.45230 0.93442 1.01070 0.96855
##   Standard Deviations: 0.72555 0.86315 1.5554 1.1093 1.1323 0.98415&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the design is perfectly balanced, the diagonal elements of the above matrix correspond to the variances of genotypes across environments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(WinterWheatM$Yield, WinterWheatM$Genotype, var)
##  COLOSSEO     CRESO    DUILIO    GRAZIA     IRIDE  SANCARLO 
## 0.5264185 0.7450275 2.4191624 1.2304397 1.2821143 0.9685497&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can also be retreived by the ‘stability’ package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stability)
## Registered S3 methods overwritten by &amp;#39;lme4&amp;#39;:
##   method                          from
##   cooks.distance.influence.merMod car 
##   influence.merMod                car 
##   dfbeta.influence.merMod         car 
##   dfbetas.influence.merMod        car
envVarStab &amp;lt;-
  stab_measures(
    .data = WinterWheatM,
    .y = Yield,
    .gen = Genotype,
    .env = Year
  )

envVarStab$StabMeasures
## # A tibble: 6 x 7
##   Genotype  Mean GenSS   Var    CV  Ecov ShuklaVar
##   &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 COLOSSEO  6.41  3.16 0.526  11.3 1.25     0.258 
## 2 CRESO     5.97  4.47 0.745  14.4 1.01     0.198 
## 3 DUILIO    6.59 14.5  2.42   23.6 2.31     0.522 
## 4 GRAZIA    6.08  7.38 1.23   18.2 1.05     0.208 
## 5 IRIDE     6.70  7.69 1.28   16.9 0.614    0.0989
## 6 SANCARLO  6.22  5.81 0.969  15.8 0.320    0.0254&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Strictly speaking, those variances are not the environmental variances, as they also contain the within-experiment and within block random variability, which needs to be separately estimated during the first step.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Muhammad Yaseen, Kent M. Eskridge and Ghulam Murtaza (2018). stability: Stability Analysis of Genotype by Environment Interaction (GEI). R package version 0.5.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stability&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genotype experiments: fitting a stability variance model with R</title>
      <link>https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/</guid>
      <description>


&lt;p&gt;Yield stability is a fundamental aspect for the selection of crop genotypes. The definition of stability is rather complex (see, for example, Annichiarico, 2002); in simple terms, the yield is stable when it does not change much from one environment to the other. It is an important trait, that helps farmers to maintain a good income in most years.&lt;/p&gt;
&lt;p&gt;Agronomists and plant breeders are continuosly concerned with the assessment of genotype stability; this is accomplished by planning genotype experiments, where a number of genotypes is compared on randomised complete block designs, with three to five replicates. These experiments are repeated in several years and/or several locations, in order to measure how the environment influences yield level and the ranking of genotypes.&lt;/p&gt;
&lt;p&gt;I would like to show an exemplary dataset, referring to a multienvironment experiment with winter wheat. Eight genotypes were compared in seven years in central Italy, on randomised complete block designs with three replicates. The ‘WinterWheat’ dataset is available in the ‘aomisc’ package, which is the accompanying package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset and turns the ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
## Loading required package: drc
## Loading required package: MASS
## Loading required package: drcData
## 
## &amp;#39;drc&amp;#39; has been loaded.
## Please cite R and &amp;#39;drc&amp;#39; if used for a publication,
## for references type &amp;#39;citation()&amp;#39; and &amp;#39;citation(&amp;#39;drc&amp;#39;)&amp;#39;.
## 
## Attaching package: &amp;#39;drc&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     gaussian, getInitial
data(WinterWheat)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 7    35     1   SIMETO  4.99 1996
## 8    33     1    SOLEX  6.08 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that this is a multienvironment experiment as it is repeated in several years: each year is an ‘environment’ in itself. Furthermore, please note that the year effect (i.e. the environment effect) is of random nature: we select the years, but we cannot control the weather conditions.&lt;/p&gt;
&lt;div id=&#34;defining-a-linear-mixed-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defining a linear mixed model&lt;/h1&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the following linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + \gamma_{kj} + g_i + e_j +  ge_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt; is the interaction effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, while &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the residual random term, which is assumed as normally distributed with variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As I said before, the block effect, the environment effect and the ‘genotype x environment’ interaction are usually regarded as random. Therefore, they are assumed as normally distributed, with means equal to 0 and variances respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{\gamma}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{e}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. Indeed, the above model is a Linear Mixed Model (LMM).&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. It is clear that this value is a measure of instability: if it is high, genotypes may respond differently to different environments. In this way, each genotype can be favored in some specific environments and disfavored in some others. Shukla (1974) has suggested that we should allow &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt; assume a different value for each genotype and use these components as a measure of stability (stability variances). According to Shukla, a genotype is considered stable when its stability variance is lower than &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown that stability variances can be obtained within the mixed model framework, by appropriately coding the variance-covariance matrix for random effects. He gave SAS code to accomplish this task and, to me, it was not straightforward to transport his code into R. I finally succeeded and I though I should better share my code, just in case it helps someone save a few headaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-stability-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a stability variance model&lt;/h1&gt;
&lt;p&gt;As we have to model the variance-covariance of random effects, we need to use the ‘lme’ function in the ‘nlme’ package (Pinheiro and Bates, 2000). The problem is that random effects are crossed and they are not easily coded with this package. After an extensive literature search, I could find the solution in the aforementioned book (at pag. 162-163) and in Galecki and Burzykowski (2013). The trick is made by appropriately using the ‘pdMat’ construct (‘pdBlocked’ and ‘pdIdent’). In the code below, I have built a block-diagonal variance-covariance matrix for random effects, where blocks and genotypes are nested within years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdIdent(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix)
## Year = pdIdent(1), pdIdent(Block - 1), pdIdent(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      1.07314201 1.0359257
## Block1           0.01641744 0.1281306
## Block2           0.01641744 0.1281306
## Block3           0.01641744 0.1281306
## GenotypeCOLOSSEO 0.17034091 0.4127238
## GenotypeCRESO    0.17034091 0.4127238
## GenotypeDUILIO   0.17034091 0.4127238
## GenotypeGRAZIA   0.17034091 0.4127238
## GenotypeIRIDE    0.17034091 0.4127238
## GenotypeSANCARLO 0.17034091 0.4127238
## GenotypeSIMETO   0.17034091 0.4127238
## GenotypeSOLEX    0.17034091 0.4127238
## Residual         0.14880400 0.3857512&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wee see that the variance component for the ‘genotype x environment’ interaction is the same for all genotypes and equal to 0.170.&lt;/p&gt;
&lt;p&gt;Allowing for a different variance component per genotype is relatively easy, by replacing ‘pdIdent()’ with ‘pdDiag()’, as shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix2 &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdDiag(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix2)
## Year = pdIdent(1), pdIdent(Block - 1), pdDiag(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      0.86592829 0.9305527
## Block1           0.01641744 0.1281305
## Block2           0.01641744 0.1281305
## Block3           0.01641744 0.1281305
## GenotypeCOLOSSEO 0.10427267 0.3229128
## GenotypeCRESO    0.09588553 0.3096539
## GenotypeDUILIO   0.47612340 0.6900170
## GenotypeGRAZIA   0.15286445 0.3909788
## GenotypeIRIDE    0.11860160 0.3443858
## GenotypeSANCARLO 0.02575029 0.1604690
## GenotypeSIMETO   0.42998504 0.6557324
## GenotypeSOLEX    0.06713590 0.2591060
## Residual         0.14880439 0.3857517&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we have now different variance components and we can classify some genotypes as stable (e.g. Sancarlo, Solex and Creso) and some others as unstable (e.g. Duilio and Simeto).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-the-means&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working with the means&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit a stability variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retreived the means for genotypes in all years, we can fit a stability variance model, although we have to use a different approach, with respect to the one we used with the whole dataset. In this case, we need to model the variance of residuals, introducing within-group (within-year) heteroscedasticity. The ‘weights’ argument can be used, together with the ‘pdIdent()’ variance function, to allow for a different variance for each genotype. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
model.mixM &amp;lt;- lme(Yield ~ Genotype, random = ~ 1|Year, data = WinterWheatM,
                 weights = varIdent(form=~1|Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code to retrieve the within-year variances is not obvious, unfortunately. However, you can use the folllowing snippet as a guidance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vF &amp;lt;- model.mixM$modelStruct$varStruct
sdRatios &amp;lt;- c(1, coef(vF, unconstrained = F))
names(sdRatios)[1] &amp;lt;- &amp;quot;COLOSSEO&amp;quot;
scalePar &amp;lt;- model.mixM$sigma
sigma2i &amp;lt;- (scalePar * sdRatios)^2
sigma2i
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.15387857 0.14548985 0.52571780 0.20246664 0.16820264 0.07535112 
##     SIMETO      SOLEX 
## 0.47958756 0.11673900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code outputs ‘sigma2i’, which does not contain the stability variances. Indeed, we should remove the within-experiment error (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;), which can only be estimated from the whole dataset. Indeed, if we take the estimate of 0.1488 (see code above), divide by three (the number of blocks) and subtract from ‘sigma2i’, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2i - model.mix2$sigma^2/3
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.10427711 0.09588839 0.47611634 0.15286517 0.11860118 0.02574966 
##     SIMETO      SOLEX 
## 0.42998610 0.06713754&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which are the stability variances, as obtained with the analyses of the whole dataset.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Annichiarico, P., 2002. Genotype x Environment Interactions - Challenges and Opportunities for Plant Breeding and Cultivar Recommendations. Plant Production and protection paper, Food &amp;amp; Agriculture Organization of the United Nations (FAO), Roma.&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Shukla, G.K., 1972. Some statistical aspects of partitioning genotype-environmental components of variability. Heredity 29, 237–245.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>https://www.statforbiology.com/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_correlationindependence2/</guid>
      <description>


&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;
&lt;div id=&#34;a-routine-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A ‘routine’ experiment&lt;/h1&gt;
&lt;p&gt;I will use the same example as presented &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in my previous post&lt;/a&gt;, which should allow us to compare the results with those obtained by using more traditional methods of data analyses. It is a genotype experiment, laid out in randomised complete blocks, with 27 wheat genotypes and three replicates. For each plot, the collegue who made the experiment recorded several traits, including yield (Yield) and weight of thousand kernels (TKW). The dataset ‘WheatQuality.csv’ is available on ‘gitHub’; it consists of 81 records (plots) and just as many couples of measures in all. The code below loads the necessary packages, the data and transforms the numeric variable ‘Block’ into a factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(plyr)
library(nlme)
library(asreml)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;In my previous post&lt;/a&gt;, I used the above dataset to calculate the Pearson’s correlation coefficient between Yield and TKW for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;plot measurements,&lt;/li&gt;
&lt;li&gt;residuals,&lt;/li&gt;
&lt;li&gt;treatment/block means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Piepho (2018) showed that, for an experiment like this one, the above correlations can be estimated by coding a multiresponse mixed model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijk} = \mu_i + \beta_{ik} + \tau_{ij} + \epsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ijk}\)&lt;/span&gt; is the response for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the rootstock &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the mean for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{ik}\)&lt;/span&gt; is the effect of the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ij}\)&lt;/span&gt; is the effect of genotype &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; is the residual for each of 81 observations for two traits.&lt;/p&gt;
&lt;p&gt;In the above model, the residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; need to be normally distributed and heteroscedastic, with trait-specific variances. Furthermore, residuals belonging to the same plot (the two observed traits) need to be correlated (correlation of errors).&lt;/p&gt;
&lt;p&gt;Hans-Peter Piepho, in his paper, put forward the idea that the ‘genotype’ and ‘block’ effects for the two traits can be taken as random, even though they might be of fixed nature, especially the genotype effect. This idea makes sense, because, for this application, we are mainly interested in variances and covariances. Both random effects need to be heteroscedastic (trait-specific variance components) and there must be a correlation between the two traits.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge, there is no way to fit such a complex model with the ‘nlme’ package and related ‘lme()’ function (I’ll gave a hint later on, for a simpler model). Therefore, I decided to use the package ‘asreml’ (Butler et al., 2018), although this is not freeware. With the function ‘asreml()’, we need to specify the following components.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The response variables. When we set a bivariate model with ‘asreml’, we can ‘cbind()’ Yield and TKW and use the name ‘trait’ to refer to them.&lt;/li&gt;
&lt;li&gt;The fixed model, that only contains the trait effect. The specification is, therefore, ‘cbind(Yield, TKW) ~ trait - 1’. Following Piepho (2018), I removed the intercept, to separately estimate the means for the two traits.&lt;/li&gt;
&lt;li&gt;The random model, that is composed by the interactions ‘genotype x trait’ and ‘block x trait’. For both, I specified a general unstructured variance covariance matrix, so that the traits are heteroscedastic and correlated. Therefore, the random model is ~ Genotype:us(trait) + Block:us(trait).&lt;/li&gt;
&lt;li&gt;The residual structure, where the observations in the same plot (the term ‘units’ is used in ‘asreml’ to represent the observational units, i.e. the plots) are heteroscedastic and correlated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait, init = c(3.7, -0.25, 1.7)) + 
          Block:us(trait, init = c(77, 38, 53)),
        residual = ~ units:us(trait, init = c(6, 0.16, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:22 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -641.556           1.0    160 18:20:22    0.0 (3 restrained)
##  2      -548.767           1.0    160 18:20:22    0.0
##  3      -448.970           1.0    160 18:20:22    0.0
##  4      -376.952           1.0    160 18:20:22    0.0
##  5      -334.100           1.0    160 18:20:22    0.0
##  6      -317.511           1.0    160 18:20:22    0.0
##  7      -312.242           1.0    160 18:20:22    0.0
##  8      -311.145           1.0    160 18:20:22    0.0
##  9      -311.057           1.0    160 18:20:22    0.0
## 10      -311.056           1.0    160 18:20:22    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   component  std.error    z.ratio
## Block:trait!trait_Yield:Yield     3.7104778  3.9364268  0.9426005
## Block:trait!trait_TKW:Yield      -0.2428390  1.9074544 -0.1273105
## Block:trait!trait_TKW:TKW         1.6684568  1.8343662  0.9095549
## Genotype:trait!trait_Yield:Yield 77.6346623 22.0956257  3.5135761
## Genotype:trait!trait_TKW:Yield   38.8322972 15.0909109  2.5732242
## Genotype:trait!trait_TKW:TKW     53.8616088 15.3520661  3.5084274
## units:trait!R                     1.0000000         NA         NA
## units:trait!trait_Yield:Yield     6.0939037  1.1951128  5.0990195
## units:trait!trait_TKW:Yield       0.1635551  0.7242690  0.2258209
## units:trait!trait_TKW:TKW         4.4717901  0.8769902  5.0990195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The box above shows the results about the variance-covariance parameters. In order to get the correlations, I specified the necessary combinations of variance-covariance parameters. It is necessary to remember that estimates, in ‘asreml’, are named as V1, V2, … Vn, according to their ordering in model output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parms &amp;lt;- mod.asreml$vparameters
vpredict(mod.asreml, rb ~ V2 / (sqrt(V1)*sqrt(V3) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate        SE
## rb -0.09759916 0.7571335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, rt ~ V5 / (sqrt(V4)*sqrt(V6) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate       SE
## rt 0.6005174 0.130663&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, re ~ V9 / (sqrt(V8)*sqrt(V10) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate        SE
## re 0.03133109 0.1385389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the estimates are very close to those obtained by using the Pearson’s correlation coefficients (see my previous post). The advantage of this mixed model solution is that we can also test hypotheses in a relatively reliable way. For example, I tested the hypothesis that residuals are not correlated by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;coding a reduced model where residuals are heteroscedastic and independent, and&lt;/li&gt;
&lt;li&gt;comparing this reduced model with the complete model by way of a REML-based Likelihood Ratio Test (LRT).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Removing the correlation of residuals is easily done, by changing the correlation structure from ‘us’ (unstructured variance-covariance matrix) to ‘idh’ (diagonal variance-covariance matrix).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:us(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:23 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.023           1.0    160 18:20:23    0.0 (2 restrained)
##  2      -383.859           1.0    160 18:20:23    0.0
##  3      -344.687           1.0    160 18:20:23    0.0
##  4      -321.489           1.0    160 18:20:23    0.0
##  5      -312.488           1.0    160 18:20:23    0.0
##  6      -311.167           1.0    160 18:20:23    0.0
##  7      -311.083           1.0    160 18:20:23    0.0
##  8      -311.082           1.0    160 18:20:23    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml2  1      0.05107    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, I tried to further reduce the model to test the significance of the correlation between block means and genotype means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:24 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.027           1.0    160 18:20:24    0.0 (2 restrained)
##  2      -383.866           1.0    160 18:20:24    0.0
##  3      -344.694           1.0    160 18:20:24    0.0
##  4      -321.497           1.0    160 18:20:24    0.0
##  5      -312.496           1.0    160 18:20:24    0.0
##  6      -311.175           1.0    160 18:20:24    0.0
##  7      -311.090           1.0    160 18:20:24    0.0
##  8      -311.090           1.0    160 18:20:24    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml3  2     0.066663    0.6399&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml4 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:idh(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:25 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -406.458           1.0    160 18:20:25    0.0 (2 restrained)
##  2      -394.578           1.0    160 18:20:25    0.0
##  3      -352.769           1.0    160 18:20:25    0.0
##  4      -327.804           1.0    160 18:20:25    0.0
##  5      -318.007           1.0    160 18:20:25    0.0
##  6      -316.616           1.0    160 18:20:25    0.0
##  7      -316.549           1.0    160 18:20:25    0.0
##  8      -316.549           1.0    160 18:20:25    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)   
## mod.asreml/mod.asreml4  3       10.986  0.003364 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that only the genotype means are significantly correlated.&lt;/p&gt;
&lt;p&gt;An alternative (and more useful) way to code the same model is by using the ‘corgh’ structure, instead of ‘us’. These two structures are totally similar, apart from the fact that the first one uses the correlations, instead of the covariances. Another difference, which we should consider when giving starting values, is that correlations come before variances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:26 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -632.445           1.0    160 18:20:26    0.0 (3 restrained)
##  2      -539.383           1.0    160 18:20:26    0.0 (2 restrained)
##  3      -468.810           1.0    160 18:20:26    0.0 (1 restrained)
##  4      -422.408           1.0    160 18:20:26    0.0
##  5      -371.304           1.0    160 18:20:26    0.0
##  6      -336.191           1.0    160 18:20:26    0.0
##  7      -317.547           1.0    160 18:20:26    0.0
##  8      -312.105           1.0    160 18:20:26    0.0
##  9      -311.118           1.0    160 18:20:26    0.0
## 10      -311.057           1.0    160 18:20:26    0.0
## 11      -311.056           1.0    160 18:20:26    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09759916  0.7571335
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60051740  0.1306748
## Genotype:trait!trait_Yield                77.63466334 22.0981962
## Genotype:trait!trait_TKW                  53.86160957 15.3536792
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.03133109  0.1385389
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of this parameterisation is that we can test our hypotheses by easily setting up contraints on correlations. One way to do this is to run the model with the argument ‘start.values = T’. In this way I could derive a data frame (‘mod.init$parameters’), with the starting values for REML maximisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the starting values
mod.init &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset, start.values = T)
init &amp;lt;- mod.init$vparameters
init&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                    Component Value Constraint
## 1  Genotype:trait!trait!TKW:!trait!Yield.cor -0.10          U
## 2                 Genotype:trait!trait_Yield  3.80          P
## 3                   Genotype:trait!trait_TKW  1.80          P
## 4     Block:trait!trait!TKW:!trait!Yield.cor  0.60          U
## 5                    Block:trait!trait_Yield 77.00          P
## 6                      Block:trait!trait_TKW 53.00          P
## 7                              units:trait!R  1.00          F
## 8     units:trait!trait!TKW:!trait!Yield.cor  0.03          U
## 9                    units:trait!trait_Yield  6.00          P
## 10                     units:trait!trait_TKW  4.50          P&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘init’ data frame has three columns: (i) names of parameters, (ii) initial values and (iii) type of constraint (U: unconstrained, P = positive, F = fixed). Now, if we take the seventh row (correlation of residuals), set the initial value to 0 and set the third column to ‘F’ (meaning: keep the initial value fixed), we are ready to fit a model without correlation of residuals (same as the ‘model.asreml2’ above). What I had to do was just to pass this data frame as the starting value matrix for a new model fit (see the argument ‘R.param’, below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init2 &amp;lt;- init
init2[8, 2] &amp;lt;- 0
init2[8, 3] &amp;lt;- &amp;quot;F&amp;quot;

mod.asreml.r2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:28 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1136.066           1.0    160 18:20:28    0.0 (1 restrained)
##  2      -939.365           1.0    160 18:20:28    0.0 (1 restrained)
##  3      -719.371           1.0    160 18:20:28    0.0 (1 restrained)
##  4      -550.513           1.0    160 18:20:28    0.0
##  5      -427.355           1.0    160 18:20:28    0.0
##  6      -353.105           1.0    160 18:20:28    0.0
##  7      -323.421           1.0    160 18:20:28    0.0
##  8      -313.616           1.0    160 18:20:28    0.0
##  9      -311.338           1.0    160 18:20:28    0.0
## 10      -311.087           1.0    160 18:20:28    0.0
## 11      -311.082           1.0    160 18:20:28    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r2)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09516456  0.7572040
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60136047  0.1305180
## Genotype:trait!trait_Yield                77.63463977 22.0809306
## Genotype:trait!trait_TKW                  53.86159936 15.3451466
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.00000000         NA
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r2, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)
## mod.asreml.r/mod.asreml.r2  1     0.051075    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is even more interesting is that ‘asreml’ permits to force the parameters to be linear combinations of one another. For instance, we can code a model where the residual correlation is contrained to be equal to the treatment correlation. To do so, we have to set up a two-column matrix (M), with row names matching the component names in the ‘asreml’ parameter vector. The matrix M should contain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the first column, the equality relationships (same number, same value)&lt;/li&gt;
&lt;li&gt;in the second column, the coefficients for the multiplicative relationships&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we would set the matrix M as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;firstCol &amp;lt;- c(1, 2, 3, 4, 5, 6, 7, 1, 8, 9)
secCol &amp;lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
M &amp;lt;- cbind(firstCol, secCol)
dimnames(M)[[1]] &amp;lt;- mod.init$vparameters$Component
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           firstCol secCol
## Genotype:trait!trait!TKW:!trait!Yield.cor        1      1
## Genotype:trait!trait_Yield                       2      1
## Genotype:trait!trait_TKW                         3      1
## Block:trait!trait!TKW:!trait!Yield.cor           4      1
## Block:trait!trait_Yield                          5      1
## Block:trait!trait_TKW                            6      1
## units:trait!R                                    7      1
## units:trait!trait!TKW:!trait!Yield.cor           1      1
## units:trait!trait_Yield                          8      1
## units:trait!trait_TKW                            9      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that in ‘firstCol’, the 1st and 8th element are both equal to 1, which contraints them to assume the same value. We can now pass the matrix M as the value of the ‘vcc’ argument in the model call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init, vcc = M)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:29 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1122.762           1.0    160 18:20:29    0.0 (1 restrained)
##  2      -900.308           1.0    160 18:20:29    0.0
##  3      -665.864           1.0    160 18:20:29    0.0
##  4      -492.020           1.0    160 18:20:29    0.0
##  5      -383.085           1.0    160 18:20:29    0.0
##  6      -336.519           1.0    160 18:20:29    0.0 (1 restrained)
##  7      -319.561           1.0    160 18:20:29    0.0
##  8      -315.115           1.0    160 18:20:29    0.0
##  9      -314.540           1.0    160 18:20:29    0.0
## 10      -314.523           1.0    160 18:20:29    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r3)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            component  std.error    z.ratio
## Block:trait!trait!TKW:!trait!Yield.cor    -0.1146908  0.7592678 -0.1510545
## Block:trait!trait_Yield                    3.6991785  3.9364622  0.9397216
## Block:trait!trait_TKW                      1.6601799  1.8344090  0.9050217
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.2336876  0.1117699  2.0907919
## Genotype:trait!trait_Yield                70.5970531 19.9293722  3.5423621
## Genotype:trait!trait_TKW                  48.9763800 13.8464106  3.5371174
## units:trait!R                              1.0000000         NA         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.2336876  0.1117699  2.0907919
## units:trait!trait_Yield                    6.3989855  1.2811965  4.9945387
## units:trait!trait_TKW                      4.6952670  0.9400807  4.9945358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r3, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)   
## mod.asreml.r/mod.asreml.r3  1       6.9336   0.00423 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the residual and treatment correlations are equal in this latter model. We also see that this reduced model fits significantly worse than the complete model ‘mod.asreml.r’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-freeware&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going freeware&lt;/h1&gt;
&lt;p&gt;Considering that the block means are not correlated, if we were willing to take the ‘block’ effect as fixed, we could fit the resulting model also with the ‘nlme’ package and the function ‘lme()’ (Pinheiro and Bates, 2000). However, we should cast the model as a univariate model.&lt;/p&gt;
&lt;p&gt;To this aim, the two variables (Yield and TKW) need to be piled up and a new factor (‘Trait’) needs to be introduced to identify the observations for the two traits. Another factor is also necessary to identify the different plots, i.e. the observational units. To perform such a restructuring, I used the ‘melt()’ function in the ‘reshape2’ package Wickham, 2007) and assigned the name ‘Y’ to the response variable, that is now composed by the two original variables Yield and TKW, one on top of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$Plot &amp;lt;- 1:81
mdataset &amp;lt;- melt(dataset[,c(-3,-6)], variable.name= &amp;quot;Trait&amp;quot;, value.name=&amp;quot;Y&amp;quot;, id=c(&amp;quot;Genotype&amp;quot;, &amp;quot;Block&amp;quot;, &amp;quot;Plot&amp;quot;))
mdataset$Block &amp;lt;- factor(mdataset$Block)
head(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait    Y
## 1 arcobaleno     1    1   TKW 44.5
## 2 arcobaleno     2    2   TKW 42.8
## 3 arcobaleno     3    3   TKW 42.7
## 4       baio     1    4   TKW 40.6
## 5       baio     2    5   TKW 42.7
## 6       baio     3    6   TKW 41.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait     Y
## 157  vesuvio     1   76 Yield 54.37
## 158  vesuvio     2   77 Yield 55.02
## 159  vesuvio     3   78 Yield 53.41
## 160 vitromax     1   79 Yield 54.39
## 161 vitromax     2   80 Yield 50.97
## 162 vitromax     3   81 Yield 48.83&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fixed model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y ~ Trait*Block&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to introduce a totally unstructured variance-covariance matrix for the random effect, I used the ‘pdMat’ construct:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;random = list(Genotype = pdSymm(~Trait - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relating to the residuals, heteroscedasticity can be included by using the ‘weight()’ argument and the ‘varIdent’ variance function, which allows a different standard deviation per trait:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weight = varIdent(form = ~1|Trait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also allowed the residuals to be correlated within each plot, by using the ‘correlation’ argument and specifying a general ‘corSymm()’ correlation form. Plots are nested within genotypes, thus I used a nesting operator, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;correlation = corSymm(form = ~1|Genotype/Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Y ~ Trait*Block, 
                 random = list(Genotype = pdSymm(~Trait - 1)),
                 weight = varIdent(form=~1|Trait), 
                 correlation = corCompSymm(form=~1|Genotype/Plot),
                 data = mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retreiving the variance-covariance matrices needs some effort, as the function ‘getVarCov()’ does not appear to work properly with this multistratum model. First of all, we can retreive the variance-covariance matrix for the genotype random effect (G) and the corresponding correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Variance structure for random effects
obj &amp;lt;- mod
G &amp;lt;- matrix( as.numeric(getVarCov(obj)), 2, 2 )
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]
## [1,] 53.86081 38.83246
## [2,] 38.83246 77.63485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.6005237
## [2,] 0.6005237 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we can retreive the ‘conditional’ variance-covariance matrix (R), that describes the correlation of errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conditional variance-covariance matrix (residual error)
V &amp;lt;- corMatrix(obj$modelStruct$corStruct)[[1]] #Correlation for residuals
sds &amp;lt;- 1/varWeights(obj$modelStruct$varStruct)[1:2]
sds &amp;lt;- obj$sigma * sds #Standard deviations for residuals (one per trait)
R &amp;lt;- t(V * sds) * sds #Going from correlation to covariance
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 4.4718234 0.1635375
## [2,] 0.1635375 6.0939251&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]
## [1,] 1.00000000 0.03132756
## [2,] 0.03132756 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total correlation matrix is simply obtained as the sum of G and R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tr &amp;lt;- G + R
cov2cor(Tr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.5579906
## [2,] 0.5579906 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are the same as those obtained by using ‘asreml’. Hope these snippets are useful!&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Butler, D., Cullis, B.R., Gilmour, A., Gogel, B., Thomson, R., 2018. ASReml-r reference manual - version 4. UK.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-effects models in s and s-plus. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Wickham, H., 2007. Reshaping data with the reshape package. Journal of Statistical Software 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>