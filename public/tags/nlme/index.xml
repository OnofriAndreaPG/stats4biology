<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlme on Fixing the bridge between biologists and statisticians</title>
    <link>http://localhost:4503/tags/nlme/</link>
    <description>Recent content in Nlme on Fixing the bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2023, @AndreaOnofri</copyright>
    <lastBuildDate>Fri, 15 Mar 2024 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://localhost:4503/tags/nlme/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Here is why I don&#39;t care about the Levene&#39;s test</title>
      <link>http://localhost:4503/2024/stat_lm_checkassumptions/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4503/2024/stat_lm_checkassumptions/</guid>
      <description>


&lt;p&gt;During my stat courses, I never give my students any information about the Levene’s test (Levene and Howard, 1960), or other similar tests for homoscedasticity, unless I am specifically prompted to do so. It is not that I intend to underrate the tremendous importance of checking for the basic assumptions of linear model! On the contrary, I always show my students several methods for the graphical inspection of model residuals, but I do not share the same aching desire for a P-value, that most of my colleagues seem to possess.&lt;/p&gt;
&lt;p&gt;I understand that the Levene’s test could have played an important role at the time when it was devised; at that time, biologists/agronomists used to only have ANOVA and regression at their disposal and the Levene’s or other formal tests were strictly necessary to assess whether those powerful techniques (ANOVA/regression) could be reliably used, in place of other less powerful non-parametric methods of data analyses. At that time, the only sensible question they could ask the data was: ‘are the variances homogeneous across the experimental groups?’. And the P-level from a Levene’s test provided the perfect answer.&lt;/p&gt;
&lt;p&gt;Today, things have vastly changed: we have very powerful computers and we have the freeware statistical environment R, where an awful lot of advanced models are put at our disposal. There is nothing that prevents us from asking our body of data several interesting questions, such as:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;do the treatments affect the variances of the experimental groups?&lt;/li&gt;
&lt;li&gt;Are the variances, somehow, related to the expected responses?&lt;/li&gt;
&lt;li&gt;Is a heteroscedastic model more reasonable than a homoscedastic model?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nowadays, we can answer all these (and other) questions by an appropriate comparison of alternative models: for example, let’s consider a genotype experiment with kamut, resulting in the dataset ‘HetGenotypes.csv’, which is available in an external repository. It is a simple fully randomised experiment with three replicates, aimed at comparing the yield of 15 kamut genotypes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/HetGenotypes.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName)
dataset &amp;lt;- dataset |&amp;gt;
  transform(Genotype, factor(Genotype))
head(dataset)
##   Genotype Yield
## 1        A 8.129
## 2        A 8.616
## 3        A 7.932
## 4        B 6.188
## 5        B 6.767
## 6        B 6.635&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It would be rather common to start the analyses by fitting a simple ‘ANOVA’ model, where the yield measurements (&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;) are regarded as functions of the ovarell mean (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) plus the genotype effects (&lt;span class=&#34;math inline&#34;&gt;\(\alpha_j\)&lt;/span&gt;), while errors (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;) are regarded as gaussian, independent and homoscedastic (a simple ANOVA model, indeed):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i = \mu + \alpha_j + \varepsilon_i; \quad \quad  \textrm{with:} \quad \quad\varepsilon_i  \sim N(0, \sigma)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For reasons that will be clearer later on, we fit this model by using the &lt;code&gt;gls()&lt;/code&gt; function in the &lt;code&gt;nlme&lt;/code&gt; package (same syntax as with the usual &lt;code&gt;lm()&lt;/code&gt; function). Immediately afterwards, we plot the residuals, which reveals that there are strong differences of variability across groups, but residuals are not clearly related to yield level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
library(ggplot2)
mod.aov &amp;lt;- gls(Yield ~ Genotype, data = dataset)

g1 &amp;lt;- ggplot(dataset) +
  geom_boxplot(mapping = aes(y = residuals(mod.aov), x = Genotype)) +
  theme_bw()
g2 &amp;lt;- ggplot() +
  geom_point(mapping = aes(y = residuals(mod.aov), x = fitted(mod.aov))) +
  theme_bw()
library(gridExtra)
gridExtra::grid.arrange(g1, g2, nrow = 1) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_lm_checkAssumptions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My question at this stage: is it sensible to ask whether the variances are the same across groups? Using Tukey’s word (Tukey, 1991), such a question appears to be ‘rather foolish’, as the answer is already clear from the previous figure. However, it is sensible to ask whether a more complex model, allowing for heteroscedastic of within group errors, is warranted. Such a model would be expressed as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i = \mu + \alpha_j + \varepsilon_i; \quad \quad  \textrm{with:} \quad \quad\varepsilon_i  \sim N(0, \sigma_j)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; is allowed to assume a different value for each genotype; nowadays, fitting such a model is widely feasible, by using the same &lt;code&gt;gls()&lt;/code&gt; function as we have used before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.het &amp;lt;- gls(Yield ~ Genotype, data = dataset,
               weights = varIdent(form = ~1|Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You see it: no fitting difficulties of any kind. This second model, with respect to the first one, may be more respectful of what we observed in our experiment, but it has the cost of requiring 14 extra parameters, to estimate 15 variances, instead of just one. Is this extra-cost justified by the data at hand?&lt;/p&gt;
&lt;p&gt;We can directly compare the two models by using the AIC (Akaike’s information criterion):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.aov, mod.het)
##         Model df       AIC      BIC    logLik   Test  L.Ratio p-value
## mod.aov     1 16 106.18435 128.6035 -37.09218                        
## mod.het     2 30  97.44571 139.4816 -18.72285 1 vs 2 36.73865   8e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the AIC is lower for the heteroscedastic model and we also see that the Likelihood Ratio Test confirms that the more complex model is much more likely, in spite of the cost induced by the need for extra parameters. Thus I keep the most complex model and go ahead with that.&lt;/p&gt;
&lt;p&gt;In conclusion, this is why I do not care about the Levene’s test: it provides a good answer to the wrong question (BTW: the Levene’s test in this circumstance turns out to be non-significant…)!&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Akaike, H., 1974. A new look at the statistical model identification. IEEE Transactions on Automatic Control 19, 716–723. &lt;a href=&#34;https://doi.org/doi:10.1109/TAC.1974.1100705&#34; class=&#34;uri&#34;&gt;https://doi.org/doi:10.1109/TAC.1974.1100705&lt;/a&gt;, MR 0423716&lt;/li&gt;
&lt;li&gt;Levene and Howard, 1960. Robust tests for equality of variances, In Ingram Olkin; Harold Hotelling; et al. (eds.), Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling, Stanford University Press.&lt;/li&gt;
&lt;li&gt;Tukey, J.W., 1991. The phylosophy of multiple comparisons. Statistical Science 6, 100–116.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Meta-analysis for a single study. Is it possible?</title>
      <link>http://localhost:4503/2022/stat_met_metanalyses/</link>
      <pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4503/2022/stat_met_metanalyses/</guid>
      <description>


&lt;p&gt;We all know that the word meta-analysis encompasses a body of statistical techniques to combine quantitative evidence from several independent studies. However, I have recently discovered that meta-analytic methods can also be used to analyse the results of a single research project. That happened a few months ago, when I was reading a paper from Damesa et al. (2017), where the authors describe some interesting methods of data analyses for multi-environment genotype experiments. These authors gave a few nice examples with related SAS code, that is rooted in mixed models. As an R enthusiast, I was willing to reproduce their analyses with R, but I could not succeed, until I realised that I could make use of the package ‘metafor’ and its bunch of meta-analityc methods.&lt;/p&gt;
&lt;p&gt;In this post, I will share my R coding, for those of you who are interested in meta-analytic methods and multi-environment experiments. Let’s start by having a look at the example that motivated my interest (Example 1 in Damesa et al., 2017, p. 849).&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;Twenty-two different genotypes of maize were compared in Ethiopia, in relation to their yield level, in four sites (Dhera, Melkassa, Mieso, and Ziway). At all sites, there were 11 incomplete blocks in each of three replicates. The data are available in Damesa et al. (2017) as supplemental material; I have put this data at your disposal in my web repository, to reproduce this example; let’s load the data first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
library(sommer)
library(emmeans)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/Damesa2017.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(1:5, .fns = factor))
head(dataset)
##   site rep block plot genotype row col yield
## 1    1   1     1    1        6   1   1  9.93
## 2    1   1     1    2       22   1   2  6.51
## 3    1   1     2    3       17   1   3  7.92
## 4    1   1     2    4       14   1   4  9.28
## 5    1   1     3    5       12   1   5  7.56
## 6    1   1     3    6       10   1   6  9.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a typical multi-environment experiment: we have three blocking factors (‘site’, ‘rep’ and ‘block’) and one treatment factor (‘genotype’), as well as the ‘yield’ response variable. Let’s see how this dataset can be analysed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-golden-standard-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The ‘golden standard’ analysis&lt;/h1&gt;
&lt;p&gt;In most situations with multi-environment experiments, we are interested in broad space inference about genotypes, which means that we want to determine the best genotypes across the whole set of environments. Accordingly, the ‘site’ and ‘site x genotype’ effects must be regarded as random, while the ‘genotype’ effect is fixed. Furthermore, we need to consider the ‘design’ effects, that (in this specific case) are the ‘reps within sites’ and the ‘blocks within reps within sites’ random effects. Finally, we have the residual error term (‘plots within blocks within reps within sites’), that is always included by default.&lt;/p&gt;
&lt;p&gt;So far, so good, but we have to go slightly more complex; for this type of studies, the variances for replicates, blocks, and residual error should be site specific, which is usually the most realistic assumption. In the end, we need to estimate:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;22 genotype means with standard errors&lt;/li&gt;
&lt;li&gt;one variance component for the site effect&lt;/li&gt;
&lt;li&gt;one variance component for the ‘genotype x site’ interaction&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the ‘rep’ effect&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the ‘block within rep’ effect&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the residual error&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we work with the &lt;code&gt;lme()&lt;/code&gt; function in the &lt;code&gt;nlme&lt;/code&gt; package, we have to create a couple of ‘dummy’ variables (‘one’ and ‘GE’), in order to reference the crossed random effects (see Galecki and Burzykowski, 2013).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# One stage analysis
dataset$one &amp;lt;- 1L
dataset$GE &amp;lt;- with(dataset, genotype:site)
model.mix &amp;lt;- lme(yield ~ genotype - 1, 
             random = list(one = pdIdent(~ site - 1),
                           one = pdIdent(~ GE - 1),
                           rep = pdDiag(~ site - 1),
                           block = pdDiag(~ site - 1)),
                              data = dataset,
             weights = varIdent(form = ~1|site))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The means for genotypes are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mg &amp;lt;- emmeans(model.mix, ~ genotype)
mg
##  genotype emmean   SE  df lower.CL upper.CL
##  1          5.15 1.65 210    1.900     8.40
##  2          5.54 1.65 210    2.296     8.79
##  3          5.19 1.65 210    1.939     8.44
##  4          4.59 1.65 210    1.341     7.84
##  5          4.82 1.65 210    1.568     8.07
##  6          4.66 1.65 210    1.411     7.91
##  7          4.64 1.65 210    1.388     7.88
##  8          4.36 1.65 210    1.110     7.61
##  9          5.03 1.65 210    1.785     8.28
##  10         4.84 1.65 210    1.592     8.09
##  11         4.54 1.65 210    1.290     7.79
##  12         4.87 1.65 210    1.622     8.12
##  13         4.84 1.65 210    1.593     8.09
##  14         4.29 1.65 210    1.045     7.54
##  15         4.47 1.65 210    1.224     7.72
##  16         4.37 1.65 210    1.123     7.62
##  17         4.07 1.65 210    0.819     7.32
##  18         4.95 1.65 210    1.697     8.19
##  19         4.71 1.65 210    1.466     7.96
##  20         4.86 1.65 210    1.612     8.11
##  21         4.13 1.65 210    0.885     7.38
##  22         4.63 1.65 210    1.380     7.88
## 
## Degrees-of-freedom method: containment 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the variance components are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(model.mix)
##          Variance          StdDev      
## one =    pdIdent(site - 1)             
## site1    1.045428e+01      3.233309e+00
## site2    1.045428e+01      3.233309e+00
## site3    1.045428e+01      3.233309e+00
## site4    1.045428e+01      3.233309e+00
## one =    pdIdent(GE - 1)               
## GE1:1    1.052944e-01      3.244909e-01
## GE1:2    1.052944e-01      3.244909e-01
## GE1:3    1.052944e-01      3.244909e-01
## GE1:4    1.052944e-01      3.244909e-01
## GE2:1    1.052944e-01      3.244909e-01
## GE2:2    1.052944e-01      3.244909e-01
## GE2:3    1.052944e-01      3.244909e-01
## GE2:4    1.052944e-01      3.244909e-01
## GE3:1    1.052944e-01      3.244909e-01
## GE3:2    1.052944e-01      3.244909e-01
## GE3:3    1.052944e-01      3.244909e-01
## GE3:4    1.052944e-01      3.244909e-01
## GE4:1    1.052944e-01      3.244909e-01
## GE4:2    1.052944e-01      3.244909e-01
## GE4:3    1.052944e-01      3.244909e-01
## GE4:4    1.052944e-01      3.244909e-01
## GE5:1    1.052944e-01      3.244909e-01
## GE5:2    1.052944e-01      3.244909e-01
## GE5:3    1.052944e-01      3.244909e-01
## GE5:4    1.052944e-01      3.244909e-01
## GE6:1    1.052944e-01      3.244909e-01
## GE6:2    1.052944e-01      3.244909e-01
## GE6:3    1.052944e-01      3.244909e-01
## GE6:4    1.052944e-01      3.244909e-01
## GE7:1    1.052944e-01      3.244909e-01
## GE7:2    1.052944e-01      3.244909e-01
## GE7:3    1.052944e-01      3.244909e-01
## GE7:4    1.052944e-01      3.244909e-01
## GE8:1    1.052944e-01      3.244909e-01
## GE8:2    1.052944e-01      3.244909e-01
## GE8:3    1.052944e-01      3.244909e-01
## GE8:4    1.052944e-01      3.244909e-01
## GE9:1    1.052944e-01      3.244909e-01
## GE9:2    1.052944e-01      3.244909e-01
## GE9:3    1.052944e-01      3.244909e-01
## GE9:4    1.052944e-01      3.244909e-01
## GE10:1   1.052944e-01      3.244909e-01
## GE10:2   1.052944e-01      3.244909e-01
## GE10:3   1.052944e-01      3.244909e-01
## GE10:4   1.052944e-01      3.244909e-01
## GE11:1   1.052944e-01      3.244909e-01
## GE11:2   1.052944e-01      3.244909e-01
## GE11:3   1.052944e-01      3.244909e-01
## GE11:4   1.052944e-01      3.244909e-01
## GE12:1   1.052944e-01      3.244909e-01
## GE12:2   1.052944e-01      3.244909e-01
## GE12:3   1.052944e-01      3.244909e-01
## GE12:4   1.052944e-01      3.244909e-01
## GE13:1   1.052944e-01      3.244909e-01
## GE13:2   1.052944e-01      3.244909e-01
## GE13:3   1.052944e-01      3.244909e-01
## GE13:4   1.052944e-01      3.244909e-01
## GE14:1   1.052944e-01      3.244909e-01
## GE14:2   1.052944e-01      3.244909e-01
## GE14:3   1.052944e-01      3.244909e-01
## GE14:4   1.052944e-01      3.244909e-01
## GE15:1   1.052944e-01      3.244909e-01
## GE15:2   1.052944e-01      3.244909e-01
## GE15:3   1.052944e-01      3.244909e-01
## GE15:4   1.052944e-01      3.244909e-01
## GE16:1   1.052944e-01      3.244909e-01
## GE16:2   1.052944e-01      3.244909e-01
## GE16:3   1.052944e-01      3.244909e-01
## GE16:4   1.052944e-01      3.244909e-01
## GE17:1   1.052944e-01      3.244909e-01
## GE17:2   1.052944e-01      3.244909e-01
## GE17:3   1.052944e-01      3.244909e-01
## GE17:4   1.052944e-01      3.244909e-01
## GE18:1   1.052944e-01      3.244909e-01
## GE18:2   1.052944e-01      3.244909e-01
## GE18:3   1.052944e-01      3.244909e-01
## GE18:4   1.052944e-01      3.244909e-01
## GE19:1   1.052944e-01      3.244909e-01
## GE19:2   1.052944e-01      3.244909e-01
## GE19:3   1.052944e-01      3.244909e-01
## GE19:4   1.052944e-01      3.244909e-01
## GE20:1   1.052944e-01      3.244909e-01
## GE20:2   1.052944e-01      3.244909e-01
## GE20:3   1.052944e-01      3.244909e-01
## GE20:4   1.052944e-01      3.244909e-01
## GE21:1   1.052944e-01      3.244909e-01
## GE21:2   1.052944e-01      3.244909e-01
## GE21:3   1.052944e-01      3.244909e-01
## GE21:4   1.052944e-01      3.244909e-01
## GE22:1   1.052944e-01      3.244909e-01
## GE22:2   1.052944e-01      3.244909e-01
## GE22:3   1.052944e-01      3.244909e-01
## GE22:4   1.052944e-01      3.244909e-01
## rep =    pdDiag(site - 1)              
## site1    8.817499e-02      2.969427e-01
## site2    1.383338e+00      1.176154e+00
## site3    4.189717e-09      6.472802e-05
## site4    1.442336e-02      1.200973e-01
## block =  pdDiag(site - 1)              
## site1    3.312025e-01      5.755020e-01
## site2    4.746751e-01      6.889667e-01
## site3    5.408242e-09      7.354075e-05
## site4    6.953371e-02      2.636925e-01
## Residual 1.346652e+00      1.160454e+00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, apart from some differences relating to the optimisation method, the results are equal to those reported in Tables 1 and 2 of Damesa et al. (2017).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-stage-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two-stage analyses&lt;/h1&gt;
&lt;p&gt;The above analysis is fully correct, but, in some circumstances may be unfeasible. In particular, we may have problems when:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the number of sites is very high, and&lt;/li&gt;
&lt;li&gt;different experimental designs have been used in different sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In these circumstances, it is advantageous to break the analysis in two stages, as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;first stage: we separately analyse the different experiments and obtain the means for all genotypes in all sites;&lt;/li&gt;
&lt;li&gt;second stage: we jointly analyse the genotype means from all sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This two-stage analysis is far simpler, because the data are only pooled at the second stage, where possible design constraints are no longer important (they are considered only at the first stage). However, this two-stage analysis does not necessarily lead to the same results as the one-stage analysis, unless the whole information obtained at the first stage is carried forward to the second one (fully efficient two-stage analysis).&lt;/p&gt;
&lt;p&gt;In more detail, genotypic variances and correlations, as observed in the first stage, should not be neglected in the second stage. Damesa et al. (2017) demonstrate that the best approach is to take the full variance-covariance matrix of genotypes at the first stage and bring it forward to the second stage. They give the coding with SAS, but, how do we do it, with R?&lt;/p&gt;
&lt;p&gt;First of all, we perform the first stage of analysis, using the &lt;code&gt;by()&lt;/code&gt; function to analyse the data separately for each site. In each site, we fit a mixed model, where the genotype is fixed, while the replicates and the incomplete blocks within replicates are random effects. Of course, this coding works because the experimental design is the same at all sites, while it should be modified in other cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First stage
model.1step &amp;lt;- by(dataset, dataset$site,
                  function(df) lme(yield ~ genotype - 1, 
             random = ~1|rep/block, 
             data = df) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we use the function &lt;code&gt;lapply()&lt;/code&gt; to get the variance components. The results are similar to those obtained in one-stage analysis (see also Damesa et al., 2017, Table 1)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the variance components
lapply(model.1step, VarCorr)
## $`1`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 0.1003720    0.3168153
## block =     pdLogChol(1)          
## (Intercept) 0.2505444    0.5005441
## Residual    1.2361933    1.1118423
## 
## $`2`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 1.4012207    1.1837317
## block =     pdLogChol(1)          
## (Intercept) 0.4645211    0.6815579
## Residual    0.2020162    0.4494621
## 
## $`3`
##             Variance     StdDev      
## rep =       pdLogChol(1)             
## (Intercept) 2.455710e-10 1.567071e-05
## block =     pdLogChol(1)             
## (Intercept) 1.825915e-09 4.273072e-05
## Residual    1.054905e+00 1.027085e+00
## 
## $`4`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 0.01412879   0.1188646
## block =     pdLogChol(1)          
## (Intercept) 0.07196842   0.2682693
## Residual    0.11262234   0.3355925&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can retrieve the genotypic means at all sites:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the means
sitmeans &amp;lt;- lapply(model.1step, 
                function(el) 
                  data.frame(emmeans(el, ~genotype)))
sitmeans &amp;lt;- do.call(rbind, sitmeans)
sitmeans$site &amp;lt;- factor(rep(1:4, each = 22))
head(sitmeans)
##     genotype   emmean        SE df lower.CL  upper.CL site
## 1.1        1 8.253672 0.7208426 12 6.683091  9.824253    1
## 1.2        2 7.731118 0.7208426 12 6.160537  9.301699    1
## 1.3        3 7.249198 0.7208426 12 5.678617  8.819779    1
## 1.4        4 8.565262 0.7208426 12 6.994681 10.135843    1
## 1.5        5 8.560002 0.7208426 12 6.989421 10.130583    1
## 1.6        6 9.510255 0.7208426 12 7.939674 11.080836    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance-covariance matrix for genotype means is obtained, for each site, by using the &lt;code&gt;vcov()&lt;/code&gt; function. Afterwords, we build a block diagonal matrix using the four variance-covariance matrices as the building blocks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the vcov matrices
Omega &amp;lt;- lapply(model.1step, vcov)
Omega &amp;lt;- Matrix::bdiag(Omega)
round(Omega[1:8, 1:8], 3)
## 8 x 8 sparse Matrix of class &amp;quot;dsCMatrix&amp;quot;
##                                                     
## [1,] 0.520 0.061 0.037 0.034 0.033 0.035 0.034 0.033
## [2,] 0.061 0.520 0.061 0.037 0.033 0.034 0.033 0.033
## [3,] 0.037 0.061 0.520 0.061 0.033 0.033 0.034 0.033
## [4,] 0.034 0.037 0.061 0.520 0.033 0.033 0.033 0.033
## [5,] 0.033 0.033 0.033 0.033 0.520 0.035 0.034 0.061
## [6,] 0.035 0.034 0.033 0.033 0.035 0.520 0.061 0.034
## [7,] 0.034 0.033 0.034 0.033 0.034 0.061 0.520 0.033
## [8,] 0.033 0.033 0.033 0.033 0.061 0.034 0.033 0.520&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can proceed to the second stage, which can be performed by using the &lt;code&gt;rma.mv()&lt;/code&gt; function in the &lt;code&gt;metafor&lt;/code&gt; package, as shown in the box below. We see that we inject the variance-covariance matrix coming from the first stage into the second. That’s why this is a meta-analytic technique: we are behaving as if we had obtained the data from the first stage from literature!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Second stage (fully efficient)
mod.meta &amp;lt;- metafor::rma.mv(emmean, Omega, 
                            mods = ~ genotype - 1,
                            random = ~ 1|site/genotype,
                    data = sitmeans, method=&amp;quot;REML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this fit we get the remaining variance components (for the ‘sites’ and for the ‘sites x genotypes’ interaction) and the genotypic means, which correspond to those obtained from one-step analysis, apart from small differences relating to the optimisation method (see also Tables 1 and 2 in Damesa et al., 2017). That’s why Damesa and co-authors talk about &lt;strong&gt;fully efficient two-stage analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance components
mod.meta$sigma2
## [1] 10.4538773  0.1271925
 r
head(mod.meta$beta)
##               [,1]
## genotype1 5.134780
## genotype2 5.509773
## genotype3 5.147052
## genotype4 4.593256
## genotype5 4.844761
## genotype6 4.691955&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A possible approximation to this fully-efficient method is also shown in Damesa et al. (2017) and consists of approximating the variance-covariance matrix of genotypic means (‘Omega’) by using a vector of weights, which can be obtained by taking the diagonal elements of the inverse of the ‘Omega’ matrix. To achieve these, we can use the R coding in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;siij &amp;lt;- diag(solve(Omega))
mod.meta2 &amp;lt;- metafor::rma.mv(emmean, 1/siij,
                            mods = ~ genotype - 1,
                            random = ~ 1|site/genotype,
                    data = sitmeans, method=&amp;quot;REML&amp;quot;) 
mod.meta2$sigma2
## [1] 10.422928  0.127908
 r
head(mod.meta2$beta)
##               [,1]
## genotype1 5.112614
## genotype2 5.431455
## genotype3 5.151905
## genotype4 4.583911
## genotype5 4.811698
## genotype6 4.704518&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we have fully reproduced the results relating to the Example 1 in the paper we used as the reference for this post. Hope this was useful.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Damesa, T.M., Möhring, J., Worku, M., Piepho, H.-P., 2017. One Step at a Time: Stage-Wise Analysis of a Series of Experiments. Agronomy Journal 109, 845. &lt;a href=&#34;https://doi.org/10.2134/agronj2016.07.0395&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2134/agronj2016.07.0395&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Lenth R (2022). Emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.7.5-090001, &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pinheiro JC, Bates DM (2000). Mixed-Effects Models in S and S-PLUS.Springer, New York. &lt;a href=&#34;doi:10.1007/b98882&#34; class=&#34;uri&#34;&gt;doi:10.1007/b98882&lt;/a&gt;. &lt;a href=&#34;https://doi.org/10.1007/b98882&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/b98882&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. Journal of Statistical Software, 36(3), 1-48. &lt;a href=&#34;https://doi.org/10.18637/jss.v036.i03&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v036.i03&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for the experimental design in linear/nonlinear regression analyses</title>
      <link>http://localhost:4503/2020/stat_nlmm_designconstraints/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4503/2020/stat_nlmm_designconstraints/</guid>
      <description>


&lt;p&gt;In this post, I am going to talk about an issue that is often overlooked by agronomists and biologists. The point is that field experiments are very often laid down in blocks, using split-plot designs, strip-plot designs or other types of designs with grouping factors (blocks, main-plots, sub-plots). We know that these grouping factors should be appropriately accounted for in data analyses: ‘analyze them as you have randomized them’ is a common saying attributed to Ronald Fisher. Indeed, observations in the same group are correlated, as they are more alike than observations in different groups. What happens if we neglect the grouping factors? We break the independence assumption and our inferences are invalid (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;To my experience, field scientists are totally aware of this issue when they deal with ANOVA-type models (e.g., see Jensen et al., 2018). However, a brief survey of literature shows that there is not the same awareness, when field scientists deal with linear/nonlinear regression models. Therefore, I decided to sit down and write this post, in the hope that it may be useful to obtain more reliable data analyses.&lt;/p&gt;
&lt;div id=&#34;an-example-with-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with linear regression&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the ‘yieldDensity.csv’ dataset, that is available on gitHub. It represents an experiment where sunflower was tested with increasing weed densities (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 10 blocks. a swift plot shows that yield is linearly related to weed density, which calls for linear regression analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(lattice)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/yieldDensityB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yield
## 1     1       0 29.90
## 2     2       0 34.23
## 3     3       0 37.12
## 4     4       0 26.37
## 5     5       0 34.48
## 6     6       0 33.70
 r
plot(yield ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We might be tempted to neglect the block effect and run a linear regression analysis of yield against density. This is clearly wrong (I am violating the independence assumption) and inefficient, as any block-to-block variability goes into the residual error term, which is, therefore, inflated.&lt;/p&gt;
&lt;p&gt;Some of my collegues would take the means for densities and use those to fit a linear regression model (two-steps analysis). By doing so, block-to-block variability is cancelled out and the analysis becomes more efficient. However, such a solution is not general, as it is not feasible, e.g., when we have unbalanced designs and heteroscedastic data. With the appropriate approach, sound analyses can also be made in two-steps (Damesa et al., 2017). From my point of view, it is reasonable to search for more general solutions to deal with one-step analyses.&lt;/p&gt;
&lt;p&gt;Based on our experience with traditional ANOVA models, we might think of taking the block effect as fixed and fit it as and additive term. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg &amp;lt;- lm(yield ~ block + density, data=dataset)
summary(mod.reg)
## 
## Call:
## lm(formula = yield ~ block + density, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6062 -0.8242 -0.3315  0.7505  4.6244 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 29.10462    0.57750  50.397  &amp;lt; 2e-16 ***
## block2       4.57750    0.74668   6.130 4.81e-08 ***
## block3       7.05875    0.74668   9.453 4.49e-14 ***
## block4      -3.98000    0.74668  -5.330 1.17e-06 ***
## block5       6.17625    0.74668   8.272 6.37e-12 ***
## block6       5.92750    0.74668   7.938 2.59e-11 ***
## block7       1.23750    0.74668   1.657  0.10199    
## block8       1.25500    0.74668   1.681  0.09733 .  
## block9       2.34875    0.74668   3.146  0.00245 ** 
## block10      2.25125    0.74668   3.015  0.00359 ** 
## density     -0.26744    0.00701 -38.149  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.493 on 69 degrees of freedom
## Multiple R-squared:  0.9635,	Adjusted R-squared:  0.9582 
## F-statistic: 181.9 on 10 and 69 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With regression, this solution is not convincing. Indeed, the above model assumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected. Is this a reasonable assumption? I vote no.&lt;/p&gt;
&lt;p&gt;Let’s check this by fitting a different regression model per block (ten different slopes + ten different intercepts):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg2 &amp;lt;- lm(yield ~ block/density + block, data=dataset)
anova(mod.reg, mod.reg2)
## Analysis of Variance Table
## 
## Model 1: yield ~ block + density
## Model 2: yield ~ block/density + block
##   Res.Df    RSS Df Sum of Sq      F  Pr(&amp;gt;F)  
## 1     69 153.88                              
## 2     60 115.75  9    38.135 2.1965 0.03465 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-level confirms that the block had a significant effect both on the intercept and on the slope. To describe such an effect we need 20 parameters in the model, which is not very parsimonious. And above all: which regression line do we use for predictions? Taking the block effect as fixed is clearly sub-optimal with regression models.&lt;/p&gt;
&lt;p&gt;The question is: can we fit a simpler and clearer model? The answer is: yes. Why don’t we take the block effect as random? This is perfectly reasonable. Let’s do it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = ~ density|block, data=dataset)
summary(modMix.1)
## Linear mixed-effects model fit by REML
##   Data: dataset 
##        AIC      BIC    logLik
##   340.9166 355.0569 -164.4583
## 
## Random effects:
##  Formula: ~density | block
##  Structure: General positive-definite, Log-Cholesky parametrization
##             StdDev     Corr  
## (Intercept) 3.16871858 (Intr)
## density     0.02255249 0.09  
## Residual    1.38891957       
## 
## Fixed effects:  yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0370844 69  30.65311       0
## density     -0.26744 0.0096629 69 -27.67704       0
##  Correlation: 
##         (Intr)
## density -0.078
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9923722 -0.5657555 -0.1997103  0.4961675  2.6699060 
## 
## Number of Observations: 80
## Number of Groups: 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above fit shows that the random effects (slope and intercept) are sligthly correlated (r = 0.091). We might like to try a simpler model, where random effects are independent. To do so, we need to consider that the above model is equivalent to the following model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s just two different ways to code the very same model. However, this latter coding, based on a ‘pdMat’ structure, can be easily modified to remove the correlation. Indeed, ‘pdSymm’ specifies a totally unstructured variance-covariance matrix for random effects and it can be replaced by ‘pdDiag’, which specifies a diagonal matrix, where covariances (off-diagonal terms) are constrained to 0. The coding is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.2 &amp;lt;- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)
summary(modMix.2)
## Linear mixed-effects model fit by REML
##   Data: dataset 
##       AIC      BIC   logLik
##   338.952 350.7355 -164.476
## 
## Random effects:
##  Formula: ~density | block
##  Structure: Diagonal
##         (Intercept)    density Residual
## StdDev:    3.198263 0.02293222 1.387148
## 
## Fixed effects:  yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0460272 69  30.39105       0
## density     -0.26744 0.0097463 69 -27.44019       0
##  Correlation: 
##         (Intr)
## density -0.139
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9991173 -0.5451478 -0.1970267  0.4925090  2.6700386 
## 
## Number of Observations: 80
## Number of Groups: 10
 r
anova(modMix.1, modMix.2)
##          Model df      AIC      BIC    logLik   Test    L.Ratio p-value
## modMix.1     1  6 340.9166 355.0569 -164.4583                          
## modMix.2     2  5 338.9520 350.7355 -164.4760 1 vs 2 0.03535079  0.8509&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model could be further simplified. For example, the code below shows how we could fit models with either random intercept or random slope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Model with only random intercept
modMix.3 &amp;lt;- lme(yield ~ density, random = list(block = ~1), data=dataset)

#Alternative
#random = ~ 1|block

#Model with only random slope
modMix.4 &amp;lt;- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)

#Alternative
#random = ~density - 1 | block&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with nonlinear regression&lt;/h1&gt;
&lt;p&gt;The problem may become trickier if we have a nonlinear relationship. Let’s have a look at another similar dataset (‘YieldLossB.csv’), that is also available on gitHub. It represents another experiment where sunflower was grown with the same increasing densities of another weed (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 8 blocks. In this case, the yield loss was recorded and analysed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/YieldLossB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yieldLoss
## 1     1       0     1.532
## 2     2       0    -0.661
## 3     3       0    -0.986
## 4     4       0    -0.697
## 5     5       0    -2.264
## 6     6       0    -1.623
 r
plot(yieldLoss ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A swift plot shows that the relationship between density and yield loss is not linear. Literature references (Cousens, 1985) show that this could be modelled by using a rectangular hyperbola:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[YL = \frac{i \, D}{1 + \frac{i \, D}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(YL\)&lt;/span&gt; is the yield loss, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is weed density, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the slope at the origin of axes and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum asymptotic yield loss. This function, together with self-starters, is available in the ‘NLS.YL()’ function in the ‘aomisc’ package, which is the accompanying package for this blog. If you do not have this package, please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this link&lt;/a&gt; to download it.&lt;/p&gt;
&lt;p&gt;The problem is the very same as above: the block effect may produce random fluctuations for both model parameters. The only difference is that we need to use the ‘nlme()’ function instead of ‘lme()’. With nonlinear mixed models, I strongly suggest you use a ‘groupedData’ object, which permits to avoid several problems. The second line below shows how to turn a data frame into a ‘groupedData’ object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
datasetG &amp;lt;- groupedData(yieldLoss ~ 1|block, dataset)
nlin.mix &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
						fixed = list(i ~ 1, A ~ 1),
            random = i + A ~ 1|block)
summary(nlin.mix)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   474.8228 491.5478 -231.4114
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite, Log-Cholesky parametrization
##          StdDev    Corr 
## i        0.1112841 i    
## A        4.0444816 0.195
## Residual 1.4142267      
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23238 0.0382246 104 32.24035       0
## A 68.52305 1.9449780 104 35.23076       0
##  Correlation: 
##   i     
## A -0.408
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4416777 -0.7049390 -0.1805691  0.3385459  2.8788990 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly to linear mixed models, the above coding implies correlated random effects (r = 0.194). Alternatively, the above model can be coded by using a ’pdMat construct, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix2 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
						      fixed = list(i ~ 1, A ~ 1),
                  random = pdSymm(list(i ~ 1, A ~ 1)))
summary(nlin.mix2)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   474.8225 491.5474 -231.4112
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite
##          StdDev    Corr 
## i        0.1112837 i    
## A        4.0468443 0.194
## Residual 1.4141993      
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23242  0.038225 104 32.24114       0
## A 68.52050  1.945186 104 35.22568       0
##  Correlation: 
##   i     
## A -0.409
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4413852 -0.7049358 -0.1805300  0.3385252  2.8787256 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can try to simplify the model, for example by excluding the correlation between random effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix3 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
						      fixed = list(i ~ 1, A ~ 1),
                  random = pdDiag(list(i ~ 1, A ~ 1)))
summary(nlin.mix3)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   472.9076 486.8451 -231.4538
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: Diagonal
##                 i        A Residual
## StdDev: 0.1172791 4.389173 1.408963
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23243 0.0393514 104 31.31852       0
## A 68.57655 1.9905549 104 34.45097       0
##  Correlation: 
##   i     
## A -0.459
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.3577292 -0.6849962 -0.1785860  0.3255925  2.8592764 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a little fantasy, we can easily code several alternative models to represent alternative hypotheses about the observed data. Obviously, the very same method can be used (and SHOULD be used) to account for other grouping factors, such as main-plots in split-plot designs or plots in repeated measure designs.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Cousens, R., 1985. A simple model relating yield loss to weed density. Annals of Applied Biology 107, 239–252. &lt;a href=&#34;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jensen, S.M., Schaarschmidt, F., Onofri, A., Ritz, C., 2018. Experimental design matters for statistical analysis: how to handle blocking: Experimental design matters for statistical analysis. Pest Management Science 74, 523–534. &lt;a href=&#34;https://doi.org/10.1002/ps.4773&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1002/ps.4773&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Testing for interactions in nonlinear regression</title>
      <link>http://localhost:4503/2019/stat_nlmm_interaction/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4503/2019/stat_nlmm_interaction/</guid>
      <description>


&lt;p&gt;Factorial experiments are very common in agriculture and they are usually laid down to test for the significance of interactions between experimental factors. For example, genotype assessments may be performed at two different nitrogen fertilisation levels (e.g. high and low) to understand whether the ranking of genotypes depends on nutrient availability. For those of you who are not very much into agriculture, I will only say that such an assessment is relevant, because we need to know whether we can recommend the same genotypes, e.g., both in conventional agriculture (high nitrogen availability) and in organic agriculture (relatively lower nitrogen availability).&lt;/p&gt;
&lt;p&gt;Let’s consider an experiment where we have tested three genotypes (let’s call them A, B and C, for brevity), at two nitrogen rates (‘high’ an ‘low’) in a complete block factorial design, with four replicates. Biomass subsamples were taken from each of the 24 plots in eight different times (Days After Sowing: DAS), to evaluate the growth of biomass over time.&lt;/p&gt;
&lt;p&gt;The dataset is available on gitHub and the following code loads it and transforms the ‘Block’ variable into a factor. For this post, we will use several packages, including ‘aomisc’, the accompanying package for this blog. Please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page for downloading and installing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(lattice)
library(nlme)
library(aomisc)
dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/growthNGEN.csv&amp;quot;,
  header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
dataset$N &amp;lt;- factor(dataset$N)
dataset$GEN &amp;lt;- factor(dataset$GEN)
head(dataset)
##   Id DAS Block Plot GEN   N  Yield
## 1  1   0     1    1   A Low  2.786
## 2  2  15     1    1   A Low  5.871
## 3  3  30     1    1   A Low 13.265
## 4  4  45     1    1   A Low 16.926
## 5  5  60     1    1   A Low 22.812
## 6  6  75     1    1   A Low 25.346&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is composed by the following variables:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;‘Id’: a numerical code for observations&lt;/li&gt;
&lt;li&gt;‘DAS’: i.e., Days After Sowing. It is the moment when the sample was collected&lt;/li&gt;
&lt;li&gt;‘Block’, ‘Plot’, ‘GEN’ and ‘N’ represent respectively the block, plot, genotype and nitrogen level for each observation&lt;/li&gt;
&lt;li&gt;‘Yield’ represents the harvested biomass.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It may be useful to take a look at the observed growth data, as displayed on the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the growth is sygmoidal (presumably logistic) and that the variance of observations increases over time, i.e. the variance is proportional to the expected response.&lt;/p&gt;
&lt;p&gt;The question is: how do we analyse this data? Let’s build a model in a sequential fashion.&lt;/p&gt;
&lt;div id=&#34;the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The model&lt;/h1&gt;
&lt;p&gt;We could empirically assume that the relationship between biomass and time is logistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijkl} = \frac{d_{ijkl}}{1 + exp\left[b \left( X_{ijkl} - e_{ijkl}\right)\right]} + \varepsilon_{ijkl}\quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the observed biomass yield at time &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, for the i-th genotype, j-th nitrogen level, k-th block and l-th plot, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum asymptotic biomass level when time goes to infinity, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at inflection point, while &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the time when the biomass yield is equal to &lt;span class=&#34;math inline&#34;&gt;\(d/2\)&lt;/span&gt;. We are mostly interested in the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;: the first one describes the yield potential of a genotype, while the second one gives a measure of the speed of growth.&lt;/p&gt;
&lt;p&gt;There are repeated measures in each plot and, therefore, model parameters may show some variability, depending on the genotype, nitrogen level, block and plot. In particular, it may be acceptable to assume that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is pretty constant and independent on the above factors, while &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; may change according to the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left\{ {\begin{array}{*{20}{c}}
d_{ijkl} = \mu_d + g_{di} + N_{dj} + gN_{dij} + \theta_{dk} + \zeta_{dl}\\
e_{ijkl} = \mu_e + q_{ei} + N_{ej} + gN_{eij} + \theta_{ek} + \zeta_{el}
\end{array}} \right. \quad \quad (2) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for each parameter, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the fixed effect of the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed effect of j-th nitrogen level, &lt;span class=&#34;math inline&#34;&gt;\(gN\)&lt;/span&gt; is the fixed interaction effect, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the random effect of blocks, while &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; is the random effect of plots within blocks. These two equations are totally equivalent to those commonly used for linear mixed models, in the case of a two-factor factorial block design, wherein &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; would be the residual error term. Indeed, in principle, we could also think about a two-steps fitting procedure, where we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;fit the logistic model to the data for each plot and obtain estimates for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;use these estimates to fit a linear mixed model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will not pursue this two-steps technique here and we will concentrate on one-step fitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-wrong-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A wrong method&lt;/h1&gt;
&lt;p&gt;If the observations were independent (i.e. no blocks and no repeated measures), this model could be fit by using conventional nonlinear regression. My preference goes to the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2015).&lt;/p&gt;
&lt;p&gt;The coding is reported below: ‘Yield’ is a function of (&lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt;) DAS, by way of a three-parameters logistic function (‘fct = L.3()’). Different curves have to be fitted for different combinations of genotype and nitrogen levels (‘curveid = N:GEN’), although these curves should be partly based on common parameter values (‘pmodels = …). The ’pmodels’ argument requires a few additional comments. It must be a vector with as many element as there are parameters in the model (three, in this case: &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;). Each element represents a linear function of variables and refers to the parameters in alphabetical order, i.e. the first element refers to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the second refers to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the third to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not dependent on any variable (‘~ 1’) and thus a constant value is fitted across curves; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; depend on a fully factorial combination of genotype and nitrogen level (~ N*GEN = ~N + GEN + N:GEN). Finally, we used the argument ‘bcVal = 0.5’ to specify that we intend to use a Transform-Both-Sides technique, where a logarithmic transformation is performed for both sides of the equations. This is necessary to account for heteroscedasticity, but it does not affect the scale of parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive1 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = GEN:N,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N*GEN), bcVal = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model may be useful for other circumstances (no blocks and no repeated measures), but it is wrong in our example. Indeed, observations are clustered within blocks and plots; by neglecting this, we violate the assumption of independence of model residuals. A swift plot of residuals against fitted values shows that there are no problems with heteroscedasticity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Considering the above, we have to use a different model, here, although I will show that this naive fit may turn out useful.&lt;/p&gt;
&lt;div id=&#34;nonlinear-mixed-model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nonlinear mixed model fitting&lt;/h2&gt;
&lt;p&gt;In order to account for the clustering of observations, we switch to a Nonlinear Mixed-Effect model (NLME). A good choice is the ‘nlme()’ function in the ‘nlme’ package (Pinheiro and Bates, 2000), although the syntax may be cumbersome, at times. I will try to help, listing and commenting the most important arguments for this function. We need to specify the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A deterministic function. In this case, we use the ‘nlsL.3()’ function in the ‘aomisc’ package, which provides a logistic growth model with the same parameterisation as the ‘L.3()’ function in the ‘drm’ package.&lt;/li&gt;
&lt;li&gt;Linear functions for model parameters. The ‘fixed’ argument in the ‘nlme’ function is very similar to the ‘pmodels’ argument in the ‘drm’ function above, in the sense that it requires a list, wherein each element is a linear function of variables. The only difference is that the parameter name needs to be specified on the left side of the function.&lt;/li&gt;
&lt;li&gt;Random effects for model parameters. These are specified by using the ‘random’ argument. In this case, the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; are expected to show random variability from block to block and from plot to plot, within a block. For the sake of simplicity, as the parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not affected by the genotype and nitrogen level, we also expect that it does not show any random variability across blocks and plots.&lt;/li&gt;
&lt;li&gt;Starting values for model parameters. Self starting routines are not used by ‘nlme()’ and thus we need to specify a named vector, holding the initial values of model parameters. In this case, I decided to use the output from the ‘naive’ nonlinear regression above, which, therefore, turns out useful.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The transformation of both sides of the equation is made explicitely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
modnlme1 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), 
                 data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N*GEN),
                    start = coef(modNaive1), 
                 control = list(msMaxIter = 400))
summary(modnlme1)$tTable
##                     Value   Std.Error  DF    t-value      p-value
## b              0.05652837 0.002157629 228 26.1993017 1.044744e-70
## d.(Intercept) 33.91575345 1.222612873 228 27.7403863 5.073988e-75
## d.NLow        -3.18382833 1.592502937 228 -1.9992606 4.676721e-02
## d.GENB        18.90014652 1.864712716 228 10.1356881 3.602004e-20
## d.GENC        -1.15934036 1.686405289 228 -0.6874625 4.924900e-01
## d.NLow:GENB   -5.99216674 2.455759122 228 -2.4400466 1.544863e-02
## d.NLow:GENC   -5.82864839 2.217534817 228 -2.6284360 9.160827e-03
## e.(Intercept) 55.20071251 2.320784619 228 23.7853664 1.087154e-63
## e.NLow        -9.06217439 3.127165895 228 -2.8978873 4.123120e-03
## e.GENB        -4.47038044 2.761533560 228 -1.6188036 1.068720e-01
## e.GENC         4.00746113 3.084383636 228  1.2992745 1.951620e-01
## e.NLow:GENB   -4.71367433 4.055953643 228 -1.1621618 2.463848e-01
## e.NLow:GENC    2.23951083 4.609547667 228  0.4858418 6.275459e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4503/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plots above, we see that the overall fit is good. Fixed effects and variance components for random effects are obtained as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(modnlme1)$tTable
##                     Value   Std.Error  DF    t-value      p-value
## b              0.05652837 0.002157629 228 26.1993017 1.044744e-70
## d.(Intercept) 33.91575345 1.222612873 228 27.7403863 5.073988e-75
## d.NLow        -3.18382833 1.592502937 228 -1.9992606 4.676721e-02
## d.GENB        18.90014652 1.864712716 228 10.1356881 3.602004e-20
## d.GENC        -1.15934036 1.686405289 228 -0.6874625 4.924900e-01
## d.NLow:GENB   -5.99216674 2.455759122 228 -2.4400466 1.544863e-02
## d.NLow:GENC   -5.82864839 2.217534817 228 -2.6284360 9.160827e-03
## e.(Intercept) 55.20071251 2.320784619 228 23.7853664 1.087154e-63
## e.NLow        -9.06217439 3.127165895 228 -2.8978873 4.123120e-03
## e.GENB        -4.47038044 2.761533560 228 -1.6188036 1.068720e-01
## e.GENC         4.00746113 3.084383636 228  1.2992745 1.951620e-01
## e.NLow:GENB   -4.71367433 4.055953643 228 -1.1621618 2.463848e-01
## e.NLow:GENC    2.23951083 4.609547667 228  0.4858418 6.275459e-01
VarCorr(modnlme1)
##               Variance                     StdDev       Corr  
## Block =       pdLogChol(list(d ~ 1,e ~ 1))                    
## d.(Intercept) 4.134390e-08                 2.033320e-04 d.(In)
## e.(Intercept) 1.851943e-08                 1.360861e-04 -0.001
## Plot =        pdLogChol(list(d ~ 1,e ~ 1))                    
## d.(Intercept) 3.396536e-09                 5.827981e-05 d.(In)
## e.(Intercept) 1.023108e-09                 3.198605e-05 0     
## Residual      1.750623e-01                 4.184044e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s go back to our initial aim: testing the significance of the ‘genotype x nitrogen’ interaction. Indeed, we have two available tests: on for the parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and one for the parameter &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. First of all, we code two ‘reduced’ models, where the genotype and nitrogen effects are purely addictive. To do so, we change the specification of the fixed effects from ’~ N*GEN’ to ‘~ N + GEN’. Also in this case, we use a ‘naive’ nonlinear regression fit to get starting values for model parameters, to be used in the following NLME model fitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive2 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), 
                 data = dataset,
                 curveid = N:GEN,
                 pmodels = c( ~ 1,  ~ N + GEN,  ~ N * GEN), 
                 bcVal = 0.5)

modnlme2 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), 
                 data = dataset,
                 random = d + e ~ 1|Block/Plot,
                 fixed = list(b ~ 1, d ~ N + GEN, e ~ N*GEN),
                 start = coef(modNaive2), 
                 control = list(msMaxIter = 200))

modNaive3 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N + GEN), bcVal = 0.5)

modnlme3 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)),
                 data = dataset,
                 random = d + e ~ 1|Block/Plot,
                 fixed = list(b ~ 1, d ~ N*GEN, e ~ N + GEN),
                 start = coef(modNaive3), control = list(msMaxIter = 200))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s consider the first reduced model ‘modnlme2’. In this model, the ‘genotype x nitrogen’ interaction has been removed for the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter. We can compare this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme2)
##          Model df      AIC      BIC    logLik   Test L.Ratio p-value
## modnlme1     1 20 329.1496 400.6686 -144.5748                       
## modnlme2     2 18 334.2187 398.5857 -149.1093 1 vs 2 9.06907  0.0107&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This test is significant, but the AIC value is very close for the two models. Considering that a LRT in mixed models is usually rather liberal, it should be possible to conclude that the ‘genotype x nitrogen’ interaction is not significant and, therefore, the ranking of genotypes in terms of yield potential, as measured by the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter should be independent on nitrogen level.&lt;/p&gt;
&lt;p&gt;Let’s now consider the second reduced model ‘modnlme3’. In this second model, the ‘genotype x nitrogen’ interaction has been removed for the ‘e’ parameter. We can compare also this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme3)
##          Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## modnlme1     1 20 329.1496 400.6686 -144.5748                        
## modnlme3     2 18 328.2446 392.6117 -146.1223 1 vs 2 3.095066  0.2128&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this second test, the lack of significance for the ‘genotype x nitrogen’ interaction seems to be less questionable than in the first one.&lt;/p&gt;
&lt;p&gt;I would like to conclude by drawing your attention to the ‘medrm’ function in the ‘medrc’ package, which can also be used to fit this type of nonlinear mixed-effects models.&lt;/p&gt;
&lt;p&gt;Happy coding with R!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J.C., Gerhard, D., 2015. Dose-Response Analysis Using R. PLOS ONE 10, e0146021. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0146021&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0146021&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>