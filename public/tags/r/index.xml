<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on The broken bridge between biologists and statisticians</title>
    <link>https://www.statforbiology.com/tags/r/</link>
    <description>Recent content in R on The broken bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2018, @AndreaOnofri</copyright>
    <lastBuildDate>Wed, 29 Mar 2023 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://www.statforbiology.com/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Subsampling in field experiments</title>
      <link>https://www.statforbiology.com/2023/stat_lmm_subsampling_tkw/</link>
      <pubDate>Wed, 29 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_lmm_subsampling_tkw/</guid>
      <description>


&lt;p&gt;Subsampling is very common in field experiments in agriculture. It happens when we collect several random samples from each plot and we submit them to some sort of measurement process. Some examples? Let’s look at the following (very incomplete) list.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We collect the whole grain yield in each plot, select four subsamples and measure, in each subsample, the oil content or some other relevant chemical property.&lt;/li&gt;
&lt;li&gt;We collect, from each plot, four plants and measure their height.&lt;/li&gt;
&lt;li&gt;We collect a representative soil samples from each plot and analyse the residual content in some xenobiotic substance, by making four repeated measurements.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The presence of sub-samples is a good thing, as long as true-replicates are also available. Indeed, the precision of our experiment increases, although the process of data analysis must be run properly. We should bear in mind that, if the experimental design is, e.g., a Randomised Complete Block with four replicates, for all the above examples we end up with sixteen values for each experimental treatment (four replicates and four sub-samples per replicate). It must be clear that the four sub-samples are not like true-replicates, because the experimental treatments were not independently allocated to each of them. The four subsamples are sub-replicates (or pseudo-replicates) and we should account for this when we analyse our data.&lt;/p&gt;
&lt;div id=&#34;a-motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A motivating example&lt;/h1&gt;
&lt;p&gt;Let’s consider a dataset from an experiment where we had 30 genotypes in three blocks and recorded the Weight of Thousand Kernels (TKW) in three subsamples per plot, which were labelled by using the ‘Sample’ variable. In the box below, we load the data and all the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(tidyverse)
library(nlme)
library(emmeans)

filePath &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/TKW.csv&amp;quot;
TKW &amp;lt;- read.csv(filePath)
TKW &amp;lt;- TKW %&amp;gt;% 
  mutate(across(1:4, .fns = factor))
head(TKW)
##   Plot Block  Genotype Sample  TKW
## 1    1     1 Meridiano      1 28.6
## 2    2     1     Solex      1 33.3
## 3    3     1  Liberdur      1 22.3
## 4    4     1  Virgilio      1 28.1
## 5    5     1   PR22D40      1 26.7
## 6    6     1    Ciccio      1 34.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-wrong-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The wrong analysis&lt;/h1&gt;
&lt;p&gt;A naive analysis would be to perform an ANOVA on all data, without making any distinction between true-replicates and sub-replicates. Let’s do this by using the code shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Naive analysis
mod &amp;lt;- lm(TKW ~ Block + Genotype, data=TKW)
anova(mod)
## Analysis of Variance Table
## 
## Response: TKW
##            Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block       2  110.3  55.169   7.510 0.0006875 ***
## Genotype   29 7224.7 249.129  33.913 &amp;lt; 2.2e-16 ***
## Residuals 238 1748.4   7.346                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
summary(mod)$sigma
## [1] 2.710373
pairwise &amp;lt;- as.data.frame(pairs(emmeans(mod, ~Genotype)))
sum(pairwise$p.value &amp;lt; 0.05)
## [1] 225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the Root Mean Square Error is 2.71, the F test for the genotypes is highly significant and there are 225 significant pairwise comparisons among the 30 genotypes.&lt;/p&gt;
&lt;p&gt;As we said, this is simple, but it is also &lt;strong&gt;terribly wrong&lt;/strong&gt;. By putting true-replicates and pseudo-replicates on an equal footing, we have forgotten that the 270 observations are grouped by plot and that the observations in the same plot are more alike than the observations in different plots, because they share the same ‘origin’. We say that the observations in each plot are correlated and, therefore, the basic assumption of independence of residuals is violated. Our analysis is invalid and our manuscript can be very likely rejected.&lt;/p&gt;
&lt;p&gt;But, why are the editors so critical when we mistake pseudo-replicates for true-replicates? We’ll see this in a few minutes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-correct-way-to-go&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The correct way to go&lt;/h1&gt;
&lt;p&gt;A fully correct model for our dataset is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{ijks} = \mu + \alpha_i + \beta_j + \gamma_{k} + \varepsilon_{s}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the thousand kernel weight for the i&lt;sup&gt;th&lt;/sup&gt; genotype, j&lt;sup&gt;th&lt;/sup&gt; block, k&lt;sup&gt;th&lt;/sup&gt; plot and s&lt;sup&gt;th&lt;/sup&gt; sub-sample, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the effect of the i&lt;sup&gt;th&lt;/sup&gt; genotype, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the effect of the j&lt;sup&gt;th&lt;/sup&gt; block, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of the the k&lt;sup&gt;th&lt;/sup&gt; plot and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the effect of the s&lt;sup&gt;th&lt;/sup&gt; subsample. The presence of the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; element accounts for the plot as a grouping factor and restores the independence of model residuals.&lt;/p&gt;
&lt;p&gt;Obviously, the difference between plots (for a given genotype and block) must be regarded as a random effect, as well as the difference between subplots, within each plot. Indeed. we have two random effects and, therefore, this is a mixed model. These two random effects are assumed to be normal, independent from each other, with mean equal to 0 and variances respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;. (BTW: I am regarding the block as fixed! You may not agree, but this does not change what I am going to say later…).&lt;/p&gt;
&lt;p&gt;We can fit this mixed model by using the &lt;code&gt;lme()&lt;/code&gt; function in the &lt;code&gt;nlme&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Mixed model fit
mod.mix &amp;lt;- lme(TKW ~ Block + Genotype, 
               random=~1|Plot, data=TKW)
sigma2.p &amp;lt;- as.numeric( VarCorr(mod.mix) )[1]
sigma2.e &amp;lt;- as.numeric( VarCorr(mod.mix) )[2]
sigma2.p
## [1] 8.891984
sigma2.e
## [1] 0.8452593
anova(mod.mix)
##             numDF denDF   F-value p-value
## (Intercept)     1   180 11563.536  &amp;lt;.0001
## Block           2    58     2.005  0.1439
## Genotype       29    58     9.052  &amp;lt;.0001
pairwise &amp;lt;- as.data.frame(pairs(emmeans(mod.mix, ~Genotype)))
sum(pairwise$p.value &amp;lt; 0.05)
## [1] 91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already see several differences with respect to the previous fit:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the ‘naive’ model, we have only one estimate for &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, that is 7.346 with 238 DF. In this case the correct term to test for the genotype effect is &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_p\)&lt;/span&gt;, that is equal to 8.892 with 58 DF. Clearly, the naive analysis strongly overestimates the number of DF: the observations taken in the same plot are correlated and they do not contribute full information.&lt;/li&gt;
&lt;li&gt;The RMSE for the mixed model is equal to 2.98 and it is higher than that from the ‘naive’ fit. The variability within plot is much smaller.&lt;/li&gt;
&lt;li&gt;The number of significant pairwise comparisons between genotypes has dropped to 91.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may wonder why &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_p\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt; is the correct error term for the genotypes. Because it is the only correct estimate of the random difference between the true-replicates that were independently submitted to the same treatment.&lt;/p&gt;
&lt;p&gt;We can now understand why the editors reject our manuscript if we do not analyse the data properly: we may strongly overestimate the precision of our experiment and, consequently, commit a lot of false positive errors!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simpler-alternative&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A simpler alternative&lt;/h1&gt;
&lt;p&gt;We strongly recommend the previous method of data analysis. But, should you like to avoid the use of mixed models by all means (tell me,… why?), whenever the number of sub-samples is the same for all plots, we can also reach correct results by proceeding in two-steps. In the first step, we calculate the means of sub-samples for each plot and, in the second step, we submit the plot means to ANOVA, by considering the genotype and the block as fixed factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First step, to get an estimate of the within plot error
mod1step &amp;lt;- lm(TKW ~ Plot, data=TKW)
summary(mod1step)$sigma^2
## [1] 0.8452593
# Calculate the means per plot
avgTKW &amp;lt;- as.data.frame(emmeans(mod1step, ~Plot))
head(avgTKW)
##   Plot   emmean        SE  df lower.CL upper.CL
## 1    1 28.56667 0.5308042 180 27.51927 29.61407
## 2    2 34.86667 0.5308042 180 33.81927 35.91407
## 3    3 23.66667 0.5308042 180 22.61927 24.71407
## 4    4 29.40000 0.5308042 180 28.35260 30.44740
## 5    5 27.30000 0.5308042 180 26.25260 28.34740
## 6    6 34.96667 0.5308042 180 33.91927 36.01407
# Recover infos on experimental design
Genotype &amp;lt;- plyr::ddply(TKW, ~Plot,                   
        function(dataSet){as.character(dataSet[[&amp;quot;Genotype&amp;quot;]][1])} )[,2]
Block &amp;lt;-  plyr::ddply(TKW, ~Plot, 
                  function(dataSet){as.factor(dataSet[[&amp;quot;Block&amp;quot;]][1])} )[,2]
dataset2step &amp;lt;- data.frame(Block, Genotype, TKW = avgTKW$emmean)
dataset2step$Block &amp;lt;- factor(dataset2step$Block)

#Second step
mod2step &amp;lt;- lm(TKW ~ Genotype + Block, data = dataset2step)
anova(mod2step)
## Analysis of Variance Table
## 
## Response: TKW
##           Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Genotype  29 2408.24  83.043  9.0522 9.943e-13 ***
## Block      2   36.78  18.390  2.0046    0.1439    
## Residuals 58  532.08   9.174                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
pairwise &amp;lt;- as.data.frame(pairs(emmeans(mod2step, ~Genotype)))
sum(pairwise$p.value &amp;lt; 0.05)
## [1] 91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are totally the same as with a mixed model fit, although all Mean Squares in ANOVA are fractions of those obtained by mixed model analyses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please, remember that this simple solution is only feasible when we have the same number of subsamples per each plot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting threshold models to seed germination data</title>
      <link>https://www.statforbiology.com/2023/stat_drcte_12-htt2step/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_drcte_12-htt2step/</guid>
      <description>


&lt;p&gt;In previous posts we have shown that we can use time-to-event curves to describe the germination pattern of a seed population (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_2-methods/&#34;&gt;see here&lt;/a&gt;). We have also shown that these curves can be modified to include the effects of external/internal factors/covariates, such as the genotype, the species, the humidity content and temperature in the substrate (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;see here&lt;/a&gt; and &lt;a href=&#34;https://www.statforbiology.com/2023/stat_drcte_10-examplehtte/&#34;&gt;here&lt;/a&gt;). These modified time-to-event curves can be fitted in ‘one-step’, i.e., we start from the germination data with the appropriate shape (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_3-reshapingdata/&#34;&gt;see here&lt;/a&gt;), fit the model and retrieve the estimates of model parameters ( &lt;a href=&#34;https://www.statforbiology.com/2023/stat_drcte_10-examplehtte/&#34;&gt;go to here for an example&lt;/a&gt; ).&lt;/p&gt;
&lt;p&gt;In some cases, we may be interested in following a different approach, that is accomplished in two-steps. Let’s consider an example where we have performed germination assays at 10 different temperatures: instead of building a time-to-event model that contains the temperature as an external covariate, we could:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;independently fit a different time-to-event curve to the germination data at each temperature, so that we have ten independent time-to-event curves;&lt;/li&gt;
&lt;li&gt;derive from each curve a summary statistic of interest, such as the germination rate for the 50-th percentile (GR50: &lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_9-quantiles/&#34;&gt;see here&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fit, e.g., a thermal-time model to those derived statistics (second step of data analyses).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fitting models in two-steps has always been a common practice in agriculture/biology (see, e.g., multienvironment genotype experiments): it is usually simpler, quicker and requires lower computing power than one-step fitting. The drawback is that some infomation may be lost between the two steps, and, for this reason, one-step and two-steps model fitting do not necessarily lead to the same results. But, we’ll make this point in a future post.&lt;/p&gt;
&lt;div id=&#34;definition-of-threshold-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Definition of threshold models&lt;/h1&gt;
&lt;p&gt;Threshold models are used to describe a relationship where the response variable changes abruptly, following a small change in the predictor. A typical threshold model looks like that in the Figure below, where we see three threshold levels:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X1 = 5.5\)&lt;/span&gt;: at this threshold, the response changes abruptly from ‘flat’ to linearly increasing;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X2 = 23.1\)&lt;/span&gt;: at this threshold, the response changes abruptly from linearly increasing to linearly decreasing;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X3 = 37.2\)&lt;/span&gt;: at this threshold, the response changes abruptly from linearly decreasing to ‘flat’.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may recognise a ‘broken-stick’ pattern, although threshold models can also be curvilinear, as we will see later.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_12-HTT2step_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I have already considered threshold models in a previous post (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_seedgermination_htt2step/&#34;&gt;see here&lt;/a&gt;) and I have already mentioned that thermal-time, hydro-time and hydro-thermal-time models for seed germination can also be cast as threshold models; if we consider, e.g., the Germination Rate (GR) as the response variable and the environmental temperature as the predictor, the relationship could be very close to that represented in Figure 1 and the three thresholds would, respectively, be the &lt;em&gt;base temperature&lt;/em&gt; (T_b_), the &lt;em&gt;optimal temperature&lt;/em&gt; (T_o_) and the &lt;em&gt;ceiling temperature&lt;/em&gt; (T_c_). On the other hand, if we consider the effect of soil humidity on GR, we should expect a response pattern with only one threshold, i.e. the &lt;em&gt;base water potential&lt;/em&gt; level (e.g. the first half of the figure above, up to the maximum response level).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-possibly-incomplete-list-of-threshold-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A (possibly incomplete) list of threshold models&lt;/h1&gt;
&lt;p&gt;I have made a review of literature, searching for all threshold models that have been used so far in seed germination studies. For all those models, I have built the related R functions, together with self-starting routines, which can be used for nonlinear regression fitting with the &lt;code&gt;drm()&lt;/code&gt; function in the &lt;code&gt;drc&lt;/code&gt; package (Ritz et al., 2019). The availability of self-starting routines will free you from the hassle of having to provide initial guesses for model parameters. All these R functions are available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package (Onofri et al., 2018) and their names, with links to the relevant parts of the appendix to this post are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#grpsilin---grt.gh&#34;&gt;GRPsi.Lin() - GRT.GH()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grpsipol---grpsipol2&#34;&gt;GRPsi.Pol() - GRPsi.Pol2()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pmaxpsi1-and-pmaxt1&#34;&gt;PmaxPsi1() - PmaxT1()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grt.bs&#34;&gt;GRT.BS()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grt.rf&#34;&gt;GRT.RF()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grt.m&#34;&gt;GRT.M()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grt.ex&#34;&gt;GRT.Ex()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grt.yl&#34;&gt;GRT.YL()&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It may be helpful to look at the shapes of the above models in the Figure below, while the equations are motivated in the appendix, at the end of this post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_12-HTT2step_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s look at a few examples of two-steps fitting. But, before working through this, you will need to install and load the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, by using the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# installing drcSeedGerm package, if not yet available
# library(devtools)
# install_github(&amp;quot;onofriandreapg/drcSeedGerm&amp;quot;)

# loading package
library(drcSeedGerm)
library(tidyverse)
library(lmtest)
library(sandwich)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1&lt;/h1&gt;
&lt;p&gt;This dataset describes the germination of rapeseed (cv. Excalibur) at different water potential levels in the substrate. It has been already used for fitting a hydro-time model in one step (&lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_6-ht1step/&#34;&gt;see here&lt;/a&gt;); in this present post, we try a different line of attack.&lt;/p&gt;
&lt;p&gt;First of all, we remove all dishes with water potential levels higher than -0.7 MPa, because the germinations were too quick to obtain a reliable estimate of the whole time-to-event curve. Next, we independently (‘separate = T’) fit a time to event model to the data observed in each dish. Lately, for each time to event curve, we retrieve the maximum proportion of germinated seeds (Pmax, i.e. the ‘d’ parameter of the time-to-event curve) and the germination rates for the 10&lt;sup&gt;th&lt;/sup&gt;, 30&lt;sup&gt;th&lt;/sup&gt; and 50&lt;sup&gt;th&lt;/sup&gt; percentile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First-step of data analyses
data(rape2G)
rape2G &amp;lt;- rape2G %&amp;gt;% 
  dplyr::filter(Psi &amp;lt;=-0.7 &amp;amp; CV == &amp;quot;Excalibur&amp;quot;) %&amp;gt;% 
  mutate(Dish2 = paste(Dish, Psi, sep = &amp;quot;:&amp;quot;))

# model fit
mod.first &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, 
                   data = rape2G,
                   fct = LL.3(), curveid = Dish2, 
                   separate = T)

# Retrieve maximum proportion of germinated seeds
Pmax &amp;lt;- coef(mod.first)[substr(names(coef(mod.first)), 1, 1) == &amp;quot;d&amp;quot;]
PmaxList &amp;lt;- tibble(Pmax =  Pmax) %&amp;gt;% 
  mutate(temp = names(Pmax), .before = Pmax) %&amp;gt;% 
  separate(col = &amp;quot;temp&amp;quot;, into = c(&amp;quot;n&amp;quot;, &amp;quot;Dish&amp;quot;, &amp;quot;Psi&amp;quot;),
           sep = &amp;quot;:&amp;quot;) %&amp;gt;% 
  mutate(Psi = as.numeric(Psi)) %&amp;gt;% 
  select(-1)
head(PmaxList)
## # A tibble: 6 × 3
##   Dish    Psi  Pmax
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 64     -0.7 0.901
## 2 65     -0.7 0.986
## 3 66     -0.7 0.922
## 4 67     -0.8 0.914
## 5 68     -0.8 0.887
## 6 69     -0.8 0.835
# Retrieve the GR values
GR &amp;lt;- quantile(mod.first, rate = T, probs = c(0.1, 0.3, 0.5))
GRlist &amp;lt;- tibble(temp = row.names(GR), GR, row.names = NULL) %&amp;gt;% 
  separate(col = &amp;quot;temp&amp;quot;, into = c(&amp;quot;Dish&amp;quot;, &amp;quot;Psi&amp;quot;, &amp;quot;g&amp;quot;),
           sep = &amp;quot;:&amp;quot;) %&amp;gt;% 
  mutate(Psi = as.numeric(Psi)) %&amp;gt;% 
  remove_rownames()
head(GRlist)
## # A tibble: 6 × 5
##   Dish    Psi g     Estimate     SE
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 64     -0.7 10%      0.581 0.0895
## 2 64     -0.7 30%      0.416 0.0397
## 3 64     -0.7 50%      0.333 0.0239
## 4 65     -0.7 10%      0.718 0.149 
## 5 65     -0.7 30%      0.468 0.0595
## 6 65     -0.7 50%      0.357 0.0330&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we are ready to move on to the second step of data analysis. Relating to the Pmax value, we can see that this values stays constant and equal to 0 up to -1 MPa and increases steadily afterwords. We can model this behaviour by using the &lt;code&gt;PmaxPsi1()&lt;/code&gt; function, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modPmax &amp;lt;- drm(Pmax ~ Psi, data = PmaxList,
               fct = PmaxPsi1())
plot(modPmax, log = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_12-HTT2step_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeftest(modPmax, vcov. = sandwich)
## 
## t test of coefficients:
## 
##                     Estimate Std. Error   t value  Pr(&amp;gt;|t|)    
## G:(Intercept)      1.0737381  0.0746708   14.3796 2.608e-11 ***
## Psib:(Intercept)  -1.0053956  0.0016495 -609.5109 &amp;lt; 2.2e-16 ***
## sigma:(Intercept)  0.1367469  0.0272873    5.0114 9.058e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regarding the germination percentiles, a look at the data shows that, for all percentiles, germination rates stay constant up to -1 MPa and, afterwords, they increase linearly. We can model this behaviour by using the &lt;code&gt;GRPsiLin()&lt;/code&gt; equation and, following Bradford (2002), we code a common base water potential level for the different germination percentiles. The code is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modGR &amp;lt;- drm(Estimate ~ Psi, data = GRlist,
               fct = GRPsi.Lin(), curveid = g,
             pmodels = list(~1, ~g - 1))
plot(modGR, log = &amp;quot;&amp;quot;,
             legendPos = c(-1.3, 0.7))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_12-HTT2step_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeftest(modGR, vcov. = sandwich)
## 
## t test of coefficients:
## 
##                   Estimate Std. Error  t value  Pr(&amp;gt;|t|)    
## Psib:(Intercept) -0.989504   0.031937 -30.9832 &amp;lt; 2.2e-16 ***
## thetaH:g10%       0.446550   0.081366   5.4881 8.977e-07 ***
## thetaH:g30%       0.649827   0.099213   6.5498 1.557e-08 ***
## thetaH:g50%       0.852974   0.134058   6.3627 3.209e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Usually, we are interested in the base osmotic potential level (&lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt;) that is given in the output of the &lt;code&gt;coeftest()&lt;/code&gt; method. We used &lt;code&gt;coeftest()&lt;/code&gt; in the &lt;code&gt;lmtest&lt;/code&gt; package for reasons that will be clearer later on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2&lt;/h1&gt;
&lt;p&gt;This second dataset was obtained from a germination assays with barley, where three replicates of 50 seeds were placed in Petri dishes and assayed at 9 constant temperature levels (1, 3, 7, 10, 15, 20, 25, 30, 35, 40 °C). Germinated seeds were counted and removed daily for 10 days. We have already presented this analysis in a previous paper (Onofri et al., 2018), although in this post we use a different (and updated) coding.&lt;/p&gt;
&lt;p&gt;Also in this second example, the first step of data analysis is based on loading the data and fitting a separate time-to-event curve to the data at each temperature level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(barley)
barley &amp;lt;- barley %&amp;gt;% 
  mutate(TempF = factor(Temp))

mod1 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, fct=W2.3(),
      curveid = TempF,
      data = barley,
      separate = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we retrieve the germination rates for the 10^th, 30&lt;sup&gt;th&lt;/sup&gt; and 50&lt;sup&gt;th&lt;/sup&gt; percentile; for analogy with the published paper, we restrict the percentiles to the germinated fraction, althoug it might be better to avoid such a restriction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GR &amp;lt;- quantile(mod1, rate = T, restricted = T, 
               probs = c(0.1, 0.3, 0.5),
                display = F)
GRlist &amp;lt;- tibble(temp = row.names(GR), GR, row.names = NULL) %&amp;gt;% 
  separate(col = &amp;quot;temp&amp;quot;, into = c(&amp;quot;Temp&amp;quot;, &amp;quot;g&amp;quot;),
           sep = &amp;quot;:&amp;quot;) %&amp;gt;% 
  mutate(Temp = as.numeric(Temp)) %&amp;gt;% 
  remove_rownames()
head(GRlist)
## # A tibble: 6 × 4
##    Temp g     Estimate      SE
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     1 10%     0.0982 0.00328
## 2     1 30%     0.0777 0.00177
## 3     1 50%     0.0682 0.00125
## 4     3 10%     0.124  0.00307
## 5     3 30%     0.105  0.00173
## 6     3 50%     0.0962 0.00126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The behaviour of germination rates against temperature can be described, e.g., by using the threshold model proposed by Masin et al. (2017), that is implemented in the R function &lt;code&gt;GRT.Ex()&lt;/code&gt;, as shown in the box below. Preliminary trials show that the three percentiles share the same ‘k’ parameter and base temperature level, which we can request by using the ‘pmodels’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modGR &amp;lt;- drm(Estimate ~ Temp, data = GRlist,
               fct = GRT.Ex(),
             curveid = g, pmodels = list(~1, ~1, ~g - 1, ~g -1))
coeftest(modGR, vcov. = sandwich)
## 
## t test of coefficients:
## 
##                 Estimate Std. Error   t value  Pr(&amp;gt;|t|)    
## k:(Intercept)   0.876849   0.138731    6.3205 4.577e-06 ***
## Tb:(Intercept) -0.553318   0.774412   -0.7145    0.4836    
## Tc:g10%        35.200942   0.021761 1617.5850 &amp;lt; 2.2e-16 ***
## Tc:g30%        33.235421   0.321386  103.4129 &amp;lt; 2.2e-16 ***
## Tc:g50%        32.198489   0.225161  143.0021 &amp;lt; 2.2e-16 ***
## ThetaT:g10%    25.454497   1.788195   14.2347 1.378e-11 ***
## ThetaT:g30%    40.458899   2.001762   20.2116 2.630e-14 ***
## ThetaT:g50%    51.413920   2.415663   21.2836 1.025e-14 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
plot(modGR, log = &amp;quot;&amp;quot;, type = &amp;quot;all&amp;quot;, xlim = c(0, 40),
     ylim = c(0, 1.3),
     ylab = &amp;quot;GR&amp;quot;, xlab = &amp;quot;Temperature (°C)&amp;quot;,
     legendPos = c(12, 0.9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_12-HTT2step_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warning-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Warning message!&lt;/h1&gt;
&lt;p&gt;When we collect data about the response of germination rates to temperature and use them to parameterise nonlinear regression models by using nonlinear least squares, the basic assumption of homoscedasticity is rarely tenable. &lt;strong&gt;We should not forget this!&lt;/strong&gt;. In the above examples I used a robust variance-covariance sandwich estimator (Zeileis, 2006; see the use of the &lt;code&gt;coeftest()&lt;/code&gt; method, instead of the &lt;code&gt;summary()&lt;/code&gt; method), although other techniques can be successfully used to deal with this problem.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Alvarado, V., Bradford, K.J., 2002. A hydrothermal time model explains the cardinal temperatures for seed germination. Plant, Cell and Environment 25, 1061–1069.&lt;/li&gt;
&lt;li&gt;Baty, F., Ritz, C., Charles, S., Brutsche, M., Flandrois, J. P., Delignette-Muller, M.-L., 2014. A toolbox for nonlinear regression in R: the package nlstools. Journal of Statistical Software, 65, 5, 1-21.&lt;/li&gt;
&lt;li&gt;Bradford, K.J., 2002. Applications of hydrothermal time to quantifying and modelling seed germination and dormancy. Weed Science 50, 248–260.&lt;/li&gt;
&lt;li&gt;Catara, S., Cristaudo, A., Gualtieri, A., Galesi, R., Impelluso, C., Onofri, A., 2016. Threshold temperatures for seed germination in nine species of Verbascum (Scrophulariaceae). Seed Science Research 26, 30–46.&lt;/li&gt;
&lt;li&gt;Garcia-Huidobro, J., Monteith, J.L., Squire, R., 1982. Time, temperature and germination of pearl millet (&lt;em&gt;Pennisetum typhoides&lt;/em&gt; S &amp;amp; H.). 1. Constant temperatures. Journal of Experimental Botany 33, 288–296.&lt;/li&gt;
&lt;li&gt;Kropff, M.J., van Laar, H.H. 1993. Modelling crop-weed interactions. CAB International, Books.&lt;/li&gt;
&lt;li&gt;Masin, R., Onofri, A., Gasparini, V., Zanin, G., 2017. Can alternating temperatures be used to estimate base temperature for seed germination? Weed Research 57, 390–398.&lt;/li&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C., 2019. Dose-Response Analysis Using R. CRC Press&lt;/li&gt;
&lt;li&gt;Rowse, H.R., Finch-Savage, W.E., 2003. Hydrothermal threshold models can describe the germination response of carrot (Daucus carota) and onion (Allium cepa) seed populations across both sub- and supra-optimal temperatures. New Phytologist 158, 101–108.&lt;/li&gt;
&lt;li&gt;Zeileis, A., 2006. Object-oriented computation of sandwich estimators. Journal of Statistical Software, 16, 9, 1-16.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix-description-of-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix: description of models&lt;/h1&gt;
&lt;div id=&#34;grpsilin---grt.gh&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRPsiLin() - GRT.GH()&lt;/h2&gt;
&lt;p&gt;The equation behind &lt;code&gt;GRPsiLin()&lt;/code&gt; has been used to describe the effect of environmental humidity (&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, in MPa) on germination rate (Bradford, 2002):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max\left[\Psi, \Psi_b\right] - \Psi_b}{\theta_H}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameter &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; is the &lt;em&gt;base water potential&lt;/em&gt; (in MPa), representing the minimum level of humidity in the substrate to trigger the germination process. The other parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt; (in MPa day or MPa hour) is the hydro-time constant.&lt;/p&gt;
&lt;p&gt;A totally similar equation (with different parameter names) has been used by Garcia-Huidobro et al (1982), to describe the effect of sub-optimal temperatures (T in °C) on the germination rate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max \left[T, T_b\right] - T_b}{\theta_T}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(T_b\)&lt;/span&gt; is the base temperature and &lt;span class=&#34;math inline&#34;&gt;\(\theta_T\)&lt;/span&gt; is the thermal time (in °C d). This second model is available &lt;code&gt;GRT.GH()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample code (not executed)
# Tlev &amp;lt;- c(2, 5, 10, 15, 20, 25)
# GR &amp;lt;- c(0, 0, 0.21, 0.49, 0.68, 0.86)
# modGH &amp;lt;- drm(GR ~ Tlev, fct = GRT.GH())
# library(sandwich); library(lmtest)
# coeftest(modGH, vcov = sandwich)
# plot(modGH, log=&amp;quot;&amp;quot;, xlim = c(0, 25), legendPos = c(5, 1.2),
#      xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;grpsipol---grpsipol2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRPsiPol() - GRPsiPol2()&lt;/h2&gt;
&lt;p&gt;In my experience, I have found that the relationship between GR and water potential in the substrate may, sometimes, be curvilinear. For these situations, I have successfully used the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max\left[\Psi,\Psi_b\right]^2 - \Psi^2_b}{\theta_H}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\left(\max\left[\Psi, \Psi_b\right] - \Psi_b \right)^2}{ \theta_H }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both models can be fitted in R, by using the two functions &lt;code&gt;GRPsi.Pol()&lt;/code&gt; and &lt;code&gt;GRPsi.Pol2()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample code (not executed)
# Psi &amp;lt;- c(-2, -1.5, -1.2, -1, -0.8, -0.6, -0.4, -0.25,
#          -0.12, -0.06, -0.03, 0)
# GR &amp;lt;- c(0, 0, 0, 0, 0.0585, 0.094, 0.1231, 0.1351,
#         0.1418, 0.1453, 0.1458, 0.1459)
# Psi2 &amp;lt;- c(-0.5, -0.6, -0.7, -0.8, -0.9, -1, -1.1, -1.2,
#           -1.5)
# GR2 &amp;lt;- c(1.4018, 1.0071, 0.5614, 0.3546, 0.2293, 0, 0,
#          0, 0)
# 
# 
# modHT &amp;lt;- drm(GR ~ Psi, fct = GRPsiPol())
# modHT2 &amp;lt;- drm(GR2 ~ Psi2, fct = GRPsiPol2())
# par(mfrow = c(1,2))
# plot(modHT, log=&amp;quot;&amp;quot;, legendPos = c(-1.5, 0.15), 
#      ylim = c(0, 0.20), xlab = &amp;quot;Water potential (MPa)&amp;quot;)
# plot(modHT2, log=&amp;quot;&amp;quot;, legendPos=c(-1.3, 1), 
#      xlab = &amp;quot;Water potential (MPa)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;pmaxpsi1-and-pmaxt1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PmaxPsi1() and PmaxT1()&lt;/h2&gt;
&lt;p&gt;All the previous models tend to go up to infinity when the predictor value (temperature or water potential) goes to infinity. In some instances, we may need an asymptotic model, to describe the response of the maximum proportion of germinated seeds to soil humidity (Onofri et al., 2018).&lt;/p&gt;
&lt;p&gt;In practice, we could use a shifted exponential model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \pi = G \, \left[ 1 - exp \left( \frac{ \max\left[\Psi, \Psi_b\right] - \Psi_b }{\sigma} \right) \right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is the proportion of germinated seeds, &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the fraction of non-germinable seeds (e.g., dormant seeds) and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; describes how quickly the population of seeds responds to increased humidity in the substrate. This model can be fitted by using the R function the self-starters &lt;code&gt;PmaxPsi1()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we reverse the sign of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; in the previous equation, we obtain a decreasing trend, which might be useful to describe the effect of super-optimal temperatures on the proportion of germinated seeds, going down to 0 at the ceiling temperature threshold. Also this model is available in R, under the name &lt;code&gt;PmaxT1()&lt;/code&gt;. &lt;code&gt;PmaxPsi1()&lt;/code&gt; &lt;code&gt;PmaxT1()&lt;/code&gt; are two equivalent R functions, apart from the name of model parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample code (not executed)
# par(mfrow = c(1,2))
# # Pmax vs Psi
# Psi &amp;lt;- seq(-2.2, 0, by = 0.2)
# Pmax &amp;lt;- c(0, 0, 0.076, 0.413, 0.514, 0.643, 0.712,
#           0.832, 0.865, 0.849, 0.89, 0.90)
# mod &amp;lt;- drm(Pmax ~ Psi, fct = PmaxPsi1())
# plot(mod, log = &amp;quot;&amp;quot;, xlab = &amp;quot;Water potential (MPa)&amp;quot;, 
#      ylab = &amp;quot;Proportion of germinating seeds&amp;quot;)
# 
# # Pmax vs temperature
# Tval &amp;lt;- c(0, 2.5, 5, 7.5, 10, 12.5, 15, 17.5,
#           20, 22.5, 25, 27.5, 30, 32.5, 35)
# Pmax2 &amp;lt;- c(0.79, 0.81, 0.807, 0.776, 0.83,
#            0.73, 0.744, 0.73, 0.828, 0.818,
#            0.805, 0.706, 0.41, 0.002, 0)
# mod2 &amp;lt;- drm(Pmax2 ~ Tval, fct = PmaxT1())
# plot(mod2, log = &amp;quot;&amp;quot;, xlab = &amp;quot;Temperature (°C)&amp;quot;, 
#      ylab = &amp;quot;Proportion of germinating seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;grt.bs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRT.BS()&lt;/h2&gt;
&lt;p&gt;A broken-stick trend, as the one depicted in the first Figure above was used by Alvarado and Bradford (2002) to model the effect of temperature on the germination rate. Their equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max \left[\min \left[T,T_o \right], Tb \right] - T_b}{\theta_{T}} \, \left\{ 1 - k \left( \max \left[ T,T_o \right] - T_o \right) \right\}\]&lt;/span&gt;
The right factor is only meaningful when it is positive, that happens when &lt;span class=&#34;math inline&#34;&gt;\(T &amp;lt; T_c\)&lt;/span&gt;, i.e. when the environmental temperature is lower than the ceiling temperature. On this basis, the ceiling temperature is equal to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T_c = \frac{1}{k} + T_o\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above equation can be easily fitted with the &lt;code&gt;GRT.BS()&lt;/code&gt; function in the ‘drcSeedGerm’ package. We have also implemented the reparameterised equation, where the parameter ‘k’ is replaced with &lt;span class=&#34;math inline&#34;&gt;\(1/(T_c - T_b)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\min \left[T,T_o \right] - T_b}{\theta_{T}} \, \left\{ 1 - \frac {\min \left[\max \left[ T,T_o \right], T_c \right] - T_o}{T_c - T_o} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This reparameterised equation is available as &lt;code&gt;GRT.BSb()&lt;/code&gt;; it is handy, because the ceiling temperature is included as a parameter, but its fitting properties are not as good as those of the previous equation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample code
# Tval &amp;lt;- c(2, 5, 10, 15, 20, 25, 30, 35, 40)
# GR &amp;lt;- c(0, 0, 0.209, 0.435, 0.759, 0.821, 0.417, 0.145, 0)
# 
# modBS &amp;lt;- drm(GR ~ Tval, fct = GRT.BS())
# plot(modBS, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
#      legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)
# coeftest(modBS, vcov = sandwich)
# 
# # Reparameterised (self-starter is less accurate)
# modBS &amp;lt;- drm(GR ~ Tval, fct = GRT.BSb())
# plot(modBS, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
#      legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)
# coeftest(modBS, vcov = sandwich)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;grt.rf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRT.RF()&lt;/h2&gt;
&lt;p&gt;Broken-stick trends may not be reasonable for biological processes, which might be better described by curvilinear equations. Rowse and Finch-Savage (2003) proposed another equation with two components: the first one depicts a linear increase of the GR value with temperature, which is off-set by the second component, starting from &lt;span class=&#34;math inline&#34;&gt;\(T = T_d\)&lt;/span&gt;, which is close to (but not coincident with) &lt;span class=&#34;math inline&#34;&gt;\(T_o\)&lt;/span&gt;. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{ \max \left( T, T_b \right) - T_b}{\theta_{T}} \left\{ 1 - k \left[ \max \left(T,T_d\right) - T_d \right] \right\}\]&lt;/span&gt;
The optimal temperature can be derived as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T_o = \frac{1 + kT_b + kT_d}{2k}\]&lt;/span&gt;
while the ceiling temperature is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T_c = \frac{1}{k} + T_d\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this equation, you will find the &lt;code&gt;GRT.RF()&lt;/code&gt; self-starter in the ‘drcSeedGerm’ package. We also provide the self-starter &lt;code&gt;GRT.RFb()&lt;/code&gt;, where the parameters ‘k’ is replaced by $ 1/(T_c - T_d)$:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{ \max \left[ T, T_b \right] - T_b}{\theta_{T}} \left\{ 1 - \frac{\left[ \max \left(T,T_d\right) - T_d \right]}{T_c - T_d}  \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This reparameterised equation contains the ceiling temperature as a parameter, but its fitting properties are as good as those pf the previous equation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;grt.m&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRT.M()&lt;/h2&gt;
&lt;p&gt;According to Mesgaran et al (2017), the negative and positive effects of temperature coexist for all temperatures above &lt;span class=&#34;math inline&#34;&gt;\(T_b\)&lt;/span&gt;. Their proposed equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{ \max \left( T, T_b \right) - T_b}{\theta_{T}} \left\{ 1 - k \left[ \min \left(T,T_c\right) - T_b \right] \right\}\]&lt;/span&gt;
This equation is only defined from base to ceiling temperature, while it is 0 elsewhere. The ceiling temperature is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T_c = \frac{1}{k} + T_b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is also easy to see that the GR value is a second-order polynomial function of &lt;span class=&#34;math inline&#34;&gt;\(T - T_b\)&lt;/span&gt; and, therefore, the curve is symmetric around the optimal temperature value, which can be derived as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_o = \frac{T_c - T_b}{2} + T_b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this model, the self-starting function in &lt;code&gt;drcSeedGerm&lt;/code&gt; is &lt;code&gt;GRT.M()&lt;/code&gt;. The model can also be reparameterised to include the ceiling temperature as an explicit parameter:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max \left[ T, T_b \right] - T_b}{\theta_{T}} \left[ 1 - \frac{\min \left[ T, T_c \right] - T_b}{T_c - T_b}  \right]\]&lt;/span&gt;
This reparameterised model is available as &lt;code&gt;GRT.Mb()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sample code (not executed)
# Tval &amp;lt;- c(2, 5, 10, 15, 20, 25, 30, 35, 40)
# GR &amp;lt;- c(0, 0, 0.209, 0.435, 0.759, 0.821, 0.417, 0.145, 0)
# modM &amp;lt;- drm(GR ~ Tval, fct = GRT.Mb())
# plot(modM, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
#      legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)
# coeftest(modM, vcov. = sandwich)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;grt.ex&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRT.Ex()&lt;/h2&gt;
&lt;p&gt;All the equations above use a product, wherein the first term represents the accumulation of thermal time and the second term may be seen as a switch-off term that is 1 either when &lt;span class=&#34;math inline&#34;&gt;\(T &amp;lt; T_o\)&lt;/span&gt; (Alvarado-Bradford equation) or &lt;span class=&#34;math inline&#34;&gt;\(T &amp;lt; T_d\)&lt;/span&gt; (Rowse-Fintch-Savage equation) or &lt;span class=&#34;math inline&#34;&gt;\(T = T_b\)&lt;/span&gt; (Mesgaran equation) and decreases progressively as temperature increases. In all the above equations, the switch-off term is linear, although we can use other types of switch-off terms, to obtain more flexible models. One possibility is to use an exponential switch-off term, as in the equation below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ GR = \frac{\max \left[T, T_b \right] - T_b}{\theta_T} \left\{ \frac{1 - \exp \left[ k (\min \left[T, T_c \right] - T_c) \right]}{1 - \exp \left[ k (T_b - T_c) \right]}  \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the switch-off parameter: the lower the value, the higher the negative effect of temperature at super-optimal levels. The response of GR to temperature is highly asymmetric with a slow increase below &lt;span class=&#34;math inline&#34;&gt;\(T_o\)&lt;/span&gt; and a steep drop afterwards.&lt;/p&gt;
&lt;p&gt;I have successfully used this model in Catara et al (2016) and Masin et al (2017). The self-starting function in R is &lt;code&gt;GRT.Ex()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Sample code
# Tval &amp;lt;- c(2, 5, 10, 15, 20, 25, 30, 35, 40)
# GR &amp;lt;- c(0, 0, 0.209, 0.435, 0.759, 0.821, 0.917, 0.445, 0)
# 
# modExb &amp;lt;- drm(GR ~ Tval, fct = GRT.Ex())
# summary(modExb)
# plot(modExb, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
#     legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;grt.yl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GRT.YL()&lt;/h2&gt;
&lt;p&gt;Another switch-off function can be derived from the simple yield loss function devised by Kropff and van Laar (1993). It is very flexible, as it may depict different types of relationships between temperature and base water potential, according to the value taken by the parameter &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR(g, T) = \frac{\max \left[T, T_b\right] - T_b}{\theta_T} \left( 1 - \frac{q \frac{\min \left[T, T_c\right] -T_b}{T_c- T_b} }{1 + (q-1) \frac{T-T_b}{T_c- T_b}}  \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In R, this model can be fitted by using the self-starter &lt;code&gt;GRT.YL()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample code
# Tval &amp;lt;- c(2, 5, 10, 15, 20, 25, 30, 35, 40)
# GR &amp;lt;- c(0, 0, 0.209, 0.435, 0.759, 0.821, 0.917, 0.445, 0)
# modYL &amp;lt;- drm(GR ~ Tval, fct = GRT.YL())
# plot(modYL, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
#      legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;#a-possibly-incomplete-list-of-threshold-models&#34;&gt;Go up&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting thermal-time-models to seed germination data</title>
      <link>https://www.statforbiology.com/2023/stat_drcte_11-examplette/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_drcte_11-examplette/</guid>
      <description>


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;a-motivating-examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A motivating examples&lt;/h1&gt;
&lt;p&gt;In recent times, we wanted to model the effect of temperature on seed germination for &lt;em&gt;Hordeum vulgare&lt;/em&gt; and we made an assay with three replicated Petri dishes (50 seeds each) at 9 constant temperature levels (1, 3, 7, 10, 15, 20, 25, 30, 35, 40 °C). Germinated seeds were counted and removed daily for 10 days. This unpublished dataset is available as &lt;code&gt;barley&lt;/code&gt; in the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, which needs to be installed from github (see below), together with the &lt;code&gt;drcte&lt;/code&gt; package for time-to-event model fitting. The following code loads the necessary packages, loads the datasets and shows the first six lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Installing packages (only at first instance)
# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)
# install_github(&amp;quot;OnofriAndreaPG/drcte&amp;quot;)
library(drcSeedGerm)
library(tidyverse)
data(barley)
head(barley)
##   Dish Temp timeBef timeAf nSeeds nCum propCum
## 1    1    1       0      1      0    0       0
## 2    1    1       1      2      0    0       0
## 3    1    1       2      3      0    0       0
## 4    1    1       3      4      0    0       0
## 5    1    1       4      5      0    0       0
## 6    1    1       5      6      0    0       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this dataset, ‘timeAf’ represents the moment when germinated seeds were counted, while ’timeBef’ represents the previous inspection time (or the beginning of the assay). The column ’nSeeds’ is the number of seeds that germinated during each time interval between ‘timeBef’ and ‘timeAf. For the analyses, we will also make use of the ’Temp’ (temperature) and ‘Dish’ (Petri dish) columns.&lt;/p&gt;
&lt;p&gt;This dataset was already analysed in Onofri et al. (2018; Example 3 and 4) by using the same methodology, but a different R coding (see the Supplemental Material in that paper), that is now outdated. This post will show you the updated coding.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-first-thermal-time-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A first thermal-time model&lt;/h1&gt;
&lt;p&gt;As we have shown in previous posts (see &lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_6-ht1step/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.statforbiology.com/2023/stat_drcte_10-examplehtte/&#34;&gt;here&lt;/a&gt;), the effect of environmental covariates (temperature, in this case) can be simply included by independently coding a different time-to-event curve to each level of that covariate. In other words, considering that a parametric time-to-event curve is defined as a cumulative probability function (&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;), with three parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \Phi \left( b, d, e \right)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(P(t)\)&lt;/span&gt; is the cumulative probability of germination at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum germinated proportion, the most obvious extension is to allow for different &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; values for each of the i&lt;sup&gt;th&lt;/sup&gt; temperature levels (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, T) = \Phi \left( b_i, d_i, e_i \right)\]&lt;/span&gt;
Although the above approach is possible, we will not purse it here, as it is clearly sub-optimal; indeed, such an approach wrongly considers the temperature as a factor, i.e. a set of nominal classes with no intrinsic orderings and distances. Obviosly, we should better regard the temperature as a continuous variable, by coding a time-to-event model where the three parameters are expressed as continuous functions of &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, T) = \Phi \left[ b(T), d(T), e(T) \right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the ‘phalaris’ dataset (Onofri et al., 2018; example 3) we used a log-logistic cumulative distribution function, with the following sub-models:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \left\{ {\begin{array}{{l}}
P(t, T) = \frac{ d(T) }{1 + \exp \left\{ b \left[ \log(t) - \log( e(T) ) \right] \right\} } \\
d(T) = G \, \left[ 1 - \exp \left( - \frac{ T_c - T }{\sigma_{T_c}} \right) \right] \\
1/[e(T)] = GR_{50}(T) = \frac{T - T_b }{\theta_T} \left[1 - \frac{T - T_b}{T_c - T_b}\right]  \\
\end{array}} \right. \quad \quad (\textrm{TTEM})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Please, note that the shape parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; has been regarded as independent from temperature; the parameters are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;, that is the germinable fraction, accounting for the fact that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; may not reach 1, regardless of time and temperature;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T_c\)&lt;/span&gt;, that is the ceiling temperature for germination;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{T_c}\)&lt;/span&gt;, that represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(T_c\)&lt;/span&gt; within the seed lot;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T_b\)&lt;/span&gt;, that is the base temperature for germination;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_T\)&lt;/span&gt;, that is the thermal-time parameter;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, that is the scale parameters for the log-logistic distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can get more information from our original paper (Onofri et al., 2018). This thermal-time model was implemented in R as the &lt;code&gt;TTEM()&lt;/code&gt; function, and it is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package; we can fit it by using the &lt;code&gt;drmte()&lt;/code&gt; function in the &lt;code&gt;drcte&lt;/code&gt; package, with no need to provide starting values for model parameters, because a self-starting routine is available. The &lt;code&gt;summary()&lt;/code&gt; method can be used to retrieve the parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Thermal-Time-to-event model fit
modTTEM &amp;lt;- drmte( nSeeds ~ timeBef + timeAf + Temp, data=barley,
               fct=TTEM())
summary(modTTEM, units = Dish)
## 
## Model fitted: Thermal-time model with shifted exponential for Pmax and Mesgaran model for GR50
## 
## Robust estimation: Cluster robust sandwich SEs 
## 
## Parameter estimates:
## 
##                       Estimate Std. Error t value  Pr(&amp;gt;|t|)    
## G:(Intercept)         12.46391    2.40502  5.1825 2.772e-07 ***
## Tc:(Intercept)        82.66899   23.47042  3.5223  0.000452 ***
## sigmaTc:(Intercept) 1018.92708  322.67791  3.1577  0.001649 ** 
## Tb:(Intercept)        -1.89327    0.38741 -4.8870 1.236e-06 ***
## ThetaT:(Intercept)    43.41025    4.37185  9.9295 &amp;lt; 2.2e-16 ***
## b:(Intercept)          5.62759    0.69149  8.1383 1.520e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is always important not to neglect a graphical inspection of model fit. The &lt;code&gt;plot()&lt;/code&gt; method does not work with time-to-event curves with additional covariates (apart from time). However, we can retrieve the fitted data by using the &lt;code&gt;plotData()&lt;/code&gt; function and use those predictions within the &lt;code&gt;ggplot()&lt;/code&gt; function. The box below shows the appropriate coding; the red circles in the graphs represent the observed means, while the black lines are model predictions).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- plotData(modTTEM)

ggplot() +
  geom_point(data = tab$plotPoints, mapping = aes(x = timeAf, y = CDF),
            col = &amp;quot;red&amp;quot;) +
  geom_line(data = tab$plotFits, mapping = aes(x = timeAf, y = CDF)) +
  facet_wrap( ~ Temp) +
  scale_x_continuous(name = &amp;quot;Time (d)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_11-ExampleTTE_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The previous graph shows that the &lt;code&gt;TTEM()&lt;/code&gt; model, that we had successfully used with other datasets, failed to provide an adequate description of the germination time-course for barley. Therefore, we modified one of the sub-models, by adopting the approach highlighted in Rowse and Finch-Savage (2003), where:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[1/[e(T)] = GR_{50}(T) = \left\{ {\begin{array}{ll}
\frac{T - T_b}{\theta_T} &amp;amp; \textrm{if} \,\,\, T_b &amp;lt; T &amp;lt; T_d \\
\frac{T - T_b}{\theta_T} \left[ 1 - \frac{T - T_d}{T_c - T_d}  \right] &amp;amp; \textrm{if} \,\,\, T_d &amp;lt; T &amp;lt; T_c \\
0 &amp;amp; \textrm{if} \,\,\, T \leq T_b \,\,\, or \,\,\, T \geq T_c
\end{array}} \right. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, we also made a further improvement, by coding another model where also the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter was allowed to change with temperature, according to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma(T) = \frac{1}{b(T)} = \frac{1}{b_0} + s (T - T_b)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These two improved models were coded as the &lt;code&gt;TTERF()&lt;/code&gt; function (with the only change in the ‘e’ submodel), and as the &lt;code&gt;TTERFc()&lt;/code&gt; function (with a change in both the ‘b’ and ‘e’ submodels), which are available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package. The code below is used to fit both models and explore the resulting fits: the symbols show the observed means, the blue line represents the ‘TTERF’ fit and the red line represents the ‘TTERFc’ fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modTTERF &amp;lt;- drmte( nSeeds ~ timeBef + timeAf + Temp, data=barley,
               fct=TTERF())
modTTERFc &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Temp, data=barley,
               fct=TTERFc())
AIC(modTTEM, modTTERF, modTTERFc)
##            df      AIC
## modTTEM   804 6478.917
## modTTERF  803 5730.622
## modTTERFc 802 5602.532
tab2 &amp;lt;- plotData(modTTERF)
tab3 &amp;lt;- plotData(modTTERFc)

ggplot() +
  geom_point(data = tab$plotPoints, mapping = aes(x = timeAf, y = CDF),
            col = &amp;quot;red&amp;quot;) +
  geom_line(data = tab2$plotFits, mapping = aes(x = timeAf, y = CDF),
            col = &amp;quot;blue&amp;quot;) +
  geom_line(data = tab3$plotFits, mapping = aes(x = timeAf, y = CDF),
            col = &amp;quot;red&amp;quot;) +
  facet_wrap( ~ Temp) +
  scale_x_continuous(name = &amp;quot;Time (d)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_11-ExampleTTE_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that, with respect to the TTEM model, both the two improved models provide a better fit.&lt;/p&gt;
&lt;p&gt;It should be clear that this modelling approach is rather flexible, by appropriately changing one or more submodels, and it can fit the germination pattern of several species in several environmental conditions.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;li&gt;Rowse, H.R., Finch-Savage, W.E., 2003. Hydrothermal threshold models can describe the germination response of carrot (Daucus carota) and onion (Allium cepa) seed populations across both sub- and supra-optimal temperatures. New Phytologist 158, 101–108.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting a hydro-thermal-time-model to seed germination data</title>
      <link>https://www.statforbiology.com/2023/stat_drcte_10-examplehtte/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2023/stat_drcte_10-examplehtte/</guid>
      <description>


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;germination-assay&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Germination assay&lt;/h1&gt;
&lt;p&gt;This dataset was obtained from previously published work (Mesgaran et al., 2017) with &lt;em&gt;Hordeum spontaneum&lt;/em&gt; [C. Koch] Thell. The germination assay was conducted using four replicates of 20 seeds tested at six different water potential levels (0, −0.3, −0.6, −0.9, −1.2 and −1.5 MPa). Osmotic potentials were produced using variable amount of polyethylene glycol (PEG, molecular weight 8000) adjusted for the temperature level. Petri dishes were incubated at six constant temperature levels (8, 12, 16, 20, 24 and 28 °C), under a photoperiod of 12 h. Germinated seeds (radicle protrusion &amp;gt; 3 mm) were counted and removed daily for 20 days.&lt;/p&gt;
&lt;p&gt;This dataset is available as &lt;code&gt;hordeum&lt;/code&gt; in the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, which needs to be installed from github (see below), together the package &lt;code&gt;drcte&lt;/code&gt;, which is necessary to fit time-to-event models. The following code loads the necessary packages, loads the dataset &lt;code&gt;rape&lt;/code&gt; and shows the first six lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Installing packages (only at first instance)
# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)
# install_github(&amp;quot;OnofriAndreaPG/drcte&amp;quot;)
library(drcSeedGerm)
library(tidyverse)
data(hordeum)
head(hordeum)
##   temp water Dish timeBef timeAf nViable nSeeds nCum
## 1    8  -1.5    1       0     24      20      0    0
## 2    8  -1.5    1      24     48      20      0    0
## 3    8  -1.5    1      48     72      20      0    0
## 4    8  -1.5    1      72     96      20      0    0
## 5    8  -1.5    1      96    120      20      0    0
## 6    8  -1.5    1     120    144      20      1    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminary-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminary analyses&lt;/h1&gt;
&lt;p&gt;First of all, it is necessary to mention that &lt;strong&gt;this dataset was already analysed in Onofri et al. (2018; Example 2) by using the same methodology, although, in that paper, the R implementation was different (see Supplemental Material) and it is now outdated.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the above data frame, ‘timeAf’ represents the moment when germinated seeds were counted, while ’timeBef’ represents the previous inspection time (or the beginning of the assay). The column ’nSeeds’ is the number of seeds that germinated during the time interval between ‘timeBef’ and ‘timeAf. The ’nCum’ column contains the cumulative number of germinated seeds and it is not necessary for time-to-event model fitting, although we can use it for plotting purposes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hordeum &amp;lt;- hordeum %&amp;gt;% 
  mutate(propCum = nCum/nViable)

ggplot(data = hordeum, mapping = aes(timeAf, propCum)) +
  geom_point() +
  facet_grid(temp ~ water) +
  scale_x_continuous(name = &amp;quot;Time (d)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_10-ExampleHTTE_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the germination time-course is strongly affected by both temperature and water potential in the substrate and, consequently, our obvious interest is to model the effects of those environmental covariates. In our manuscript, we started from the idea that a parametric time-to-event curve is defined as a cumulative probability function (&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;), with three parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \Phi \left( b, d, e \right)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(P(t)\)&lt;/span&gt; is the cumulative probability of germination at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum germinated proportion. The most obvious extension is to allow for different &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; values for each of the i&lt;sup&gt;th&lt;/sup&gt; combinations of water potential (&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;) and temperature level (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi, T) = \Phi \left( b_i, d_i, e_i \right)\]&lt;/span&gt;
From the graph above, we see several ‘degenerated’ time-to-event curves, where no germinations occurred (e.g., see the graph at -1.5 MPa and 28°C). In order to avoid problems with those curves, we can use the &lt;code&gt;drmte()&lt;/code&gt; function and set the &lt;code&gt;separate = TRUE&lt;/code&gt; argument, so that the different curves are fitted independently of one another and the degenerated curves are recognised and skipped, without stopping the execution in R. In particular, where no time-course of events can be estimated, it is assumed that there is no progress to germination during the study-period and that the cumulative proportion of germinated seeds remains constant across that period. Consequently, the &lt;code&gt;drmte()&lt;/code&gt; function resorts to fitting a simpler model, where the only &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter is estimated (that is the maximum fraction of germinated seeds).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hordeum &amp;lt;- hordeum %&amp;gt;% 
  mutate(comb = factor( factor(water):factor(temp)))
mod1 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, data = hordeum,
             curveid = comb, fct = loglogistic(),
             separate = TRUE)
summary(mod1)
## 
## Model fitted: Separate fitting of several time-to-event curves
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##             Estimate Std. Error t-value   p-value    
## b:-1.5:8    5.577670   0.671086  8.3114 &amp;lt; 2.2e-16 ***
## d:-1.5:8    0.705845   0.051801 13.6260 &amp;lt; 2.2e-16 ***
## e:-1.5:8  203.532986   8.746552 23.2701 &amp;lt; 2.2e-16 ***
## b:-1.5:12   4.036639   0.633750  6.3694 1.897e-10 ***
## d:-1.5:12   0.566163   0.061406  9.2201 &amp;lt; 2.2e-16 ***
## e:-1.5:12 232.198040  17.347102 13.3854 &amp;lt; 2.2e-16 ***
## b:-1.5:16   4.130712   1.174395  3.5173 0.0004359 ***
## d:-1.5:16   0.226248   0.057013  3.9683 7.238e-05 ***
## e:-1.5:16 293.584384  41.294139  7.1096 1.164e-12 ***
## d:-1.5:20   0.000000   0.000000     NaN       NaN    
## d:-1.5:24   0.000000   0.000000     NaN       NaN    
## d:-1.5:28   0.000000   0.000000     NaN       NaN    
## b:-1.2:8    4.761454   0.531157  8.9643 &amp;lt; 2.2e-16 ***
## d:-1.2:8    0.803436   0.044972 17.8653 &amp;lt; 2.2e-16 ***
## e:-1.2:8  152.794391   7.123662 21.4489 &amp;lt; 2.2e-16 ***
## b:-1.2:12   4.165847   0.518344  8.0368 9.050e-16 ***
## d:-1.2:12   0.667099   0.053329 12.5092 &amp;lt; 2.2e-16 ***
## e:-1.2:12 145.571114   8.562685 17.0006 &amp;lt; 2.2e-16 ***
## b:-1.2:16   3.848053   0.556564  6.9140 4.713e-12 ***
## d:-1.2:16   0.536017   0.057474  9.3262 &amp;lt; 2.2e-16 ***
## e:-1.2:16 175.692653  13.009048 13.5054 &amp;lt; 2.2e-16 ***
## b:-1.2:20   4.679284   1.377885  3.3960 0.0006838 ***
## d:-1.2:20   0.191881   0.049814  3.8520 0.0001172 ***
## e:-1.2:20 291.173467  34.870683  8.3501 &amp;lt; 2.2e-16 ***
## d:-1.2:24   0.000000   0.000000     NaN       NaN    
## d:-1.2:28   0.000000   0.000000     NaN       NaN    
## b:-0.9:8    5.237479   0.550871  9.5076 &amp;lt; 2.2e-16 ***
## d:-0.9:8    0.887920   0.035345 25.1218 &amp;lt; 2.2e-16 ***
## e:-0.9:8  111.384281   4.427060 25.1599 &amp;lt; 2.2e-16 ***
## b:-0.9:12   4.368990   0.475050  9.1969 &amp;lt; 2.2e-16 ***
## d:-0.9:12   0.850876   0.039968 21.2889 &amp;lt; 2.2e-16 ***
## e:-0.9:12  99.501087   4.833892 20.5841 &amp;lt; 2.2e-16 ***
## b:-0.9:16   3.581796   0.400722  8.9383 &amp;lt; 2.2e-16 ***
## d:-0.9:16   0.816041   0.043892 18.5922 &amp;lt; 2.2e-16 ***
## e:-0.9:16 105.233306   6.461986 16.2850 &amp;lt; 2.2e-16 ***
## b:-0.9:20   3.536663   0.502112  7.0436 1.874e-12 ***
## d:-0.9:20   0.572734   0.056895 10.0665 &amp;lt; 2.2e-16 ***
## e:-0.9:20 154.543645  11.924187 12.9605 &amp;lt; 2.2e-16 ***
## b:-0.9:24   3.113060   0.947598  3.2852 0.0010191 ** 
## d:-0.9:24   0.233165   0.062870  3.7087 0.0002084 ***
## e:-0.9:24 269.509412  55.515112  4.8547 1.206e-06 ***
## d:-0.9:28   0.000000   0.000000     NaN       NaN    
## b:-0.6:8    5.077994   0.538321  9.4330 &amp;lt; 2.2e-16 ***
## d:-0.6:8    0.900202   0.033548 26.8334 &amp;lt; 2.2e-16 ***
## e:-0.6:8   92.146203   3.725399 24.7346 &amp;lt; 2.2e-16 ***
## b:-0.6:12   5.564019   0.586966  9.4793 &amp;lt; 2.2e-16 ***
## d:-0.6:12   0.937528   0.027059 34.6476 &amp;lt; 2.2e-16 ***
## e:-0.6:12  74.982399   2.818636 26.6024 &amp;lt; 2.2e-16 ***
## b:-0.6:16   4.144136   0.458813  9.0323 &amp;lt; 2.2e-16 ***
## d:-0.6:16   0.837853   0.041271 20.3014 &amp;lt; 2.2e-16 ***
## e:-0.6:16  75.109147   3.897277 19.2722 &amp;lt; 2.2e-16 ***
## b:-0.6:20   4.399408   0.510297  8.6213 &amp;lt; 2.2e-16 ***
## d:-0.6:20   0.725331   0.049946 14.5224 &amp;lt; 2.2e-16 ***
## e:-0.6:20  83.735884   4.468212 18.7404 &amp;lt; 2.2e-16 ***
## b:-0.6:24   3.269465   0.443121  7.3783 1.603e-13 ***
## d:-0.6:24   0.607528   0.055700 10.9071 &amp;lt; 2.2e-16 ***
## e:-0.6:24 125.859897   9.985513 12.6042 &amp;lt; 2.2e-16 ***
## b:-0.6:28   2.959772   0.767672  3.8555 0.0001155 ***
## d:-0.6:28   0.265633   0.059199  4.4871 7.220e-06 ***
## e:-0.6:28 233.440197  40.981613  5.6962 1.225e-08 ***
## b:-0.3:8    6.489283   0.700089  9.2692 &amp;lt; 2.2e-16 ***
## d:-0.3:8    0.962474   0.021243 45.3069 &amp;lt; 2.2e-16 ***
## e:-0.3:8   72.248403   2.326579 31.0535 &amp;lt; 2.2e-16 ***
## b:-0.3:12   5.571444   0.614984  9.0595 &amp;lt; 2.2e-16 ***
## d:-0.3:12   0.962476   0.021243 45.3075 &amp;lt; 2.2e-16 ***
## e:-0.3:12  56.804335   2.154440 26.3662 &amp;lt; 2.2e-16 ***
## b:-0.3:16   3.759741   0.406837  9.2414 &amp;lt; 2.2e-16 ***
## d:-0.3:16   0.925252   0.029452 31.4157 &amp;lt; 2.2e-16 ***
## e:-0.3:16  53.997403   3.004606 17.9715 &amp;lt; 2.2e-16 ***
## b:-0.3:20   3.455788   0.382078  9.0447 &amp;lt; 2.2e-16 ***
## d:-0.3:20   0.863032   0.038527 22.4004 &amp;lt; 2.2e-16 ***
## e:-0.3:20  56.589306   3.525940 16.0494 &amp;lt; 2.2e-16 ***
## b:-0.3:24   3.219012   0.384905  8.3631 &amp;lt; 2.2e-16 ***
## d:-0.3:24   0.739176   0.049321 14.9869 &amp;lt; 2.2e-16 ***
## e:-0.3:24  72.448658   5.145402 14.0803 &amp;lt; 2.2e-16 ***
## b:-0.3:28   3.384884   0.449578  7.5290 5.110e-14 ***
## d:-0.3:28   0.591722   0.055517 10.6583 &amp;lt; 2.2e-16 ***
## e:-0.3:28 111.482975   8.597283 12.9672 &amp;lt; 2.2e-16 ***
## b:0:8       8.055166   0.890908  9.0415 &amp;lt; 2.2e-16 ***
## d:0:8       0.962496   0.021229 45.3391 &amp;lt; 2.2e-16 ***
## e:0:8      64.579539   1.810276 35.6739 &amp;lt; 2.2e-16 ***
## b:0:12      4.597148   0.515642  8.9154 &amp;lt; 2.2e-16 ***
## d:0:12      0.962502   0.021236 45.3239 &amp;lt; 2.2e-16 ***
## e:0:12     45.258150   2.124034 21.3076 &amp;lt; 2.2e-16 ***
## b:0:16      4.519281   0.504988  8.9493 &amp;lt; 2.2e-16 ***
## d:0:16      0.937575   0.027033 34.6821 &amp;lt; 2.2e-16 ***
## e:0:16     41.805944   2.052607 20.3672 &amp;lt; 2.2e-16 ***
## b:0:20      3.833745   0.419743  9.1335 &amp;lt; 2.2e-16 ***
## d:0:20      0.925095   0.029446 31.4163 &amp;lt; 2.2e-16 ***
## e:0:20     43.588297   2.434550 17.9040 &amp;lt; 2.2e-16 ***
## b:0:24      4.103341   0.467512  8.7770 &amp;lt; 2.2e-16 ***
## d:0:24      0.875057   0.036978 23.6641 &amp;lt; 2.2e-16 ***
## e:0:24     47.161280   2.550713 18.4894 &amp;lt; 2.2e-16 ***
## b:0:28      2.895784   0.341099  8.4896 &amp;lt; 2.2e-16 ***
## d:0:28      0.764633   0.047737 16.0176 &amp;lt; 2.2e-16 ***
## e:0:28     63.034588   4.939641 12.7610 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-better-modelling-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A better modelling approach&lt;/h1&gt;
&lt;p&gt;The previous approach is clearly sub-optimal, as the temperature and water potential levels are regarded as factors, i.e. as nominal classes with no intrinsic orderings and distances. It would be much better to recognise that temperature and water potential are continuous variables and, consequently, code a time-to-event model where the three parameters are expressed as continuous functions of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi, T) = \Phi \left[ b(\Psi, T), d(\Psi, T), e(\Psi, T) \right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above mentioned manuscript (Onofri et al., 2018; example 2) we used a log-logistic cumulative distribution function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi, T) = \frac{ d(\Psi, T) }{1 + \exp \left\{ b \left[ \log(t) - \log( e(\Psi, T) ) \right] \right\} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Considering that the germination rate is the inverse of germination time, we replaced &lt;span class=&#34;math inline&#34;&gt;\(e(\Psi, T) = 1/GR_{50}(\Psi, T)\)&lt;/span&gt; and used the following sub-models:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(\Psi, T) = \textrm{max} \left\{ G \, \left[ 1 - \exp \left( \frac{ \Psi - \Psi_b - k(T - T_b )}{\sigma_{\Psi_b}} \right) \right]; 0 \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR_{50}(\Psi, T) = \textrm{max} \left\{ \frac{T - T_b }{\theta_{HT}} \left[\Psi - \Psi_b - k(T - T_b )\right]; 0 \right\}\]&lt;/span&gt;
Please, note that the shape parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; has been regarded as independent from the environmental covariates. It may be useful to note that the the parameters are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b}\)&lt;/span&gt;, that is the median base water potential in the seed lot,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(T_{b}\)&lt;/span&gt;, that is the base temperature for germination,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_HT\)&lt;/span&gt;, that is the hydro-thermal-time parameter,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b}\)&lt;/span&gt;, that represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;, that is the germinable fraction, accounting for the fact that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; may not reach 1, regardless of time and water potential.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; that are parameters of shape.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can get more information from our original paper (Onofri et al., 2018). This hydro-thermal-time model was implemented in R as the &lt;code&gt;HTTEM()&lt;/code&gt; function, and it is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package; we can fit it by using the &lt;code&gt;drmte()&lt;/code&gt; function in the &lt;code&gt;drcte&lt;/code&gt; package, but we need to provide starting values for model parameters, because the self-starting routine is not yet available. Finally, the &lt;code&gt;summary()&lt;/code&gt; method can be used to retrieve the parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Complex model and slow fitting
modHTTE &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + water + temp,
                 data=hordeum,
                 fct = HTTEM(),
  start=c(0.8,-2, 0.05, 3, 0.2, 2000, 0.5))
summary(modHTTE, robust = T, units = Dish)
## 
## Model fitted: Hydro-thermal-time-model (Mesgaran et al., 2017)
## 
## Robust estimation: Cluster robust sandwich SEs 
## 
## Parameter estimates:
## 
##                          Estimate  Std. Error  t value Pr(&amp;gt;|t|)    
## G:(Intercept)          9.8820e-01  1.1576e-02  85.3692   &amp;lt;2e-16 ***
## Psib:(Intercept)      -2.9133e+00  3.4812e-02 -83.6874   &amp;lt;2e-16 ***
## kt:(Intercept)         7.4228e-02  1.2666e-03  58.6015   &amp;lt;2e-16 ***
## Tb:(Intercept)        -7.4525e-01  3.5254e-01  -2.1139   0.0346 *  
## sigmaPsib:(Intercept)  5.5284e-01  2.8976e-02  19.0790   &amp;lt;2e-16 ***
## ThetaHT:(Intercept)    1.3091e+03  4.0638e+01  32.2130   &amp;lt;2e-16 ***
## b:(Intercept)          4.1650e+00  1.1332e-01  36.7548   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is always important not to neglect a graphical inspection of model fit. The &lt;code&gt;plot()&lt;/code&gt; method does not work with time-to-event curves with additional covariates (apart from time). However, we can retrieve the fitted data by using the &lt;code&gt;plotData()&lt;/code&gt; function and use those predictions within the &lt;code&gt;ggplot()&lt;/code&gt; function. The box below shows the appropriate coding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- plotData(modHTTE)

ggplot() +
  geom_point(data = tab$plotPoints, mapping = aes(x = timeAf, y = CDF),
            col = &amp;quot;red&amp;quot;) +
  geom_line(data = tab$plotFits, mapping = aes(x = timeAf, y = CDF)) +
  facet_grid(temp ~ water) +
  scale_x_continuous(name = &amp;quot;Time (d)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_10-ExampleHTTE_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Mesgaran, M.B., Mashhadi, H.R., Alizadeh, H., Hunt, J., Young, K.R., Cousens, R.D., 2013. Importance of distribution function selection for hydrothermal time models of seed germination. Weed Research 53, 89–101. &lt;a href=&#34;https://doi.org/10.1111/wre.12008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The coefficient of determination: is it the R-squared or r-squared?</title>
      <link>https://www.statforbiology.com/2022/stat_general_r2/</link>
      <pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_general_r2/</guid>
      <description>


&lt;p&gt;We often use the &lt;strong&gt;coefficient of determination&lt;/strong&gt; as a swift ‘measure’ of goodness of fit for our regression models. Unfortunately, there is no unique symbol for such a coefficient and both &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; are used in literature, almost interchangeably. Such an interchangeability is also endorsed by the Wikipedia (see at: &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34;&gt;https://en.wikipedia.org/wiki/Coefficient_of_determination&lt;/a&gt; ), where both symbols are reported as the abbreviations for this statistical index.&lt;/p&gt;
&lt;p&gt;As an editor of several International Journals, I should not agree with such an approach; indeed, the two symbols &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; mean two different things, and they are not necessarily interchangeable, because, depending on the setting, either of the two may be wrong or ambiguous. Let’s pay a little attention to such an issue.&lt;/p&gt;
&lt;div id=&#34;what-are-the-r-and-r-indices&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are the ‘r’ and ‘R’ indices?&lt;/h1&gt;
&lt;p&gt;When studying the relationship between quantitative variables, we have two main statistical indices:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;strong&gt;Pearson’s (simple linear) correlation coefficient&lt;/strong&gt;, that is almost always indicated as the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; coefficient. Correlation is different from regression, as it does not assume any sort of dependency between two quantitative variables and it is only meant to express their joint variability;&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;coefficient of multiple correlation&lt;/strong&gt;, that is usually indicated with &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and represents (definition from Wikipedia) a &lt;em&gt;measure of how well a given variable can be predicted using a linear function of a set of other variables&lt;/em&gt;. Although &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is based on correlation (it is the correlation between the observed values for the dependent variable and the predictions made by the model), it is used in the context of multiple regression, where we are studying a dependency relationship.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And, what about the &lt;strong&gt;coefficient of determination&lt;/strong&gt;? It is yet another concept and another index, measuring (again from Wikipedia) the &lt;em&gt;proportion of the variation in the dependent variable that is predictable from the independent variable(s)&lt;/em&gt;. As you see, we are still in the context of regression and our aim is to describe the goodness of fit.&lt;/p&gt;
&lt;p&gt;To start with, let’s abbreviate the coefficient of determination as &lt;span class=&#34;math inline&#34;&gt;\(CD\)&lt;/span&gt;, in order to avoid any confusion with &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;; this index can be be obtained as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ CD_1 = \frac{SS_{reg}}{SS_{tot}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ CD_2 = 1 - \frac{SS_{res}}{SS_{tot}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where: &lt;span class=&#34;math inline&#34;&gt;\(SS_{reg}\)&lt;/span&gt; is the regression sum of squares, &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt; is the total sum of squares and &lt;span class=&#34;math inline&#34;&gt;\(SS_{res}\)&lt;/span&gt; is the residual sum of squares, after a linear regression fit. The second formula is preferable: sum of squares are always positive and, thus, we clearly see that &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; may not be higher than 1 (this is less obvious, for &lt;span class=&#34;math inline&#34;&gt;\(CD_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So far, so good; we have three different indices and three different symbols: &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CD\)&lt;/span&gt;. But, in practice, things did not go that smoothly! The early statisticians, instead of proposing a brand new symbol for the coefficient of determination, made the choice of highlighting the connections with &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;. For example, Sokal and Rohlf, in their very famous biometry book, at page 570 (2nd. Edition) demonstrated that the coefficient of determination could be obtained as the square of the coefficient of correlation and, thus, they used the symbol &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt;. Later, in the same book (pag. 660), these same authors demonstrated that the coefficient of multiple correlation &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; was equal to the positive square root of the &lt;strong&gt;coefficient of multiple determination&lt;/strong&gt;, for which they used the symbol &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Obviously, Sokal and Rohlf (and other authors of other textbooks) meant to say that the coefficient of determination, depending on the context, can be obtained either as the square of the correlation coefficient, or as the square of the multiple correlation coefficient. An uncareful interpretation led to the idea that the coefficient of determination can be indicated either as the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; or as the &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; and that the two symbols are always interchangeable. But, is this really true? No, it depends on the context.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple linear regression&lt;/h1&gt;
&lt;p&gt;Let’s have a look at the following example: we fit a simple linear regression model to a dataset and retrieve the coefficient of determination by using the &lt;code&gt;summary()&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 1:20
Y &amp;lt;- c(17.79, 18.81, 19.02, 14.14, 16.72, 16.05, 13.99, 13.26,
       12.48, 11.33, 11.07, 9.68, 9.19, 9.44, 9.75, 7.71, 6.47, 
       5.22, 4.55, 7.7)
mod &amp;lt;- lm(Y ~ X)
summary(mod)$r.squared # Coeff. determination with R
## [1] 0.9270622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very easy to see that &lt;span class=&#34;math inline&#34;&gt;\(R = |r|\)&lt;/span&gt; and it is also easy to demonstrate that &lt;span class=&#34;math inline&#34;&gt;\(r^2 = CD_1\)&lt;/span&gt; (look, e.g., at Sokal and Rohlf for a mathematical proof). Furthermore, due to the equality &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot} = SS_{reg} + SS_{res}\)&lt;/span&gt;, it is also easy to see that &lt;span class=&#34;math inline&#34;&gt;\(CD_1 = CD_2\)&lt;/span&gt;. We are ready to draw our first conclusion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SSreg &amp;lt;- sum((predict(mod) - mean(Y))^2)
SStot &amp;lt;- sum((Y - mean(Y))^2)
SSres &amp;lt;- sum(residuals(mod)^2)
SSreg/SStot
## [1] 0.9270622
1 - SSres/SStot
## [1] 0.9270622
r.coef &amp;lt;- cor(X, Y)
R.coef &amp;lt;- cor(Y, fitted(mod))
r.coef^2
## [1] 0.9270622
R.coef^2
## [1] 0.9270622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 1. For simple linear regression, the coefficient of determination is always equal to &lt;span class=&#34;math inline&#34;&gt;\(R^2 = r^2\)&lt;/span&gt; and both symbols are acceptable (and correct).&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-regression-and-multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomial regression and multiple regression&lt;/h1&gt;
&lt;p&gt;Apart from simple linear regression, for all other types of linear models, provided that an intercept is fitted, it is not, in general, true that &lt;span class=&#34;math inline&#34;&gt;\(R = |r|\)&lt;/span&gt;, while it is, in general, true that that the coefficient of determination is equal to the squared coefficient of multiple correlation &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;. I’ll show a swift example with a polynomial regression in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lm(Y ~ X + I(X^2))
cor.coef &amp;lt;- cor(X, Y)
R.coef &amp;lt;- cor(Y, fitted(mod2))

# R and r are not equal
cor.coef; R.coef
## [1] -0.9628407
## [1] 0.9652451
# The coefficient of determination is R2
R.coef^2; summary(mod2)$r.squared
## [1] 0.931698
## [1] 0.931698&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, when we have several predictors (e.g., multiple regression), the correlation coefficient is not unique and we could calculate as many &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; values as there are predictors in the model.&lt;/p&gt;
&lt;p&gt;In the box below I show another example where I analysed the ‘mtcars’ dataset by using multiple regression; we see that &lt;span class=&#34;math inline&#34;&gt;\(R^2 = CD_1 = CD_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
mod &amp;lt;- lm(mpg ~ wt+disp+hp - 1, data = mtcars)  
summary(mod)$r.squared # Coeff. determination with R
## [1] 0.8328665
SSreg &amp;lt;- sum((predict(mod) - mean(mtcars$mpg))^2)
SStot &amp;lt;- sum((mtcars$mpg - mean(mtcars$mpg))^2)
SSres &amp;lt;- sum(residuals(mod)^2)
SSreg/SStot
## [1] 0.9852479
1 - SSres/SStot
## [1] -1.084229
R.coef &amp;lt;- cor(mtcars$mpg, fitted(mod))
R.coef^2
## [1] 0.002746157&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to draw our second conclusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 2: with all linear models, apart from simple linear regression, the &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; symbol should not be used for the coefficient of determination, because this latter index IS NOT, in general, equal to the square of the coefficient of correlation. The &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; symbol is a much better choice.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models-with-no-intercept&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Linear models with no intercept&lt;/h1&gt;
&lt;p&gt;The situation becomes much more complex for linear models with no intercept. For these models, the squared multiple correlation coefficient IS NOT ALWAYS equal to the proportion of variance accounted for. Let’s look at the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lm(Y ~ - 1 + X)
summary(mod2)$r.squared # Proportion of variance accounted for
## [1] 0.4390065
R.coef &amp;lt;- cor(Y, fitted(mod2))
R.coef^2
## [1] 0.9270622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, the coefficient of determination IS NOT ALWAYS the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;; however, the coefficient of determination can be calculated by using &lt;span class=&#34;math inline&#34;&gt;\(CD_1 = CD_2\)&lt;/span&gt;, provided that &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(SS_{reg}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SS_{res}\)&lt;/span&gt; are obtained in a way that accounts for the missing intercept. Schabenberger and Pierce recommend the following equations and the symbols they use clearly reflect that those equations do not return the squared multiple correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R^2_{noint} = \frac{\sum_{i = 1}^{n}{\hat{y_i}^2}}{\sum_{i = 1}^{n}{y_i^2}} \quad \textrm{or} \quad R^{2*}_{noint} =1 - \frac{SS_{res}}{\sum_{i = 1}^{n}{y_i^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SSreg &amp;lt;- sum(fitted(mod2)^2)
SStot &amp;lt;- sum(Y^2)
SSres &amp;lt;- sum(residuals(mod2)^2)
SSreg/SStot # R^2 ok
## [1] 0.4390065
1 - SSres/SStot # R^2 ok
## [1] 0.4390065&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are ready for our third conclusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 3: in the case of models with no intercept, neither the &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; nor the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; symbols should be used for the coefficient of determination. The proportion of variability accounted for by the model can be calculated by using a modified formula and should be reported by using a different symbol (e.g. &lt;span class=&#34;math inline&#34;&gt;\(R^2_{noint}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(R^2_0\)&lt;/span&gt; or similar).&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nonlinear regression&lt;/h1&gt;
&lt;p&gt;With this class of models, we have two main problems:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;they do not have an intercept term, at least, not in the usual sense. Consequently, the square of the multiple coefficient of correlation does not represent the proportion of variance accounted for by the model;&lt;/li&gt;
&lt;li&gt;the equality &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot} = SS_{reg} + SS_{res}\)&lt;/span&gt; may not hold and thus the equations for &lt;span class=&#34;math inline&#34;&gt;\(CD_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; may produce different results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In contrast to linear models with no intercept, for nonlinear models we do not have any general modified formula that consistently returns the proportion of variance accounted for by the model (i.e., the coefficient of determination). However, Schabenberger and Pierce (2002) suggested that we can still use &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; as a swift measure of goodness of fit, but they also proposed that we use the term ‘Pseudo-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;’ instead oft &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;. Why ‘Pseudo’?. For two good reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the ’Pseudo-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;’cannot exceed 1, but it may lower than 0;&lt;/li&gt;
&lt;li&gt;the ‘Pseudo-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;’ &lt;strong&gt;cannot be interpreted as the proportion of variance explained by the model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In R, the ‘Pseudo-R&lt;sup&gt;2&lt;/sup&gt;’ can be calculated by using the &lt;code&gt;R2.nls()&lt;/code&gt; function in the ‘aomisc’ package, for nonlinear models fitted with both the &lt;code&gt;nls()&lt;/code&gt; and &lt;code&gt;drm()&lt;/code&gt; functions (this latter function is in the ‘drc’ package).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
X &amp;lt;- c(0.1, 5, 7, 22, 28, 39, 46, 200)
Y &amp;lt;- c(1, 13.66, 14.11, 14.43, 14.78, 14.86, 14.78, 14.91)

# nls fit
library(aomisc)
model &amp;lt;- nls(Y ~ SSmicmen(X, Vm, K))
R2nls(model)$PseudoR2
## [1] 0.9930399
# It is not the R2, in strict sense
R.coef &amp;lt;- cor(Y, fitted(model))
R.coef^2
## [1] 0.9957255
# It cannot be calculated by the usual expression!
SSreg &amp;lt;- sum(fitted(model) - mean(Y))
SStot &amp;lt;- sum( (Y - mean(Y))^2 )
SSreg/SStot
## [1] 0.003622004
# It can be calculated by using the alternative form
# that is no longer equivalent
SSres &amp;lt;- sum(residuals(model)^2)
1 - SSres/SStot
## [1] 0.9930399&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may now come to our final conclusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 4. With nonlinear models, we should never use either &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; because they are both wrong. If we need a swift measure of goodness of fit, we can use the &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; index above, but we should not name it as the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;, because, in general, it does not correspond to the coefficient of determination. We should better use the term Pseudo-R&lt;sup&gt;2&lt;/sup&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hope this was useful; for those who are interested in the use of the Pseud-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; in nonlinear regression, I hav already published one post at this link: &lt;a href=&#34;https://www.statforbiology.com/2021/stat_nls_r2/&#34;&gt;https://www.statforbiology.com/2021/stat_nls_r2/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Schabenberger, O., Pierce, F.J., 2002. Contemporary statistical models for the plant and soil sciences. Taylor &amp;amp; Francis, CRC Press, Books.&lt;/li&gt;
&lt;li&gt;Sokal, R.R., Rohlf F.J., 1981. Biometry. Second Edition, W.H. Freeman and Company, USA.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multi-environment split-plot experiments</title>
      <link>https://www.statforbiology.com/2022/stat_lmm_2-wayssplitrepeated/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_lmm_2-wayssplitrepeated/</guid>
      <description>


&lt;p&gt;Have you made a split-plot field experiment? Have you repeated such an experiment in two (or more) years/locations? Have you run into troubles, because the reviewer told you that your ANOVA model was invalid? If so, please, stop for awhile and read: this post might help you understand what was wrong with your analyses.&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;Let’s think of a field experiment, where 6 genotypes of faba bean were compared under two different sowing times (autumn and spring). For the ease of organisation, such an experiment was laid down as a &lt;strong&gt;split-plot&lt;/strong&gt;, in a randomised complete block design; sowing times were randomly allocated to main-plots, while genotypes were randomly allocated to sub-plots. Let’s stop for a moment… does this sound strange to you? Do you need further information about split-plot designs? You can get some general information &lt;a href=&#34;https://www.statforbiology.com/_statbookeng/designing-experiments.html#setting-up-a-field-experiment&#34;&gt;at this link&lt;/a&gt; and hints on how to analyse the results at this &lt;a href=&#34;https://www.statforbiology.com/_statbookeng/plots-of-different-sizes.html&#34;&gt;other link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The above experiment was repeated in three years and two locations (six environments in all), in order to explore the environmental variability of results (we will not make any distinction between years and locations, for the sake of this post). In the end, we recorded crop yield and produced a dataset with 288 record (6 environments by 2 sowing times by 6 genotypes by 4 blocks). If you are interested in more detail about this experiment, you can find them in Stagnari et al., (2007).&lt;/p&gt;
&lt;p&gt;The resulting dataset (‘fabaBeanSplitMet.csv’) is available in a public online repository and contains six columns, the ‘Location’, the ‘Year’, the ‘Sowing’ time, the ‘Genotype’, the ‘Block’ and the response variable, i.e. the ‘Yield’. After loading the dataset, we need to recode the independent variables into factors and create the new ‘Environment’ factor, as the combination of ‘Year’ and ‘Location’ levels. In the box below, we use the ‘dplyr’ package to accomplish this preliminary step (Wickham et al., 2022).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
rm(list=ls())
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/fabaBeanSplitMet.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Location, Year, Sowing, Genotype, Block),
                .fns = factor),
         Environment = factor(Location:Year))
head(dataset)
##   Location      Year Sowing  Genotype Block Yield       Environment
## 1  papiano 2002-2003 autumn    Chiaro     1  2.05 papiano:2002-2003
## 2  papiano 2002-2003 autumn    Chiaro     2  2.50 papiano:2002-2003
## 3  papiano 2002-2003 autumn    Chiaro     3  2.64 papiano:2002-2003
## 4  papiano 2002-2003 autumn    Chiaro     4  2.45 papiano:2002-2003
## 5  papiano 2002-2003 autumn Collameno     1  2.01 papiano:2002-2003
## 6  papiano 2002-2003 autumn Collameno     2  2.19 papiano:2002-2003&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-valid-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building a valid model&lt;/h1&gt;
&lt;p&gt;A model is identified by listing all the effects which we need to explain the observed yield. In this case, considering the aims of our experiment, it is pretty easy to grasp the importance of the ‘sowing date’ effect, the ‘genotype’ effect and their interaction. These are the so-called treatment factors and we have no doubt that they should be included in our model. Furthermore, we should also be interested to know whether those treatment effects interact with the environment effect, so we should clearly add to the model the ‘sowing time by environment’, ‘genotype by environment’ and ‘sowing time by genotype by environment’ interactions.&lt;/p&gt;
&lt;p&gt;At this step, it is possible that we have no specific interest in any other effects, apart from those we have just mentioned; however, if we stop now, our model is still incomplete and, therefore, invalid. Indeed, we should also think about possible grouping factors. You may wonder: what are the grouping factors? This aspect needs particular attention.&lt;/p&gt;
&lt;p&gt;In split-plot and other very common types of designs, the experimental units are not completely randomised, but they are organised (‘grouped’, indeed) by way of some innate attribute, such as the environment or block or main-plot, which they belong to. These attributes are known as ‘grouping factors (see Piepho et al., 2003) and they introduce a sort of ’parenthood’, so that some observations are more alike than others, because they belong to the same ‘group’ (e.g., same block or same main-plot). If we neglect the effects of ‘grouping factors’, these ‘parenthood’ effects remain in the residuals, which will be correlated. The correlation of residuals represent an important violation of the basic assumptions for linear model fitting and, therefore, the model will be invalid and our paper will be rejected. One first conclusion: &lt;strong&gt;please, do never forget the grouping factors, if you want your paper to be accepted!&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What are the grouping factors in this case? First of all we have the environments (six levels), then we have the blocks within each environment (24 levels in all) and, finally, we have the main-plots within each block and within each environment (48 levels, in all). In this latter respect, we can see that each main-plot can be uniquely identified by the combination of one environment, one block and one sowing time. Consequently, the final (valid) model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Environment + Sowing + Environment:Sowing + Genotype + 
        Environment:Genotype + Environment:Sowing:Genotype + 
        Environment:Block + Environment:Block:Sowing&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R, we can abbreviate as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Environment * Sowing * Genotype + 
        Environment:Block + Environment:Block:Sowing&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sorry, I know I am running the risk of being regarded as a boring professor; but, please, remember: &lt;strong&gt;failing to include any of the above mentioned effects in the model, unless they are clearly non-significant, leads to totally invalid results!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, we need to take a very important decision: which factors are fixed and which factors are random? The rule is that all factors that reference randomisation units (to which treatments are allocated) NEED TO BE RANDOM, while, for the other factors, we can make our own subjective choice. Here, the main-plot factor, to which we allocated the sowing dates, needs to be taken as random. For the other factors, we make the most traditional choice of taking them as fixed, although we need to consider that, in other instances, it might be appropriate to regard the ‘environment’ and ‘block’ effects as random (relating to block effects, you may read Dixon, 2016, for interesting information).&lt;/p&gt;
&lt;p&gt;If I were to suggest a simple package to fit the above model, I’d say that you should favour the &lt;code&gt;lmer()&lt;/code&gt; function in the &lt;code&gt;lme4&lt;/code&gt; package, where the random effects are coded by using the ‘(1|effect)’ notation, as shown in the box below; before fitting, we load the ‘lme4’ package, together with the ‘lmerTest’ package, which gives us extra-flexibility to produce an ANOVA table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(lmerTest)
modMix &amp;lt;- lmer(Yield ~ Environment * Sowing * Genotype +
               Environment:Block + (1|Environment:Block:Sowing),
               data = dataset)
anova(modMix)
## Type III Analysis of Variance Table with Satterthwaite&amp;#39;s method
##                             Sum Sq Mean Sq NumDF DenDF  F value    Pr(&amp;gt;F)    
## Environment                 71.084  14.217     5    18 125.4463 2.430e-13 ***
## Sowing                      45.947  45.947     1    18 405.4282 8.574e-14 ***
## Genotype                     8.030   1.606     5   180  14.1707 1.091e-11 ***
## Environment:Sowing          11.022   2.204     5    18  19.4520 1.086e-06 ***
## Environment:Genotype         9.468   0.379    25   180   3.3418 1.454e-06 ***
## Sowing:Genotype              5.340   1.068     5   180   9.4231 5.388e-08 ***
## Environment:Block            4.398   0.244    18    18   2.1560    0.0561 .  
## Environment:Sowing:Genotype  7.912   0.316    25   180   2.7925 4.513e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, multiple comparison testing can be done with the ‘emmeans’ package as we have shown elsewhere. Transforming the environment or block effects into random effects is pretty straightforward, by changing the R notation; please remember that, if you regard the environment as random, all its interactions should also be regarded most often regardes as well as random.&lt;/p&gt;
&lt;p&gt;Did I menage to make myself clear? If not, drop me a line to the address below. Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Stagnari, F., Onofri, A., Jemison, J.J., Monotti, M., 2007. Improved multivariate analyses to discriminate the behaviour of faba bean varieties. Agronomy For Sustainable Development 27, 387–397.&lt;/li&gt;
&lt;li&gt;Wickham H, François R, Henry L, Müller K (2022). Dplyr: A Grammar of Data Manipulation. R package version 1.0.9, &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Meta-analysis for a single study. Is it possible?</title>
      <link>https://www.statforbiology.com/2022/stat_met_metanalyses/</link>
      <pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_met_metanalyses/</guid>
      <description>


&lt;p&gt;We all know that the word meta-analysis encompasses a body of statistical techniques to combine quantitative evidence from several independent studies. However, I have recently discovered that meta-analytic methods can also be used to analyse the results of a single research project. That happened a few months ago, when I was reading a paper from Damesa et al. (2017), where the authors describe some interesting methods of data analyses for multi-environment genotype experiments. These authors gave a few nice examples with related SAS code, that is rooted in mixed models. As an R enthusiast, I was willing to reproduce their analyses with R, but I could not succeed, until I realised that I could make use of the package ‘metafor’ and its bunch of meta-analityc methods.&lt;/p&gt;
&lt;p&gt;In this post, I will share my R coding, for those of you who are interested in meta-analytic methods and multi-environment experiments. Let’s start by having a look at the example that motivated my interest (Example 1 in Damesa et al., 2017, p. 849).&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;Twenty-two different genotypes of maize were compared in Ethiopia, in relation to their yield level, in four sites (Dhera, Melkassa, Mieso, and Ziway). At all sites, there were 11 incomplete blocks in each of three replicates. The data are available in Damesa et al. (2017) as supplemental material; I have put this data at your disposal in my web repository, to reproduce this example; let’s load the data first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
library(sommer)
library(emmeans)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/Damesa2017.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(1:5, .fns = factor))
head(dataset)
##   site rep block plot genotype row col yield
## 1    1   1     1    1        6   1   1  9.93
## 2    1   1     1    2       22   1   2  6.51
## 3    1   1     2    3       17   1   3  7.92
## 4    1   1     2    4       14   1   4  9.28
## 5    1   1     3    5       12   1   5  7.56
## 6    1   1     3    6       10   1   6  9.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a typical multi-environment experiment: we have three blocking factors (‘site’, ‘rep’ and ‘block’) and one treatment factor (‘genotype’), as well as the ‘yield’ response variable. Let’s see how this dataset can be analysed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-golden-standard-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The ‘golden standard’ analysis&lt;/h1&gt;
&lt;p&gt;In most situations with multi-environment experiments, we are interested in broad space inference about genotypes, which means that we want to determine the best genotypes across the whole set of environments. Accordingly, the ‘site’ and ‘site x genotype’ effects must be regarded as random, while the ‘genotype’ effect is fixed. Furthermore, we need to consider the ‘design’ effects, that (in this specific case) are the ‘reps within sites’ and the ‘blocks within reps within sites’ random effects. Finally, we have the residual error term (‘plots within blocks within reps within sites’), that is always included by default.&lt;/p&gt;
&lt;p&gt;So far, so good, but we have to go slightly more complex; for this type of studies, the variances for replicates, blocks, and residual error should be site specific, which is usually the most realistic assumption. In the end, we need to estimate:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;22 genotype means with standard errors&lt;/li&gt;
&lt;li&gt;one variance component for the site effect&lt;/li&gt;
&lt;li&gt;one variance component for the ‘genotype x site’ interaction&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the ‘rep’ effect&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the ‘block within rep’ effect&lt;/li&gt;
&lt;li&gt;four variance components (one per site) for the residual error&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we work with the &lt;code&gt;lme()&lt;/code&gt; function in the &lt;code&gt;nlme&lt;/code&gt; package, we have to create a couple of ‘dummy’ variables (‘one’ and ‘GE’), in order to reference the crossed random effects (see Galecki and Burzykowski, 2013).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# One stage analysis
dataset$one &amp;lt;- 1L
dataset$GE &amp;lt;- with(dataset, genotype:site)
model.mix &amp;lt;- lme(yield ~ genotype - 1, 
             random = list(one = pdIdent(~ site - 1),
                           one = pdIdent(~ GE - 1),
                           rep = pdDiag(~ site - 1),
                           block = pdDiag(~ site - 1)),
                              data = dataset,
             weights = varIdent(form = ~1|site))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The means for genotypes are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mg &amp;lt;- emmeans(model.mix, ~ genotype)
mg
##  genotype emmean   SE  df lower.CL upper.CL
##  1          5.15 1.65 210    1.900     8.40
##  2          5.54 1.65 210    2.296     8.79
##  3          5.19 1.65 210    1.939     8.44
##  4          4.59 1.65 210    1.341     7.84
##  5          4.82 1.65 210    1.568     8.07
##  6          4.66 1.65 210    1.411     7.91
##  7          4.64 1.65 210    1.388     7.88
##  8          4.36 1.65 210    1.110     7.61
##  9          5.03 1.65 210    1.785     8.28
##  10         4.84 1.65 210    1.592     8.09
##  11         4.54 1.65 210    1.290     7.79
##  12         4.87 1.65 210    1.622     8.12
##  13         4.84 1.65 210    1.593     8.09
##  14         4.29 1.65 210    1.045     7.54
##  15         4.47 1.65 210    1.224     7.72
##  16         4.37 1.65 210    1.123     7.62
##  17         4.07 1.65 210    0.819     7.32
##  18         4.95 1.65 210    1.697     8.19
##  19         4.71 1.65 210    1.466     7.96
##  20         4.86 1.65 210    1.612     8.11
##  21         4.13 1.65 210    0.885     7.38
##  22         4.63 1.65 210    1.380     7.88
## 
## Degrees-of-freedom method: containment 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the variance components are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(model.mix)
##          Variance          StdDev      
## one =    pdIdent(site - 1)             
## site1    1.045428e+01      3.233309e+00
## site2    1.045428e+01      3.233309e+00
## site3    1.045428e+01      3.233309e+00
## site4    1.045428e+01      3.233309e+00
## one =    pdIdent(GE - 1)               
## GE1:1    1.052944e-01      3.244909e-01
## GE1:2    1.052944e-01      3.244909e-01
## GE1:3    1.052944e-01      3.244909e-01
## GE1:4    1.052944e-01      3.244909e-01
## GE2:1    1.052944e-01      3.244909e-01
## GE2:2    1.052944e-01      3.244909e-01
## GE2:3    1.052944e-01      3.244909e-01
## GE2:4    1.052944e-01      3.244909e-01
## GE3:1    1.052944e-01      3.244909e-01
## GE3:2    1.052944e-01      3.244909e-01
## GE3:3    1.052944e-01      3.244909e-01
## GE3:4    1.052944e-01      3.244909e-01
## GE4:1    1.052944e-01      3.244909e-01
## GE4:2    1.052944e-01      3.244909e-01
## GE4:3    1.052944e-01      3.244909e-01
## GE4:4    1.052944e-01      3.244909e-01
## GE5:1    1.052944e-01      3.244909e-01
## GE5:2    1.052944e-01      3.244909e-01
## GE5:3    1.052944e-01      3.244909e-01
## GE5:4    1.052944e-01      3.244909e-01
## GE6:1    1.052944e-01      3.244909e-01
## GE6:2    1.052944e-01      3.244909e-01
## GE6:3    1.052944e-01      3.244909e-01
## GE6:4    1.052944e-01      3.244909e-01
## GE7:1    1.052944e-01      3.244909e-01
## GE7:2    1.052944e-01      3.244909e-01
## GE7:3    1.052944e-01      3.244909e-01
## GE7:4    1.052944e-01      3.244909e-01
## GE8:1    1.052944e-01      3.244909e-01
## GE8:2    1.052944e-01      3.244909e-01
## GE8:3    1.052944e-01      3.244909e-01
## GE8:4    1.052944e-01      3.244909e-01
## GE9:1    1.052944e-01      3.244909e-01
## GE9:2    1.052944e-01      3.244909e-01
## GE9:3    1.052944e-01      3.244909e-01
## GE9:4    1.052944e-01      3.244909e-01
## GE10:1   1.052944e-01      3.244909e-01
## GE10:2   1.052944e-01      3.244909e-01
## GE10:3   1.052944e-01      3.244909e-01
## GE10:4   1.052944e-01      3.244909e-01
## GE11:1   1.052944e-01      3.244909e-01
## GE11:2   1.052944e-01      3.244909e-01
## GE11:3   1.052944e-01      3.244909e-01
## GE11:4   1.052944e-01      3.244909e-01
## GE12:1   1.052944e-01      3.244909e-01
## GE12:2   1.052944e-01      3.244909e-01
## GE12:3   1.052944e-01      3.244909e-01
## GE12:4   1.052944e-01      3.244909e-01
## GE13:1   1.052944e-01      3.244909e-01
## GE13:2   1.052944e-01      3.244909e-01
## GE13:3   1.052944e-01      3.244909e-01
## GE13:4   1.052944e-01      3.244909e-01
## GE14:1   1.052944e-01      3.244909e-01
## GE14:2   1.052944e-01      3.244909e-01
## GE14:3   1.052944e-01      3.244909e-01
## GE14:4   1.052944e-01      3.244909e-01
## GE15:1   1.052944e-01      3.244909e-01
## GE15:2   1.052944e-01      3.244909e-01
## GE15:3   1.052944e-01      3.244909e-01
## GE15:4   1.052944e-01      3.244909e-01
## GE16:1   1.052944e-01      3.244909e-01
## GE16:2   1.052944e-01      3.244909e-01
## GE16:3   1.052944e-01      3.244909e-01
## GE16:4   1.052944e-01      3.244909e-01
## GE17:1   1.052944e-01      3.244909e-01
## GE17:2   1.052944e-01      3.244909e-01
## GE17:3   1.052944e-01      3.244909e-01
## GE17:4   1.052944e-01      3.244909e-01
## GE18:1   1.052944e-01      3.244909e-01
## GE18:2   1.052944e-01      3.244909e-01
## GE18:3   1.052944e-01      3.244909e-01
## GE18:4   1.052944e-01      3.244909e-01
## GE19:1   1.052944e-01      3.244909e-01
## GE19:2   1.052944e-01      3.244909e-01
## GE19:3   1.052944e-01      3.244909e-01
## GE19:4   1.052944e-01      3.244909e-01
## GE20:1   1.052944e-01      3.244909e-01
## GE20:2   1.052944e-01      3.244909e-01
## GE20:3   1.052944e-01      3.244909e-01
## GE20:4   1.052944e-01      3.244909e-01
## GE21:1   1.052944e-01      3.244909e-01
## GE21:2   1.052944e-01      3.244909e-01
## GE21:3   1.052944e-01      3.244909e-01
## GE21:4   1.052944e-01      3.244909e-01
## GE22:1   1.052944e-01      3.244909e-01
## GE22:2   1.052944e-01      3.244909e-01
## GE22:3   1.052944e-01      3.244909e-01
## GE22:4   1.052944e-01      3.244909e-01
## rep =    pdDiag(site - 1)              
## site1    8.817499e-02      2.969427e-01
## site2    1.383338e+00      1.176154e+00
## site3    4.245188e-09      6.515511e-05
## site4    1.442336e-02      1.200973e-01
## block =  pdDiag(site - 1)              
## site1    3.312025e-01      5.755020e-01
## site2    4.746751e-01      6.889667e-01
## site3    5.498857e-09      7.415428e-05
## site4    6.953371e-02      2.636925e-01
## Residual 1.346652e+00      1.160454e+00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, apart from some differences relating to the optimisation method, the results are equal to those reported in Tables 1 and 2 of Damesa et al. (2017).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-stage-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two-stage analyses&lt;/h1&gt;
&lt;p&gt;The above analysis is fully correct, but, in some circumstances may be unfeasible. In particular, we may have problems when:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the number of sites is very high, and&lt;/li&gt;
&lt;li&gt;different experimental designs have been used in different sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In these circumstances, it is advantageous to break the analysis in two stages, as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;first stage: we separately analyse the different experiments and obtain the means for all genotypes in all sites;&lt;/li&gt;
&lt;li&gt;second stage: we jointly analyse the genotype means from all sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This two-stage analysis is far simpler, because the data are only pooled at the second stage, where possible design constraints are no longer important (they are considered only at the first stage). However, this two-stage analysis does not necessarily lead to the same results as the one-stage analysis, unless the whole information obtained at the first stage is carried forward to the second one (fully efficient two-stage analysis).&lt;/p&gt;
&lt;p&gt;In more detail, genotypic variances and correlations, as observed in the first stage, should not be neglected in the second stage. Damesa et al. (2017) demonstrate that the best approach is to take the full variance-covariance matrix of genotypes at the first stage and bring it forward to the second stage. They give the coding with SAS, but, how do we do it, with R?&lt;/p&gt;
&lt;p&gt;First of all, we perform the first stage of analysis, using the &lt;code&gt;by()&lt;/code&gt; function to analyse the data separately for each site. In each site, we fit a mixed model, where the genotype is fixed, while the replicates and the incomplete blocks within replicates are random effects. Of course, this coding works because the experimental design is the same at all sites, while it should be modified in other cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First stage
model.1step &amp;lt;- by(dataset, dataset$site,
                  function(df) lme(yield ~ genotype - 1, 
             random = ~1|rep/block, 
             data = df) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we use the function &lt;code&gt;lapply()&lt;/code&gt; to get the variance components. The results are similar to those obtained in one-stage analysis (see also Damesa et al., 2017, Table 1)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the variance components
lapply(model.1step, VarCorr)
## $`1`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 0.1003720    0.3168153
## block =     pdLogChol(1)          
## (Intercept) 0.2505444    0.5005441
## Residual    1.2361933    1.1118423
## 
## $`2`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 1.4012207    1.1837317
## block =     pdLogChol(1)          
## (Intercept) 0.4645211    0.6815579
## Residual    0.2020162    0.4494621
## 
## $`3`
##             Variance     StdDev      
## rep =       pdLogChol(1)             
## (Intercept) 2.457639e-10 1.567686e-05
## block =     pdLogChol(1)             
## (Intercept) 1.824486e-09 4.271400e-05
## Residual    1.054905e+00 1.027085e+00
## 
## $`4`
##             Variance     StdDev   
## rep =       pdLogChol(1)          
## (Intercept) 0.01412879   0.1188646
## block =     pdLogChol(1)          
## (Intercept) 0.07196842   0.2682693
## Residual    0.11262234   0.3355925&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can retrieve the genotypic means at all sites:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the means
sitmeans &amp;lt;- lapply(model.1step, 
                function(el) 
                  data.frame(emmeans(el, ~genotype)))
sitmeans &amp;lt;- do.call(rbind, sitmeans)
sitmeans$site &amp;lt;- factor(rep(1:4, each = 22))
head(sitmeans)
##     genotype   emmean        SE df lower.CL  upper.CL site
## 1.1        1 8.253672 0.7208426 12 6.683091  9.824253    1
## 1.2        2 7.731118 0.7208426 12 6.160537  9.301699    1
## 1.3        3 7.249198 0.7208426 12 5.678617  8.819779    1
## 1.4        4 8.565262 0.7208426 12 6.994681 10.135843    1
## 1.5        5 8.560002 0.7208426 12 6.989421 10.130583    1
## 1.6        6 9.510255 0.7208426 12 7.939674 11.080836    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance-covariance matrix for genotype means is obtained, for each site, by using the &lt;code&gt;vcov()&lt;/code&gt; function. Afterwords, we build a block diagonal matrix using the four variance-covariance matrices as the building blocks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the vcov matrices
Omega &amp;lt;- lapply(model.1step, vcov)
Omega &amp;lt;- Matrix::bdiag(Omega)
round(Omega[1:8, 1:8], 3)
## 8 x 8 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                                                     
## [1,] 0.520 0.061 0.037 0.034 0.033 0.035 0.034 0.033
## [2,] 0.061 0.520 0.061 0.037 0.033 0.034 0.033 0.033
## [3,] 0.037 0.061 0.520 0.061 0.033 0.033 0.034 0.033
## [4,] 0.034 0.037 0.061 0.520 0.033 0.033 0.033 0.033
## [5,] 0.033 0.033 0.033 0.033 0.520 0.035 0.034 0.061
## [6,] 0.035 0.034 0.033 0.033 0.035 0.520 0.061 0.034
## [7,] 0.034 0.033 0.034 0.033 0.034 0.061 0.520 0.033
## [8,] 0.033 0.033 0.033 0.033 0.061 0.034 0.033 0.520&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can proceed to the second stage, which can be performed by using the &lt;code&gt;rma.mv()&lt;/code&gt; function in the &lt;code&gt;metafor&lt;/code&gt; package, as shown in the box below. We see that we inject the variance-covariance matrix coming from the first stage into the second. That’s why this is a meta-analytic technique: we are behaving as if we had obtained the data from the first stage from literature!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Second stage (fully efficient)
mod.meta &amp;lt;- metafor::rma.mv(emmean, Omega, 
                            mods = ~ genotype - 1,
                            random = ~ 1|site/genotype,
                    data = sitmeans, method=&amp;quot;REML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this fit we get the remaining variance components (for the ‘sites’ and for the ‘sites x genotypes’ interaction) and the genotypic means, which correspond to those obtained from one-step analysis, apart from small differences relating to the optimisation method (see also Tables 1 and 2 in Damesa et al., 2017). That’s why Damesa and co-authors talk about &lt;strong&gt;fully efficient two-stage analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Variance components
mod.meta$sigma2
## [1] 10.4538773  0.1271925
head(mod.meta$beta)
##               [,1]
## genotype1 5.134780
## genotype2 5.509773
## genotype3 5.147052
## genotype4 4.593256
## genotype5 4.844761
## genotype6 4.691955&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A possible approximation to this fully-efficient method is also shown in Damesa et al. (2017) and consists of approximating the variance-covariance matrix of genotypic means (‘Omega’) by using a vector of weights, which can be obtained by taking the diagonal elements of the inverse of the ‘Omega’ matrix. To achieve these, we can use the R coding in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;siij &amp;lt;- diag(solve(Omega))
mod.meta2 &amp;lt;- metafor::rma.mv(emmean, 1/siij,
                            mods = ~ genotype - 1,
                            random = ~ 1|site/genotype,
                    data = sitmeans, method=&amp;quot;REML&amp;quot;) 
mod.meta2$sigma2
## [1] 10.422928  0.127908
head(mod.meta2$beta)
##               [,1]
## genotype1 5.112614
## genotype2 5.431455
## genotype3 5.151905
## genotype4 4.583911
## genotype5 4.811698
## genotype6 4.704518&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we have fully reproduced the results relating to the Example 1 in the paper we used as the reference for this post. Hope this was useful.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Damesa, T.M., Möhring, J., Worku, M., Piepho, H.-P., 2017. One Step at a Time: Stage-Wise Analysis of a Series of Experiments. Agronomy Journal 109, 845. &lt;a href=&#34;https://doi.org/10.2134/agronj2016.07.0395&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2134/agronj2016.07.0395&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Lenth R (2022). Emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.7.5-090001, &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pinheiro JC, Bates DM (2000). Mixed-Effects Models in S and S-PLUS.Springer, New York. &lt;a href=&#34;doi:10.1007/b98882&#34; class=&#34;uri&#34;&gt;doi:10.1007/b98882&lt;/a&gt;. &lt;a href=&#34;https://doi.org/10.1007/b98882&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/b98882&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. Journal of Statistical Software, 36(3), 1-48. &lt;a href=&#34;https://doi.org/10.18637/jss.v036.i03&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v036.i03&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Should I say &#39;&#39;there is no difference&#39;&#39; or &#39;&#39;the difference is not significant&#39;&#39;?</title>
      <link>https://www.statforbiology.com/2022/stat_general_plevels/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_general_plevels/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In a recent manuscript we wrote a sentence similar to the following: “&lt;em&gt;On average, the genotype A gave a yield of 12.4 tons per hectare, while the genotype B gave 10.6 tons per hectare and such a difference was not significant (P = 0.20)&lt;/em&gt;”. Perhaps I should point out that we were talking about maize yields… One of the reviewers complained that “&lt;em&gt;This is an example of expression having no place in a scientific paper&lt;/em&gt;” and that we should write: “&lt;em&gt;… no difference in yield was found between A and B (P = 0.20)&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;This is one of the cases where I cannot agree with the reviewer… and here is why.&lt;/p&gt;
&lt;p&gt;I think that we should make a clear distinction between the experiment and the ‘whole picture’. If we stick to the experiment and look at the 8 plots we harvested (yes, we had four replicated plots per each genotype, in a completely randomised design), we should conclude that the average yield WAS different and such a difference WAS equal to 1.8 tons per hectare. Any doubt about this? Therefore, it would seem to be perfectly legitimate for us to say that there was a difference.&lt;/p&gt;
&lt;p&gt;However, nobody wants to reach conclusions that are only valid for eights plots; the reason why we make experiments is that we always want to reach general conclusions! Therefore, we have to look at the ‘whole picture’, i.e. the two populations of all possible plots we could have inspected for the two genotypes in the very same environmental conditions. Can we say that, in general, the two genotypes (the two populations of plots) are different?&lt;/p&gt;
&lt;p&gt;Obviously, if we do not have (or do not intend to use) any other preliminary information about the two genotypes (no prior knowledge), we have to reach general conclusions based solely on our eight plots. What can we reasonably say about the two genotypes, looking at the eight data?&lt;/p&gt;
&lt;p&gt;In a ‘Fisherian’ context (you may know that Fisher was a very renown scientist, to whom we owe a great part of our approaches to experimental methods), we start by making a ‘null’ hypothesis, i.e. that the two genotypes are, indeed, the same genotype (thus, they have the same average yield). We can all understand that, even if the ‘null’ were absolutely true, growing this unique genotype in two groups of four independent plots would never lead to exactly the same average yield, due to the usual plot-to-plot variability. For example, in the box below, I have drawn two samples of 4 yields from the same population and found a difference of about 1 ton. When I have taken a second pair of samples, the difference was 1.5 tons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
sample1 &amp;lt;- rnorm(4, (12.4 + 10.6)/2, 2)
sample2 &amp;lt;- rnorm(4, (12.4 + 10.6)/2, 2)
mean(sample1) - mean(sample2)
## [1] -1.002351
# Second sampling
sample1 &amp;lt;- rnorm(4, (12.4 + 10.6)/2, 2)
sample2 &amp;lt;- rnorm(4, (12.4 + 10.6)/2, 2)
mean(sample1) - mean(sample2)
## [1] -1.533741&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we may wonder what happens if we take 100’000 pairs of samples? Let’s do a Monte Carlo simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff &amp;lt;- c()
for(i in 1:100000){
  sample1 &amp;lt;- rnorm(4, (12.4 + 10.6)/2, 2)
  sample2 &amp;lt;- rnorm(4, (12.4 + 10.6)/2, 2)
  diff[i] &amp;lt;- mean(sample1) - mean(sample2)
}
sum(diff &amp;gt; 1.8) + sum(diff &amp;lt; -1.8)
## [1] 20069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our experiment we observed a difference equal to 1.8 tons per hectare and Monte Carlo simulation shows that, taking two perfectly equal genotypes and making an experiment like ours, there is more than 20% probability (20,069 cases out of 100,000) that we observe a difference as high as 1.8 (in absolute value) or higher. We need to admit that such a probability is rather high; consequently, we need to conclude that our observation is compatible with the ‘null’ and there is no evidence to reject it. Therefore, we cannot conclude that the two genotypes are different. This is true for the eight plots, but it is not true in general; at least, such a conclusion is not supported by the data at hand (and we have to let the data speak!).&lt;/p&gt;
&lt;p&gt;But, can we say that there is no difference between between the two genotypes, as advocated by the reviewer? Let’s consider a possible alternative hypothesis, i.e. that the two genotypes are different and the difference is 1.8 tons. Monte Carlo simulation shows that there is a considerable amount of cases (49,146 out of 100,000) where a hypothetical experiment leads to differences that are equal to 1.8 or less, in absolute value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff &amp;lt;- c()
for(i in 1:100000){
  sample1 &amp;lt;- rnorm(4, 12.4, 2)
  sample2 &amp;lt;- rnorm(4, 10.6, 2)
  diff[i] &amp;lt;- mean(sample1) - mean(sample2)
}
sum(diff &amp;lt; 1.8 &amp;amp; diff &amp;gt; -1.8)
## [1] 49146&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, our experiment is not only compatible with the ‘null’, it is also compatible with the ‘alternative’ hypothesis, i.e. that the two genotypes are different.&lt;/p&gt;
&lt;p&gt;You may object that my reasoning is flawed; indeed, while there is only one possible ‘null’, there is an infinite amount of ‘alternatives’ to consider and we have no prior knowledge to favour one of those, as I did before. You are right and, in fact, we always work with the ‘null’ and test whether we can reject it; if we fail, we have to accept it. However, my point is that, even though we accept the null, we cannot say that this is true, because there is no way to be sure about this. Particularly when the sample size is small or the effect under study is highly variable.&lt;/p&gt;
&lt;p&gt;As the bottom line, I strongly encourage that a cautious language is used: the absence of evidence should never be taken as the evidence of absence (Altman and Bland, 1995)!&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;p&gt;Altman, D.G., Bland, J.M., 1995. Statistics notes: Absence of evidence is not evidence of absence. BMJ 311, 485.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R (a tutorial). Part 6</title>
      <link>https://www.statforbiology.com/2022/stat_drcte_6-ht1step/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_drcte_6-ht1step/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;fitting-time-to-event-models-with-environmental-covariates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting time-to-event models with environmental covariates&lt;/h1&gt;
&lt;p&gt;In the previous post we have shown that time-to-event curves (e.g., germination or emergence curves) can be used to describe the time course of germinations/emergences for a seed lot (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;this post&lt;/a&gt;). We have also seen that the effects of experimental factors on seed germination can be accounted for by coding a different time-to-event curve for each factor level (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;this post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, we would like to consider the environmental variables, that are, perhaps, the most important factors to trigger germination/emergence. For example, let’s consider either humidity content in the substrate, or temperature, or oxygen availability; it is clear that these variables play a fundamental role in determining germination extent and velocity and, therefore, they are very much studied by seed scientists. In principle, germination assays with environmental variables are straightforward to set up: several Petri dishes are submitted to different environmental conditions and germinations are inspected over time. What is the best method to analyse the resulting data and retrieve some important parameters, such as threshold temperatures (base, optimal or ceiling temperature) or base water potential?&lt;/p&gt;
&lt;p&gt;It is important to anticipate that most environmental variables can be expressed on a quantitative scale; obviously when we make an experiment we are forced into selecting a subset of all possible, e.g., temperatures, such as 15, 20, 30°C, but that does not mean that we are not interested to what happens at, e.g., 18 or 22°C. From this point of view, quantitative variables are very different from qualitative variables, such as the different plant species that we have compared in &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;our previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post we will see an example of how we can account for the effects of water content in the substrate and include it in our time-to-event models. Of course, the same approach can be followed also with other types of environmental variables and, more generally, quantitative variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-hydro-time-to-event-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting hydro-time-to-event models&lt;/h1&gt;
&lt;p&gt;Let’s consider the following example: the germination of rapeseed (&lt;em&gt;Brassica napus&lt;/em&gt; L. var. &lt;em&gt;oleifera&lt;/em&gt;, cv. Excalibur) was tested at fourteen different water potential levels (0, -0.03, -0.15, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1, -1.1, -1.2, -1.5 MPa), which were created by using a polyethylene glycol solution (PEG 6000). For each water potential level, three replicated Petri dishes with 50 seeds each were incubated at 20°C. Germinated seeds were counted every 2-3 days for 14 days and they were removed from the dishes after germination.&lt;/p&gt;
&lt;p&gt;The dataset was published by Pace et al. (2012) and it is available as &lt;code&gt;rape&lt;/code&gt; in the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, which needs to be installed from github (see below). Furthermore, the package &lt;code&gt;drcte&lt;/code&gt; is necessary to fit time-to-event models and it should also be installed from gitHub. The following code loads the necessary packages, loads the dataset &lt;code&gt;rape&lt;/code&gt; and shows the first six lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)
# install_github(&amp;quot;OnofriAndreaPG/drcte&amp;quot;)
library(drcSeedGerm)
library(drcte)
library(ggplot2)
data(rape)
head(rape)
##   Psi Dish timeBef timeAf nSeeds nCum propCum
## 1   0    1       0      3     49   49    0.98
## 2   0    1       3      4      0   49    0.98
## 3   0    1       4      5      0   49    0.98
## 4   0    1       5      7      0   49    0.98
## 5   0    1       7     10      0   49    0.98
## 6   0    1      10     14      0   49    0.98&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above data.frame, ‘timeAf’ represents the moment when germinated seeds were counted, while ’timeBef’ represents the previous inspection time (or the beginning of the assay). The column ’nSeeds’ is the number of seeds that germinated during the time interval between ‘timeBef’ and ‘timeAf. The ’propCum’ column contains the cumulative proportions of germinated seeds and it is not necessary for time-to-event model fitting, although we can use it for plotting purposes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rape, aes(timeAf, propCum)) +
  geom_point() +
  facet_wrap(~Psi) +
  scale_x_continuous(name = &amp;quot;Time (d)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_6-HT1step_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The germination time-course is strongly affected by the water potential in the substrate, as this determines the ability of seeds to absorb water and, consequently, trigger the germination and emergence processes. Therefore, our obvious interest is to understand how the environmental factor affects the time-course of germination.&lt;/p&gt;
&lt;p&gt;We have shown that a parametric time-to-event curve is defined as a cumulative probability function (&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;), with three parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \Phi \left( b, d, e \right)\]&lt;/span&gt;
Considering &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;our previous post&lt;/a&gt;, the most obvious extension of the above model is to allow for different &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; value for each water potential level:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi_i) = \Phi \left( b_i, d_i, e_i \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first problem is that, for some water potential levels, germination did not occur and, for other levels, it occurred very quickly, so that no time-course of events could be observed (e.g., see the graph at 0 or -0.03 MPa). We say that we have ‘degenerated’ time-to-event curves.&lt;/p&gt;
&lt;p&gt;If we fit those curves by using the ‘curveid’ argument, we are forced into fitting the same time-to-event model to all water potential levels (as shown in our previous post), and, therefore, the presence of degenerated curves provokes an error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Not run
# mod0 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, data = rape,
#              curveid = Psi, fct = loglogistic())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This problem can be circumvented by using the &lt;code&gt;separate = TRUE&lt;/code&gt; argument; in this case, the different curves are fitted independent of one another and we are not tied to fitting the same model for all water potential levels. Errors are raised when trying to fit parametric time-to-event models, but they do not stop the execution in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, data = rape,
             curveid = Psi, fct = loglogistic(),
             separate = TRUE)
## Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
##   non-finite finite-difference value [3]
## Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
##   non-finite value supplied by optim
## Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
##   non-finite value supplied by optim
## Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
##   non-finite value supplied by optim
coef(mod1)
##          b:-1          d:-1          e:-1        b:-0.9        d:-0.9 
##  6.439202e+01  4.666194e-02  8.364749e+00  5.832909e+00  5.501028e-01 
##        e:-0.9        b:-0.8        d:-0.8        e:-0.8        b:-0.7 
##  5.870221e+00  3.732950e+00  8.788932e-01  4.471057e+00  3.599409e+00 
##        d:-0.7        e:-0.7        b:-0.6        d:-0.6        e:-0.6 
##  9.359289e-01  2.730251e+00 -1.439224e+00 -2.343929e+03  7.160646e-01 
##        b:-0.5        d:-0.5        e:-0.5       b:-0.15       d:-0.15 
## -6.019417e-01 -1.486343e+03  6.107110e-02 -5.594961e-01 -2.775981e+03 
##       e:-0.15 
##  2.952136e-02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In particular, for the cases where a time-course of events cannot be estimated, the &lt;code&gt;drmte()&lt;/code&gt; function resorts to fitting a simpler model, where only the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter is estimated (that is the maximum fraction of germinated seeds). In the box above, we can see the estimated parameters but no standard errors, which can be obtained by using the &lt;code&gt;summary()&lt;/code&gt; method, although there are some statistical issues that we will consider in a following post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-better-modelling-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A better modelling approach&lt;/h1&gt;
&lt;p&gt;The previous approach is clearly sub-optimal. First of all, the different water potential levels are assumed as independent, with no ordering and distances. In other words, we have a time-to-event curve for, e.g. -0.9 MPa and -0.8 MPa, but we have no information about the time-to-event curve for any water potential levels in between. Furthermore, we have no estimates of some relevant hydro-time parameters, such as the &lt;em&gt;base water potential&lt;/em&gt;, that is fundamental to predict the germination/emergence in field conditions.&lt;/p&gt;
&lt;p&gt;In order to account for the very nature of the water potential variable, we could code a time-to-event model where the three parameters are continuous functions of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi) = \Phi \left( b(\Psi), d(\Psi), e(\Psi) \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We followed such an approach in a relatively recent publication (Onofri et al., 2018) and we also spoke about this in a &lt;a href=&#34;https://www.statforbiology.com/2020/stat_seedgermination_ht1step/&#34;&gt;recent post&lt;/a&gt;. In detail, we considered a log-logistic cumulative distribution function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{ d }{1 + \exp \left\{ b \left[ \log(t) - \log( e ) \right] \right\} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum germinated proportion. Considering that the germination rate is the inverse of germination time, we replaced &lt;span class=&#34;math inline&#34;&gt;\(e = 1/GR_{50}\)&lt;/span&gt; and wrote the three parameters as functions of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi) = \frac{ d(\Psi) }{1 + \exp \left\{ b(\Psi) \left[ \log(t) - \log(1 / \left[ GR_{50}(\Psi) \right] ) \right] \right\} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\begin{array}{l}
GR_{50}(\Psi) = \textrm{max} \left( \frac{\Psi - \Psi_{b}}{\theta_H}; 0 \right)\\
d(\Psi ) = \textrm{max} \left\{ G \, \left[ 1 - \exp \left( \frac{ \Psi - \Psi_b }{\sigma_{\Psi_b}} \right) \right]; 0 \right\}\\
b(\Psi) = b
\end{array}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b}\)&lt;/span&gt;, that is the median base water potential in the seed lot (in &lt;em&gt;MPa&lt;/em&gt;),&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt;, that is the hydro-time constant (in &lt;em&gt;MPa day&lt;/em&gt; or &lt;em&gt;MPa hour&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b}\)&lt;/span&gt;, that represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;, that is the germinable fraction, accounting for the fact that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; may not reach 1, regardless of time and water potential.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; (slope parameter) that is assumed to be constant and independent on &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the end, our hydro-time model is composed by four sub-models:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a cumulative probability function (log-logistic, in our example), based on the three parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = 1/GR50\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(GR50\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, although, this was indeed a simple identity model &lt;span class=&#34;math inline&#34;&gt;\(b(\Psi) = b\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This hydro-time-to-event model was implemented in R as the &lt;code&gt;HTE1()&lt;/code&gt; function, and it is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, together with the appropriate self-starting routine. It can be fitted by using the &lt;code&gt;drmte()&lt;/code&gt; function in the &lt;code&gt;drcte&lt;/code&gt; package and the &lt;code&gt;coef()&lt;/code&gt; function can be used to retrieve the parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modHTE &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi, 
                data = rape, fct = HTE1())
coef(modHTE)
##         G:(Intercept)      Psib:(Intercept) sigmaPsib:(Intercept) 
##             0.9577918            -1.0397239             0.1108891 
##    thetaH:(Intercept)         b:(Intercept) 
##             0.9061385             4.0273963&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said before, we are also interested in standard errors for model parameters; we will address this issue in another post. It is important to note that this model gives us the ability of predicting germination at any water potential levels and it is not restrained to the values that we included in the experimental design. Furthermore, we have reliable estimates of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt;, which we can use for prediction purposes in field conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-modelling-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Another modelling approach&lt;/h1&gt;
&lt;p&gt;Another type of hydro-time model was proposed by Bradford (2002) and later extended by Mesgaran et al., (2013). These authors, instead of modifying a traditional log-logistic distribution to include the environmental covariate, wrote a totally new cumulative distribution function, based on theoretical underpinnings relating to the distribution of base water potential within a seed population. Their model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \Phi \left\{ \frac{\Psi - (\theta_H / t) -\Psi_b }{\sigma_{\Psi_b}} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is a gaussian cumulative distribution function for base water potential. More information on how this model can be obtained from the original papers; it is, however, important to highlight that it is assumed that base water potential changes from seed to seeds within the population, according to a gaussian distribution function. The cumulative distribution function of event times is indirectly modelled, but it is not, in itself, gaussian (you see that &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is at the denominator).&lt;/p&gt;
&lt;p&gt;Mesgaran et al (2013) suggested that &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; may not be gaussian and proposed several alternatives, so that, in all, we have six possible hydro-time-to-event models, which we have implemented within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;gaussian (function &lt;code&gt;HTnorm()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;logistic (function &lt;code&gt;HTL()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Gumbel (function &lt;code&gt;HTG()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;log-logistic (function &lt;code&gt;HTLL()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Weibull (Type I) (function &lt;code&gt;HTW1()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Weibull (Type II) (function &lt;code&gt;HTW2()&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These equations are given at the end of this post. The code to fit those models is given below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi, 
              data = rape, fct = HTnorm())
mod2 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
              data = rape, fct = HTL())
mod3 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
              data = rape, fct = HTG())
mod4 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTLL())
mod5 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTW1())
mod6 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTW2())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the best model for this dataset? Let’s use the Akaike’s Information Criterion (AIC: the lowest, the best) to decide; we see that &lt;code&gt;modHTE&lt;/code&gt; was the best fitting one, followed by &lt;code&gt;mod4&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod1, mod2, mod3, mod4, mod5, mod6, modHTE)
##         df      AIC
## mod1   291 3516.914
## mod2   291 3300.824
## mod3   291 3097.775
## mod4   290 2886.608
## mod5   290 2889.306
## mod6   290 3009.023
## modHTE 289 2832.481&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important not to neglect a graphical inspection of model fit. The &lt;code&gt;plot()&lt;/code&gt; method does not work with time-to-event curves with additional covariates (apart from time). However, we can retrieve the fitted data by using the &lt;code&gt;plotData()&lt;/code&gt; function and use those predictions within the &lt;code&gt;ggplot()&lt;/code&gt; function. The box below shows the appropriate coding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
tab &amp;lt;- plotData(modHTE)

ggplot() +
  geom_point(data = rape, mapping = aes(x = timeAf, y = propCum),
            col = &amp;quot;red&amp;quot;) +
  geom_line(data = tab$plotFits, mapping = aes(x = timeAf, y = CDF)) +
  facet_wrap(~ Psi) +
  scale_x_continuous(name = &amp;quot;Time (d)&amp;quot;) +
  scale_y_continuous(name = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_6-HT1step_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bradford, K.J., 2002. Applications of hydrothermal time to quantifying and modeling seed germination and dormancy. Weed Science 50, 248–260.&lt;/li&gt;
&lt;li&gt;Mesgaran, M.B., Mashhadi, H.R., Alizadeh, H., Hunt, J., Young, K.R., Cousens, R.D., 2013. Importance of distribution function selection for hydrothermal time models of seed germination. Weed Research 53, 89–101. &lt;a href=&#34;https://doi.org/10.1111/wre.12008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Neve, P., Cousens, R.D., 2014. Experimental design and parameter estimation for threshold models in seed germination. Weed Research 54, 425–435. &lt;a href=&#34;https://doi.org/10.1111/wre.12095&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12095&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pace, R., Benincasa, P., Ghanem, M.E., Quinet, M., Lutts, S., 2012. Germination of untreated and primed seeds in rapeseed (brassica napus var oleifera del.) under salinity and low matric potential. Experimental Agriculture 48, 238–251.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C. (2019) Dose-Response Analysis Using R CRC Press.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;further-detail&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further detail&lt;/h1&gt;
&lt;p&gt;Let us conclude this page by giving some detail on all other models in Mesgaran et al (2913; slightly reparameterised). In same cases &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; has been replaced by &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is the location parameter of the cumulative distribution function of base water potential, but it is not the median value. On the other hand, &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; is the shifting parameter for all logarithm based distributions; indeed, logarithm based distribution are only defined for strictly positive variables, while we know that water potential usually assumes negative values. The shifting parameters is used to shift the cumulative distribution function to the right, so that negative values are allowed.&lt;/p&gt;
&lt;div id=&#34;htl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTL()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \frac{1}{1 + exp \left[ - \frac{  \Psi  - \left( \theta _H/t \right) - \Psi_{b} } {\sigma}  \right] }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htg&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTG()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \exp \left\{ { - \exp \left[ { - \left( {\frac{{\Psi - (\theta _H / t ) - \mu }}{\sigma }} \right)} \right]} \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htll&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTLL()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \frac{1}{1 + \exp \left\{ \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b} + \delta)  }{\sigma}\right\} }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htw1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTW1()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = exp \left\{ - \exp \left[ - \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b} + \delta)  }{\sigma}\right] \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htw2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTW2()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = 1 - exp \left\{ - \exp \left[ \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b} + \delta)  }{\sigma}\right] \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R (a tutorial). Part 7</title>
      <link>https://www.statforbiology.com/2022/stat_drcte_7-summary/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_drcte_7-summary/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts expand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;exploring-the-results-of-a-time-to-event-fit-model-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring the results of a time-to-event fit: model parameters&lt;/h1&gt;
&lt;p&gt;In the previous post we have shown that time-to-event curves (e.g., germination or emergence curves) can be used to describe the time course of germinations/emergences for a seed lot (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;this post&lt;/a&gt;). We have also seen that the effects of experimental factors on seed germination can be accounted for by coding a different time-to-event curve for each factor level (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;this post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Once we have fit a time-to-event model, we are usually interested in exploring the results, to get all possible information from the fitted model. If we have fitted a parametric model, the value of the estimated parameters is usually of extreme interest, as it gives information on the main traits of germination/emergence (e.g., capability, speed and uniformity).&lt;/p&gt;
&lt;p&gt;For example, let’s consider the hydro-time model we have fitted in our previous post &lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_6-ht1step/&#34;&gt;at this link&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \Phi \left\{ \frac{\Psi - (\theta_H / t) -\Psi_b }{\sigma_{\Psi_b}} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is the cumulative proportion of germinated seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and water potential &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is a gaussian cumulative distribution function for base water potential, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b}\)&lt;/span&gt; is the median base water potential in the seed lot (in MPa), &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt; is the hydro-time constant (in MPa day or MPa hour) and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b}\)&lt;/span&gt; represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population. Clearly, these parameters have a clear biological meaning and getting to know about their value represents the reason why we have fitted such a model. The box below shows the code we used in our previous post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drcSeedGerm)
library(drcte)
data(rape)
modHTE &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi, 
                data = rape, fct = HTnorm())
coef(modHTE)
##    ThetaH:(Intercept)    Psib50:(Intercept) sigmaPsib:(Intercept) 
##             0.7510691            -0.9069810             0.2369954&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, we got parameter estimates, but we are not happy with this. We also need standard errors, to present along with estimates in our papers. The easiest way to obtain parameters and their standard errors altogether is to use the &lt;code&gt;summary()&lt;/code&gt; method for ‘drcte’ objects:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(modHTE)
## 
## Model fitted: Hydrotime model with normal distribution of Psib (Bradford et al., 2002)
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##                         Estimate Std. Error t-value   p-value    
## ThetaH:(Intercept)     0.7510691  0.0394604  19.034 &amp;lt; 2.2e-16 ***
## Psib50:(Intercept)    -0.9069810  0.0118081 -76.810 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.2369954  0.0071406  33.190 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, the above standard errors are not correct: indeed, they are obtained assuming that the observational units (i.e., the seeds) are independent, while they are clustered within randomisation units (Petri dishes, in this case). Consequently, seeds in the same Petri dish are more similar than seeds in different Petri dishes (there is intra-class correlation, we say). How can we obtain standard errors that account for such lack of independence?&lt;/p&gt;
&lt;p&gt;If we look at the literature about survival analysis (that is where we borrowed our methods from), we can see that cluster robust sandwich estimators of standard errors have proven useful and reliable (Yu and Peng, 2008). Therefore, we have implemented them in ‘drcte’; the ‘units’ argument in the &lt;code&gt;summary()&lt;/code&gt; method can be used to provide a variable for the Petri dishes and calculate cluster-robust SEs, by way of the facilities provided in the ‘sandwich’ package (Zeileis et al. 2020).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(modHTE, robust = T, units = Dish)
## 
## Model fitted: Hydrotime model with normal distribution of Psib (Bradford et al., 2002)
## 
## Robust estimation: Cluster robust sandwich SEs 
## 
## Parameter estimates:
## 
##                        Estimate Std. Error  t value  Pr(&amp;gt;|t|)    
## ThetaH:(Intercept)     0.751069   0.131968   5.6913 3.075e-08 ***
## Psib50:(Intercept)    -0.906981   0.039530 -22.9444 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.236995   0.031309   7.5696 4.974e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the difference between ‘naive’ and cluster robust SEs is remarkable.&lt;/p&gt;
&lt;p&gt;There might be other methods to obtain cluster robust standard errors (e.g., jackknife and bootstrap methods) and we are looking for ways to implement them in a reliable way. So far, we recommend that you make sure that your standard errors for model parameters do not neglect the clustering of seeds within randomisation units (petri dishes, pots, boxes or plots).&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Yu, B., Peng, Y., 2008. Mixture cure models for multivariate survival data. Computational Statistics and Data Analysis 52, 1524–1532.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M., &amp;amp; Ritz, C. (2022). A unified framework for the analysis of germination, emergence, and other time-to-event data in weed science. Weed Science, 1-13. &lt;a href=&#34;doi:10.1017/wsc.2022.8&#34; class=&#34;uri&#34;&gt;doi:10.1017/wsc.2022.8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zeileis, A., Köll, S., Graham, N., 2020. Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R. J. Stat. Soft. 95. &lt;a href=&#34;https://doi.org/10.18637/jss.v095.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v095.i01&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R (a tutorial). Part 8</title>
      <link>https://www.statforbiology.com/2022/stat_drcte_8-predict/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_drcte_8-predict/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts expand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;predictions-from-a-parametric-time-to-event-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predictions from a parametric time-to-event model&lt;/h1&gt;
&lt;p&gt;In previous posts we have shown that time-to-event models (e.g., germination or emergence models) can be used to describe the time course of germinations/emergences for a seed lot (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;this post&lt;/a&gt;) or for several seed lots, submitted to different experimental treatments (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;this post&lt;/a&gt;). We have seen that fitted models can be used to extract information of biological relevance (&lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_7-summary/&#34;&gt;this post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Another key aspect is to use a fitted model to make predictions: what fraction of germinated/emerged seeds will we find in, e.g., one/two weeks? And in one month? For example, let’s consider the hydro-time model we have fitted in some previous posts (the first one is &lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_6-ht1step/&#34;&gt;at this link&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \Phi \left\{ \frac{\Psi - (\theta_H / t) -\Psi_b }{\sigma_{\Psi_b}} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above model, &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is the cumulative proportion of germinated seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and water potential &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is a gaussian cumulative distribution function for base water potential, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b}\)&lt;/span&gt; is the median base water potential in the seed lot (in MPa), &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt; is the hydro-time constant (in MPa day or MPa hour) and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b}\)&lt;/span&gt; represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population.&lt;/p&gt;
&lt;p&gt;The code below can be used to fit the above model to the ‘rape’ dataset in the ‘drcSeedGerm’ package and retreive the estimated parameters, with robust standard errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drcSeedGerm)
library(drcte)
data(rape)
modHTE &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi, 
                data = rape, fct = HTnorm())
summary(modHTE, units = Dish)
## 
## Model fitted: Hydrotime model with normal distribution of Psib (Bradford et al., 2002)
## 
## Robust estimation: Cluster robust sandwich SEs 
## 
## Parameter estimates:
## 
##                        Estimate Std. Error  t value  Pr(&amp;gt;|t|)    
## ThetaH:(Intercept)     0.751069   0.131968   5.6913 3.075e-08 ***
## Psib50:(Intercept)    -0.906981   0.039530 -22.9444 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.236995   0.031309   7.5696 4.974e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we may wonder: if we have a seed lot with the above characteristics (&lt;span class=&#34;math inline&#34;&gt;\(\theta = 0.751\)&lt;/span&gt; MPa d, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b_{50}} = -0.907\)&lt;/span&gt; MPa and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b} = 0.237\)&lt;/span&gt;), what will the proportion of germinated seeds be, e.g., at 1, 3, 5, 7 days after water imbibition, when the base water potential in the substrate is, e.g., 0, -0.25 and -0.5 MPa? To predict this from the model object we can build a data frame with the values of predictors (see below the use of the &lt;code&gt;expand.grid()&lt;/code&gt; function) and use it as the ‘newdata’ argument to the &lt;code&gt;predict()&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newd &amp;lt;- expand.grid(time = c(1, 3, 5, 7), 
                    psi = c(0, -0.25, -0.5))
predict(modHTE, newdata = newd)
##    time   psi Prediction
## 1     1  0.00 0.74468884
## 2     3  0.00 0.99720253
## 3     5  0.00 0.99929640
## 4     7  0.00 0.99962993
## 5     1 -0.25 0.34568239
## 6     3 -0.25 0.95689600
## 7     5 -0.25 0.98375378
## 8     7 -0.25 0.98981312
## 9     1 -0.50 0.07326801
## 10    3 -0.50 0.74565419
## 11    5 -0.50 0.86069050
## 12    7 -0.50 0.89697826&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With time-to-event models, the ‘newdata’ argument takes a data frame, where the first column is always time and the succeeding columns, wherever needed, represent the environmental covariates, in the same order as they appear in the model definition. If several models have been simultaneously fitted by using the ‘curveid’ argument (not in this case, though), predictions are made for all models, always using the same ‘newdata’.&lt;/p&gt;
&lt;p&gt;We can also obtain standard errors and confidence intervals for the predictions, by adding the &lt;code&gt;se.fit = TRUE&lt;/code&gt; and &lt;code&gt;interval = TRUE&lt;/code&gt; arguments. We also recommend to add the &lt;code&gt;robust = T&lt;/code&gt; argument, so that we obtain robust standard errors, accounting for the clustering of seeds within Petri dishes (lack of independence). With parametric time-to-event models, robust standard errors are obtained by using a cluster-robust sandwich variance-covariance matrix (Zeileis et al. 2020); in this case, a clustering variable needs to be provided with the &lt;code&gt;units&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Naive standard errors and confidence intervals
predict(modHTE, newdata = newd, se.fit = T, interval = T)
##    time   psi Prediction           SE      Lower     Upper
## 1     1  0.00 0.74468884 0.0452433821 0.65601344 0.8333642
## 2     3  0.00 0.99720253 0.0008053309 0.99562411 0.9987809
## 3     5  0.00 0.99929640 0.0002384085 0.99882913 0.9997637
## 4     7  0.00 0.99962993 0.0001358778 0.99936362 0.9998963
## 5     1 -0.25 0.34568239 0.0488012921 0.25003362 0.4413312
## 6     3 -0.25 0.95689600 0.0060636345 0.94501149 0.9687805
## 7     5 -0.25 0.98375378 0.0027953655 0.97827496 0.9892326
## 8     7 -0.25 0.98981312 0.0019555787 0.98598025 0.9936460
## 9     1 -0.50 0.07326801 0.0182524957 0.03749378 0.1090422
## 10    3 -0.50 0.74565419 0.0143630866 0.71750306 0.7738053
## 11    5 -0.50 0.86069050 0.0098002073 0.84148245 0.8798986
## 12    7 -0.50 0.89697826 0.0084761895 0.88036524 0.9135913&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cluster robust standard errors and confidence intervals
predict(modHTE, newdata = newd, se.fit = T, interval = T,
        robust = T, units = Dish)
##    time   psi Prediction           SE      Lower     Upper
## 1     1  0.00 0.74468884 0.1450176755 0.46045941 1.0289183
## 2     3  0.00 0.99720253 0.0035012373 0.99034023 1.0040648
## 3     5  0.00 0.99929640 0.0011070141 0.99712670 1.0014661
## 4     7  0.00 0.99962993 0.0006427237 0.99837022 1.0008897
## 5     1 -0.25 0.34568239 0.1576339911 0.03672545 0.6546393
## 6     3 -0.25 0.95689600 0.0251911861 0.90752218 1.0062698
## 7     5 -0.25 0.98375378 0.0129567170 0.95835908 1.0091485
## 8     7 -0.25 0.98981312 0.0093141326 0.97155775 1.0080685
## 9     1 -0.50 0.07326801 0.0622953823 0.00000000 0.1953647
## 10    3 -0.50 0.74565419 0.0498559815 0.64793826 0.8433701
## 11    5 -0.50 0.86069050 0.0424570614 0.77747619 0.9439048
## 12    7 -0.50 0.89697826 0.0388178876 0.82089660 0.9730599&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are currently studying a way to avoid that confidence intervals return unrealistic predictions (see above some values that are higher than 1). We may note that cluster robust standard errors are higher than naive standard errors: the seed in the same Petri dish are correlated and, thus, they do not contribute full information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions-from-non-parametric-time-to-event-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predictions from non-parametric time-to-event models&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;predict()&lt;/code&gt; method can also be used to make predictions from NPMLE and KDE fits. In this case, no environmental covariates are admissible and, therefore, we can provide ‘newdata’ as a vector of times to make predictions. In the code below we fit the NPMLE of a time-to-event model to four species of the genus &lt;em&gt;Verbascum&lt;/em&gt;, for which the data are available as the ‘verbascum’ data frame. We also make predictions relating to the proportion of germinated seeds at 1, 3, 5, and 7 days from water imbibition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(verbascum)
mod &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, fct = NPMLE(),
             curveid = Species, data = verbascum)

# Define the values for predictions
newd &amp;lt;- c(1, 3, 5, 7)
predict(mod, newdata = newd, se.fit = T, interval = T,
        robust = T, units = Dish)
##      Species newdata Prediction         SE     Lower     Upper
## 1   arcturus       1       0.00 0.00000000 0.0000000 0.0000000
## 2   arcturus       3       0.00 0.00000000 0.0000000 0.0000000
## 3   arcturus       5       0.00 0.00000000 0.0000000 0.0000000
## 4   arcturus       7       0.00 0.00000000 0.0000000 0.0000000
## 5  blattaria       1       0.00 0.00000000 0.0000000 0.0000000
## 6  blattaria       3       0.09 0.04876853 0.0100000 0.1861875
## 7  blattaria       5       0.73 0.06088854 0.6100000 0.8300000
## 8  blattaria       7       0.80 0.05475904 0.6900000 0.9000000
## 9   creticum       1       0.00 0.00000000 0.0000000 0.0000000
## 10  creticum       3       0.33 0.08704392 0.1738125 0.5123750
## 11  creticum       5       0.97 0.02327631 0.9200000 1.0000000
## 12  creticum       7       0.97 0.02327631 0.9200000 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standard errors are estimated by using a resampling (bootstrap) approach, that is performed at the group level, whenever a grouping variable is provided, by way of the ‘units’ argument (Davison and Hinkley, 1997).&lt;/p&gt;
&lt;p&gt;For KDE models, we can make predictions in the very same way, although we are still unsure about the most reliable way to obtain standard errors. For this reason, the use of the ‘predict’ method with this type of non-parametric models does not yet provide standard errors and confidence intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions-from-a-time-to-event-model-from-literature&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predictions from a time-to-event model from literature&lt;/h1&gt;
&lt;p&gt;In some cases, we do not have a fitted model, but we have some literature information. For example, we have seen a manuscript where the authors say that, for a certain species, emergences appeared to follow a log-logistic time-course with the following parameters: ‘b’ (the slope at inflection point) equal to -1, ‘d’ (maximum germinated proportion) equal to 0.83 and ‘e’ (median germination time for the germinated fraction) equal to 12.3. Considering that a log-logistic time-to-event model is represented as LL.3(), we can make predictions by using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(LL.3(), coefs = c(-1, 0.83, 12.3),
        newdata = c(1, 3, 5, 7, 10))
##   newdata prediction
## 1       1 0.06240602
## 2       3 0.16274510
## 3       5 0.23988439
## 4       7 0.30103627
## 5      10 0.37219731&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, stay tuned for other posts in this series.
Thank you for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Davison, A.C., Hinkley, D.V., 1997. Bootstrap methods and their application. Cambridge University Press, UK.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M., &amp;amp; Ritz, C. (2022). A unified framework for the analysis of germination, emergence, and other time-to-event data in weed science. Weed Science, 1-13. &lt;a href=&#34;doi:10.1017/wsc.2022.8&#34; class=&#34;uri&#34;&gt;doi:10.1017/wsc.2022.8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zeileis, A., Köll, S., Graham, N., 2020. Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R. J. Stat. Soft. 95. &lt;a href=&#34;https://doi.org/10.18637/jss.v095.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v095.i01&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R (a tutorial). Part 9</title>
      <link>https://www.statforbiology.com/2022/stat_drcte_9-quantiles/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2022/stat_drcte_9-quantiles/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts expand on a manuscript that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper. In order to work throughout this post, you need to install the ‘drcte’ and ‘drcSeedGerm’ packages, by using the code provided &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_1-intro/&#34;&gt;in this page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;quantiles-from-time-to-event-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantiles from time-to-event models&lt;/h1&gt;
&lt;p&gt;We have previously shown that time-to-event models (e.g., germination or emergence models) can be used to describe the time course of germinations/emergences for a seed lot (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;this post&lt;/a&gt;) or for several seed lots, submitted to different experimental treatments (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/&#34;&gt;this post&lt;/a&gt;). We have seen that fitted models can be used to extract information of biological relevance (&lt;a href=&#34;https://www.statforbiology.com/2022/stat_drcte_7-summary/&#34;&gt;this post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A time-to-event model is, indeed, a cumulative probability function for germination time and, therefore, we might be interested in finding the quantiles. But, what are the ‘quantiles’? It is a set of ‘cut-points’ that divide the distribution of event-times into a set of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; intervals with equal probability. For example the 100-quantiles, also named as the percentiles, divide the distribution of event-times into &lt;span class=&#34;math inline&#34;&gt;\(q = 100\)&lt;/span&gt; groups. Some of these cut-off points may be particularly relevant: for example the 50-th percentile corresponds to the time required to reach 50% germination (T50) and it is regarded as a good measure of germination velocity. Other common percentiles are the T10, or the T30, which are used to express the germination velocity for the quickest seeds in the lot.&lt;/p&gt;
&lt;p&gt;Extracting some relevant percentiles from the time-to-event curve is regarded as an important task, to sintetically describe the germination/emergence velocity of seed populations. To this aim, we have included the &lt;code&gt;quantile()&lt;/code&gt; method in the &lt;code&gt;drcte&lt;/code&gt; package, that addresses most of the peculiarities of seed germination/emergence data. In this post, we will show the usage of this function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;peculiarities-of-seed-science-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Peculiarities of seed science data&lt;/h1&gt;
&lt;p&gt;I know that you are looking forward to getting the quantiles for your time-to-event curve. Please, hang on for a while… we need to become aware of a couple of issues, that are specific to germination/emergence data and are not covered in literature for other types of time-to-event data (e.g., survival data).&lt;/p&gt;
&lt;div id=&#34;quantiles-and-restricted-quantiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantiles and ‘restricted’ quantiles&lt;/h2&gt;
&lt;p&gt;Due to the presence of the ungerminated/unemerged fraction, quantiles suffer from the intrinsic ambiguity that we could calculate them either for the whole sample, or for the germinated fraction. For example, let’s think that we have a seed lot where the maximum percentage of germination is 60% and thus 40% of seeds are dormant. How do we define the 50&lt;sup&gt;th&lt;/sup&gt; percentile?&lt;/p&gt;
&lt;p&gt;In general, we should consider the whole population, including the ungerminated fraction, where the event is not observed; accordingly, the, e.g., 50&lt;sup&gt;th&lt;/sup&gt; percentile (T50) should be defined as the time to 50% germination. Obviously, with such a definition the, e.g., T50 cannot be estimated when the maximum germinated fraction is lower than 50%.&lt;/p&gt;
&lt;p&gt;On the other hand, for certain applications, it might be ok to remove the ungerminated fraction prior to estimating the quantiles; in this case, for our example where the maximum germinated fracion is 60%, the T50 would be defined as the time to 30% germination, that is 50% of the maximum germinated fraction.&lt;/p&gt;
&lt;p&gt;Due to such an ambiguity, we should talk about quantiles and ‘restricted’ quantiles. The graph below should help clarify such a difference.&lt;/p&gt;
&lt;p&gt;As a general suggestion, we should never use restricted quantiles for seed germination/emergence studies, especially when the purpose is to make comparisons across treatment groups (Bradford, 2002; Keshtkar et al. 2021).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_9-Quantiles_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;germinationemergence-rates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Germination/emergence rates&lt;/h2&gt;
&lt;p&gt;The quantiles of germination times (e.g., T10, T30 or T50) are very common measures of germination velocity, although they may be rather counterintuitive, because a high germination time implies low velocity. Another common measure of velocity is the Germination Rate, that is the inverse of germination time (e.g., GR10 = 1/T10).&lt;/p&gt;
&lt;p&gt;The quantiles of germination rates (e.g., GR10, GR30, GR50…) represents the daily progress to germination for a given subpopulation and they are used as the basis for hydro-thermal-time modelling. Therefore, their determination for a seed lot is also very relevant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-quantiles-with-drcte&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting the quantiles with ‘drcte’&lt;/h1&gt;
&lt;div id=&#34;parametric-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parametric models&lt;/h2&gt;
&lt;p&gt;In a previous post we have used the code below to fit a log-logistic time-to-event model to the germination data for three species of the Verbascum genus:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drcte)
library(drcSeedGerm)
data(verbascum)
mod1 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, fct = LL.3(),
             curveid = Species, data = verbascum)
summary(mod1, units = Dish)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0
## 
## Robust estimation: Cluster robust sandwich SEs 
## 
## Parameter estimates:
## 
##               Estimate Std. Error  t value  Pr(&amp;gt;|t|)    
## b:arcturus   -9.930005   1.111135  -8.9368 4.286e-16 ***
## b:blattaria  -7.198025   0.578038 -12.4525 &amp;lt; 2.2e-16 ***
## b:creticum  -11.033749   0.943374 -11.6961 &amp;lt; 2.2e-16 ***
## d:arcturus    0.356648   0.047915   7.4433 3.715e-12 ***
## d:blattaria   0.840064   0.025593  32.8242 &amp;lt; 2.2e-16 ***
## d:creticum    0.969990   0.017290  56.1015 &amp;lt; 2.2e-16 ***
## e:arcturus   12.059189   0.486010  24.8126 &amp;lt; 2.2e-16 ***
## e:blattaria   4.031763   0.199741  20.1850 &amp;lt; 2.2e-16 ***
## e:creticum    3.200655   0.103161  31.0259 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It may be useful to rank the species in terms of their germination velocity and, to that purpose, we could estimate the times to 30% and 50% germination (T30 and T50), that are the 30&lt;sup&gt;th&lt;/sup&gt; and 50&lt;sup&gt;th&lt;/sup&gt; percentiles of the time-to-event distribution. We can use the &lt;code&gt;quantile()&lt;/code&gt; method, where the probability levels are passed in as the vector ‘probs’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(mod1, probs = c(0.3, 0.5))
## 
## Estimated quantiles
## 
##               Estimate Std. Error
## arcturus:30%   14.2634   0.992685
## arcturus:50%       NaN        NaN
## blattaria:30%   3.7156   0.106630
## blattaria:50%   4.2536   0.118066
## creticum:30%    2.9759   0.058279
## creticum:50%    3.2187   0.059480&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may note that the T50 is not estimable with &lt;em&gt;Verbascum arcturus&lt;/em&gt;, as the maximum germinated proportion (&lt;em&gt;d&lt;/em&gt; parameter for the time-to-event model above) is 0.36. Standard errors are obtained by using the delta method and they are invalid whenever the experimental units (seeds) are clustered within containers, such as the Petri dishes.&lt;/p&gt;
&lt;p&gt;For all these cases, we should prefer cluster-robust standard errors (Zeileis et al. 2020), which can be obtained by setting the extra argument ‘robust = TRUE’ and providing a clustering variable as the &lt;code&gt;units&lt;/code&gt; argument. By setting ‘interval = TRUE’ we can also obtain confidence intervals for the desired probability level (0.95, by default).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(mod1, probs = c(0.3, 0.5), robust = T,
         units = Dish,
         interval = T)
## 
## Estimated quantiles
## 
##               Estimate Std. Error   Lower   Upper
## arcturus:30%   14.2634   0.755294 12.7830 15.7437
## arcturus:50%       NaN        NaN     NaN     NaN
## blattaria:30%   3.7156   0.188941  3.3452  4.0859
## blattaria:50%   4.2536   0.209027  3.8439  4.6632
## creticum:30%    2.9759   0.085151  2.8090  3.1428
## creticum:50%    3.2187   0.104743  3.0134  3.4240&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may note that cluster robust standard errors are higher than naive standard errors: the seed in the same Petri dish are correlated and, thus, they do not contribute full information.&lt;/p&gt;
&lt;p&gt;If we are interested in the germination rates G30 and G50, we can set the argument ‘rate = T’, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(mod1, probs = c(0.3, 0.5), robust = T,
         units = Dish,
         interval = T, rate = T)
## 
## Estimated quantiles
## 
##               Estimate        SE    Lower    Upper
## arcturus:30%   0.07011 0.0037126 0.062833 0.077386
## arcturus:50%   0.00000 0.0000000 0.000000 0.000000
## blattaria:30%  0.26914 0.0136861 0.242315 0.295963
## blattaria:50%  0.23510 0.0115531 0.212454 0.257741
## creticum:30%   0.33604 0.0096153 0.317191 0.354882
## creticum:50%   0.31069 0.0101106 0.290872 0.330505&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parametric-models-with-environmental-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parametric models with environmental covariates&lt;/h2&gt;
&lt;p&gt;If we have fitted a hydro-thermal time model or other models with an environmental covariate, we can also use the &lt;code&gt;quantile()&lt;/code&gt; method, and pass a value for that covariate, as shown in the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Parametric model with environmental covariate
data(rape)
modTE &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTLL())
quantile(modTE, Psi = 0, 
         probs = c(0.05, 0.10, 0.15, 0.21), 
         restricted = F, rate = T, robust = T,
         interval = T)
## 
## Estimated quantiles
## 
##       Estimate      SE  Lower  Upper
## 1:5%    1.6630 0.29134 1.0919 2.2340
## 1:10%   1.6581 0.28523 1.0990 2.2171
## 1:15%   1.6546 0.28115 1.1036 2.2056
## 1:21%   1.6513 0.27745 1.1075 2.1951&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The environmental covariate only accepts a single value; in order to vectorise, we need to use the &lt;code&gt;lapply()&lt;/code&gt; function, as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is to vectorise on Psi
psiList &amp;lt;- seq(-1, 0, 0.25)
names(psiList) &amp;lt;- as.character(psiList)
lapply(psiList, 
       function(x) quantile(modTE, Psi = x, 
                            probs = c(0.05, 0.10, 0.15, 0.21),
                            restricted = F, rate = T,
                            interval = &amp;quot;delta&amp;quot;, 
                            units = rape$Dish, 
                            display = F))
## $`-1`
##        Estimate        SE       Lower     Upper
## 1:5%  0.1854204 0.1316218 -0.07255364 0.4433945
## 1:10% 0.1805588 0.1246436 -0.06373812 0.4248557
## 1:15% 0.1770742 0.1199609 -0.05804492 0.4121932
## 1:21% 0.1737531 0.1157066 -0.05302779 0.4005339
## 
## $`-0.75`
##        Estimate        SE     Lower     Upper
## 1:5%  0.5548033 0.1603098 0.2406019 0.8690047
## 1:10% 0.5499417 0.1532949 0.2494893 0.8503941
## 1:15% 0.5464571 0.1485831 0.2552394 0.8376747
## 1:21% 0.5431360 0.1442990 0.2603152 0.8259567
## 
## $`-0.5`
##        Estimate        SE     Lower    Upper
## 1:5%  0.9241862 0.1932594 0.5454048 1.302968
## 1:10% 0.9193246 0.1863806 0.5540254 1.284624
## 1:15% 0.9158400 0.1817648 0.5595875 1.272092
## 1:21% 0.9125189 0.1775713 0.5644855 1.260552
## 
## $`-0.25`
##       Estimate        SE     Lower    Upper
## 1:5%  1.293569 0.2286380 0.8454469 1.741691
## 1:10% 1.288708 0.2219286 0.8537354 1.723680
## 1:15% 1.285223 0.2174308 0.8590664 1.711379
## 1:21% 1.281902 0.2133475 0.8637483 1.700055
## 
## $`0`
##       Estimate        SE    Lower    Upper
## 1:5%  1.662952 0.2654765 1.142628 2.183276
## 1:10% 1.658090 0.2589271 1.150603 2.165578
## 1:15% 1.654606 0.2545391 1.155718 2.153493
## 1:21% 1.651285 0.2505575 1.160201 2.142368&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;non-parametric-npmle-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-parametric (NPMLE) models&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;quantile()&lt;/code&gt; method can also be used to make predictions from NPMLE fits. This method works by assuming that the events are evenly scattered within each inspection interval (‘interpolation method’). Inferences need to be explicitly requested by using setting ‘interval = T’; in this case, standard errors are estimated by using a resampling approach, that is performed at the group level, whenever a grouping variable is provided, by way of the ‘units’ argument (Davison and Hinkley, 1997).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf, fct = NPMLE(),
             curveid = Species, data = verbascum)
quantile(mod2, probs = c(0.3, 0.5), robust = T, units = Dish,
         interval = T, rate = T)
## 
## 
## 
## 
## Estimated quantiles
## (cluster robust bootstrap-based inference)
## 
##                 n       Mean   Median        SE   Lower   Upper
## arcturus.30%  999 0.04146680 0.067397 0.0367206 0.00000 0.08547
## arcturus.50%  999 0.00062217 0.000000 0.0065345 0.00000 0.00000
## blattaria.30% 999 0.26951972 0.268293 0.0175655 0.24096 0.30726
## blattaria.50% 999 0.23306179 0.230769 0.0127207 0.21393 0.26443
## creticum.30%  999 0.34503418 0.343750 0.0205478 0.31176 0.38462
## creticum.50%  999 0.30386151 0.302469 0.0131902 0.28255 0.33333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For KDE models, quantiles are calculated from the time-to-event curve by using a bisection method. However, we are still unsure about the most reliable way to obtain standard errors and, for this reason, inferences are not provided with this type of non-parametric models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;quantiles-and-effective-doses-ed&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantiles and Effective Doses (ED)&lt;/h1&gt;
&lt;p&gt;Quantiles for time-to-event data resamble Effective Doses (ED) for dose-response data, although we discourage the use of this latter term, as the time-to-event curve is a cumulative probability function based on time, that is not a ‘dose’ in strict terms. However, the concept is similar: we need to find the stimulus (time) that permits to obtain a certain response (germination/emergence). Considering such similarity, we decided to define the &lt;code&gt;ED()&lt;/code&gt; method for ‘drcte’ objects, that is compatible with the &lt;code&gt;ED()&lt;/code&gt; method for ‘drc’ objects. However, for seed germination/emergence data, we strongly favor the use of the &lt;code&gt;quantile()&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;It’s all for this topic; thank you for reading and, please, stay tuned for other posts in this series.&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bradford KJ (2002) Applications of hydrothermal time to quantifying and modeling seed germination and dormancy. Weed Sci 50:248–260&lt;/li&gt;
&lt;li&gt;Davison, A.C., Hinkley, D.V., 1997. Bootstrap methods and their application. Cambridge University Press, UK.&lt;/li&gt;
&lt;li&gt;Keshtkar E, Kudsk P, Mesgaran MB (2021) Perspective: Common errors in dose–response analysis and how to avoid them. Pest Manag Sci 77:2599–2608&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M., &amp;amp; Ritz, C. (2022). A unified framework for the analysis of germination, emergence, and other time-to-event data in weed science. Weed Science, 1-13. &lt;a href=&#34;doi:10.1017/wsc.2022.8&#34; class=&#34;uri&#34;&gt;doi:10.1017/wsc.2022.8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zeileis, A., Köll, S., Graham, N., 2020. Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R. J. Stat. Soft. 95. &lt;a href=&#34;https://doi.org/10.18637/jss.v095.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v095.i01&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R: a tutorial. Part 5</title>
      <link>https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/</link>
      <pubDate>Thu, 23 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_drcte_5-comparinglots/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;comparing-germinationemergence-for-several-seed-lots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing germination/emergence for several seed lots&lt;/h1&gt;
&lt;p&gt;Very often, seed scientists need to compare the germination behavior of different seed populations, e.g., different plant species, or one single plant species submitted to different temperatures, light conditions, priming treatments and so on. How should such a comparison be performed? For example, if we have submitted several seed samples to different environmental conditions, how do we decide whether the germinative response is affected by those environmental conditions?&lt;/p&gt;
&lt;p&gt;If the case that we have replicates for all experimental treatments, e.g. several Petri dishes, one possible line of attack is to take a summary measure for each dish and use that for further analyses, in a two-steps fashion. For example, we could take the total number of germinated seeds (Pmax) in each dish and use the resulting values to parameterise some sort of ANOVA-like model and test the significance of all effects.&lt;/p&gt;
&lt;p&gt;This method of data analysis is known as ‘response feature analysis’ and it may be regarded as ‘very traditional’, in the sense that it is often found in literature; although it is not wrong, it is, undoubtedly, sub-optimal. Indeed, two seed lots submitted to different treatments may show the same total number of germinated seeds, but a different velocity or uniformity of germination. In other words, if we only consider, e.g., the Pmax, we can answer the question: “do the seed lots differ for their germination capability?”, but not the more general question: “are the seed lots different?”.&lt;/p&gt;
&lt;p&gt;In order to answer this latter question, we should consider the entire time-course of germination and not only one single summary statistic. &lt;strong&gt;In other words, we need a method to fit and compare several time-to-event curves.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A motivating example&lt;/h1&gt;
&lt;p&gt;Let’s take a practical approach and start from an example: a few years ago, some colleagues of mine studied the germination behavior of a plant species (&lt;em&gt;Verbascum arcturus&lt;/em&gt;), in different conditions. In detail, they considered the factorial combination of two storage periods (LONG and SHORT storage) and two temperature regimes (FIX: constant daily temperature of 20°C; ALT: alternating daily temperature regime, with 25°C during daytime and 15°C during night time, with a 12:12h photoperiod). If you are a seed scientist and are interested in this experiment, you’ll find detail in Catara &lt;em&gt;et al.&lt;/em&gt; (2016).&lt;/p&gt;
&lt;p&gt;If you are not a seed scientist, you may wonder why my colleagues made such an assay; well, there is evidence that, for some plant species, the germination ability improves over time, after seed maturation. Therefore, if we take seeds and store them for different periods of time, there might be an effect on their germination traits. Likewise, there is also evidence that germinations may be hindered if seed is not submitted to daily temperature fluctuations. For seed scientists, all these mechanisms are very important, as they permit to trigger the germinations when the environmental conditions are favorable for seedling survival.&lt;/p&gt;
&lt;p&gt;Let’s go back to our assay: the experimental design consisted of four experimental ‘combinations’ (LONG-FIX, LONG-ALT, SHORT-FIX and SHORT-ALT) and four replicates for each combination. One replicate consisted of a Petri dish, that is a small plastic box containing humid blotting paper, with 25 seeds of &lt;em&gt;V. arcturus&lt;/em&gt;. In all, there were 16 Petri dishes, which were put in climatic chambers with the appropriate conditions. During the assay, my colleagues made daily inspections: germinated seeds were counted and removed from the dishes. Inspections were made for 15 days, until no more germinations could be observed.&lt;/p&gt;
&lt;p&gt;The original dataset is available on a web repository: let’s load and have a look at it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasetOr &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/TempStorage.csv&amp;quot;,
                      header = T, check.names = F)
head(datasetOr)
##   Dish Storage Temp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
## 1    1     Low  Fix 0 0 0 0 0 0 0 0 3  4  6  0  1  0  3
## 2    2     Low  Fix 0 0 0 0 1 0 0 0 2  7  2  3  0  5  1
## 3    3     Low  Fix 0 0 0 0 1 0 0 1 3  5  2  4  0  1  3
## 4    4     Low  Fix 0 0 0 0 1 0 3 0 0  3  1  1  0  4  4
## 5    5    High  Fix 0 0 0 0 0 0 0 0 1  2  5  4  2  3  0
## 6    6    High  Fix 0 0 0 0 0 0 0 0 2  2  7  8  1  2  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have one row per Petri dish; the first three columns show, respectively, the dish number, storage and temperature conditions. The next 15 columns represent the inspection times (from 1 to 15) and contain the counts of germinated seeds. The research question is:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is germination behavior affected by storage and temperature conditions?&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reshaping-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reshaping the data&lt;/h1&gt;
&lt;p&gt;The original dataset for our example is in a WIDE format and, as we have shown in a previous post (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_3-reshapingdata/&#34;&gt;go to this link&lt;/a&gt;), it is necessary to reshape it in LONG GROUPED format, by using the &lt;code&gt;melt_te()&lt;/code&gt; function in the ‘drcte’ package. As I said in my previous posts, this package can be installed from gitHub (see the code at: &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_1-intro/&#34;&gt;this link&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;melt_te()&lt;/code&gt; function needs to receive the columns storing the counts (‘count_cols = 4:18’), the columns storing the factor variables (‘treat_cols = c(“Dish”, “Storage”, “Temp”)’), a vector of monitoring times (‘monitimes = 1:15’) and a vector with the total number of seeds in each Petri dish (‘n.subjects = rep(25,16)’).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drcte)
dataset &amp;lt;- melt_te(datasetOr, count_cols = 4:18, 
                   treat_cols = c(&amp;quot;Dish&amp;quot;, &amp;quot;Storage&amp;quot;, &amp;quot;Temp&amp;quot;), 
                   monitimes = 1:15, n.subjects = rep(25, 16))
head(dataset, 16)
##      Dish Storage Temp Units timeBef timeAf count nCum propCum
## 1       1     Low  Fix     1       0      1     0    0    0.00
## 1.1     1     Low  Fix     1       1      2     0    0    0.00
## 1.2     1     Low  Fix     1       2      3     0    0    0.00
## 1.3     1     Low  Fix     1       3      4     0    0    0.00
## 1.4     1     Low  Fix     1       4      5     0    0    0.00
## 1.5     1     Low  Fix     1       5      6     0    0    0.00
## 1.6     1     Low  Fix     1       6      7     0    0    0.00
## 1.7     1     Low  Fix     1       7      8     0    0    0.00
## 1.8     1     Low  Fix     1       8      9     3    3    0.12
## 1.9     1     Low  Fix     1       9     10     4    7    0.28
## 1.10    1     Low  Fix     1      10     11     6   13    0.52
## 1.11    1     Low  Fix     1      11     12     0   13    0.52
## 1.12    1     Low  Fix     1      12     13     1   14    0.56
## 1.13    1     Low  Fix     1      13     14     0   14    0.56
## 1.14    1     Low  Fix     1      14     15     3   17    0.68
## 1.15    1     Low  Fix     1      15    Inf     8   NA      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the resulting data frame, the column ‘timeAf’ contains the time when the inspection was made and the column ‘count’ contains the number of germinated seeds (e.g. 9 seeds were counted at day 9). These seeds did not germinate exactly at day 9; they germinated within the interval between two inspections, that is between day 8 and day 9. The beginning of the interval is given as the variable ‘timeBef’. Apart from these three columns, we have the columns for the blocking factor (‘Dish’ and ‘Units’; this latter column is added by the R function, but it is not useful in this case) and for the treatment factors (‘Storage’ and ‘Temp’) plus two other additional columns (‘nCum’ and ‘propCum’), which we are not going to use for our analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-several-time-to-event-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting several time-to-event curves&lt;/h1&gt;
&lt;p&gt;In this case, we have reasons to believe that the germination time-course can be described by using a parametric log-logistic time-to-event model, which can be estimated by using either the &lt;code&gt;drm()&lt;/code&gt; function in the ‘drc’ package (Ritz et al., 2019) or the &lt;code&gt;drmte()&lt;/code&gt; function in the ‘drcte’ package (Onofri et al., submitted). In both cases, we have to include the experimental factor (‘curveid’ argument), to specify that we want to fit a different curve for each combination of storage and temperature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(1:3, .fns = factor))
mod1 &amp;lt;- drmte(count ~ timeBef + timeAf, fct = loglogistic(),
            data = dataset, 
            curveid = Temp:Storage)
summary(mod1)
## 
## Model fitted: Log-logistic distribution of event times
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##             Estimate Std. Error t-value   p-value    
## b:Fix:Low   4.974317   0.819632  6.0690 1.287e-09 ***
## b:Fix:High 11.476618   1.254439  9.1488 &amp;lt; 2.2e-16 ***
## b:Alt:Low   7.854558   5.239825  1.4990    0.1339    
## b:Alt:High 10.600439   1.014061 10.4534 &amp;lt; 2.2e-16 ***
## d:Fix:Low   0.998474   0.150189  6.6481 2.968e-11 ***
## d:Fix:High  0.861711   0.038987 22.1027 &amp;lt; 2.2e-16 ***
## d:Alt:Low   1.405930   5.607576  0.2507    0.8020    
## d:Alt:High  0.948113   0.024298 39.0208 &amp;lt; 2.2e-16 ***
## e:Fix:Low  12.009974   0.987039 12.1677 &amp;lt; 2.2e-16 ***
## e:Fix:High 10.906963   0.190532 57.2447 &amp;lt; 2.2e-16 ***
## e:Alt:Low  17.014976  13.214513  1.2876    0.1979    
## e:Alt:High  9.585255   0.166937 57.4183 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a previous post I have described a log-logistic time-to-event model (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;see here&lt;/a&gt;), which has a sygmoidal shape, with the three parameters &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. These parameters represent, respectively, the slope at inflection point, the higher asymptote (i.e. the maximum proportion of germinated seeds) and the median germination time. As we have four curves, we have a number of 12 estimated parameters.&lt;/p&gt;
&lt;p&gt;We see that the optimization routines returns an unreasonable value for the higher asymptote for one of the curves (&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; = 1.40 with Alt:Low); it is unreasonable because the maximum proportion of germinated seeds may not exceed 1. Therefore, we should refit the model by adding a constraint (&lt;span class=&#34;math inline&#34;&gt;\(d \le 1\)&lt;/span&gt;) for all the four curves. We can do so by setting the ‘upperl’ argument to 1 for the 5&lt;sup&gt;th&lt;/sup&gt; through 8&lt;sup&gt;th&lt;/sup&gt; estimands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- drmte(count ~ timeBef + timeAf, fct = loglogistic(),
            data = dataset, 
            curveid = Temp:Storage,
            upperl = c(NA, NA, NA, NA, 1, 1, 1, 1, NA, NA, NA, NA))
summary(mod1)
## 
## Model fitted: Log-logistic distribution of event times
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##             Estimate Std. Error t-value   p-value    
## b:Fix:Low   4.979665   0.818414  6.0845 1.168e-09 ***
## b:Fix:High 11.471625   1.254025  9.1478 &amp;lt; 2.2e-16 ***
## b:Alt:Low   8.408186   2.232712  3.7659 0.0001659 ***
## b:Alt:High 10.605807   1.014523 10.4540 &amp;lt; 2.2e-16 ***
## d:Fix:Low   0.997284   0.149235  6.6826 2.347e-11 ***
## d:Fix:High  0.861709   0.038999 22.0955 &amp;lt; 2.2e-16 ***
## d:Alt:Low   1.000000   0.881644  1.1342 0.2566920    
## d:Alt:High  0.948132   0.024282 39.0468 &amp;lt; 2.2e-16 ***
## e:Fix:Low  12.004534   0.981279 12.2336 &amp;lt; 2.2e-16 ***
## e:Fix:High 10.907108   0.190613 57.2213 &amp;lt; 2.2e-16 ***
## e:Alt:Low  15.903079   2.872327  5.5367 3.083e-08 ***
## e:Alt:High  9.585297   0.166873 57.4406 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
plot(mod1, log = &amp;quot;&amp;quot;, legendPos = c(6, 1), xlab = &amp;quot;Time&amp;quot;,
     ylab = &amp;quot;Cumulative probability of germination&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_5-ComparingLots_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the graph we see that there are visible differences between the fitted curves (the legend considers the curves in alphabetical order, i.e. 1: Fix-Low, 2: Fix-High, 3: Alt-Low and 4: Alt-High). Now, the question is: could we say that those differences are only due to chance (null hypothesis)?&lt;/p&gt;
&lt;p&gt;In order to make such a test, we could compare the logarithm of the likelihood for the fitted model with the logarithm of the likelihood for a ‘reduced’ model, where all curves have been pooled into one common curve for all treatment levels. The higher the log-likelihood difference, the lowest the probability that the null is true (Likelihood Ratio Test; LRT).&lt;/p&gt;
&lt;p&gt;A LRT for parametric models can be done with the &lt;code&gt;compCDF()&lt;/code&gt; function in the ‘drcte’ package, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compCDF(mod1)
## 
## 
## Likelihood ratio test
## NULL: time-to-event curves are equal
## 
## Observed LR value:  202.8052
## Degrees of freedom:  9
## P-value:  8.551556e-39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the LR value, that relates to the difference between the two log-likelihoods, is rather high and equal to 202; when the null is true, this LR value has an approximate Chi-square distribution; accordingly, we see that the P-level is very low and, thus, the null should be rejected.&lt;/p&gt;
&lt;p&gt;It general, the results of LRTs should be taken with care, particularly when the observed data are not independent from one another. Unfortunately, the lack of independence is an intrinsic characteristic of germination/emergence assays, where seeds are, most often, clustered within Petri dishes or other types of containers.&lt;/p&gt;
&lt;p&gt;In this example, we got a very low p-level, which leaves us rather confident that the difference between time-to-event curves is significant. More generally, instead of relying on a chi-square approximation, we should better use a grouped-permutation approach. This technique is based on the idea that, when the difference between curves is not significant, we should be able to freely permute the labels (treatment level) among Petri dishes (clustering units) and, consequently, build an empirical distribution for the LR statistic under the null (permutation distribution). The p-level is related to the proportion of LR values in the permutation distribution that are higher than the observed value (i.e.: 202.8)&lt;/p&gt;
&lt;p&gt;In the code below, we show how we can do this. The code is rather slow and, therefore, we should not use a very high number of permutation; the default is 199, that gives us a minimum p-value of 0.005. We see that we can confirm that the difference between curves is highly significant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compCDF(mod1, type = &amp;quot;permutation&amp;quot;, units = dataset$Dishes)
## 
## 
## Likelihood ratio test (permutation based)
## NULL: time-to-event curves are equal
## 
## Observed LR value:  202.8052
## Degrees of freedom:  9
## Naive P-value:  8.551556e-39
## Permutation P-value (B = 199): 0.005&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-non-parametric-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing non-parametric curves&lt;/h1&gt;
&lt;p&gt;In the above example, we have decided to fit a parametric time-to-event model. However, in other situations, we might be interested in fitting a non-parametric time-to-event model (NPMLE; &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;see here&lt;/a&gt;) and compare the curves for different treatment levels. In practice, nothing changes with respect to the approach I have shown above: first of all, we fit the NPMLEs with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNP &amp;lt;- drmte(count ~ timeBef + timeAf, fct = NPMLE(),
            data = dataset, 
            curveid = Temp:Storage)
plot(modNP, log = &amp;quot;&amp;quot;, legendPos = c(6, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_5-ComparingLots_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, we compare the non-parametric curves, in the very same fashion as above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compCDF(modNP, units = dataset$Units)
## Exact Wilcoxon test (permutation form)
## NULL: time-to-event curves are equal
## 
##      level   n   Scores
## 1  Fix:Low 100   2.5350
## 2 Fix:High 100   6.4600
## 3  Alt:Low 100 -51.2275
## 4 Alt:High 100  42.2325
## 
## Observed T value:  44.56
## Permutation P-value (B = 199): 0.005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, with NPMLEs, a different test statistic is used in the background; the default one is the Wilcoxon rank sum score, although two types of log-rank scores are also implemented (Sun’s scores and Finkelstein’s scores; see Fay and Shaw 2010). Permutation based P-values are calculated and reported.&lt;/p&gt;
&lt;p&gt;The approach is exactly the same with Kernel Density Estimators (KDE; &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/&#34;&gt;see here&lt;/a&gt;). First we fit the four curves curves, by including the experimental factor as the ‘curveid’ argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modKD &amp;lt;- drmte(count ~ timeBef + timeAf, fct = KDE(),
            data = dataset, 
            curveid = Temp:Storage)
plot(modKD, log = &amp;quot;&amp;quot;, legendPos = c(6, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_5-ComparingLots_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Second, we compare those curves, by using the &lt;code&gt;compCDF()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compCDF(modKD, units = dataset$Units)
## 
## Permutation test based on a Cramer-von-Mises type distance (Barreiro-Ures et al., 2019)
## NULL HYPOTHESIS: time-to-event curves are equal
## 
##      level   n         D
## 1  Fix:Low 100 0.0579661
## 2 Fix:High 100 0.1347005
## 3  Alt:Low 100 4.1378209
## 4 Alt:High 100 3.8581487
## 
## Observed D value =  2.0472
## P value =  0.005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, a Cramér‐von Mises type distance among curves is used (Barreiro-Ures et al., 2019), which, roughly speaking, is based on the integrated distance between the KDEs for the different groups and the pooled KDE for all groups. Permutation based P-values are also calculated and reported.&lt;/p&gt;
&lt;p&gt;We do hope that this post helps you test your hypotheses about seed germination/emergence in a reliable way.&lt;/p&gt;
&lt;p&gt;Thanks for reading and, please, accept my best:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.casaonofri.it/_Figures/greetings.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Barreiro‐Ures, D, M Francisco‐Fernández, R Cao, BB Fraguela, R Doallo, JL González‐Andújar, M Reyes (2019) Analysis of interval‐grouped data in weed science: The binnednp Rcpp package. Ecol Evol 9:10903–10915&lt;/li&gt;
&lt;li&gt;Catara, S., Cristaudo, A., Gualtieri, A., Galesi, R., Impelluso, C., Onofri, A., 2016. Threshold temperatures for seed germination in nine species of Verbascum (Scrophulariaceae). Seed Science Research 26, 30–46.&lt;/li&gt;
&lt;li&gt;Fay, MP, PA Shaw (2010) Exact and Asymptotic Weighted Logrank Tests for Interval Censored Data: The interval R Package. Journal of Statistical Software 36:1–34&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C. (2019) Dose-Response Analysis Using R CRC Press&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R: a tutorial. Part 4</title>
      <link>https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_drcte_4-time-to-eventcurves/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;time-to-event-models-for-seed-germinationemergence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Time-to-event models for seed germination/emergence&lt;/h1&gt;
&lt;p&gt;The individual seeds within a population do not germinate/emerge altogether at the same moment; this is an undisputed fact, resulting from seed-to-seed variability in germination/emergence time. Accordingly, the primary reason why we organise germination assays is to describe the progress to germination for the whole population, by using some appropriate time-to-event model.&lt;/p&gt;
&lt;p&gt;What is a time-to-event model? It is a model that describes the probability that the event (germination/emergence, in our case) occurs at any time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; or before that time:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t) = \Phi(T \le t)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, it may be easier to think that &lt;span class=&#34;math inline&#34;&gt;\(P(t)\)&lt;/span&gt; is the proportion of germinated/emerged seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. In statistical terms, &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is a Cumulative Distribution Function (CDF), with the following characteristics:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is constrained from 0 to +&lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the response &lt;span class=&#34;math inline&#34;&gt;\(P(t)\)&lt;/span&gt; is constrained from 0 to 1&lt;/li&gt;
&lt;li&gt;the response &lt;span class=&#34;math inline&#34;&gt;\(P(t)\)&lt;/span&gt; is monotonically increasing&lt;/li&gt;
&lt;li&gt;due to the possible presence of a final fraction of individuals without the event (e.g., ungerminated seeds), &lt;span class=&#34;math inline&#34;&gt;\(P(t)\)&lt;/span&gt; may not necessarily reach 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first task to fit a time-to-event model is to select a form for &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;. In general, we have three possibilities:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a parametric maximum likelihood model (ML)&lt;/li&gt;
&lt;li&gt;a non-parametric maximum likelihood model (NPMLE)&lt;/li&gt;
&lt;li&gt;a kernel density estimator (KDE)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s have a closer look at those three options.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parametric-time-to-event-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parametric time-to-event models&lt;/h1&gt;
&lt;p&gt;The CDF for parametric time-to-event models is characterised by a pre-defined, usually sigmoidal, shape. Right-skewed CDFs have proven useful, such as the log-normal, log-logistic or Weibull, which are only defined for &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 0\)&lt;/span&gt; (see #1 above). These CDFs contain a location (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and a scale (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;) parameter: the former is a measure of central tendency (e.g., the median for the log-logistic and log-normal CDFs) while the latter is a measure of how fast the curve grows during time. For seed germination/emergence, most often, a third parameter is necessary, to describe the fraction of dormant or nonviable seeds (&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; d \leq 1\)&lt;/span&gt;). As an example, we show a log-logistic CDF, that is also used in ecotoxicology, for dose-response studies:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{d}{1 + exp\left\{ b \right[ \log(t) - log(e)\left] \right\}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Fitting the above parametric model implies that, based on the observed data, we need to assign a specific value to the parameters &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, so that an appropriate likelihood function is maximised (Maximum Likelihood estimation). Relating to the estimation process, we should necessarily take into account that germination/emergence data are censored data; neglecting this fact has some important consequences, as I have described &lt;a href=&#34;https://www.statforbiology.com/2021/stat_drcte_2-methods/&#34;&gt;in this post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In practice, parametric time-to-event models can be fitted by the &lt;code&gt;drmte()&lt;/code&gt; function in the ‘drcte’ package. As an example, let’s consider a factitious dataset relating to an assay where the germinations of 30 seeds were counted daily for 15 days. A log-logistic time-to-event model can be easily fit as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/oneFlush.csv&amp;quot;)
head(dataset)
##   timeBef timeAf counts
## 1       0      1      3
## 2       1      2      2
## 3       2      3      3
## 4       3      4      4
## 5       4      5      3
## 6       5      6      2
library(drcte)
te.mod &amp;lt;- drmte(counts ~ timeBef + timeAf, fct = LL.3(),
                data = dataset)
# Alternative
# te.mod &amp;lt;- drm(counts ~ timeBef + timeAf, fct = LL.3(),
#                 data = dataset, type = &amp;quot;event&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that we have directly considered the observed counts as the response variable, with no preliminary transformation. Furthermore, we have two predictors that represent the extremes of each time interval (‘timeBef’ and ‘timeAf’), by which we associate each count to the whole uncertainty interval during which germinations took place and not to a precise time instant. In other words, we fully respect our censored data.&lt;/p&gt;
&lt;p&gt;The very same model can be fitted by using the &lt;code&gt;drm()&lt;/code&gt; function in the ‘drc’ package, although we should add the argument &lt;code&gt;type = &#34;event&#34;&lt;/code&gt;, as this latter package is not specific to time-to-event methods.&lt;/p&gt;
&lt;p&gt;The usual &lt;code&gt;coef()&lt;/code&gt;, &lt;code&gt;summary()&lt;/code&gt;, &lt;code&gt;print()&lt;/code&gt; and &lt;code&gt;plot()&lt;/code&gt; methods can be used for ‘drcte’ objects as for any other model object in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(te.mod)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.52553    0.44166 -3.4541 0.0005521 ***
## d:(Intercept)  0.97842    0.16027  6.1048  1.03e-09 ***
## e:(Intercept)  4.91400    1.68058  2.9240 0.0034558 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
plot(te.mod, ylim = c(0, 1), xlim = c(0, 15),
     xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_4-Time-to-EventCurves_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;nonparametric-time-to-event-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nonparametric time-to-event models&lt;/h2&gt;
&lt;p&gt;In some cases we are not willing to assume that the germination curve has a certain predefined shape, but we need an extra degree of flexibility. For example, emergences may proceed in successive flushes that are not easily described by using a sigmoidal curve. In these cases, we can fit a non-parametric model, whose shape is not pre-defined, but it is built by closely following the observed data. In survival analyses, time-to-event curves for interval censored data are estimated by using Nonparametric Maximum Likelihood Estimators (NPMLE), that are potentially interesting also for plant science.&lt;/p&gt;
&lt;p&gt;For example, we can consider another factitious dataset, where the germination took place in two distinct flushes; a non-parametric maximum likelihood model can be fit by using the &lt;code&gt;NPMLE()&lt;/code&gt; function, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/twoFlushes.csv&amp;quot;)
head(dataset)
##   timeBef timeAf nEmerg
## 1       0      2      0
## 2       2      4      1
## 3       4      6     10
## 4       6      8      9
## 5       8     10      4
## 6      10     12      0
te.npmle &amp;lt;- drmte(nEmerg ~ timeBef + timeAf, fct = NPMLE(),
                data = dataset)
summary(te.npmle)
## 
## Model fitted: NPML estimator for time-to-event data
## 
## Robust estimation: no 
## 
## Turnbull&amp;#39;s intervals and masses:
## 
##            count   pdf   cdf Naive.SE
## 1.(2,4]     1.00  0.02  0.02   0.0198
## 1.(4,6]    10.00  0.20  0.22   0.0586
## 1.(6,8]     9.00  0.18  0.40   0.0693
## 1.(8,10]    4.00  0.08  0.48   0.0707
## 1.(14,16]   1.00  0.02  0.50   0.0707
## 1.(18,20]   1.00  0.02  0.52   0.0707
## 1.(20,22]   1.00  0.02  0.54   0.0705
## 1.(22,24]   4.00  0.08  0.62   0.0686
## 1.(24,26]   7.00  0.14  0.76   0.0604
## 1.(26,28]   6.00  0.12  0.88   0.0460
## 1.(28,30]   3.00  0.06  0.94   0.0336
## 1.(32,Inf)  3.00  0.06  1.00   0.0000
par(mfrow = c(1,2))

plot(te.npmle, ylim = c(0, 1),
     xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;,
     main = &amp;quot;Interpolation method&amp;quot;, npmle.points = T)
plot(te.npmle, ylim = c(0, 1),
     xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;,
     npmle.type = &amp;quot;midpoint&amp;quot;, shading = F,
     main = &amp;quot;Midpoint imputation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_4-Time-to-EventCurves_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With NPMLE, the time-to-event curve is only defined at the end of each time interval, while it is undefined elsewhere and it is (optionally) represented by a shaded area (Figure above, left). This shaded area reflects the uncertainty due to censoring.&lt;/p&gt;
&lt;p&gt;Although we cannot know at what moment the nonparametric curve went up within the grey interval, we can make some reasonable assumptions. For example, we could assume that events are evenly spread within the interval, which is the approach taken in the ‘interval’ package (Figure above, left panel). In the ‘survival’ package and, most commonly, in survival analysis, it is assumed that the curve goes up in the middle of the interval (midpoint imputation; Figure above, right panel).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kernel-density-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kernel density estimators&lt;/h2&gt;
&lt;p&gt;NPMLEs are very flexible and they can be used to describe the progress to germination/emergence with, virtually, every type of datasets, also when the events took place in distinct flushes. However, we may be reluctant to accept a time-to-event curve with a stairstep shape, especially for prediction purposes. A further possibility to describe the time to event curve with a smooth and flexible model is to use a so-called Kernel Density Estimator (KDE). A KDE is built by considering the observed data, a Kernel function (usually gaussian) and a bandwith value, that controls the degree of smoothing; a nice post to see how Kernel density estimation works in practice can be found &lt;a href=&#34;https://mathisonian.github.io/kde/&#34;&gt;at this link here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the box below we show that a KDE can be fitted to the observed data (same example as above) by using the &lt;code&gt;drmte()&lt;/code&gt; function and setting the ‘fct’ argument to &lt;code&gt;KDE()&lt;/code&gt;. Please, note that this type of KDE is specifically modified to comply with censoring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;te.kde &amp;lt;- drmte(nEmerg ~ timeBef + timeAf, fct = KDE(),
                data = dataset)
summary(te.kde)
## 
## Model fitted: Kernel estimator for the distribution function
## 
## Robust estimation: no 
## 
## Bandwidth estimates:
## 
##               Estimate
## h:(bandwidth)   1.7871
plot(te.kde, ylim = c(0, 1),
     xlab = &amp;quot;Time&amp;quot;, ylab = &amp;quot;Cumulative proportion of germinated seeds&amp;quot;)
paf &amp;lt;- KDE.fun(seq(1,35, 1), dataset$timeBef, dataset$timeAf, dataset$nEmerg, 
               h = 0.5)
lines(paf ~ seq(1,35, 1), col = &amp;quot;red&amp;quot;)
paf &amp;lt;- KDE.fun(seq(1,35, 1), dataset$timeBef, dataset$timeAf, dataset$nEmerg, 
               h = 3.0)
lines(paf ~ seq(1,35, 1), col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_4-Time-to-EventCurves_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have augmented the above graph with three estimators: the blue one has a bandwidth &lt;span class=&#34;math inline&#34;&gt;\(h = 3\)&lt;/span&gt;, the red one has &lt;span class=&#34;math inline&#34;&gt;\(b = 0.5\)&lt;/span&gt; and the black one has &lt;span class=&#34;math inline&#34;&gt;\(b = 1.7871\)&lt;/span&gt;. Our aim was to show the effect of bandwidth selection on the resulting time-to-event curve, although it is usually necessary to apply an appropriate algorithm for the selection. The function &lt;code&gt;drmte()&lt;/code&gt;, by default, uses the AMISE method and, unless you know what you are doing, we recommend that you stick to such an algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ml-npmle-or-kde&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ML, NPMLE or KDE?&lt;/h1&gt;
&lt;p&gt;There is no rule to select the type of time-to-event model for seed germination/emergence and you will have to make your own choice and defend it at the publication stage. As a swift suggestion, I would say that a parametric model is to be preferred, unless it shows some visible signs of lack of fit.&lt;/p&gt;
&lt;p&gt;Whatever model you select, fitting a time-to-event model may be the most unambiguous way to describe the progress to germination/emergence. In a future post, we will see that we can also compare time-to-event models for different experimental treatments and or environmental conditions, which is, most often, the central step in our process of data analyses, for seed germination/emergence assays.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Barreiro-Ures D, Francisco-Fernández M, Cao R, Fraguela BB, Doallo R, González-Andújar JL, Reyes M (2019) Analysis of interval-grouped data in weed science: The binnednp Rcpp package. Ecol Evol 9:10903–10915&lt;/li&gt;
&lt;li&gt;Fay MP, Shaw PA (2010) Exact and Asymptotic Weighted Logrank Tests for Interval Censored Data: The interval R Package. J Stat Softw 36:1–34&lt;/li&gt;
&lt;li&gt;Ritz C, Jensen SM, Gerhard D, Streibig JC (2019) Dose-response analysis using RCRC Press. USA&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Biplots are everywhere: where do they come from?</title>
      <link>https://www.statforbiology.com/2021/stat_multivar_svd_biplots/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_multivar_svd_biplots/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Principal Component Analysis (PCA) is perhaps the most widespread multivariate technique in biology and it is used to summarise the results of experiments in a wide range of disciplines, from agronomy to botany, from entomology to plant pathology. Whenever possible, the results are presented by way of a biplot, an ubiquitous type of graph with a formidable descriptive value. Indeed, carefully drawn biplots can be used to represent, altogether, the experimental subjects, the experimental variables and their reciprocal relationships (distances and correlations).&lt;/p&gt;
&lt;p&gt;However, biplots are not created equal and the interpretational rules may change dramatically, depending on how the data were processed. As a reviewer/editor, I often feel that the authors have perfomed their Principal Component Analyses by using the default options in their favourite R function, but they are not totally aware of how the data were processed to reach the final published result. Therefore, their interpretation of biplots is, sometimes, more or less abused.&lt;/p&gt;
&lt;p&gt;I thought that it might be helpful to offer a ‘user-friendly’ explanation of the basic interpretational rules for biplots, with no overwhelming mathematical detail.&lt;/p&gt;
&lt;div id=&#34;a-simple-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A simple example&lt;/h1&gt;
&lt;p&gt;Let’s look at a simple (perhaps too simple), but realistic example. This is an herbicide trial, where we compared weed control ability of nine sugarbeet herbicides. Ground covering for six weed species was recorded; the weed species were &lt;em&gt;Polygonum lapathyfolium&lt;/em&gt;, &lt;em&gt;Chenopodium polyspermum&lt;/em&gt;, &lt;em&gt;Echinochloa crus-galli&lt;/em&gt;, &lt;em&gt;Amaranthus retroflexus&lt;/em&gt;, &lt;em&gt;Xanthium strumarium&lt;/em&gt; and &lt;em&gt;Polygonum aviculare&lt;/em&gt;; they were identified by using their BAYER code (the first three letters of the genus name and the first two letters of the species name). The aim of the experiment was to ordinate herbicide treatments in terms of their weed control spectra.&lt;/p&gt;
&lt;p&gt;The dataset is available in an online repository (see below). It is a dataframe, although we’d better convert it into the matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; for further analyses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;WeedPop &amp;lt;- read.csv(&amp;quot;https://www.statforbiology.com/_data/WeedPop.csv&amp;quot;,
                    header = T)
X &amp;lt;- WeedPop[,2:7]
row.names(X) &amp;lt;- WeedPop[,1]
X
##   POLLA CHEPO ECHCG AMARE XANST POLAV
## A   0.1    33    11     0   0.1   0.1
## B   0.1     3     3     0   0.1   0.0
## C   7.0    19    19     4   7.0   1.0
## D  18.0     3    28    19  12.0   6.0
## E   5.0     7    28     3  10.0   1.0
## F  11.0     9    33     7  10.0   6.0
## G   8.0    13    33     6  15.0  15.0
## H  18.0     5    33     4  19.0  12.0
## I   6.0     6    38     3  10.0   6.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s submit the matrix &lt;strong&gt;X&lt;/strong&gt; to a PCA, by using the default options in the &lt;code&gt;prcomp()&lt;/code&gt; function. In this case, the data is centered prior to analyses, but it is not standardised to unit variance.&lt;/p&gt;
&lt;p&gt;As the results, we obtain two matrices, that we will call &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt;; they respectively contain the &lt;em&gt;subject-scores&lt;/em&gt; (or principal component scores) and the &lt;em&gt;trait-scores&lt;/em&gt; (this is the rotation matrix). These two matrices, by multiplication, return the original centered data matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pcaMod &amp;lt;- prcomp(X)
G &amp;lt;- pcaMod$x
E &amp;lt;- pcaMod$rotation
print(G, digits = 3)
##      PC1      PC2    PC3    PC4      PC5    PC6
## A -26.97  12.2860  3.129  0.271 -0.00398  0.581
## B -20.81 -17.3167 -2.935 -3.081 -1.29287  0.475
## C  -9.95   3.6671  3.035  0.819  2.38378 -0.801
## D  12.69  -7.3364 11.756  3.732 -1.45392 -0.180
## E   1.13  -2.2854 -5.436  3.412  1.86926 -2.247
## F   8.05   1.6470 -0.455  2.895  0.22101  1.607
## G   9.46   7.3174 -1.213 -5.072 -4.98975 -0.976
## H  16.34   0.0632  0.799 -7.213  4.07606  0.509
## I  10.07   1.9578 -8.680  4.237 -0.80960  1.032
print(E, digits = 3)
##          PC1     PC2     PC3     PC4     PC5     PC6
## POLLA  0.349 -0.0493  0.5614 -0.1677  0.5574  0.4710
## CHEPO -0.390  0.8693  0.2981 -0.0013  0.0480 -0.0327
## ECHCG  0.688  0.4384 -0.3600  0.4322 -0.0113  0.1324
## AMARE  0.213 -0.1200  0.6808  0.4155 -0.4893 -0.2547
## XANST  0.372  0.1050  0.0499 -0.4107  0.2631 -0.7813
## POLAV  0.263  0.1555  0.0184 -0.6662 -0.6150  0.2903
print(G %*% t(E), digits = 3)
##    POLLA CHEPO  ECHCG  AMARE  XANST  POLAV
## A -8.033 22.11 -14.11 -5.111 -9.144 -5.133
## B -8.033 -7.89 -22.11 -5.111 -9.144 -5.233
## C -1.133  8.11  -6.11 -1.111 -2.244 -4.233
## D  9.867 -7.89   2.89 13.889  2.756  0.767
## E -3.133 -3.89   2.89 -2.111  0.756 -4.233
## F  2.867 -1.89   7.89  1.889  0.756  0.767
## G -0.133  2.11   7.89  0.889  5.756  9.767
## H  9.867 -5.89   7.89 -1.111  9.756  6.767
## I -2.133 -4.89  12.89 -2.111  0.756  0.767&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;biplot-and-interpretational-rules&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Biplot and interpretational rules&lt;/h1&gt;
&lt;p&gt;We can draw a biplot by using the first two columns in &lt;strong&gt;G&lt;/strong&gt; for the markers and the first two columns in &lt;strong&gt;E&lt;/strong&gt; for the arrowtips. In the box below I used the &lt;code&gt;biplot.default()&lt;/code&gt; method in R; I decided not to use the &lt;code&gt;biplot.prcomp()&lt;/code&gt; method, in order to avoid any further changes in &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(G[,1:2], E[,1:2], xlab=&amp;quot;PC1 (64%)&amp;quot;, 
       ylab=&amp;quot;PC2 (16%)&amp;quot;, main = &amp;quot;Default biplot \n\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_SVD_biplots_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, I know, the biplot does not look nice for publication (but, better solutions with ‘ggplot’ exist! Google for them!). Please, note that the markers are the experimental subjects and the arrows are the observed variables, which is the most common choice.&lt;/p&gt;
&lt;p&gt;The interpretational rules are based on:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the distances between markers;&lt;/li&gt;
&lt;li&gt;the distances from markers to the origin of axes;&lt;/li&gt;
&lt;li&gt;the lengths of arrows;&lt;/li&gt;
&lt;li&gt;the angles between arrows;&lt;/li&gt;
&lt;li&gt;the relative positions of arrows;&lt;/li&gt;
&lt;li&gt;the relative positions of arrows and markers.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By looking at those six characteristics, we should be able to derive information on:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the similarity of objects;&lt;/li&gt;
&lt;li&gt;the existence of groupings;&lt;/li&gt;
&lt;li&gt;the contribution of the experimental variables to the observed groupings;&lt;/li&gt;
&lt;li&gt;the relationship between experimental variables;&lt;/li&gt;
&lt;li&gt;the original value of each subject in each variable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What is the problem, then? The problem is that, depending on how the data were processed during the PCA, we obtain different types of biplots, conveying different information. Therefore, the interpretational rules muxt change according to the type of biplot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-possible-options-for-pca&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The possible options for PCA&lt;/h1&gt;
&lt;p&gt;During my lecturing activities I have noted that most PhD students are perfectly aware that, prior to PCA, we have the options of either:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;centering the data, or&lt;/li&gt;
&lt;li&gt;standardising them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In general, students are also aware that such a first decision impacts on the results of a PCA and on their interpretation.&lt;/p&gt;
&lt;p&gt;What it is less known is that, independent on data transformation, the results of a PCA are not unique, but they may change, depending on how the calculations are performed. Indeed, with a PCA, we look for two appropriate matrices, which:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;allow for the best description of our dataset in reduced rank space and&lt;/li&gt;
&lt;li&gt;by multiplication return the original centered or standardised data matrix (i.e. &lt;span class=&#34;math inline&#34;&gt;\(Y = G \, E^T\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By using the default options in &lt;code&gt;prcomp()&lt;/code&gt;, we have found &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt;, but there is an infinite number of matrix couples that obey the two requirements above. These couples can be found by ‘scaling’ &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt;, according to our specific research needs. What does ‘scaling’ mean? It is easy: the columns of &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt; can be, respectively, multiplied and divided by some selected constant values, to obtain two new matrices, that also represent an acceptable solution to a PCA, as long as their product returns the original data matrix.&lt;/p&gt;
&lt;p&gt;Let’s see the most common types of scalings.&lt;/p&gt;
&lt;div id=&#34;scaling-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scaling 1&lt;/h2&gt;
&lt;p&gt;If we take &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt; the way they are, with no changes, we say that we are using the so called &lt;strong&gt;Scaling 1&lt;/strong&gt;, that is, by far, the the most common type of scaling and it is also know as &lt;strong&gt;Principal Component Scaling&lt;/strong&gt; or &lt;strong&gt;row-scaling&lt;/strong&gt;. Let’s have a closer look at the columns of &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt; and, for each column, we calculate the norm (sum of squared elements):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# norms of column-vectors in G
normsG &amp;lt;- sqrt(diag(crossprod(G)))
normsG
##       PC1       PC2       PC3       PC4       PC5       PC6 
## 44.091847 24.153689 16.523462 11.827405  7.428198  3.338675
# norms of column-vectors in E
normsE &amp;lt;- sqrt(diag(crossprod(E)))
normsE
## PC1 PC2 PC3 PC4 PC5 PC6 
##   1   1   1   1   1   1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, with scaling 1, the column-vectors in &lt;strong&gt;E&lt;/strong&gt; have unit norms, while the column-vectors in &lt;strong&gt;G&lt;/strong&gt; have positive and decreasing norms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scaling-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scaling 2&lt;/h2&gt;
&lt;p&gt;If we divide each column in &lt;strong&gt;G&lt;/strong&gt; for the respective column-norm, we obtain a new &lt;strong&gt;G2&lt;/strong&gt; matrix, with column-norms equal to unity. At the same time, if we multiply each column in &lt;strong&gt;E&lt;/strong&gt; for the same amount, we obtain a new &lt;strong&gt;E2&lt;/strong&gt; matrix, where the column norms are the same as the column norms of &lt;strong&gt;G&lt;/strong&gt;. We do this scaling, by using the &lt;code&gt;sweep()&lt;/code&gt; function and we also show that &lt;strong&gt;G2&lt;/strong&gt; and &lt;strong&gt;E2&lt;/strong&gt;, by matrix multiplication, return the original centered data matrix (and so, they are an acceptable solution for a PCA).&lt;/p&gt;
&lt;p&gt;This second scaling is known as &lt;strong&gt;Scaling 2&lt;/strong&gt; or &lt;strong&gt;column-scaling&lt;/strong&gt;; it is less common, but, nonetheless, it is the default in the &lt;code&gt;summary.rda()&lt;/code&gt; method in the ‘vegan’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scaling 2
G2 &amp;lt;- sweep(G, 2, normsG, &amp;quot;/&amp;quot;)
E2 &amp;lt;- sweep(E, 2, normsG, &amp;quot;*&amp;quot;)
round(G2, digits = 3)
##      PC1    PC2    PC3    PC4    PC5    PC6
## A -0.612  0.509  0.189  0.023 -0.001  0.174
## B -0.472 -0.717 -0.178 -0.260 -0.174  0.142
## C -0.226  0.152  0.184  0.069  0.321 -0.240
## D  0.288 -0.304  0.711  0.316 -0.196 -0.054
## E  0.026 -0.095 -0.329  0.288  0.252 -0.673
## F  0.183  0.068 -0.028  0.245  0.030  0.481
## G  0.214  0.303 -0.073 -0.429 -0.672 -0.292
## H  0.371  0.003  0.048 -0.610  0.549  0.152
## I  0.228  0.081 -0.525  0.358 -0.109  0.309
round(E2, digits = 3)
##           PC1    PC2    PC3    PC4    PC5    PC6
## POLLA  15.383 -1.191  9.277 -1.983  4.141  1.572
## CHEPO -17.193 20.997  4.925 -0.015  0.357 -0.109
## ECHCG  30.349 10.589 -5.948  5.112 -0.084  0.442
## AMARE   9.373 -2.899 11.250  4.914 -3.634 -0.850
## XANST  16.387  2.537  0.824 -4.858  1.954 -2.609
## POLAV  11.594  3.757  0.305 -7.879 -4.569  0.969
sqrt(diag(crossprod(G2)))
## PC1 PC2 PC3 PC4 PC5 PC6 
##   1   1   1   1   1   1
sqrt(diag(crossprod(E2)))
##       PC1       PC2       PC3       PC4       PC5       PC6 
## 44.091847 24.153689 16.523462 11.827405  7.428198  3.338675
round(G2 %*% t(E2), digits = 3)
##    POLLA  CHEPO   ECHCG  AMARE  XANST  POLAV
## A -8.033 22.111 -14.111 -5.111 -9.144 -5.133
## B -8.033 -7.889 -22.111 -5.111 -9.144 -5.233
## C -1.133  8.111  -6.111 -1.111 -2.244 -4.233
## D  9.867 -7.889   2.889 13.889  2.756  0.767
## E -3.133 -3.889   2.889 -2.111  0.756 -4.233
## F  2.867 -1.889   7.889  1.889  0.756  0.767
## G -0.133  2.111   7.889  0.889  5.756  9.767
## H  9.867 -5.889   7.889 -1.111  9.756  6.767
## I -2.133 -4.889  12.889 -2.111  0.756  0.767&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scaling-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scaling 3&lt;/h2&gt;
&lt;p&gt;With scaling 1, the the values in &lt;strong&gt;G&lt;/strong&gt; were, on average, much higher than the values in &lt;strong&gt;E&lt;/strong&gt;; otherwise, with scaling 2, the situation was reversed. In order to have the two matrices in comparable scales, we could think of dividing each column in &lt;strong&gt;G&lt;/strong&gt; for the square root of the respective column-norm and, at the same time, multiply each column in &lt;strong&gt;E&lt;/strong&gt; for the same amount. The two new matrices &lt;strong&gt;G3&lt;/strong&gt; and &lt;strong&gt;E3&lt;/strong&gt; have the same column norms (they are now in comparable scales) and, by multiplication, they return the original centered data matrix.&lt;/p&gt;
&lt;p&gt;This scaling is known as &lt;strong&gt;symmetrical-scaling&lt;/strong&gt;; it is common in several applications of PCA, such as AMMI and GGE analyses for the evaluation of the stability of genotypes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scaling 3
G3 &amp;lt;- sweep(G, 2, sqrt(normsG), &amp;quot;/&amp;quot;)
E3 &amp;lt;- sweep(E, 2, sqrt(normsG), &amp;quot;*&amp;quot;)
round(G3, digits = 3)
##      PC1    PC2    PC3    PC4    PC5    PC6
## A -4.062  2.500  0.770  0.079 -0.001  0.318
## B -3.134 -3.523 -0.722 -0.896 -0.474  0.260
## C -1.498  0.746  0.747  0.238  0.875 -0.438
## D  1.910 -1.493  2.892  1.085 -0.533 -0.099
## E  0.170 -0.465 -1.337  0.992  0.686 -1.230
## F  1.212  0.335 -0.112  0.842  0.081  0.880
## G  1.424  1.489 -0.298 -1.475 -1.831 -0.534
## H  2.460  0.013  0.197 -2.097  1.496  0.279
## I  1.516  0.398 -2.135  1.232 -0.297  0.565
round(E3, digits = 3)
##          PC1    PC2    PC3    PC4    PC5    PC6
## POLLA  2.317 -0.242  2.282 -0.577  1.519  0.861
## CHEPO -2.589  4.272  1.212 -0.004  0.131 -0.060
## ECHCG  4.570  2.155 -1.463  1.486 -0.031  0.242
## AMARE  1.412 -0.590  2.768  1.429 -1.334 -0.465
## XANST  2.468  0.516  0.203 -1.412  0.717 -1.428
## POLAV  1.746  0.764  0.075 -2.291 -1.676  0.530
sqrt(diag(crossprod(G3)))
##      PC1      PC2      PC3      PC4      PC5      PC6 
## 6.640169 4.914640 4.064906 3.439099 2.725472 1.827204
sqrt(diag(crossprod(E3)))
##      PC1      PC2      PC3      PC4      PC5      PC6 
## 6.640169 4.914640 4.064906 3.439099 2.725472 1.827204
round(G3 %*% t(E3), digits = 3)
##    POLLA  CHEPO   ECHCG  AMARE  XANST  POLAV
## A -8.033 22.111 -14.111 -5.111 -9.144 -5.133
## B -8.033 -7.889 -22.111 -5.111 -9.144 -5.233
## C -1.133  8.111  -6.111 -1.111 -2.244 -4.233
## D  9.867 -7.889   2.889 13.889  2.756  0.767
## E -3.133 -3.889   2.889 -2.111  0.756 -4.233
## F  2.867 -1.889   7.889  1.889  0.756  0.767
## G -0.133  2.111   7.889  0.889  5.756  9.767
## H  9.867 -5.889   7.889 -1.111  9.756  6.767
## I -2.133 -4.889  12.889 -2.111  0.756  0.767&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scaling-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scaling 4&lt;/h2&gt;
&lt;p&gt;Scaling 4 is similar to scaling 2, but the matrix &lt;strong&gt;G&lt;/strong&gt;, instead of to unit norm, is scaled to unit variance, by dividing each column by the corresponding standard deviation. Likewise, &lt;strong&gt;E&lt;/strong&gt; is multiplied by the same amounts, so that &lt;strong&gt;E4&lt;/strong&gt; has column norms equal to the standard deviations of &lt;strong&gt;G&lt;/strong&gt;. The two matrices &lt;strong&gt;G4&lt;/strong&gt; and &lt;strong&gt;E4&lt;/strong&gt;, by multiplication, return the original centered matrix.&lt;/p&gt;
&lt;p&gt;This scaling is rather common in some applications of PCA, such as factor analysis; the elements in &lt;strong&gt;G4&lt;/strong&gt; are known as &lt;strong&gt;factor scores&lt;/strong&gt;, while those in &lt;strong&gt;E4&lt;/strong&gt; are the so-called &lt;strong&gt;loadings&lt;/strong&gt;. It is available, e.g., in the &lt;code&gt;dudi.pca()&lt;/code&gt; function in the ‘ade4’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scaling 4
G4 &amp;lt;- sweep(G, 2, apply(G, 2, sd), &amp;quot;/&amp;quot;)
E4 &amp;lt;- sweep(E, 2, apply(G, 2, sd), &amp;quot;*&amp;quot;)
round(G4, digits = 3)
##      PC1    PC2    PC3    PC4    PC5    PC6
## A -1.730  1.439  0.536  0.065 -0.002  0.492
## B -1.335 -2.028 -0.502 -0.737 -0.492  0.402
## C -0.638  0.429  0.519  0.196  0.908 -0.678
## D  0.814 -0.859  2.012  0.893 -0.554 -0.153
## E  0.073 -0.268 -0.930  0.816  0.712 -1.904
## F  0.516  0.193 -0.078  0.692  0.084  1.362
## G  0.607  0.857 -0.208 -1.213 -1.900 -0.827
## H  1.048  0.007  0.137 -1.725  1.552  0.431
## I  0.646  0.229 -1.486  1.013 -0.308  0.874
round(E4, digits = 3)
##          PC1    PC2    PC3    PC4    PC5    PC6
## POLLA  5.439 -0.421  3.280 -0.701  1.464  0.556
## CHEPO -6.079  7.424  1.741 -0.005  0.126 -0.039
## ECHCG 10.730  3.744 -2.103  1.807 -0.030  0.156
## AMARE  3.314 -1.025  3.977  1.737 -1.285 -0.301
## XANST  5.794  0.897  0.291 -1.717  0.691 -0.922
## POLAV  4.099  1.328  0.108 -2.786 -1.615  0.343
apply(X, 2, var)
##     POLLA     CHEPO     ECHCG     AMARE     XANST     POLAV 
##  43.45750  95.11111 136.86111  32.61111  38.73528  29.06500
sqrt(diag(crossprod(G4)))
##      PC1      PC2      PC3      PC4      PC5      PC6 
## 2.828427 2.828427 2.828427 2.828427 2.828427 2.828427
sqrt(diag(crossprod(E4)))
##       PC1       PC2       PC3       PC4       PC5       PC6 
## 15.588822  8.539619  5.841926  4.181619  2.626265  1.180400
round(G4 %*% t(E4), digits = 3)
##    POLLA  CHEPO   ECHCG  AMARE  XANST  POLAV
## A -8.033 22.111 -14.111 -5.111 -9.144 -5.133
## B -8.033 -7.889 -22.111 -5.111 -9.144 -5.233
## C -1.133  8.111  -6.111 -1.111 -2.244 -4.233
## D  9.867 -7.889   2.889 13.889  2.756  0.767
## E -3.133 -3.889   2.889 -2.111  0.756 -4.233
## F  2.867 -1.889   7.889  1.889  0.756  0.767
## G -0.133  2.111   7.889  0.889  5.756  9.767
## H  9.867 -5.889   7.889 -1.111  9.756  6.767
## I -2.133 -4.889  12.889 -2.111  0.756  0.767&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In conclusion to this section, although we might think that two plus two is always four, unfortunately, this is not true for a PCA and we need to be aware that there are, at least, four types of possible results, depending on which type of scaling has been used for the PC scores in &lt;strong&gt;G&lt;/strong&gt; and the rotation matrix &lt;strong&gt;E&lt;/strong&gt;. Consequently, there are four types of biplots, as we will see in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;four-types-of-biplots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Four types of biplots&lt;/h1&gt;
&lt;p&gt;We have at hand four matrices for the subject-scors (&lt;strong&gt;G&lt;/strong&gt;, &lt;strong&gt;G2&lt;/strong&gt;. &lt;strong&gt;G3&lt;/strong&gt; and &lt;strong&gt;G4&lt;/strong&gt;) and four matrices for the trait-scores (&lt;strong&gt;E&lt;/strong&gt;, &lt;strong&gt;E2&lt;/strong&gt;, &lt;strong&gt;E3&lt;/strong&gt; and &lt;strong&gt;E4&lt;/strong&gt;). If we draw a biplot by using the scores in &lt;strong&gt;G&lt;/strong&gt; for the markers and the scores in &lt;strong&gt;E&lt;/strong&gt; for the arrowtips, we obtain the so-called &lt;strong&gt;distance biplot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Otherwise, if we use &lt;strong&gt;G2&lt;/strong&gt; and &lt;strong&gt;E2&lt;/strong&gt;, we get the so-called &lt;strong&gt;correlation biplot&lt;/strong&gt;. Again, If we use &lt;strong&gt;G3&lt;/strong&gt; and &lt;strong&gt;E3&lt;/strong&gt; we obtain a &lt;strong&gt;symmetrical biplot&lt;/strong&gt;, while,if we use &lt;strong&gt;G4&lt;/strong&gt; and &lt;strong&gt;E4&lt;/strong&gt; we obtain a further type of biplot, which we could name &lt;strong&gt;type 4 biplot&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The four types of biplots are drawn in the following graph.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.casaonofri.it/_Figures/fourBiplots.png&#34; style=&#34;width:90.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s see how we can interpret the above biplots, depending on the scaling. For those of you who would not like to wait ’till the end of this post to grasp the interpretational rules, I have decided to anticipate the table below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.casaonofri.it/_Figures/Summary.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-preliminary-knowledge&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some preliminary knowledge&lt;/h1&gt;
&lt;p&gt;At the beginning I promised no overwhelming math detail, but there is one thing that cannot be avoided. If you are totally allergic to math, you can skip this section and only read the final sentence. However, I suggest that you sit back, relax and patiently read through this part.&lt;/p&gt;
&lt;p&gt;The whole interpretation of biplots depends from the concept of &lt;em&gt;inner product&lt;/em&gt;, which I will try to explain below. We have seen that the results of a PCA come in the form of the two matrices &lt;strong&gt;G&lt;/strong&gt; and &lt;strong&gt;E&lt;/strong&gt;; each row of &lt;strong&gt;G&lt;/strong&gt; corresponds to a marker, while each row of &lt;strong&gt;E&lt;/strong&gt; corresponds to an arrow. We talk about &lt;strong&gt;row-vectors&lt;/strong&gt;. Obviously, as we have to plot in 2D we only take the first two elements in each row-vector, but, if we could plot in 6D, we could draw markers and arrows by taking all six elements in each row-vector.&lt;/p&gt;
&lt;p&gt;Now, let’s imagine two row-vectors, named &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;, with, respectively, co-ordinates equal to [1, 0.25] and [0.25, 1]. I have reported these two vectors as arrows in the graph below and I have highlighted the angle between the two arrows, that is &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Furthermore, I have drawn the projection of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;, as the segment OA.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.casaonofri.it/_Figures/innerProduct.png&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, we can use the co-ordinates of &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; as row-vectors in the matrix &lt;strong&gt;M&lt;/strong&gt;, so that we have a connection between the graph and the matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M &amp;lt;- matrix(c(0.25, 1, 1, 0.25), 2, 2, byrow = T)
row.names(M) &amp;lt;- c(&amp;quot;u&amp;quot;, &amp;quot;v&amp;quot;)
M
##   [,1] [,2]
## u 0.25 1.00
## v 1.00 0.25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;inner product&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(u \cdot v\)&lt;/span&gt; is defined as the sum of the products of coordinates:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[u \cdot v = (0.25 \times 1) + (1 \times 0.25) = 0.5\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Obviously, the inner product of a vector with itself is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[u \cdot u = |u|^2 = 0.25^2 + 1^2 = 1.0625\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we think of the Pythagorean theorem, we immediatly see that such a ‘self inner product’ corresponds to the squared ‘length’ of the arrow (that is the squared norm of the vector). With R, we can obtain the inner products for rows, in a pairwise fashion, by using the &lt;code&gt;tcrossprod()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tcrossprod(M)
##        u      v
## u 1.0625 0.5000
## v 0.5000 1.0625&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that the squared norms of the two row-vectors are found along the diagonal. Please, also note that the inner products for column-vectors could be obtained by using the &lt;code&gt;crossprod()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Now, how do we visualize the inner product on the graph? By using the cosine inequality, it is possible to demonstrate that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[u \cdot v = |u| \, |v| \, cos \theta\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In words: the inner product of two vectors &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; is equal to the product of their lengths &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; by the cosine of the angle between them. For an explanation of this, you can &lt;a href=&#34;https://www.youtube.com/watch?v=NMl_dD9zPCI&#34;&gt;follow this link to a nice YouTube tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is a very useful property; look at the plot above: you may remember from high school that, in a rectangular triangle, the length of OA (one cathetus) can be obtained by multiplying the length of &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; (the hypothenus) by the cosine of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[u \cdot v = \overline{OA} \, |v|\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In words: the inner product between two arrows can be visualised by multiplying the length of one arrow for the length of the projection of the other arrow on the first arrow.&lt;/p&gt;
&lt;p&gt;The inner product is also useful to calculate the angle between two vectors; indeed, considering the equation above, we can also write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\cos \theta = \frac{u \cdot v}{|u| \, |v|}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if we take the matrix M, we can calculate the cosines of the angles between the row-vectors, by using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cosAngles &amp;lt;- function(mat){
  innerProd &amp;lt;- tcrossprod(mat)
  norms &amp;lt;- apply(mat, 1, norm, type = &amp;quot;2&amp;quot;)
  sweep(sweep(innerProd, 2, norms, &amp;quot;/&amp;quot;), 1, norms, &amp;quot;/&amp;quot;) 
}
cosAngles(M)
##           u         v
## u 1.0000000 0.4705882
## v 0.4705882 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, the angle of a vector with itself is 0 and its cosine is 1.&lt;/p&gt;
&lt;p&gt;As conclusions to this section, please note the following.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;The inner product between two arrows (or between an arrow and a subject-marker) can be visualised by multiplying the length of one arrow for the length of the projection of the other arrow on the first arrow&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The squared length of an arrow, or the squared distance from a marker and the origin of axes, is obtained by summing the squares of its co-ordinates&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretational-rules-for-biplots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interpretational rules for biplots&lt;/h1&gt;
&lt;div id=&#34;rule-1-distances-between-markers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rule 1: Distances between markers&lt;/h2&gt;
&lt;p&gt;The Euclidean distances between subjects in the original &lt;strong&gt;X&lt;/strong&gt; matrix can be regarded as measures of dissimilarity. They can be calculated by using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist(X)
##           A         B         C         D         E         F         G
## B 31.048510                                                            
## C 19.288079 24.984395                                                  
## D 45.241905 38.522980 27.073973                                        
## E 33.118424 27.803237 15.459625 21.679483                              
## F 36.886718 35.182666 18.841444 16.062378 10.295630                    
## G 37.768108 39.311830 22.293497 22.000000 17.320508 11.489125          
## H 45.860986 41.732721 27.892651 18.411953 20.024984 13.820275 13.892444
## I 40.430558 37.574193 23.790755 22.649503 11.269428  8.660254 13.892444
##           H
## B          
## C          
## D          
## E          
## F          
## G          
## H          
## I 16.970563&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we consider the matrices &lt;strong&gt;G&lt;/strong&gt;, &lt;strong&gt;G2&lt;/strong&gt;, &lt;strong&gt;G3&lt;/strong&gt; and &lt;strong&gt;G4&lt;/strong&gt;, we see that only the first one preserves the Euclidean inter-object distances (the box below shows only the distances between the subject ‘I’ and all other subjects, for the sake of brevity):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.matrix(dist(G))[9,]
##         A         B         C         D         E         F         G         H 
## 40.430558 37.574193 23.790755 22.649503 11.269428  8.660254 13.892444 16.970563 
##         I 
##  0.000000
as.matrix(dist(G2))[9,] # Proportional to mahalanobis
##         A         B         C         D         E         F         G         H 
## 1.2416395 1.2894994 1.1327522 1.3499172 1.0999901 0.5584693 1.2454512 1.3227711 
##         I 
## 0.0000000
as.matrix(dist(G3))[9,] # Not proportional
##        A        B        C        D        E        F        G        H 
## 6.741577 6.606727 4.569223 5.433575 2.727196 2.141145 3.931517 4.567013 
##        I 
## 0.000000
as.matrix(dist(G4))[9,] # Mahlanobis
##        A        B        C        D        E        F        G        H 
## 3.511887 3.647255 3.203907 3.818143 3.111242 1.579590 3.522668 3.741362 
##        I 
## 0.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This leads us to the first interpretational rule: &lt;strong&gt;starting from a column-centered matrix, with a distance biplot (scaling 1), the Euclidean distances between markers approximate the Euclidean distances between subjects in the original centered matrix. Instead, with scaling 2 and 4 the Euclidean distances between markers approximate the Mahalanobis distances of subjects&lt;/strong&gt;, which represent a totally different measure of dissimilarity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-2-distances-from-markers-to-the-origin-of-axes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rule 2: Distances from markers to the origin of axes&lt;/h2&gt;
&lt;p&gt;Let’s now consider the distance of each marker from the origin of the axes. We need to consider that a hypothetical subject with average values for all original variables would be located exactly on the origin of axes, independent on the scaling. Therefore, the distance from the origin of axes shows how far is that subject from the hypothetical ‘average subject’: the farthest the distance, the most ‘notable’ the subject (very high or very low), in relation to one or more of the original variables.&lt;/p&gt;
&lt;p&gt;Therefore, the second interpretational rule is: &lt;strong&gt;starting from a column-centered matrix, independent from scaling, the Euclidean distance between a marker and the origin approximate the dissimilarity between a subject and a hypothetical subject that has average value for all original variables. With scaling 1, such dissimilarity is measured by the Euclidean distance, while with scalings 2 and 4, it is measured by the Mahalanobis distance.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-3-and-4-lengths-of-arrows-and-inner-products&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rule 3 and 4: Lengths of arrows and inner products&lt;/h2&gt;
&lt;p&gt;We have seen that the pairwise inner products of row vectors in a matrix can be obtained by using the &lt;code&gt;tcrossprod()&lt;/code&gt; function. In this respect, the inner products of row-vectors &lt;strong&gt;E1&lt;/strong&gt; and &lt;strong&gt;E3&lt;/strong&gt; (scaling 1 and 3) are totally meaningless. On the other hand, the inner products for row vectors in &lt;strong&gt;E2&lt;/strong&gt; and &lt;strong&gt;E4&lt;/strong&gt; represent, respectively, the deviances-codeviances and variance-covariances of the original observations in X.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tcrossprod(E2)
##           POLLA     CHEPO     ECHCG      AMARE     XANST      POLAV
## POLLA  347.6600 -242.4667  389.2667  225.86667  270.3267  174.93000
## CHEPO -242.4667  760.8889 -328.8889 -167.88889 -223.3556 -120.56667
## ECHCG  389.2667 -328.8889 1094.8889  211.88889  493.1556  350.36667
## AMARE  225.8667 -167.8889  211.8889  260.88889  126.7556   78.26667
## XANST  270.3267 -223.3556  493.1556  126.75556  309.8822  226.59667
## POLAV  174.9300 -120.5667  350.3667   78.26667  226.5967  232.52000
tcrossprod(E4)
##           POLLA     CHEPO     ECHCG      AMARE     XANST      POLAV
## POLLA  43.45750 -30.30833  48.65833  28.233333  33.79083  21.866250
## CHEPO -30.30833  95.11111 -41.11111 -20.986111 -27.91944 -15.070833
## ECHCG  48.65833 -41.11111 136.86111  26.486111  61.64444  43.795833
## AMARE  28.23333 -20.98611  26.48611  32.611111  15.84444   9.783333
## XANST  33.79083 -27.91944  61.64444  15.844444  38.73528  28.324583
## POLAV  21.86625 -15.07083  43.79583   9.783333  28.32458  29.065000
cov(X)
##           POLLA     CHEPO     ECHCG      AMARE     XANST      POLAV
## POLLA  43.45750 -30.30833  48.65833  28.233333  33.79083  21.866250
## CHEPO -30.30833  95.11111 -41.11111 -20.986111 -27.91944 -15.070833
## ECHCG  48.65833 -41.11111 136.86111  26.486111  61.64444  43.795833
## AMARE  28.23333 -20.98611  26.48611  32.611111  15.84444   9.783333
## XANST  33.79083 -27.91944  61.64444  15.844444  38.73528  28.324583
## POLAV  21.86625 -15.07083  43.79583   9.783333  28.32458  29.065000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consequently, the lengths of arrows (norms) are given by the square-roots of the diagonal elements in the matrix of inner products. These lengths, are proportional (scaling 2) and equal (scaling 4) to the standard deviations of the original variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(X, 2, sd)
##     POLLA     CHEPO     ECHCG     AMARE     XANST     POLAV 
##  6.592230  9.752493 11.698765  5.710614  6.223767  5.391197
sqrt(apply(E, 1, function(x) sum(x^2)))
## POLLA CHEPO ECHCG AMARE XANST POLAV 
##     1     1     1     1     1     1
sqrt(apply(E2, 1, function(x) sum(x^2)))
##    POLLA    CHEPO    ECHCG    AMARE    XANST    POLAV 
## 18.64564 27.58421 33.08911 16.15206 17.60347 15.24861
sqrt(apply(E3, 1, function(x) sum(x^2)))
##    POLLA    CHEPO    ECHCG    AMARE    XANST    POLAV 
## 3.743663 5.142620 5.471886 3.746464 3.308392 3.461031
sqrt(apply(E4, 1, function(x) sum(x^2)))
##     POLLA     CHEPO     ECHCG     AMARE     XANST     POLAV 
##  6.592230  9.752493 11.698765  5.710614  6.223767  5.391197&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This leads us to the third interpretational rule: &lt;strong&gt;starting from a column-centered matrix, the lengths of arrows approximate the standard deviations of the original variables only with a correlation biplot (scaling 2) or with a type 4 biplot.&lt;/strong&gt;. Furthermore, &lt;strong&gt;the inner products of two arrows, approximate the codeviances (Scaling 2) and the covariances (Scaling 4)&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-5-angles-between-arrows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rule 5: Angles between arrows&lt;/h2&gt;
&lt;p&gt;We have seen that the cosines of the angles between the row-vectors in the matrices &lt;strong&gt;E&lt;/strong&gt; to &lt;strong&gt;E4&lt;/strong&gt; can be calculated by taking, for each pair of rows, the inner product and the respective norms. By using the functione defined above, it is easy to see that the angles between arrows in &lt;strong&gt;E&lt;/strong&gt; and &lt;strong&gt;E3&lt;/strong&gt; are totally meaningless. Otherwise, the cosines of the angles between row-vectors in &lt;strong&gt;E2&lt;/strong&gt; and &lt;strong&gt;E4&lt;/strong&gt; are equal to the correlations between the original variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cosAngles(E2)
##            POLLA      CHEPO      ECHCG      AMARE      XANST      POLAV
## POLLA  1.0000000 -0.4714266  0.6309353  0.7499753  0.8235940  0.6152573
## CHEPO -0.4714266  1.0000000 -0.3603326 -0.3768197 -0.4599788 -0.2866398
## ECHCG  0.6309353 -0.3603326  1.0000000  0.3964563  0.8466435  0.6943966
## AMARE  0.7499753 -0.3768197  0.3964563  1.0000000  0.4458008  0.3177744
## XANST  0.8235940 -0.4599788  0.8466435  0.4458008  1.0000000  0.8441605
## POLAV  0.6152573 -0.2866398  0.6943966  0.3177744  0.8441605  1.0000000
cosAngles(E4)
##            POLLA      CHEPO      ECHCG      AMARE      XANST      POLAV
## POLLA  1.0000000 -0.4714266  0.6309353  0.7499753  0.8235940  0.6152573
## CHEPO -0.4714266  1.0000000 -0.3603326 -0.3768197 -0.4599788 -0.2866398
## ECHCG  0.6309353 -0.3603326  1.0000000  0.3964563  0.8466435  0.6943966
## AMARE  0.7499753 -0.3768197  0.3964563  1.0000000  0.4458008  0.3177744
## XANST  0.8235940 -0.4599788  0.8466435  0.4458008  1.0000000  0.8441605
## POLAV  0.6152573 -0.2866398  0.6943966  0.3177744  0.8441605  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This leads to the fourth interpretational rule: &lt;strong&gt;starting from a column-centered matrix, scaling 2 and scaling 4 result in biplots where the cosines of the angles between arrows approximate the correlation of original variables&lt;/strong&gt;. Consequently, right angles indicate no correlation, acute angles indicate positive correlatione (the smaller the angle the higher the correlation), while obtuse angles (up to 180°) indicate a negative correlation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-6-inner-products-between-markers-and-arrows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rule 6: Inner products between markers and arrows&lt;/h2&gt;
&lt;p&gt;We have seen that, by multiplication, the subject-scores and trait-scores return the original centered matrix and this is totally independent from scaling. Accordingly, we can also say that the observed value for each subject in each variable can be obtained by using the inner product of a subject-vector and a trait-vector.&lt;/p&gt;
&lt;p&gt;For example, let’s calculate the inner product between the first row in &lt;strong&gt;G&lt;/strong&gt; (subject A) and the first row in &lt;strong&gt;E&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crossprod(G[1,], E[1,])
##           [,1]
## [1,] -8.033333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is the exact value shown by A on POLLA, in the original centered data matrix.&lt;/p&gt;
&lt;p&gt;Therefore, the fifth interpretational rule is that: &lt;strong&gt;independent on scaling, we can approximate the value of one subject in one specific variable, by considering how long are the respective trait-arrow and the projection of the subject-marker on the trait-arrow.&lt;/strong&gt; If the projection is on the same direction of the trait-arrow, the original value is positive (i.e., above the mean), while if the projection is in the opposite direction, the original value is negative (i.e., below the mean).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-we-standardise-the-original-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What if we standardise the original data?&lt;/h1&gt;
&lt;p&gt;Sometimes we need to standardise our data, for example because we want to avoid that a few of the original variables (the ones with largest values) assume a preminent role in the ordination of subjects. From a practical point of view, this is very simple: we only have to add the option &lt;code&gt;scale = T&lt;/code&gt; to the selected R function for PCA.&lt;/p&gt;
&lt;p&gt;What does it change, with the interpretation of a biplot? The main thing to remember is that the starting point is a standardised matrix where each value represents the difference with respect to the column mean in standard deviation units. Furthermore, all the original columns, after standardisation, have unit standard deviation. Accordingly, there are some changes to the interpretational rules, which I have listed in the table above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;I’d like to conclude with a couple of take-home messages, to be kept in mind when publishing a biplot. Please, note that they are taken from Onofri et al., (2010):&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Always mention what kind of pre-manipulation has been performed on the original dataset (centering, normalisation, standardisation).&lt;/li&gt;
&lt;li&gt;Always mention what sort of rescaling on PCs has been used. Remember that the selection of one particular scaling option strongly affects the interpretation of the biplot, in terms of distances and angles. For example, one scaling may allow Euclidean distances between objects to be interpreted, but not those between variables, while another scaling does not permit either.&lt;/li&gt;
&lt;li&gt;Never drag and pull a PCA plot so that it fits the page layout. Remember that axes need to be equally scaled for any geometric interpretations of distances and angles.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Borcard, D., Gillet, F., Legendre, P., 2011. Numerical Ecology with R. Springer Science+Business Media, LLC 2011, New York.&lt;/li&gt;
&lt;li&gt;Gower, J.C., 1966. Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika 53, 325–338.&lt;/li&gt;
&lt;li&gt;Legendre, P., Legendre, L., 2012. Numerical ecology. Elsevier, Amsterdam (The Netherlands).&lt;/li&gt;
&lt;li&gt;Kroonenberg, P.M., 1995. Introduction to biplots for GxE tables. The University of Queensland. Research Report #51, Brisbane, Australia.&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Principal Component Analysis: a brief intro for biologists</title>
      <link>https://www.statforbiology.com/2021/stat_multivar_pca/</link>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_multivar_pca/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post I am revisiting the concept of Principal Component Analysis (PCA). You might say that there is no need for that, as the Internet is full with posts relating to such a rather old technique. However, I feel that, in those posts, the theoretical aspects are either too deeply rooted in maths or they are skipped altogether, so that the main emphasis is on interpreting the output of an R function. I think that both approaches may not be suitable for biologists: the first one may be too difficult to understand, while skipping altogether the theoretical aspects promotes the use of R as a black-box, which is dangerouse for teaching purposes. That’s why I wrote this post… I wanted to make my attempt to create a useful lesson. You will tell me whether I suceeded or not.&lt;/p&gt;
&lt;div id=&#34;what-is-principal-component-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is Principal Component Analysis?&lt;/h1&gt;
&lt;p&gt;A main part of field experiments is multivariate, in the sense that several traits are measured in each experimental unit. For example, think about yield quality in genotype experiments or about the composition of weed flora in herbicide trials: in both cases, it is very important to consider several variables altogether, otherwise, we lose all information about the relationships among the observed variables and we lose the ability of producing a convincing summary of results.&lt;/p&gt;
&lt;p&gt;Multivariate methods can help us to deal with multivariate datasets. One main task of multivariate analysis is &lt;strong&gt;ordination&lt;/strong&gt;, i.e. organising the observations so that similar subjects are near to each other, while dissimilar subjects are further away. Clearly, ordination is connected to ‘representation’ and it is aided by techniques that permit a reduction in the number of variables, with little loss in terms of descriptive ability.&lt;/p&gt;
&lt;p&gt;Principal Component Analysis (PCA) is one of these techniques. How can we reduce the number of variables? How can we use, say, two variables instead of six, without losing relevant information? One possibility is to exploit the correlations between pairs of variables: whenever this is high, both variables carry a similar amount of information and there may be no need of using both of them.&lt;/p&gt;
&lt;p&gt;Let’s make a trivial example: think about four subjects where we measured two variables (X1 and X2), with the second one being exactly twice as much the first one. As we know, these four subjects and their characteristics take the form of a 4 x 2 matrix, which is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
X &amp;lt;- c(12, 15, 17, 19)
Y &amp;lt;- c(24, 30, 34, 38)
dataMat &amp;lt;- data.frame(X, Y, row.names=letters[1:4])
print(dataMat)
##    X  Y
## a 12 24
## b 15 30
## c 17 34
## d 19 38&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I said, this example is trivial: it is indeed totally clear that the second variable does not add any additional information with respect to the first one, and thus we could as well drop it without hindering the ordination of the four subjects. Nevertheless, naively dropping a variable may not be optimal whenever the correlation is less then perfect! What should we do, then?&lt;/p&gt;
&lt;p&gt;First of all, we should note that the two variables have different means and standard deviations. Thus, we might like to center and, possibly, standardise them. The first practice is necessary, while the second one is optional and it may dramatically change the interpretation of results. Therefore, when you write your paper, please always inform the readers whether you have standardised or not! When we work on centered (not standardised) data, we usually talk about PCA on covariance matrix, while, when we work on standardised data, we talk abot PCA on correlation matrix. The reason for these namings will be clear later on.&lt;/p&gt;
&lt;p&gt;Let’s standardise the data, and represent them in the Euclidean space: in this specific case the points (subjects) lie along the bisector of first and third quadrant (all points have equal co-ordinates).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Z &amp;lt;- scale(dataMat, scale=T, center=T)[1:4,1:2]
Z
##            X          Y
## a -1.2558275 -1.2558275
## b -0.2511655 -0.2511655
## c  0.4186092  0.4186092
## d  1.0883839  1.0883839&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;pca-a-rotation-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PCA: a ‘rotation’ procedure&lt;/h2&gt;
&lt;p&gt;From the previous graph we may note that it would be useful to rotate the axes by an angle of 45°; in this case, the points will lie along the x-axis and they would all have a null co-ordinate on the y-axis. As the consequence, this second dimension would be totally useless.&lt;/p&gt;
&lt;p&gt;Rotating the axes is a matrix multiplication problem: we have to take the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; and multiply it by a rotation matrix, to get the co-ordinates of subjects in the new reference system. We do not need the details: if we want to rotate by an angle &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, the rotation matrix is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V = \left[\begin{array}{cc} \cos \alpha &amp;amp;  - \sin \alpha \\
\sin \alpha &amp;amp; \cos \alpha \end{array} \right]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our case (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 45° = \pi/4\)&lt;/span&gt;), it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V = \left[\begin{array}{cc} \frac{\sqrt{2}}{2} &amp;amp;  - \frac{\sqrt{2}}{2} \\
\frac{\sqrt{2}}{2} &amp;amp; \frac{\sqrt{2}}{2} \end{array} \right]\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-a-eigenvalue-decomposition-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PCA: a eigenvalue decomposition procedure&lt;/h2&gt;
&lt;p&gt;How do we get the rotation matrix, in general? We take the correlation matrix and submit it to eigenvalue decomposition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corX &amp;lt;- cor(dataMat)
eig &amp;lt;- eigen(corX)
eig
## eigen() decomposition
## $values
## [1] 2 0
## 
## $vectors
##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068
V &amp;lt;- eig$vectors&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the eigenvectors of this decomposition correspond to the aforementioned rotation matrix. We can now use &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; to rotate the original matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; and calculate the coordinates of the original observations in the new, rotated space. We store the subject scores in the matrix &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;U &amp;lt;- Z %*% V
print(round(U, 3) )
##     [,1] [,2]
## a -1.776    0
## b -0.355    0
## c  0.592    0
## d  1.539    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have seen that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[U = Z V\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = U V^T\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In words, we have decomposed our original standardised matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; into the product of two matrices, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. That is, we have made a &lt;strong&gt;factor decomposition&lt;/strong&gt; of our dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-a-linear-combination-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PCA: a linear combination procedure&lt;/h2&gt;
&lt;p&gt;Look at the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; matrices above. Matrix multiplication consists of a row-column operation, such as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ -1.256 \times 0.707 + (-1.256 \times 0.707) = -1.776 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that what we are doing is indeed a linear combination of the observed variables, by using the coefficients in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Whatever way we look at our PCA, we can now plot the subjects in the new, rotated space:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Plot
plot(U[,1], U[,2], cex=1.5, pch=21, bg=&amp;quot;red&amp;quot;, 
     xlim=c(-2,2), ylim=c(-2,2))
abline(h=0, lty=1)
abline(v=0, lty=1)
arrows(0, 0, V[1,1], V[1,2])
arrows(0, 0, V[2,1], V[2,2])
text(0.8, -0.8, &amp;quot;Z1&amp;quot;)
text(0.8, 0.8, &amp;quot;Z2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/figR_rotated-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The arrows represent the row-vectors of the rotation matrix: they are the directions of the original axes on the rotated space. We clearly see that the second dimension is totally unnecessary, as we already have a perfect ordination in one dimension. We have performed a rank reduction in our dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-preliminary-conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some preliminary conclusions&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;PCA consists of a rotation of the original space&lt;/li&gt;
&lt;li&gt;In the new space, the first dimension (first principal component) is selected so that it preserves the maximum amount of the original data variability, the second principal component one so that it preserves the maximum amount of the residual data variability&lt;/li&gt;
&lt;li&gt;Therefore, I can select a subspace of dimensions (components) and have the objects projected in reduced rank space&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-more-realistic-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A more realistic example&lt;/h1&gt;
&lt;p&gt;Let’s look at a simple (perhaps too simple), but real example. This is an herbicide trial, where we compared the weed control ability of nine sugarbeet herbicides. Ground covering for six weed species was recorded; the weed species were &lt;em&gt;Polygonum lapathyfolium&lt;/em&gt;, &lt;em&gt;Chenopodium polyspermum&lt;/em&gt;, &lt;em&gt;Echinochloa crus-galli&lt;/em&gt;, &lt;em&gt;Amaranthus retroflexus&lt;/em&gt;, &lt;em&gt;Xanthium strumarium&lt;/em&gt; and &lt;em&gt;Polygonum aviculare&lt;/em&gt;; they were identified by using their BAYER code (the first three letters of the genus name and the first two letters of the species name). The aim of the experiment was to ordinate herbicide treatments in terms of their weed control spectra.&lt;/p&gt;
&lt;p&gt;The dataset is online available (see below). It is a dataframe, although we’d better convert it into the matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; for further analyses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
WeedPop &amp;lt;- read.csv(&amp;quot;https://www.statforbiology.com/_data/WeedPop.csv&amp;quot;, header = T)
X &amp;lt;- WeedPop[,2:7]
row.names(X) &amp;lt;- WeedPop[,1]
X
##   POLLA CHEPO ECHCG AMARE XANST POLAV
## A   0.1    33    11     0   0.1   0.1
## B   0.1     3     3     0   0.1   0.0
## C   7.0    19    19     4   7.0   1.0
## D  18.0     3    28    19  12.0   6.0
## E   5.0     7    28     3  10.0   1.0
## F  11.0     9    33     7  10.0   6.0
## G   8.0    13    33     6  15.0  15.0
## H  18.0     5    33     4  19.0  12.0
## I   6.0     6    38     3  10.0   6.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is rather small, although a graphical representation would be very difficult with six weed species. However, there must be some correlation between one weed species and the others, as herbicides may have similar weed control spectra. Therefore, PCA might be the right tool for rank reduction and ordination.&lt;/p&gt;
&lt;div id=&#34;preliminary-actions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminary actions&lt;/h2&gt;
&lt;p&gt;First of all, we need to decide whether we want to standardise the data. In this case, some variables have higher variances with respect to others. Therefore, they might end up having higher weight in the ordination, with respect to those with lower variances. We store the standardised data into the matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Z &amp;lt;- as.matrix( scale(X, scale=T) )
print(Z, digits=3)
##     POLLA  CHEPO  ECHCG  AMARE  XANST  POLAV
## A -1.2186  2.267 -1.206 -0.895 -1.469 -0.952
## B -1.2186 -0.809 -1.890 -0.895 -1.469 -0.971
## C -0.1719  0.832 -0.522 -0.195 -0.361 -0.785
## D  1.4967 -0.809  0.247  2.432  0.443  0.142
## E -0.4753 -0.399  0.247 -0.370  0.121 -0.785
## F  0.4349 -0.194  0.674  0.331  0.121  0.142
## G -0.0202  0.216  0.674  0.156  0.925  1.812
## H  1.4967 -0.604  0.674 -0.195  1.567  1.255
## I -0.3236 -0.501  1.102 -0.370  0.121  0.142
## attr(,&amp;quot;scaled:center&amp;quot;)
## POLLA CHEPO ECHCG AMARE XANST POLAV 
##  8.13 10.89 25.11  5.11  9.24  5.23 
## attr(,&amp;quot;scaled:scale&amp;quot;)
## POLLA CHEPO ECHCG AMARE XANST POLAV 
##  6.59  9.75 11.70  5.71  6.22  5.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does this standardised matrix represent? A few hints:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;negative values indicate weed coverings below column-average&lt;/li&gt;
&lt;li&gt;positive values indicate weed coverings above column-average&lt;/li&gt;
&lt;li&gt;weed coverings are expressed as standard deviation units: a value of 1 indicate that we are one standard deviation above the column-average.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvalue-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Eigenvalue decomposition&lt;/h2&gt;
&lt;p&gt;Let’s get the eigenvalues and eigenvectors of the correlation matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca &amp;lt;- eigen( cor(X) )
d &amp;lt;- pca$values
V &amp;lt;- pca$vectors
row.names(V) &amp;lt;- colnames(WeedPop[,2:7])
colnames(V) &amp;lt;- paste(&amp;quot;PC&amp;quot;, 1:6, sep=&amp;quot;&amp;quot;)
print(V, digits=3)
##          PC1    PC2      PC3     PC4    PC5     PC6
## POLLA  0.460  0.221 -0.24985  0.1658  0.641 -0.4884
## CHEPO -0.294 -0.473 -0.82077 -0.0553  0.106  0.0492
## ECHCG  0.429 -0.318  0.04360 -0.7830 -0.178 -0.2617
## AMARE  0.340  0.599 -0.50839 -0.0570 -0.460  0.2285
## XANST  0.482 -0.252  0.05955  0.0369  0.338  0.7651
## POLAV  0.413 -0.452 -0.00164  0.5931 -0.470 -0.2300
print(d)
## [1] 3.85798046 0.93663779 0.67502915 0.30394443 0.19203762 0.03437055&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can use the rotation matrix V to get the coordinates of our subjects in the new rotated space (&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;U &amp;lt;- Z %*% V
row.names(U) &amp;lt;- LETTERS[1:9]
colnames(U) &amp;lt;- paste(&amp;quot;PC&amp;quot;, 1:6, sep=&amp;quot;&amp;quot;)
print(U, digits=3)
##      PC1     PC2     PC3     PC4     PC5     PC6
## A -3.149 -0.6934 -1.2399  0.0492  0.0360 -0.0871
## B -2.546  0.9861  1.2551  0.7437 -0.1589 -0.0553
## C -1.111  0.0642 -0.5837 -0.1334  0.4072  0.1219
## D  2.131  1.9161 -0.9096  0.0616 -0.2061  0.0262
## E -0.387  0.1079  0.6533 -0.6903  0.1889  0.3369
## F  0.776  0.0767 -0.0814 -0.3752 -0.0397 -0.2627
## G  1.463 -1.2798 -0.1703  0.5564 -0.7200  0.1704
## H  2.362 -0.6767  0.3413  0.5668  0.8051 -0.0711
## I  0.462 -0.5010  0.7353 -0.7787 -0.3124 -0.1793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the three ingredients of a PCA, i.e.:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the original standardised matrix (&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;the matrix of ‘subject-scores’ (&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;the matrix of ‘trait-scores’ (&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;: rotation matrix)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-the-characteristics-of-subject-scores-u&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are the characteristics of subject-scores (U)?&lt;/h2&gt;
&lt;p&gt;We have said that subject-scores in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; represent our subjects in the new, rotated space. We have six original variables and six PCs (indeeed, the number of PCs is equal to the lowest between the number of rows and the number of columns in the original matrix X). Let’s see some characteristics of subject scores:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(U, 2, mean)
##           PC1           PC2           PC3           PC4           PC5 
##  1.233852e-16 -9.227765e-18  1.849769e-17 -2.467765e-17  2.930056e-17 
##           PC6 
##  1.225826e-16
apply(U, 2, var)
##        PC1        PC2        PC3        PC4        PC5        PC6 
## 3.85798046 0.93663779 0.67502915 0.30394443 0.19203762 0.03437055
cor(U)
##               PC1           PC2           PC3           PC4           PC5
## PC1  1.000000e+00 -5.773238e-16 -2.028552e-16  1.397684e-16 -5.313598e-16
## PC2 -5.773238e-16  1.000000e+00 -1.295690e-15 -1.214638e-16  3.326095e-16
## PC3 -2.028552e-16 -1.295690e-15  1.000000e+00  1.851002e-16  3.505827e-16
## PC4  1.397684e-16 -1.214638e-16  1.851002e-16  1.000000e+00  7.572507e-16
## PC5 -5.313598e-16  3.326095e-16  3.505827e-16  7.572507e-16  1.000000e+00
## PC6  5.636542e-16  4.023873e-16  3.853043e-16  1.540250e-15 -4.896632e-16
##               PC6
## PC1  5.636542e-16
## PC2  4.023873e-16
## PC3  3.853043e-16
## PC4  1.540250e-15
## PC5 -4.896632e-16
## PC6  1.000000e+00
#Distances
dist(U)
##          A        B        C        D        E        F        G        H
## B 3.151284                                                               
## C 2.317658 2.722462                                                      
## D 5.905129 5.282416 3.804313                                             
## E 3.550451 2.850970 1.568730 3.587558                                    
## F 4.190097 3.867798 2.054301 2.491515 1.550448                           
## G 4.863326 4.862117 3.217457 3.425880 2.903991 1.959146                  
## H 5.808357 5.353306 3.762687 3.102872 3.224662 2.213925 1.954319         
## I 4.218553 3.727084 2.358123 3.477388 1.274494 1.158964 2.120882 2.620524
dist(Z)
##          A        B        C        D        E        F        G        H
## B 3.151284                                                               
## C 2.317658 2.722462                                                      
## D 5.905129 5.282416 3.804313                                             
## E 3.550451 2.850970 1.568730 3.587558                                    
## F 4.190097 3.867798 2.054301 2.491515 1.550448                           
## G 4.863326 4.862117 3.217457 3.425880 2.903991 1.959146                  
## H 5.808357 5.353306 3.762687 3.102872 3.224662 2.213925 1.954319         
## I 4.218553 3.727084 2.358123 3.477388 1.274494 1.158964 2.120882 2.620524&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The PCs for subject-scores have means equal to 0&lt;/li&gt;
&lt;li&gt;The PCs for subject scores have variances equal to the corresponding eigenvalue&lt;/li&gt;
&lt;li&gt;The sum of variances for the PCs is equal to the sum of variances for the original standardised variables (i.e. 6)&lt;/li&gt;
&lt;li&gt;the first PC has the highest variance, while the following PCs have progressively decreasing variances&lt;/li&gt;
&lt;li&gt;Euclidean distances among subjects are preserved in the new rotated space&lt;/li&gt;
&lt;li&gt;PCs are uncorrelated&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-the-characteristics-of-trait-scores-v&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are the characteristics of trait-scores (V)?&lt;/h2&gt;
&lt;p&gt;The rotation matrix &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; has one row for each of the original variables and six columns. It gives one relevant information: the scores are proportional to the correlation between the PC scores in U and the original variables. This may need an explanation.&lt;/p&gt;
&lt;p&gt;When we look at &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; we do not have any hints on how the components relate to the original matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. For example, we see that the subject ‘A’ has a score on PC1 equal to &lt;span class=&#34;math inline&#34;&gt;\(-3.15\)&lt;/span&gt;; what does this mean in terms of weed infestation? There were a lot of weeds or a few weeds? One way to ascertain this is to look at the correlation between the principal components in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and the original variables in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print( cor(X, U), digits = 3)
##          PC1    PC2      PC3     PC4     PC5      PC6
## POLLA  0.904  0.214 -0.20528  0.0914  0.2809 -0.09055
## CHEPO -0.577 -0.457 -0.67435 -0.0305  0.0464  0.00913
## ECHCG  0.842 -0.307  0.03582 -0.4317 -0.0780 -0.04852
## AMARE  0.668  0.580 -0.41769 -0.0314 -0.2016  0.04236
## XANST  0.946 -0.244  0.04892  0.0203  0.1480  0.14185
## POLAV  0.811 -0.438 -0.00135  0.3270 -0.2058 -0.04265&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that PC1 in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is highly correlated with POLLA, XANST and ECHCG; therefore, the subjects ‘A’, with a very low score in PC1, is expected to have a low ground covering for the aforementioned weed species. The above correlations are known as &lt;em&gt;loadings&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If we look at the rotation matrix &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, we note that trait-scores are proportional to the correlations between subject scores and the original variables (see below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cors &amp;lt;- cor(X, U)
print( cors/V, digits = 3)
##        PC1   PC2   PC3   PC4   PC5   PC6
## POLLA 1.96 0.968 0.822 0.551 0.438 0.185
## CHEPO 1.96 0.968 0.822 0.551 0.438 0.185
## ECHCG 1.96 0.968 0.822 0.551 0.438 0.185
## AMARE 1.96 0.968 0.822 0.551 0.438 0.185
## XANST 1.96 0.968 0.822 0.551 0.438 0.185
## POLAV 1.96 0.968 0.822 0.551 0.438 0.185&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, the rotation matrix V can help us understand the meaning of subject-scores in relation to the traits under study. We’ll get back to this in a few minutes.&lt;/p&gt;
&lt;p&gt;In summary, we can say that &lt;strong&gt;trait-scores relate to the correlations between subject-scores and the original variables in X&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inner-products-of-subject-scores-and-trait-scores&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inner products of subject-scores and trait-scores&lt;/h2&gt;
&lt;p&gt;We have seen that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[U = Z \, V\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the matrix of subject-scores. Thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = U V^T\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed, we have decomposed our original matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; into the product of two matrices: &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. This multiplication can be seen as the ‘inner-products’ of row-vectors in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; (subject-scores) by the row-vectors in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; (trait-scores), which return the original values in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. For example, the PC scores for the herbicide ‘A’ are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;U[1,]
##         PC1         PC2         PC3         PC4         PC5         PC6 
## -3.14916165 -0.69341377 -1.23990604  0.04915220  0.03599628 -0.08713997&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The scores for POLLA (see first row of V) are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V[1,]
##        PC1        PC2        PC3        PC4        PC5        PC6 
##  0.4600611  0.2211836 -0.2498484  0.1657640  0.6410762 -0.4884065&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The inner product for these two vectors is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-3.14916165 * 0.4600611 + 
-0.69341377 * 0.2211836 +
-1.23990604 * -0.2498484 +
0.04915220 * 0.1657640 +
0.03599628 * 0.6410762 +
-0.08713997 * -0.4884065
## [1] -1.218606&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which corresponds, exactly, to the standardised weed covering for POLLA treated with the herbicide ‘A’, i.e. the value in the first row and first column in the original standardised matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In general: &lt;strong&gt;The inner products of subject-score row-vectors and trait-score row-vectors return the observed values in the standardised matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-brief-recap&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A brief recap&lt;/h1&gt;
&lt;p&gt;At the moment, we have three main ‘ingredients’:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Our starting matrix (&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;), containing information about herbicidess and weed coverings&lt;/li&gt;
&lt;li&gt;A subject-score matrix (&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;), containing some information about the herbicides (e.g. their distances)&lt;/li&gt;
&lt;li&gt;A rotation matrix (&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;), which contains some information about the original variables (e.g how they correlate with principal components)&lt;/li&gt;
&lt;li&gt;The inner products of row-vectors in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and row-vectors in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; return the original standardised observations &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is clear that &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, if taken together, contain all information available in &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-the-matter-then&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What’s the matter then?&lt;/h1&gt;
&lt;p&gt;You might say that we have not made much gain, so far! Well, this is not true, indeed. The Eckart-Young theorem says that we do not need to take all columns in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and all columns in &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, as a reduced number of them will give us the best possible representation in reduced space. In other words, if we take, e.g., three columns of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and three columns of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, we will get the best possible representation of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; in a space of dimension equal to three. Likewise, if we take, two columns of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and two columns of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, we will get the best possible representation of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; in a space of dimension equal to two.&lt;/p&gt;
&lt;p&gt;The question is: how many columns should we take? The six original variables had a total variance of 6. Six PC in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; have a total variance of 6, so the quality of representation is 100%. Five PCs in &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; have a total variance of 5.97, so the quality of representation is 99.4%. In general we have a quality of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cumsum(d)/6 * 100
## [1]  64.29967  79.91030  91.16079  96.22653  99.42716 100.00000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that two components give a quality of representation of almost 80%, which is very good. Furthermore, according to the Kaiser criterion, we can remove all components with a variance lower than 1, i.e. those components with a lower variance than the original standardised variables. This criterion confirms that two components are more then enough to represent our original dataset. Of course, the representation is not perfect: we see that the following product:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print( U[,1:2] %*% t(V[,1:2]), digits=3)
##    POLLA   CHEPO  ECHCG  AMARE  XANST   POLAV
## A -1.602  1.2527 -1.130 -1.486 -1.343 -0.9865
## B -0.953  0.2819 -1.405 -0.275 -1.475 -1.4971
## C -0.497  0.2961 -0.497 -0.339 -0.552 -0.4878
## D  1.404 -1.5316  0.305  1.872  0.544  0.0134
## E -0.154  0.0627 -0.200 -0.067 -0.214 -0.2085
## F  0.374 -0.2641  0.308  0.310  0.354  0.2856
## G  0.390  0.1753  1.034 -0.269  1.027  1.1824
## H  0.937 -0.3741  1.228  0.398  1.309  1.2812
## I  0.102  0.1010  0.357 -0.143  0.349  0.4174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;does not return exactly our original matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, but it gives the best possible approximation in reduced rank space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;principal-components-as-an-ordination-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Principal components as an ordination method&lt;/h1&gt;
&lt;p&gt;So far, we have seen that we can use two principal components in place of the six original variables, losing only a minor part of information. We have, therefore, solved the problem of rank reduction: the two principal components can be used in further analyses, such as cluster analysis or regression analysis, wherever using a low number of meaningful variables may be convenient. In this sense, principal component analysis may be prodromic to the adoption of other statistical methods.&lt;/p&gt;
&lt;p&gt;However, in many instances, PCA is used as a method of ordination, in order to understand how similar are the subjects and which variables contribute to the observed differences among subjects. We have already mentioned that we can look at &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; to understand what was the presence of weeds with the different herbicides. Another example: the herbicide ‘H’ has a high positive score on PC1 and low negative score on PC2; therefore, it is characterised by a high ground covering (above average) for weeds with positive scores on PC1 (POLLA, ECHCG, XANST, POLAV) and negative scores on PC2 (CHEPO, ECHCG, XANST, POLAV).&lt;/p&gt;
&lt;p&gt;Instead of simply looking at the matrices &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, we can draw a typical kind of plot, which shows both subject-scores and trait-scores and, for this reason, it is known as ‘biplot’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-biplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The ‘biplot’&lt;/h1&gt;
&lt;p&gt;As I said, a ‘biplot’ shows two components (usually the first two) of both the subject-scores and the trait-scores. I’ll show the biplot for the ‘WeedPop’ dataset. Please note that the two matrices are not on comparable scales and the scores on &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are higher than those on &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. Therefore, the biplot uses a double reference system.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(U[,1:2], V[,1:2], xlab=&amp;quot;PC1 (64%)&amp;quot;, 
       ylab=&amp;quot;PC2 (16%)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/figR_biplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Subjects scores are usually represented as symbols, while trait-scores are represented as ‘vectors’. The biplot shows a huge amount of information, such as:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;similarity of subjects (distances)&lt;/li&gt;
&lt;li&gt;the original values in the standardised matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Due to the characteristics of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; (see above), the distances of subjects on the biplot approximate their Euclidean distances on the original matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. It means that distant subjects are characterised by different weed infestation levels, while close subjects are similar. What is interesting is that such a similarity/dissimilarity considers all the observed variables altogether.&lt;/p&gt;
&lt;p&gt;We can see that the herbicides ‘C’, ‘E’, ‘F’ and ‘I’ form a relatively homogeneous group, with a close-to-zero score on PC1 and different scores on PC2. A second group contains the herbicides ‘A’ and ‘B’, with negative values on PC1 and different scores on PC2. A third group contains the herbicides ‘G’ and ‘H’, with positive scores on PC1 and negative scores on PC2. The herbicide ‘D’ seems to be rather peculiar, with positive scores on both PCs.&lt;/p&gt;
&lt;p&gt;Apart from discovering groups, we need to be able to explain the rationale behind groupings. We have seen that the inner products between subject-scores and trait-scores give us the key to approximate the original matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;. How can we visualize the inner products in the biplot?&lt;/p&gt;
&lt;p&gt;Let’s take a subject-score vector &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; and a trait-score vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: their inner product is the sum of products of their coordinates, which approximates the observed standardised value of that subject for that variable:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z_{ij} \simeq u \cdot v = u_1 v_1 + u_2 v_2 + ... + u_n v_n\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We know that the cosine of the angle &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; between two vectors is equal to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cos \, \phi = \frac{u \cdot v}{\left|u\right| \cdot \left|v\right|}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\left|u\right|\)&lt;/span&gt; and $ |v|$ are the ‘lengths’ of the two vectors. Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cos \, \phi \cdot \left|u\right| \cdot \left|v\right| = u \cdot v = z_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s look at this on a graph, relating to the herbicide ‘A’ and CHEPO. Let’s draw a line along the direction of the vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;: we can identify a positive direction and a negative direction. We can also see that the projection of &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; into the direction spanned by the trait-vector &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; creates the segment &lt;span class=&#34;math inline&#34;&gt;\(OA&amp;#39;\)&lt;/span&gt;, which has length equal to &lt;span class=&#34;math inline&#34;&gt;\(cos \, \phi \cdot \left|u\right|\)&lt;/span&gt; (this is one of the trigonometric ratios in right triangles). Let’s call &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; this length; now we have: &lt;span class=&#34;math inline&#34;&gt;\(z_{ij} = l \times \left| v \right|\)&lt;/span&gt;. We see that the standardised ground covering of one weed is high where the respective trait vector is long and the projection of the subject vector is long in the positive direction. In this case, the projection is long on the positive side, which means that the herbicide ‘A’ was characterised by high ground covering on CHEPO.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;576&#34; /&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s draw all projections:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We clearly see that the herbicide ‘A’ showed the highest infestation with CHEPO, while the herbicide ‘D’ showest the lowest. ‘F’ and ‘H’ were slightly below average, while the others were slightly above average.&lt;/p&gt;
&lt;p&gt;The reader can pick up examples for other herbicides/weeds: just look at the projections of subject-vectors on trait-vectors!&lt;/p&gt;
&lt;p&gt;Let’s conclude this section with two important highlights:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;distances and inner products on biplots are only approximations to the real distances of subjects and observed values for subjects on variables. We need to make sure that the approximation is good!&lt;/li&gt;
&lt;li&gt;The two axes need to have the samwe scaling, otherwise the geometrical propoerties are not respected. Please, do not pull and push a biplot, to fit the available size on a manuscript!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;what-if-we-start-from-centered-not-standardised-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What if we start from centered (not standardised) data?&lt;/h1&gt;
&lt;p&gt;You might wander what happens if we start from centered data (but not standardised). Centered data are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;C &amp;lt;- scale(X, scale = F)
print(C, digits = 2)
##   POLLA CHEPO ECHCG AMARE XANST POLAV
## A -8.03  22.1 -14.1 -5.11 -9.14 -5.13
## B -8.03  -7.9 -22.1 -5.11 -9.14 -5.23
## C -1.13   8.1  -6.1 -1.11 -2.24 -4.23
## D  9.87  -7.9   2.9 13.89  2.76  0.77
## E -3.13  -3.9   2.9 -2.11  0.76 -4.23
## F  2.87  -1.9   7.9  1.89  0.76  0.77
## G -0.13   2.1   7.9  0.89  5.76  9.77
## H  9.87  -5.9   7.9 -1.11  9.76  6.77
## I -2.13  -4.9  12.9 -2.11  0.76  0.77
## attr(,&amp;quot;scaled:center&amp;quot;)
## POLLA CHEPO ECHCG AMARE XANST POLAV 
##   8.1  10.9  25.1   5.1   9.2   5.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This matrix reportes the data as differences with respect to the means and the variances of the different variables are not the same and equal to 1.&lt;/p&gt;
&lt;p&gt;Working with centered data involves the eigenvalue decomposition of the variance covariance matrix (Do you remember? I mentioned this a few sections above…), while the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; matrix is obtained as above, by using the eigenvectors from spectral decomposition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca &amp;lt;- eigen( cov(X) )
d &amp;lt;- pca$values
V &amp;lt;- pca$vectors
row.names(V) &amp;lt;- colnames(WeedPop[,2:7])
colnames(V) &amp;lt;- paste(&amp;quot;PC&amp;quot;, 1:6, sep=&amp;quot;&amp;quot;)
U &amp;lt;- C %*% V
print(U, digits = 3)
##      PC1      PC2    PC3    PC4      PC5    PC6
## A -26.97 -12.2860  3.129 -0.271 -0.00398 -0.581
## B -20.81  17.3167 -2.935  3.081 -1.29287 -0.475
## C  -9.95  -3.6671  3.035 -0.819  2.38378  0.801
## D  12.69   7.3364 11.756 -3.732 -1.45392  0.180
## E   1.13   2.2854 -5.436 -3.412  1.86926  2.247
## F   8.05  -1.6470 -0.455 -2.895  0.22101 -1.607
## G   9.46  -7.3174 -1.213  5.072 -4.98975  0.976
## H  16.34  -0.0632  0.799  7.213  4.07606 -0.509
## I  10.07  -1.9578 -8.680 -4.237 -0.80960 -1.032
print(V, digits = 3)
##          PC1     PC2     PC3     PC4     PC5     PC6
## POLLA  0.349  0.0493  0.5614  0.1677  0.5574 -0.4710
## CHEPO -0.390 -0.8693  0.2981  0.0013  0.0480  0.0327
## ECHCG  0.688 -0.4384 -0.3600 -0.4322 -0.0113 -0.1324
## AMARE  0.213  0.1200  0.6808 -0.4155 -0.4893  0.2547
## XANST  0.372 -0.1050  0.0499  0.4107  0.2631  0.7813
## POLAV  0.263 -0.1555  0.0184  0.6662 -0.6150 -0.2903
print(d, digits = 3)
## [1] 243.01  72.93  34.13  17.49   6.90   1.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important difference is that the variance of components in the subject-score matrix has a sum equal to the sum of variances on the original matrix X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum( apply(X, 2, var) )
## [1] 375.8411&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another important difference is that the inner products return the values in the centered matrix, i.e. the weed coverings of the different species as differences with respect to column-means. The goodness of representation and biplot are very similar to the previous ones. However, we may note that the weeds with higher variances (CHEPO and ECHCG) tend to have higher weight on the ordination, as shown by longer vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cumsum(d)/sum(d) * 100
## [1]  64.65801  84.06118  93.14164  97.79412  99.62927 100.00000
apply(X, 2, var)
##     POLLA     CHEPO     ECHCG     AMARE     XANST     POLAV 
##  43.45750  95.11111 136.86111  32.61111  38.73528  29.06500&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(U[,1:2], V[,1:2], xlab=&amp;quot;PC1 (65%)&amp;quot;, 
       ylab=&amp;quot;PC2 (15%)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_multivar_PCA_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA with R&lt;/h1&gt;
&lt;p&gt;PCA can be easily performed with R, altough there are several functions, among which it is often difficult to make a selection. We will now briefly introduce only &lt;code&gt;prcomp()&lt;/code&gt; and &lt;code&gt;princomp()&lt;/code&gt;, both in the ‘stats’ package.&lt;/p&gt;
&lt;p&gt;These two functions are different in relation to the algorithm that is used for calculation (spectral decomposition and singular value decomposition). In &lt;code&gt;princomp()&lt;/code&gt;, we need to use the argument ‘scale’ to select between a PCA on centered data (‘scale = F’) and a PCA on standardised data (‘scale = T’). In &lt;code&gt;prcomp()&lt;/code&gt;, we can make the same selection by using the argument ‘cor’. Other differences relate to the fact that &lt;code&gt;prcomp()&lt;/code&gt; shows the standard deviations of subject-scores, instead of their variances.&lt;/p&gt;
&lt;p&gt;Please, note that the output of &lt;code&gt;princomp()&lt;/code&gt; is slightly different, as this function operates on standardised data by using population-based standard deviations and not sample-based standard deviations. This might be preferable, as PCA is, fundamentally, a descriptive method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#prcomp()
pcaAnalysis&amp;lt;-prcomp(X, scale=T)
summary(pcaAnalysis)
print( pcaAnalysis$x, digits=3) #Subject-scores
print(pcaAnalysis$rotation, digits=3) #Trait-scores

#princomp()
pcaAnalysis2 &amp;lt;- princomp(X, cor=T)
print(pcaAnalysis2, digits=3)
print(pcaAnalysis2$scores, digits=3)
print(pcaAnalysis2$loadings, digits=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R: a tutorial. Part 3</title>
      <link>https://www.statforbiology.com/2021/stat_drcte_3-reshapingdata/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_drcte_3-reshapingdata/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post. If you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;reshaping-time-to-event-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reshaping time-to-event data&lt;/h1&gt;
&lt;p&gt;The first thing we should consider before working through this tutorial is the structure of germination/emergence data. To our experience, seed scientists are used to storing their datasets in several formats, that may not be immediately usable with the ‘drcte’ and ‘drc’ packages, which this tutorial is built upon. The figure below shows some of the possible formats that I have often encountered in my consulting work.&lt;/p&gt;
&lt;p style=&#34;text-align:center&#34;&gt;
&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_drcte_3-method-data.png&#34; title=&#34;fig:&#34; style=&#34;width:90.0%&#34; alt=&#34;Possible data structures for seed germination/emergence data&#34; /&gt;
&lt;/p&gt;
&lt;p&gt;Both the ‘drcte’ and ‘drc’ packages require that the data is stored in LONG GROUPED format, as shown in the figure above (panel A, top left). Each row represents a time interval, the columns ‘timeBef’ and ‘timeAf’ represent the beginning and ending of the interval, while the column ‘count’ represents the seeds that germinated/emerged during that interval of time. Other columns may be needed, to represent, e.g., the randomisation unit (e.g., Petri dish) within which the count was made and the experimental treatment level that was allocated to that unit.&lt;/p&gt;
&lt;p&gt;Apart from the LONG GROUPED format, other time-to-event packages, such as ‘survival’ (Therneau, 2021) or ‘interval’ (Fay and Shaw, 2010) require a different data format, which could be named as LONG UNGROUPED (Figure above, panel B, top right). In this format, each row represents a seed, while the columns ‘timeBef’ and ‘timeAf’ represent the beginning and ending of the interval during which that seed germinated/emerged. The column ‘count’ is not necessary, while other columns may be necessary, as for the LONG GROUPED format. Apart from the above mentioned ‘survival’ and ‘interval’ packages, this format is also compatible with the ‘drcte’ package.&lt;/p&gt;
&lt;p&gt;Both the GROUPED and UNGROUPED formats have two basic advantages: (i) they can be used with all types of time-to-event data and (ii) they obey to the principles of tidy data (Wickham, 2016). However, to my experience, seed scientists often use other formats, such as the WIDE format (Figure above, panel C, bottom left) or the NONLINEAR REGRESSION format (Figure above, panel D, bottom right), which need to be preliminary transformed into one of the LONG GROUPED or LONG UNGROUPED formats.&lt;/p&gt;
&lt;p&gt;In order to ease the transition from traditional methods of data analysis to time-to-event methods, we decided to develop a couple of service functions that can be used to reshape the data to a format that is more suitable for time-to-event analyses. Let’s see how to do this by using a ‘real-life’ example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;Seeds of &lt;em&gt;L. ornithopodioides&lt;/em&gt; were collected from natural populations, at three different maturation stages, i.e. at 27 Days After Anthesis (DAA), when seeds were still green (Stage A), at 35 DAA, when seeds were brown and soft (Stage B) and at 41 DAA, when seeds were brown and moderately hard (Stage C). Germination assays were performed by placing four replicates of 25 seeds on filter paper (Whatman no. 3) in 9-cm diameter Petri dishes, in the dark and at a constant temperature of 20°C. The filter paper was initially moistened with 5 mL of distilled water and replenished as needed during the assay. Germinated seeds were counted daily over 15 days and removed from the Petri dishes. This dataset is a subset of a bigger dataset, aimed at assessing the time when the hard coat imposes dormancy in seeds of different legume species (Gresta et al., 2011).&lt;/p&gt;
&lt;p&gt;The authors of the above study sent me the above dataset in WIDE format, where the rows represented each Petri dish and the columns represented all information about each dish, including the counts of germinated seeds, which were listed in different columns. The dataset is shown in the table below and it is available as the ‘lotusOr’ dataframe in the ‘drcte’ package.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##  Stage Dish 1  2 3 4 5 6 7 8 9 10 11 12 13 14 15
##      A    1 0  0 0 0 0 1 2 1 1  1  3  1  1  4  2
##      A    2 0  0 0 0 0 1 1 1 1  2  1  2  3  1  1
##      A    3 0  0 0 0 0 3 4 4 3  1  2  1  1  0  1
##      A    4 0  0 0 0 0 1 3 1 2  1  2  1  3  1  1
##      B    5 0  0 0 4 1 2 1 1 0  1  2  2  3  2  1
##      B    6 0  0 0 4 2 2 1 0 1  1  3  4  3  4  0
##      B    7 0  0 0 4 0 4 1 2 1  1  3  3  2  2  0
##      B    8 0  0 0 4 3 2 2 3 1  1  1  1  1  0  0
##      C    9 1 10 1 2 1 2 3 2 0  0  0  0  1  0  0
##      C   10 1 10 4 1 4 0 1 1 0  0  0  0  1  0  1
##      C   11 1 11 5 1 2 2 1 0 1  0  0  1  0  0  0
##      C   12 0 16 1 2 2 1 1 0 0  1  0  0  0  1  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The WIDE format is handy for swift calculations with a spreadsheet, but, in general, it is not ok, as: (1) it does not obey to the principle of tidy data (Wickham, 2016); (2) it is not generally efficient and it cannot be used with a set of germination assays with different monitoring schedules. Therefore, facing a dataset in the WIDE format, we need to reshape it into a LONG format.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transforming-from-wide-to-long-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Transforming from WIDE to LONG GROUPED&lt;/h1&gt;
&lt;p&gt;For such a reshaping, we could use one of the available R functions, such us &lt;code&gt;pivot_longer()&lt;/code&gt; in the ‘tidyverse’ or &lt;code&gt;melt()&lt;/code&gt; in the ‘reshape’ package. However, when reshaping the data, it is also useful to make e few transformations, such as producing cumulative counts and proportions, that might be useful to plot graphs, or for other purposes. Therefore, we developed the &lt;code&gt;melt_te()&lt;/code&gt; function; let’s look at its usage, in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasetG &amp;lt;- melt_te(lotusOr, count_cols = 3:17, treat_cols = Stage, 
                   monitimes = 1:15, n.subjects = rep(25,12))
head(datasetG, 16)
##    Stage Units timeBef timeAf count nCum propCum
## 1      A     1       0      1     0    0    0.00
## 2      A     1       1      2     0    0    0.00
## 3      A     1       2      3     0    0    0.00
## 4      A     1       3      4     0    0    0.00
## 5      A     1       4      5     0    0    0.00
## 6      A     1       5      6     1    1    0.04
## 7      A     1       6      7     2    3    0.12
## 8      A     1       7      8     1    4    0.16
## 9      A     1       8      9     1    5    0.20
## 10     A     1       9     10     1    6    0.24
## 11     A     1      10     11     3    9    0.36
## 12     A     1      11     12     1   10    0.40
## 13     A     1      12     13     1   11    0.44
## 14     A     1      13     14     4   15    0.60
## 15     A     1      14     15     2   17    0.68
## 16     A     1      15    Inf     8   NA      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;melt_te()&lt;/code&gt; function requires the following arguments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;data: the original dataframe&lt;/li&gt;
&lt;li&gt;count_cols: the positions of the columns in ‘data’, listing, for each dish, the counts of germinated seeds at each assessment time (columns)&lt;/li&gt;
&lt;li&gt;treat: the columns in ‘data’, listing, for each dish, the levels of each experimental treatment&lt;/li&gt;
&lt;li&gt;monitimes: a vector of monitoring times that needs to be of the same length as the argument ‘count_cols’&lt;/li&gt;
&lt;li&gt;subjects: a vector with the number of viable seeds per dish, at the beginning of the assay. If this argument is omitted, the function assumes that such a number is equal to the total count of germinated seeds in each dish&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The functions outputs a dataframe in LONG format, where the initial columns code for the experimental treatments, the column ‘Units’ represents the original rows (Petri dishes), ‘timeAf’ represents the time at which the observation was made, ‘timeBef’ represents the time at which the previous observation was made, ‘count’ represents the number of seeds that germinated between ‘timeBef’ and ‘timeAf’, ‘nCum’ is the cumulative count and ‘propCum’ is the cumulative proportion of germinated seeds. An extra row is added for the ungerminated seeds at the end of the assay, with ‘timeBef’ equal to the final assessment time and ‘timeAf’ equal to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transforming-from-wide-to-long-ungrouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Transforming from WIDE to LONG UNGROUPED&lt;/h1&gt;
&lt;p&gt;The LONG GROUPED format is good for the packages ‘drc’ and ‘drcte’. However, we might be interested in performing data analyses within the framework of survival analysis, e.g. with the ‘survival’ or ‘interval’ packages, that require the LONG UNGROUPED format. In order to reshape the original dataset into the LONG UNGROUPED format, we can use the same &lt;code&gt;melt_te()&lt;/code&gt; function, by setting the argument &lt;code&gt;grouped = FALSE&lt;/code&gt;. An example is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;datasetU &amp;lt;- melt_te(lotusOr, count_cols = 3:17, treat_cols = 1, 
                   monitimes = 1:15, n.subjects = rep(25,12), grouped = F)
head(datasetU, 16)
##    Stage Units timeBef timeAf
## 1      A     1       5      6
## 2      A     1       6      7
## 3      A     1       6      7
## 4      A     1       7      8
## 5      A     1       8      9
## 6      A     1       9     10
## 7      A     1      10     11
## 8      A     1      10     11
## 9      A     1      10     11
## 10     A     1      11     12
## 11     A     1      12     13
## 12     A     1      13     14
## 13     A     1      13     14
## 14     A     1      13     14
## 15     A     1      13     14
## 16     A     1      14     15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;from-long-grouped-to-long-ungrouped-and-vice-versa&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;From LONG GROUPED to LONG UNGROUPED (and vice-versa)&lt;/h1&gt;
&lt;p&gt;If necessary, we can easily reshape back and forth from the GROUPED and UNGROUPED formats, by using the funtions &lt;code&gt;ungroup_te&lt;/code&gt; and &lt;code&gt;group_te()&lt;/code&gt;. See the sample code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# From LONG GROUPED to LONG UNGROUPED
datasetU2 &amp;lt;- ungroup_te(datasetG, count)[,-c(5, 6)]
head(datasetU2, 16)
##    Stage Units timeBef timeAf
## 1      A     1       5      6
## 2      A     1       6      7
## 3      A     1       6      7
## 4      A     1       7      8
## 5      A     1       8      9
## 6      A     1       9     10
## 7      A     1      10     11
## 8      A     1      10     11
## 9      A     1      10     11
## 10     A     1      11     12
## 11     A     1      12     13
## 12     A     1      13     14
## 13     A     1      13     14
## 14     A     1      13     14
## 15     A     1      13     14
## 16     A     1      14     15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# From LONG UNGROUPED to LONG GROUPED
datasetG2 &amp;lt;- group_te(datasetU)
head(datasetG2, 16)
##    Stage Units timeBef timeAf count
## 1      A     1       5      6     1
## 2      A     1       6      7     2
## 3      A     1       7      8     1
## 4      A     1       8      9     1
## 5      A     1       9     10     1
## 6      A     1      10     11     3
## 7      A     1      11     12     1
## 8      A     1      12     13     1
## 9      A     1      13     14     4
## 10     A     1      14     15     2
## 11     A     1      15    Inf     8
## 12     A     2       5      6     1
## 13     A     2       6      7     1
## 14     A     2       7      8     1
## 15     A     2       8      9     1
## 16     A     2       9     10     2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;from-nonlinear-regression-to-long-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;From NONLINEAR REGRESSION to LONG GROUPED&lt;/h1&gt;
&lt;p&gt;In other instances, it may happen that the dataset was prepared as required by nonlinear regression analysis, i.e. listing the cumulative number of germinated seeds at each inspection time. The following Table shows an example for the first Petri dish, as available in the ‘lotusCum’ dataframe in the ‘drcte’ package.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##  Stage Dish Time nCum
##      A    1    1    0
##      A    1    2    0
##      A    1    3    0
##      A    1    4    0
##      A    1    5    0
##      A    1    6    1
##      A    1    7    3
##      A    1    8    4
##      A    1    9    5
##      A    1   10    6
##      A    1   11    9
##      A    1   12   10
##      A    1   13   11
##      A    1   14   15
##      A    1   15   17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this situation, we need to ‘decumulate’ the counts and add the beginning of each inspection interval (e.g., ‘timeBef’). We can easily do this by using the &lt;code&gt;decumulate_te()&lt;/code&gt; function in ‘drcte’. An example is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(lotusCum)
##   Stage Dish Time nSeeds nCum Prop
## 1     A    1    1      0    0 0.00
## 2     A    1    2      0    0 0.00
## 3     A    1    3      0    0 0.00
## 4     A    1    4      0    0 0.00
## 5     A    1    5      0    0 0.00
## 6     A    1    6      1    1 0.04
dataset_sd &amp;lt;- decumulate_te(lotusCum,
                            resp = nCum, 
                            treat_cols = Stage,
                            monitimes = Time,
                            units = Dish, 
                            n.subjects = rep(25, 12),
                            type = &amp;quot;count&amp;quot;)
dataset_sd &amp;lt;- decumulate_te(lotusCum,
                            resp = Prop, 
                            treat_cols = &amp;quot;Stage&amp;quot;,
                            monitimes = Time,
                            units = Dish, 
                            n.subjects = rep(25, 12),
                            type = &amp;quot;proportion&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;decumulate_te()&lt;/code&gt; function requires the following arguments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;resp: the column containing the counts/proportions of germinated seeds&lt;/li&gt;
&lt;li&gt;treat: the columns listing, for each row of data, the corresponding level of experimental factors (one factor per column)&lt;/li&gt;
&lt;li&gt;monitimes: the column listing monitoring times&lt;/li&gt;
&lt;li&gt;units: the column listing the randomisation units&lt;/li&gt;
&lt;li&gt;subjects: a vector listing the number of viable seeds, at the beginning of the assay, for each randomisation unit.&lt;/li&gt;
&lt;li&gt;type: a value specifying if ‘resp’ contains ‘counts’ or ‘proportions’&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We do hope that, with these functions, we can menage to ease your transition from traditional methods of data analysis to time-to-event methods, for seed germination/emergence assays.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Michael P. Fay, Pamela A. Shaw (2010). Exact and Asymptotic Weighted Logrank Tests for Interval Censored Data: The interval R Package. Journal of Statistical Software, 36(2), 1-34. URL &lt;a href=&#34;https://www.jstatsoft.org/v36/i02/&#34; class=&#34;uri&#34;&gt;https://www.jstatsoft.org/v36/i02/&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Gresta, F, G Avola, A Onofri, U Anastasi, A Cristaudo (2011) When Does Hard Coat Impose Dormancy in Legume Seeds? Lotus and Scorpiurus Case Study. Crop Science 51:1739–1747&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J. C., Gerhard, D. (2015) Dose-Response Analysis Using R. PLOS ONE, 10(12), e0146021&lt;/li&gt;
&lt;li&gt;Therneau T (2021). &lt;em&gt;A Package for Survival Analysis in R&lt;/em&gt;. R package version 3.2-11, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=survival&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=survival&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Wickham, H, G Grolemund (2016) R for data science: import, tidy, transform, visualize, and model data. First edition. Sebastopol, CA: O’Reilly. 492 p&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R (a tutorial). Part 2</title>
      <link>https://www.statforbiology.com/2021/stat_drcte_2-methods/</link>
      <pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_drcte_2-methods/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a follow-up post: if you are interested in other posts of this series, please go to: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. All these posts exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;div id=&#34;survival-analysis-and-germinationemergence-data-an-overlooked-connection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Survival analysis and germination/emergence data: an overlooked connection&lt;/h1&gt;
&lt;p&gt;Seed germination and emergence data describe the time until the event of interest occurs and, therefore, they can be put together in the wide group of &lt;strong&gt;time-to-event data&lt;/strong&gt;. You may wonder: what’s the matter with time-to-event data? Do they have anything special that needs our attention? The answer is, definitely, yes!&lt;/p&gt;
&lt;p&gt;Indeed, with very few exceptions, time-to-event data are affected by a peculiar form of uncertainty, which takes the name of &lt;strong&gt;censoring&lt;/strong&gt;. It relates to the fact that, due to the typical monitoring schedule, the exact time-to-event may not always be determined. Think about a germination assay, where we put, e.g., 100 seeds in a Petri dish and make daily inspections. At each inspection, we count the number of germinated seeds. In the end, what have we learnt about the germination time of each seed? It is easy to note that we do not have exact values, we only have a set of intervals. Let’s consider three possible situations.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we found a germinated seed at the first inspection time, we only know that germination took place before the inspection (left-censoring).&lt;/li&gt;
&lt;li&gt;If we find a germinated seed at the second (or later) inspection time, we only know that germination took place somewhere between the previous and the present inspection (interval-censoring).&lt;/li&gt;
&lt;li&gt;If we find an ungerminated seed at the end of the experiment, we only know that its germination time, if any, is higher than the duration of the experiment (right-censoring).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Censoring implies a lot of uncertainty, which is additional to other more common sources of uncertainty, such as the individual-to-individual variability or random errors in the manipulation process. The problem of censoring has been widely recognised in other disciplines, such as medicine, in relation to survival data (time-to-death data). In order to address that problem, a vast body of methods has developed, which goes under the name of ‘survival analysis’.&lt;/p&gt;
&lt;p&gt;In principle, time-to-germination and time-to-emergence data are totally similar to time-to-death data, apart from the fact that we usually deal with different (and less sad) events. Unfortunately, such a connection has been largely overlooked by plant biologists and, as the consequence, the problem of censoring has been most often neglected. In order to understand why this is not a good think, it is important that we become aware about the possible consequences of neglecting censoring during data analyses. That is why I decided to sit and write this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-example-a-simulated-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example: a simulated dataset&lt;/h1&gt;
&lt;p&gt;It may be useful to think about a possible mechanism by which time-to-event data arise. Let’s imagine that we have a population with 85% of germinable seeds (the other ones are dormant and, therefore, they are not immediately able to germinate). The germinable fraction is composed by seeds with variables germination times, as dictated by their genotypes and environmental conditions. Let’s assume that such a variability can be described by using a log-logistic distribution, with a median germination time &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt; days and a shape parameter &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we sample 100 seeds from that population, the sample will not necessarily reflect the characteristics of the whole population. We can do this sampling in R, by using a three-steps approach.&lt;/p&gt;
&lt;div id=&#34;step-1-the-ungerminated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: the ungerminated fraction&lt;/h2&gt;
&lt;p&gt;First, let’s simulate the number of germinated seeds, assuming a binomial distribution with a proportion of successes equal to 0.85. We use the random number generator &lt;code&gt;rbinom()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 1
d &amp;lt;- 0.85
set.seed(1234)
nGerm &amp;lt;- rbinom(1, 100, d)
nGerm
## [1] 89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, in this instance, 89 seeds germinated out of 100, which is not the expected 85%. This may be regarded as an expression of the typical random sampling variability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-germination-times-for-the-germinated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: germination times for the germinated fraction&lt;/h2&gt;
&lt;p&gt;Second, let’s simulate the germination times for these 89 germinable seeds, by drawing from a log-logistic distribution with &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt;. To this aim, we use the &lt;code&gt;rllogis()&lt;/code&gt; function in the &lt;code&gt;actuar&lt;/code&gt; package (Dutang et al., 2008):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 2
library(actuar)
b &amp;lt;- 1.6; e &amp;lt;- 4.5 
Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
Gtimes &amp;lt;- c(Gtimes, rep(NA, 100 - nGerm))
Gtimes
##   [1]  3.2936708  3.4089762  3.2842199  1.4401630  3.1381457 82.1611955
##   [7]  9.4906364  2.9226745  4.3424551  2.7006042  4.0202158  8.0519663
##  [13]  0.9492013  7.8199588  1.6163588  7.9661485  8.4641154 11.2879041
##  [19]  9.5014360  7.2786264  7.5809838 12.7421713 32.7999661  9.9691944
##  [25]  1.8137333  4.2197542  1.0218849  1.6604417 30.0352308  5.0235265
##  [31]  8.5085067  7.5367739  4.4185382 11.5555259  2.1919263 10.6509339
##  [37]  8.6857151  0.2185902  1.8377033  3.9362727  3.0864702  7.3804164
##  [43]  3.2978782  7.0100360  4.4775843  2.8328842  4.6721090  9.1258796
##  [49]  2.1485568 21.8749808  7.4265984  2.5148724  4.4491466 13.1132301
##  [55]  4.4559642  4.5684584  2.2556488 11.8783556  1.5338755  1.4106592
##  [61] 31.8419420  7.2666641 65.0154287  9.2798476  2.5988399  7.4612907
##  [67]  4.4048509 27.7439121  3.8257187 15.4967751  1.1960785 62.5152642
##  [73]  2.0169970 19.1134899  4.2891084  6.0420938 22.6521417  7.1946293
##  [79]  2.9028993  0.9241876  4.8277336 13.8068124  4.0273655 10.8651761
##  [85]  1.1509735  5.9593534  7.4009589 12.6839405  1.1698335         NA
##  [91]         NA         NA         NA         NA         NA         NA
##  [97]         NA         NA         NA         NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we have a vector hosting 100 germination times (‘Gtimes’). Please, note that I also added 11 NA values, to represent non-germinable seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-counts-of-germinated-seeds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: counts of germinated seeds&lt;/h2&gt;
&lt;p&gt;Unfortunately, due to the monitoring schedule, we cannot observe the exact germination time for each single seed in the sample; we can only count the seeds which have germinated between two assessment times. Therefore, as the third step, we simulate the observed counts, by assuming daily monitoring for 40 days; what we do, is a sort of ‘binning’ process, where we assign each seed to the time interval during which it germinated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obsT &amp;lt;- seq(1, 40, by=1) #Observation schedule
count &amp;lt;- table( cut(Gtimes, breaks = c(0, obsT)) )
count
## 
##   (0,1]   (1,2]   (2,3]   (3,4]   (4,5]   (5,6]   (6,7]   (7,8]   (8,9]  (9,10] 
##       3      11      10       8      13       2       1      12       4       5 
## (10,11] (11,12] (12,13] (13,14] (14,15] (15,16] (16,17] (17,18] (18,19] (19,20] 
##       2       3       2       2       0       1       0       0       0       1 
## (20,21] (21,22] (22,23] (23,24] (24,25] (25,26] (26,27] (27,28] (28,29] (29,30] 
##       0       1       1       0       0       0       0       1       0       0 
## (30,31] (31,32] (32,33] (33,34] (34,35] (35,36] (36,37] (37,38] (38,39] (39,40] 
##       1       1       1       0       0       0       0       0       0       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, e.g., 11 germinated seeds were counted at day 2; therefore they germinated between day 1 and day 2 and their real germination time is unknown, but included in the range between 1 and 2 (left-open and right-closed). This is a typical example of interval-censoring (see above).&lt;/p&gt;
&lt;p&gt;We can also see that, in total, we counted 86 germinated seeds and, therefore, 14 seeds were still ungerminated at the end of the assay. For this simulation exercise, we know that 11 seeds were non-germinable and three seeds were germinable, but they were not allowed enough time to germinate (look at the table above: there are 3 seeds with germination times higher than 40). In real life, this is another source of uncertainty: we might be able to ascertain whether these 14 seeds are viable or not (e.g. by using a tetrazolium test), but, if they are viable, we would never be able to tell whether they are dormant or their germination time is simply longer than the duration of the assay. In real life, we can only reach an uncertain conclusion: the germination time of the 14 ungerminated seeds is comprised between 40 days to infinity; this is an example of right-censoring.&lt;/p&gt;
&lt;p&gt;The above uncertainty affects our capability of describing the germination time-course from the observed data. We can try to picture the situation in the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_2-Methods_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the real germination time-course? The red one? The blue one? Something in between? We cannot really say this from our dataset, we are uncertain. The grey areas represent the uncertainty due to censoring. Do you think that we can reasonably neglect it?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neglecting-censoring&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neglecting censoring&lt;/h1&gt;
&lt;p&gt;For a moment, let’s forget about those grey uncertainty areas. Let’s forget about censoring; this is what it is often done in literature for germination data: we associate the observed proportion of germinated seeds to the exact time when it was observed. It is, indeed, an abuse, as the observed data tell us a different story: the observed proportion of germinated seeds might have been attained before the moment of inspection (and not necessarily at the moment of inspection). But we disregard this and fit a nonlinear regression model, i.e. a log-logistic function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
G(t) = \frac{d}{ 1 + exp \left\{ - b \right[ \log(t) - \log(e) \left] \right\}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the fraction of germinated seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the germinable fraction, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time for the germinable fraction and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The above model is sygmoidally shaped and it is symmetric on a log-time scale. The three parameters are biologically relevant, as they describe the three main features of seed germination, i.e. capability (&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;), speed (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and uniformity (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In order to fit a nonlinear regression model, we need to transform the observed data, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
propCum &amp;lt;- cumsum(counts)/100
df &amp;lt;- data.frame(time = obsT, counts = counts, propCum = propCum) 
df
##    time counts propCum
## 1     1      3    0.03
## 2     2     11    0.14
## 3     3     10    0.24
## 4     4      8    0.32
## 5     5     13    0.45
## 6     6      2    0.47
## 7     7      1    0.48
## 8     8     12    0.60
## 9     9      4    0.64
## 10   10      5    0.69
## 11   11      2    0.71
## 12   12      3    0.74
## 13   13      2    0.76
## 14   14      2    0.78
## 15   15      0    0.78
## 16   16      1    0.79
## 17   17      0    0.79
## 18   18      0    0.79
## 19   19      0    0.79
## 20   20      1    0.80
## 21   21      0    0.80
## 22   22      1    0.81
## 23   23      1    0.82
## 24   24      0    0.82
## 25   25      0    0.82
## 26   26      0    0.82
## 27   27      0    0.82
## 28   28      1    0.83
## 29   29      0    0.83
## 30   30      0    0.83
## 31   31      1    0.84
## 32   32      1    0.85
## 33   33      1    0.86
## 34   34      0    0.86
## 35   35      0    0.86
## 36   36      0    0.86
## 37   37      0    0.86
## 38   38      0    0.86
## 39   39      0    0.86
## 40   40      0    0.86&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In practice, we determine the cumulative proportion (or percentage) of germinated seeds (‘propCum’), which is used as the response variable, while the observation time (‘time’) is used as the independent variable. Then, we fit the log-logistic function by non-linear least squares regression. I’ll say this again: &lt;strong&gt;you can clearly see that, by doing so, we totally neglect the grey areas in the figure above, we only look at the observed points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let’s use the ‘drm’ function, in the ‘drc’ package (Ritz et al., 2015). The argument ‘fct’ is used to set the fitted function to log-logistic with three parameters (the equation above).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;plot()&lt;/code&gt; and &lt;code&gt;summary()&lt;/code&gt; methods can be used to plot a graph and to retrieve the estimated parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
mod &amp;lt;- drm(propCum ~ time, data = df,
           fct = LL.3() )
plot(mod, log = &amp;quot;&amp;quot;,
      xlab = &amp;quot;Time (days)&amp;quot;,
      ylab = &amp;quot;Proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_drcte_2-Methods_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.8497771  0.0702626 -26.327 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.8768793  0.0070126 125.044 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.2691575  0.1020457  51.635 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.01762168 (37 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that our estimates are very close to the real values (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; = 1.85 vs. 1.6; &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; = 5.27 vs. 4.5; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; = 0.88 vs. 0.86) and we also see that standard errors are rather small (the coefficient of variability goes from 1 to 4%). There is a difference in sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which relates to the fact that the &lt;code&gt;LL.3()&lt;/code&gt; function in ‘drc’ removes the minus sign in the equation above. Please, disregard this aspect, which stems from the fact that the ‘drc’ package is rooted in pesticide bioassays.&lt;/p&gt;
&lt;p&gt;Do you think that this analysis is not that bad? Wait a moment. Let’s see what happens if we account for censoring.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting-for-censoring&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Accounting for censoring&lt;/h1&gt;
&lt;p&gt;How can we account for censoring? The answer is simple: we should use fitting methods which are specifically devised to incorporate the uncertainty due to censoring. We said that, in medicine, the body of these methods goes under the name of survival analysis and, from this framework, we can borrow a parametric survival model. However, I will not use the name ‘survival model’ as we are not dealing with a survival process. Instead, I will use the name &lt;strong&gt;parametric time-to-event model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;My colleagues and I have extensively talked about parametric time-to-event models in two of our recent papers and related appendices (Onofri et al., 2019; Onofri et al., 2018). Therefore, I will not go into detail, now. I will just say that time-to-event models directly consider the observed counts as the response variable. As the independent variable, they consider the extremes of each time interval (‘timeBef’ and ‘timeAf’; see below). We immediately see two differences with nonlinear regression: (1) we do not need to transform the observed counts into cumulative proportions, and (2) by using an interval as the independent variable, we inject into the model the uncertainty due to censoring (the grey areas in the figure above). Let’s reshape the dataset as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(timeBef = c(0, obsT), timeAf = c(obsT, Inf), counts = c(as.numeric(counts), 100 - sum(counts)) )
df
##    timeBef timeAf counts
## 1        0      1      3
## 2        1      2     11
## 3        2      3     10
## 4        3      4      8
## 5        4      5     13
## 6        5      6      2
## 7        6      7      1
## 8        7      8     12
## 9        8      9      4
## 10       9     10      5
## 11      10     11      2
## 12      11     12      3
## 13      12     13      2
## 14      13     14      2
## 15      14     15      0
## 16      15     16      1
## 17      16     17      0
## 18      17     18      0
## 19      18     19      0
## 20      19     20      1
## 21      20     21      0
## 22      21     22      1
## 23      22     23      1
## 24      23     24      0
## 25      24     25      0
## 26      25     26      0
## 27      26     27      0
## 28      27     28      1
## 29      28     29      0
## 30      29     30      0
## 31      30     31      1
## 32      31     32      1
## 33      32     33      1
## 34      33     34      0
## 35      34     35      0
## 36      35     36      0
## 37      36     37      0
## 38      37     38      0
## 39      38     39      0
## 40      39     40      0
## 41      40    Inf     14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can you see the difference? Please, note that we also added the ungerminated seeds at the end, with an uncertainty interval going from 40 days to infinity.&lt;/p&gt;
&lt;p&gt;Time-to-event models can be easily fitted by using the &lt;code&gt;drm()&lt;/code&gt; function in the ‘drc’ package, although we should specify that we want to use a time-to-event method, by setting the &lt;code&gt;type = &#34;event&#34;&lt;/code&gt; argument. More simply, we can use the &lt;code&gt;drmte()&lt;/code&gt; function in the ‘drcte’ package, that is a specific package for time-to-event methods. Both the functions use the same syntax and, with respect to nonlinear regression, there are some important differences in the model call. In particular, a nonlinear regression model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CumulativeProportion ~ timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other hand, a time-to-event model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Count ~ timeBef + timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full model call is shown below, both with &lt;code&gt;drm()&lt;/code&gt; and with &lt;code&gt;drmte()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Time-to-event model
library(drcte)
# modTE &amp;lt;- drm(counts ~ timeBef + timeAf, data = df, 
#            fct = LL.3(), type = &amp;quot;event&amp;quot;)
modTE &amp;lt;- drmte(counts ~ timeBef + timeAf, data = df, 
           fct = LL.3())
summary(modTE)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.826006   0.194579 -9.3844 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.881476   0.036928 23.8701 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.302109   0.565273  9.3797 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the nonlinear regression fit, the estimates from a time-to-event fit are very similar, but the standard errors are much higher (the coefficient of variability now goes from 4 to 11%). That was expected: we have added the uncertainty due to censoring.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neglecting-or-accounting-for-censoring&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neglecting or accounting for censoring?&lt;/h1&gt;
&lt;p&gt;You may wonder which of the two analysis is right and which is wrong. We cannot say this from just one dataset. However, we can repeat the Monte Carlo simulation above to extract 1000 samples, fit the model by using the two methods and retrieve parameter estimates and standard errors for each sample and method. We do this by using the code below (please, be patient… it may take some time).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GermSampling &amp;lt;- function(nSeeds, timeLast, stepOss, e, b, d){
    
    #Draw a sample as above
    nGerm &amp;lt;- rbinom(1, nSeeds, d)
    Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
    Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))

    
    #Generate the observed data
    obsT &amp;lt;- seq(1, timeLast, by=stepOss) 
    counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
    propCum &amp;lt;- cumsum(counts)/nSeeds
    timeBef &amp;lt;- c(0, obsT)
    timeAf &amp;lt;- c(obsT, Inf)
    counts &amp;lt;- c(counts, nSeeds - sum(counts))
    
    #Calculate the T50 with two methods
    mod &amp;lt;- drm(propCum ~ obsT, fct = LL.3() )
    modTE &amp;lt;- drm(counts ~ timeBef + timeAf, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
    c(b1 = summary(mod)[[3]][1,1],
      ESb1 = summary(mod)[[3]][1,2],
      b2 = summary(modTE)[[3]][1,1],
      ESb2 = summary(modTE)[[3]][1,2],
      d1 = summary(mod)[[3]][2,1],
      ESd1 = summary(mod)[[3]][2,2],
      d2 = summary(modTE)[[3]][2,1],
      ESd2 = summary(modTE)[[3]][2,2],
      e1 = summary(mod)[[3]][3,1],
      ESe1 = summary(mod)[[3]][3,2],
      e2 = summary(modTE)[[3]][3,1],
      ESe2 = summary(modTE)[[3]][3,2] )
}

set.seed(1234)
result &amp;lt;- data.frame()
for (i in 1:1000) {
  res &amp;lt;- GermSampling(100, 40, 1, 4.5, 1.6, 0.85)
  result &amp;lt;- rbind(result, res)
} 
names(result) &amp;lt;- c(&amp;quot;b1&amp;quot;, &amp;quot;ESb1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;ESb2&amp;quot;,
                   &amp;quot;d1&amp;quot;, &amp;quot;ESd1&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;ESd2&amp;quot;,
                   &amp;quot;e1&amp;quot;, &amp;quot;ESe1&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;ESe2&amp;quot;)
result &amp;lt;- result[result$d2 &amp;gt; 0,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have stored our results in the data frame ‘result’. The means of estimates obtained for both methods should be equal to the real values that we used for the simulation, which will ensure that estimators are unbiased. The means of standard errors (in brackets, below) should represent the real sample-to-sample variability, which may be obtained from the Monte Carlo standard deviation, i.e. from the standard deviation of the 1000 estimates for each parameter and method.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Nonlinear regression&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.63 (0.051)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.006)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.086)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Time-to-event method&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.62 (0.187)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.579)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Real values&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.60 (0.188)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.593)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We clearly see that both nonlinear regression and the time-to-event method lead to unbiased estimates of model parameters. However, standard errors from nonlinear regression are severely biased and underestimated. On the contrary, standard errors from time-to-event method are unbiased.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Censoring is peculiar to germination assays and other time-to-event studies. It may have a strong impact on the reliability of our standard errors and, consequently, on hypotheses testing. Therefore, censoring should never be neglected and time-to-event methods should necessarily be used for data analyses. The body of time-to-event methods often goes under the name of ‘survival analysis’, which creates a direct connection between survival data and germination/emergence data.&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;C. Dutang, V. Goulet and M. Pigeon (2008). actuar: An R Package for Actuarial Science. Journal of Statistical Software, vol. 25, no. 7, 1-37.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Paolo Benincasa, M B Mesgaran, and Christian Ritz. 2018. Hydrothermal-Time-to-Event Models for Seed Germination. European Journal of Agronomy 101: 129–39.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Hans Peter Piepho, and Marcin Kozak. 2019. Analysing Censored Data in Agricultural Research: A Review with Examples and Software Tips. Annals of Applied Biology, 174, 3-13.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Ritz, C., 2022. A unified framework for the analysis of germination, emergence, and other time-to-event data in weed science. Weed Science 1–13. &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/wsc.2022.8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ritz, C., F. Baty, J. C. Streibig, and D. Gerhard. 2015. Dose-Response Analysis Using R. PLOS ONE, 10 (e0146021, 12).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing seed germination and emergence data with R (a tutorial). Part 1</title>
      <link>https://www.statforbiology.com/2021/stat_drcte_1-intro/</link>
      <pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_drcte_1-intro/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction-to-the-tutorial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to the tutorial&lt;/h1&gt;
&lt;p&gt;Germination/emergence assays are relatively easy to perform, by following standardised procedures, as described, e.g., by the International Seed Testing Association (&lt;a href=&#34;https://www.ingentaconnect.com/content/ista/rules/2019/00002019/00000001&#34;&gt;see here&lt;/a&gt; ). In short, we take a sample of seeds and we put them in an appropriate container. We put the container in the right environmental conditions (e.g., relating to humidity content and temperature) and we inspect the seeds according to a regular schedule (e.g., daily). At each inspection, we count the number of germinated/emerged seeds and remove them from the containers; inspections are performed until no new germinations/emergences are observed for a sufficient amount of time.&lt;/p&gt;
&lt;p&gt;We see that these assays are rather simple, but, in spite of such simplicity, the process of data analysis still presents several grey areas. How should we quantify the germination/emergence process? How should we compare the germination/emergence of different seed lots?&lt;/p&gt;
&lt;p&gt;A brief survey of literature shows that a plethora of methods is used, which is certainly encouraged by the wide availability of computer packages. Some of these methods have been around for quite a while (e.g., the use of germination indices or nonlinear regression), some others are relatively new (e.g., methods from survival analysis). It is clear that not all these methods are efficient or reliable, especially when they are used with little concern about the basic assumptions that each method makes.&lt;/p&gt;
&lt;p&gt;Furthermore, the use of a lot of different methods of data analysis by different research groups may serve the purpose of creativity, but are we really sure that it also serves the purpose of advancing science? Wouldn’t it be better if we could use the same reliable methods of data analysis, so that we could better understand each other, compare our results and pool them together?&lt;/p&gt;
&lt;p&gt;Therefore, together with some collegues, we decided to start defining a framework for the analysis of germination and emergence data, which might help the readers to select efficient and reliable methods and, lately, improve comparisons and communication of results within the scientific community. We decided to structure this framework as the combination of (i) a step-by-step procedure (ii) the methods to accomplish it and (iii) a user friendly software interface, based on a new R package.&lt;/p&gt;
&lt;p&gt;This is the first of a coming series of webpages, aimed at presenting the framework and related methods, provide examples together with commented R code and datasets. The webpages can be found by using the following link: &lt;a href=&#34;https://www.statforbiology.com/tags/drcte/&#34;&gt;https://www.statforbiology.com/tags/drcte/&lt;/a&gt;. These webpages exapand on a paper that we have recently published in the Journal ‘Weed Science’; please follow &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34;&gt;this link&lt;/a&gt; to the paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-r-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The R package&lt;/h1&gt;
&lt;p&gt;The analyses we propose for seed germination/emergence assays can be performed by using our new R package, namely &lt;code&gt;drcte&lt;/code&gt;. This package is heavily based on the ‘drc’ package (Ritz et al., 2015), that is a very flexible software for general model fitting purposes. Although &lt;code&gt;drc&lt;/code&gt; contains, already, several basic functions for time-to-event analyses, we felt that, in order to meet the specific needs of agricultural research, it would be useful to make some further customisation and develop some additional functions, which we implemented in the ‘drcte’ package. Furthermore, we also created the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, that contains specific functions for seed germination/emergence assays.&lt;/p&gt;
&lt;p&gt;Both the &lt;code&gt;drcte&lt;/code&gt; and &lt;code&gt;drcSeedGerm&lt;/code&gt; packages can be downloaded and installed from gitHub, by using the code proposed in the box below. It requires the ‘devtools’ package, that needs to be as well installed, if it is not already included in the system.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)
library(devtools)
install_github(&amp;quot;OnofriAndreaPG/drcte&amp;quot;)
install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do hope you may find the package and tutorial useful to your purposes.&lt;/p&gt;
&lt;p&gt;Please, stay tuned for further posts.&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Ritz, C., 2022. A unified framework for the analysis of germination, emergence, and other time-to-event data in weed science. Weed Science 1–13. &lt;a href=&#34;https://doi.org/10.1017/wsc.2022.8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/wsc.2022.8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J. C., Gerhard, D. (2015) Dose-Response Analysis Using R. PLOS ONE, 10(12), e0146021&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why are derivatives important in life? A case-study with nonlinear regression</title>
      <link>https://www.statforbiology.com/2021/stat_nls_speciesarea/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_nls_speciesarea/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In general, undergraduate students in biology/ecology courses tend to consider the derivatives as a very abstract entity, with no real usefulness in the everyday life. In my work as a teacher, I have often tried to fight against such an attitude, by providing convincing examples on how we can use the derivatives to get a better understanding about the changes on a given system.&lt;/p&gt;
&lt;p&gt;In this post I’ll tell you about a recent situation where I was involved with derivatives. A few weeks ago, a colleague of mine wrote me to ask the following question (I’m changing it a little, to make it, hopefully, more interesting). He asked: &lt;em&gt;“I am using a power curve to model how the size of the sampling area affects species richness. How can I quantify my knowledge gain?”&lt;/em&gt;. This is an interesting question, indeed, although I feel I should provide you with some background information.&lt;/p&gt;
&lt;p&gt;Ecologists and botanists are very often involved with field surveys, aimed at determining the richness of plant species in a given territory. In most cases, such territories are too big to conduct exhaustive samplings and, therefore, it is necessary to resort to sampling a smaller area. The problem is that it is clearly recognised that the wider the sampled area, the higher the number of plant species that we encounter. So, what is the minimum sampling area to conduct a reliable survey?&lt;/p&gt;
&lt;p&gt;First of all, let’s try to model the species-are relationship. In some instances, this relationship can be described by using a power curve, that is coded as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a \, X^b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the number of species, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the sampling area, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are regression parameters. In order to understand such a relationship, we can take the ‘speciesArea’ dataset in the ‘aomisc’ package, that comes from Cristaudo et al. (2015). We can fit a power curve to this dataset, by using the &lt;code&gt;drm()&lt;/code&gt; function in the ‘drc’ package, together with the &lt;code&gt;DRC.powerCurve()&lt;/code&gt; self-starter in the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
data(speciesArea)

# drm fit
model &amp;lt;- drm(numSpecies ~ Area, fct = DRC.powerCurve(),
             data = speciesArea)
summary(model)
## 
## Model fitted: Power curve (Freundlich equation) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 4.348404   0.337197  12.896 3.917e-06 ***
## b:(Intercept) 0.329770   0.016723  19.719 2.155e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.9588598 (7 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very useful to look at the resulting graph: we clearly see that the harder we work, the higher is our knowledge gain, in terms of plant richness. We may not be experts of plant surveys, but we should keep in mind that this may be really hard work, especially if we have to survey citrus groves under the sunshine of an Italian summer in Sicily! Therefore, we’d better optimise our effort and enlarge our sampling area only if this gives us a relevant payback.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_nls_SpeciesArea.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;In this respect, we see that every additional sampling effort gives a progressively lower payback; for example, an increase of 50 m&lt;sup&gt;2&lt;/sup&gt; in sampling area let us discover almost 16 new species when we begin our survey, while, when we have already sampled 200 m&lt;sup&gt;2&lt;/sup&gt;, a similar increase of sampling area let us discover only 2 additional plant species.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(model, newdata = data.frame(Area = c(50)))
## Prediction 
##    15.7979
predict(model, newdata = data.frame(Area = c(250))) -
  predict(model, newdata = data.frame(Area = c(200))) 
## Prediction 
##    1.90552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above argument motivates my colleague’s question: how do we quantify the knowledge gain in relation to the effort it costs? This is a typical situation where the first derivative of the power function comes in handy. You might remember from high school that the first derivative represents the slope of the line tangent to the function at any point on the graph. Its value represents the ratio between the knowledge gain and the increase in sampling effort and it is a very good measure of how well our additional effort is paid back in terms of knowledge gain. In other words, the higher the derivative, the higher our convenience to increase our sampling effort.&lt;/p&gt;
&lt;p&gt;But, how do we find a derivative? Years ago, it was a big relief for me to discover that R can efficiently help us with this task. In particular, we have two main functions available: &lt;code&gt;D()&lt;/code&gt; and &lt;code&gt;deriv()&lt;/code&gt;. The first one takes an expression as an argument and returns an expression, which can be evaluated to get the derivative value. For example, if we want to know the derivative value for a sampling area ranging from 1 to 100 m&lt;sup&gt;2&lt;/sup&gt;, we can use the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;powerCurve.der &amp;lt;- D(expression(a * X ^ b), &amp;quot;X&amp;quot;)
powerCurve.der
## a * (X^(b - 1) * b)
a &amp;lt;- coef(model)[1]
b &amp;lt;- coef(model)[2]
X &amp;lt;- seq(1, 100, by = 10)
eval(powerCurve.der)
##  [1] 1.43397379 0.28745425 0.18635899 0.14354434 0.11901599 0.10281978
##  [7] 0.09119265 0.08237067 0.07540802 0.06974823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It confirms what we already knew, that is our payback decreases while our effort increases; it is also interesting to note that the function &lt;code&gt;deparse()&lt;/code&gt; transforms the resulting expression into character strings, which we can pass to the &lt;code&gt;gnlht()&lt;/code&gt; function in the ‘aomisc’ package, to calculate standard errors for the estimated derivatives.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(deparse(powerCurve.der))
samplingAreas &amp;lt;- data.frame(X = seq(1, 100, by = 10))
pred &amp;lt;- gnlht(model, funList, const = samplingAreas,
              parameterNames = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;))
pred
##                   Form  X   Estimate          SE  t-value      p-value
## 1  a * (X^(b - 1) * b)  1 1.43397379 0.046072499 31.12429 9.127464e-09
## 2  a * (X^(b - 1) * b) 11 0.28745425 0.007794058 36.88121 2.800293e-09
## 3  a * (X^(b - 1) * b) 21 0.18635899 0.006470755 28.80019 1.565496e-08
## 4  a * (X^(b - 1) * b) 31 0.14354434 0.005745031 24.98583 4.196158e-08
## 5  a * (X^(b - 1) * b) 41 0.11901599 0.005240148 22.71233 8.123235e-08
## 6  a * (X^(b - 1) * b) 51 0.10281978 0.004857404 21.16764 1.321660e-07
## 7  a * (X^(b - 1) * b) 61 0.09119265 0.004552575 20.03100 1.934121e-07
## 8  a * (X^(b - 1) * b) 71 0.08237067 0.004301572 19.14897 2.637561e-07
## 9  a * (X^(b - 1) * b) 81 0.07540802 0.004089788 18.43813 3.421480e-07
## 10 a * (X^(b - 1) * b) 91 0.06974823 0.003907714 17.84886 4.276907e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;deriv()&lt;/code&gt; function is similar, but it takes a formula as an argument and, if we provide the &lt;code&gt;function.arg()&lt;/code&gt; argument, it returns a function, which is very handy for further processing. For example, we can use such function for plotting purposes, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;powerCurve.der2 &amp;lt;- deriv(~ a * X ^ b, &amp;quot;X&amp;quot;,
              function.arg = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;X&amp;quot;))

curve(attr(powerCurve.der2(4.348, 0.32977, x), &amp;quot;gradient&amp;quot;), 
      from = 0, to = 250, ylab = &amp;quot;First derivative&amp;quot;, 
      xlab = &amp;quot;Sampling area&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_SpeciesArea_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, how do we use the above information to decide how big our sampling area should be? According to Muller-Dumboise and Ellenberg (1974), the minimal sampling area should be selected so that an increase of 10% in sampling area yields an increase of 10% in the number of sampled species. In other words, the minimal sampling area should be selected so that the sampling effort in relative terms is equal to the gain in knowledge, also in relative terms.&lt;/p&gt;
&lt;p&gt;Considering that the total sampling area was 256 m&lt;sup&gt;2&lt;/sup&gt; and the total number of species was 26, the minimum sampling area should correspond to the point on the graph where the first derivative is equal to 2.6/25.6 = 26/256 = 0.1015625. Graphically, we need to find a point along the x-axis where the tangent line to the graph is parallel to the line connecting the origin of axes and the point with co-ordinates &lt;span class=&#34;math inline&#34;&gt;\(x = 256\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y = 26\)&lt;/span&gt; (see the graph below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_SpeciesArea_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In order to find this minimum sampling area, we solve the derivative function so that it returns a value of 0.1015625. We can do this by using the &lt;code&gt;uniroot()&lt;/code&gt; function as shown in the box below. The minimum sampling area is approximately equal to 52 m&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solveFun &amp;lt;- function(x) attr(powerCurve.der2(4.348, 0.32977, x),
                             &amp;quot;gradient&amp;quot;) - 0.1015625
uniroot(solveFun, lower = 0, upper = 256)$root
## [1] 51.93759&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope that I have put another brick to convince you that derivatives can help us to solve some problems in the everyday life! If you have comments, please drop me a line.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Cristaudo, A., Restuccia, A., Onofri, A., Giudice, V.L., Gresta, F., 2015. Species-area relationships and minimum area in citrus grove weed communities. Plant Biosystems 149, 337–345.&lt;/li&gt;
&lt;li&gt;Muller-Dumbois, D., Ellenberg, H., 1974. Community sampling: the relevè method., in: Aims and Methods of Vegetation Ecology. John Wiley &amp;amp; Sons, Inc., Species/Area curves, pp. 45–66.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Other useful functions for nonlinear regression: threshold models and all that</title>
      <link>https://www.statforbiology.com/2021/stat_seedgermination_htt2step/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_seedgermination_htt2step/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In a recent post I presented several equations and just as many &lt;a href=&#34;https://www.statforbiology.com/2020/stat_nls_usefulfunctions/&#34;&gt;self-starting functions for nonlinear regression analyses in R&lt;/a&gt;. Today, I would like to build upon that post and present some further equations, relating to the so-called threshold models.&lt;/p&gt;
&lt;p&gt;But, … what are threshold models? In some instances, we need to describe relationships where the response variable changes abruptly, following a small change in the predictor. A typical threshold model looks like that in the Figure below, where we see three threshold levels:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X = 5.5\)&lt;/span&gt;: at this threshold, the response changes abruptly from ‘flat’ to linearly increasing;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X = 23.1\)&lt;/span&gt;: at this threshold, the response changes abruptly from linearly increasing to linearly decreasing;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X = 37.2\)&lt;/span&gt;: at this threshold, the response changes abruptly from linearly decreasing to ‘flat’.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may recognise a ‘broken-stick’ pattern, although threshold models can also be curvilinear, as we will see later.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Threshold models are very common in biology, where they can be successfully used to describe the daily (or hourly) progress towards a certain developmental stage, as influenced by the environmental conditions, mainly humidity and temperature. In this post I will show examples relating to seed germination, but the very same concepts also apply to the growth of plants and other organisms. In biology, these models are often known as thermal-time or hydro-time models.&lt;/p&gt;
&lt;p&gt;Considering seed germination, the response variable is the Germination Rate (GR), that is the reciprocal of germination time; for example, if a seed takes 7 days to accomplish the germination process, the GR is equal to &lt;span class=&#34;math inline&#34;&gt;\(1/7 = 0.143\)&lt;/span&gt; and it represents the fraction of germination that is accomplished in one day. The GR is a good measure of velocity: the higher the value the higher the speed. If we plot the GR against, e.g., temperature, we should very likely observe a response pattern as in Figure 1; the three thresholds are, respectively, the &lt;em&gt;base temperature&lt;/em&gt; (T_b_), the &lt;em&gt;optimal temperature&lt;/em&gt; (T_o_) and the &lt;em&gt;ceiling temperature&lt;/em&gt; (T_c_). If we look at the effect of soil humidity on GR, we should expect a response pattern with only one threshold, i.e. the &lt;em&gt;base water potential&lt;/em&gt; (e.g. the first half of Figure 1, up to the maximum response level).&lt;/p&gt;
&lt;p&gt;Although the equations I am going to introduce are commonly used in the seed science literature, I am confident that you might find them useful for a lot of other applications. For all those equations, I have built the related R functions, together with self-starting routines, which can be used for nonlinear regression analyses with the &lt;code&gt;drm()&lt;/code&gt; function in the ‘drc’ package (Ritz et al., 2019). The availability of self-starting routines will free you from the hassle of having to provide initial guess for model parameters. All these R functions are available within the ‘drcSeedGerm’ package (Onofri, 2019).&lt;/p&gt;
&lt;p&gt;The post is rather long, but you do not need to read it all; have a look at the graph below to spot the shape you are interested in and use the link to reach the relevant part in this web page. But, first of all, do not forget to install (if necessary) and load the ‘drcSeedGerm’ package, by using the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#installing package, if not yet available
# library(devtools)
# install_github(&amp;quot;onofriandreapg/drcSeedGerm&amp;quot;)

# loading package
library(drcSeedGerm)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;list-of-threshold-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;List of threshold models&lt;/h1&gt;
&lt;p&gt;In this post I will show the following threshold models:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#one-threshold-linear-model&#34;&gt;One-threshold linear&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-threshold-polynomial&#34;&gt;One-threshold polynomial - 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-threshold-polynomial&#34;&gt;One-threshold polynomial - 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-threshold-exponential&#34;&gt;One-threshold exponential&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#broken-stick-model&#34;&gt;Broken-stick model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#broken-curvilinear-model&#34;&gt;Broken-curvilinear model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-model&#34;&gt;Polynomial model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exponential-switch-off-model&#34;&gt;Exponential switch-off model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hyperbolic-model&#34;&gt;Hyperbolic model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-threshold-linear-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One-threshold linear model&lt;/h1&gt;
&lt;p&gt;In some cases, we may need to model a process occurring only above a certain threshold level for the predictor variable. Models of this kind have been used to describe the effect of environmental humidity (&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, in MPa) on germination rate (Bradford, 2002):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max\left[\Psi, \Psi_b\right] - \Psi_b}{\theta_H} \quad \quad \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameter &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; is the &lt;em&gt;base water potential&lt;/em&gt; (in MPa), representing the minimum level of humidity in the substrate to trigger the germination process. The other parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt; (in MPa day or MPa hour) is the hydro-time constant.&lt;/p&gt;
&lt;p&gt;A totally similar model has been used by Garcia-Huidobro et al (1982), to describe the effect of sub-optimal temperatures (T in °C) on the germination rate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max \left[T, T_b\right] - T_b}{\theta_T} \quad \quad \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(T_b\)&lt;/span&gt; is the base temperature and &lt;span class=&#34;math inline&#34;&gt;\(\theta_T\)&lt;/span&gt; is the thermal time (in °C d).&lt;/p&gt;
&lt;p&gt;Both models can be fitted in R, by using the two functions &lt;code&gt;GRPsi.Lin()&lt;/code&gt; and &lt;code&gt;GRT.GH()&lt;/code&gt;; they are totally equivalent, apart from parameter names. In the example below I have fitted the second equation to a seed germination dataset; this type of data is usually heteroscedastic, thus, please note the use of a robust variance-covariance estimator for model parameters (Zeileis, 2006).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tlev &amp;lt;- c(2, 5, 10, 15, 20, 25)
GR &amp;lt;- c(0, 0, 0.21, 0.49, 0.68, 0.86)
modGH &amp;lt;- drm(GR ~ Tlev, fct = GRT.GH())
library(sandwich); library(lmtest)
coeftest(modGH, vcov = sandwich)
## 
## t test of coefficients:
## 
##                    Estimate Std. Error t value  Pr(&amp;gt;|t|)    
## Tb:(Intercept)      4.77160    0.31993  14.915 0.0001177 ***
## ThetaT:(Intercept) 22.83118    0.62847  36.328 3.428e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
plot(modGH, log=&amp;quot;&amp;quot;, xlim = c(0, 25), legendPos = c(5, 1.2),
     xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-threshold-polynomial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One threshold polynomial&lt;/h1&gt;
&lt;p&gt;In my experience, I have found that the relationship between GR and water potential in the substrate may, sometimes, be curvilinear. For these situations, I have successfully used the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max\left[\Psi,\Psi_b\right]^2 - \Psi^2_b}{\theta_H} \quad \quad \quad \quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\left(\max\left[\Psi, \Psi_b\right] - \Psi_b \right)^2}{ \theta_H } \quad \quad \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both models can be fitted in R, by using the two functions &lt;code&gt;GRPsi.Pol()&lt;/code&gt; and &lt;code&gt;GRPsi.Pol2()&lt;/code&gt;, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Observed data
Psi &amp;lt;- c(-2, -1.5, -1.2, -1, -0.8, -0.6, -0.4, -0.25,
         -0.12, -0.06, -0.03, 0)
GR &amp;lt;- c(0, 0, 0, 0, 0.0585, 0.094, 0.1231, 0.1351,
        0.1418, 0.1453, 0.1458, 0.1459)
Psi2 &amp;lt;- c(-0.5, -0.6, -0.7, -0.8, -0.9, -1, -1.1, -1.2,
          -1.5)
GR2 &amp;lt;- c(1.4018, 1.0071, 0.5614, 0.3546, 0.2293, 0, 0,
         0, 0)


modHT &amp;lt;- drm(GR ~ Psi, fct = GRPsiPol())
modHT2 &amp;lt;- drm(GR2 ~ Psi2, fct = GRPsiPol2())
par(mfrow = c(1,2))

plot(modHT, log=&amp;quot;&amp;quot;, legendPos = c(-1.5, 0.15), 
     ylim = c(0, 0.20), xlab = &amp;quot;Water potential (MPa)&amp;quot;)
plot(modHT2, log=&amp;quot;&amp;quot;, legendPos=c(-1.3, 1), 
     xlab = &amp;quot;Water potential (MPa)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-threshold-exponential&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One threshold: exponential&lt;/h1&gt;
&lt;p&gt;All the previous models tend to go up to infinity when the predictor value (temperature or water potential) goes to infinity. In some instances, we may need an asymptotic model, for example, to describe the oxygen uptake kinetics during a walk test (see Baty et al., 2014, although their threshold model is more complex) or to describe the response of the maximum proportion of germinated seeds to soil humidity (Onofri et al., 2018).&lt;/p&gt;
&lt;p&gt;In practice, we could use a shifted exponential model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \pi = G \, \left[ 1 - exp \left( \frac{ \max\left[\Psi, \Psi_b\right] - \Psi_b }{\sigma} \right) \right]  \quad \quad \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is the proportion of germinated seeds, &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the fraction of non-germinable seeds (e.g., dormant seeds) and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; describes how quickly the population of seeds responds to increased humidity in the substrate.&lt;/p&gt;
&lt;p&gt;If we reverse the sign of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; in Eq. 5, we obtain a decreasing trend, which might be useful to describe the effect of super-optimal temperatures on the proportion of germinated seeds, going down to 0 at the ceiling temperature threshold.&lt;/p&gt;
&lt;p&gt;Both equations can be fitted using the self-starters &lt;code&gt;PmaxPsi1()&lt;/code&gt; and &lt;code&gt;PmaxT1()&lt;/code&gt;. The two R functions are totally equal, apart from parameters names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1,2))
# Pmax vs Psi
Psi &amp;lt;- seq(-2.2, 0, by = 0.2)
Pmax &amp;lt;- c(0, 0, 0.076, 0.413, 0.514, 0.643, 0.712,
          0.832, 0.865, 0.849, 0.89, 0.90)
mod &amp;lt;- drm(Pmax ~ Psi, fct = PmaxPsi1())
plot(mod, log = &amp;quot;&amp;quot;, xlab = &amp;quot;Water potential (MPa)&amp;quot;, 
     ylab = &amp;quot;Proportion of germinating seeds&amp;quot;)

# Pmax vs temperture
Tval &amp;lt;- c(0, 2.5, 5, 7.5, 10, 12.5, 15, 17.5,
          20, 22.5, 25, 27.5, 30, 32.5, 35)
Pmax2 &amp;lt;- c(0.79, 0.81, 0.807, 0.776, 0.83,
           0.73, 0.744, 0.73, 0.828, 0.818,
           0.805, 0.706, 0.41, 0.002, 0)
mod2 &amp;lt;- drm(Pmax2 ~ Tval, fct = PmaxT1())
plot(mod2, log = &amp;quot;&amp;quot;, xlab = &amp;quot;Temperature (°C)&amp;quot;, 
     ylab = &amp;quot;Proportion of germinating seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;broken-stick-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;‘Broken-stick’ model&lt;/h1&gt;
&lt;p&gt;A broken-stick trend, as the one depicted in the Figure 1 was used by Alvarado and Bradford (2002) to model the effect of temperature on the germination rate. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\min \left[T,T_o \right] - T_b}{\theta_{T}} \, \left\{ 1 - \frac {\min \left[\max \left[ T,T_o \right], T_c \right] - T_o}{T_c - T_o} \right\} \quad \quad (6)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Base, optimal and ceiling temperatures (respectively &lt;span class=&#34;math inline&#34;&gt;\(T_b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_o\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_c\)&lt;/span&gt;) are included as parameters, together with the hydro-thermal time parameter (&lt;span class=&#34;math inline&#34;&gt;\(\theta_T\)&lt;/span&gt;). Equation 6 can be easily fitted with the &lt;code&gt;GRT.BS()&lt;/code&gt; function in the ‘drcSeedGerm’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tval &amp;lt;- c(2, 5, 10, 15, 20, 25, 30, 35, 40)
GR &amp;lt;- c(0, 0, 0.209, 0.435, 0.759, 0.821, 0.417, 0.145, 0)

modBS &amp;lt;- drm(GR ~ Tval, fct = GRT.BS())
plot(modBS, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
     legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeftest(modBS, vcov = sandwich)
## 
## t test of coefficients:
## 
##                    Estimate Std. Error t value  Pr(&amp;gt;|t|)    
## Tc:(Intercept)     36.81971    0.41113  89.556 3.290e-09 ***
## Tb:(Intercept)      6.49657    0.43146  15.057 2.340e-05 ***
## To:(Intercept)     23.21649    0.29481  78.750 6.256e-09 ***
## ThetaT:(Intercept) 18.18250    0.76345  23.816 2.431e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;broken-curvilinear-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Broken-curvilinear model&lt;/h1&gt;
&lt;p&gt;Broken-stick trends may not be reasonable for biological processes, which might be better described by curvilinear equations. Rowse and Finch-Savage (2003) proposed another equation with two components: the first one depicts a linear increase of the GR value with temperature, which is off-set by the second component, starting from &lt;span class=&#34;math inline&#34;&gt;\(T = T_d\)&lt;/span&gt;, which is close to (but not coincident with) &lt;span class=&#34;math inline&#34;&gt;\(T_o\)&lt;/span&gt;. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{ \max \left[ T, T_b \right] - T_b}{\theta_{T}} \left\{ 1 - \frac{\min \left[ \max \left[T,T_d\right], T_o \right] - T_d}{T_c - T_d}  \right\} \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The optimal temperature can be derived as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T_o = \frac{1 + kT_b + kT_d}{2k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ k = \frac{1}{T_c - T_d}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this equation, you will find the &lt;code&gt;GRT.RFb()&lt;/code&gt; self-starter in the ‘drcSeedGerm’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modRF &amp;lt;- drm(GR ~ Tval, fct = GRT.RFb())
plot(modRF, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
     legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomial model&lt;/h1&gt;
&lt;p&gt;According to Mesgaran et al (2017), the negative and positive effects of temperature coexist for all temperatures above &lt;span class=&#34;math inline&#34;&gt;\(T_b\)&lt;/span&gt;. Their proposed equation can be parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR = \frac{\max \left[ T, T_b \right] - T_b}{\theta_{T}} \left[ 1 - \frac{\min \left[ T, T_c \right] - T_b}{T_c - T_b}  \right] \quad \quad \quad \quad (8)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that the GR value is a second-order polynomial function of &lt;span class=&#34;math inline&#34;&gt;\(T - T_b\)&lt;/span&gt; and, therefore, the curve is symmetric around the optimal temperature value, which can be derived as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_o = \frac{T_c - T_b}{2} + T_b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For this equation, the self-starting function is &lt;code&gt;GRT.M()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modM &amp;lt;- drm(GR ~ Tval, fct = GRT.M())
plot(modM, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
     legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exponential-switch-off-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exponential switch-off model&lt;/h1&gt;
&lt;p&gt;Equations 6 to 8 use a product, wherein the first term represents the accumulation of thermal time and the second term may be seen as a switch-off term that is 1 either when &lt;span class=&#34;math inline&#34;&gt;\(T &amp;lt; T_o\)&lt;/span&gt; (Equation 6) or &lt;span class=&#34;math inline&#34;&gt;\(T &amp;lt; T_d\)&lt;/span&gt; (Equation 7) or &lt;span class=&#34;math inline&#34;&gt;\(T = T_b\)&lt;/span&gt; (Equation 8) and decreases progressively as temperature increases. In all the above equations, the switch-off term is linear, although we can use other types of switch-off terms, to obtain more flexible models. One possibility is to use an exponential switch-off term, as in the equation below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ GR = \frac{\max \left[T, T_b \right] - T_b}{\theta_T} \left\{ \frac{1 - \exp \left[ k (\min \left[T, T_c \right] - T_c) \right]}{1 - \exp \left[ k (T_b - T_c) \right]}  \right\} \quad \quad \quad (9)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the switch-off parameter: the lower the value, the higher the negative effect of temperature at super-optimal levels. The response of GR to temperature is highly asymmetric with a slow increase below &lt;span class=&#34;math inline&#34;&gt;\(T_o\)&lt;/span&gt; and a steep drop afterwards.&lt;/p&gt;
&lt;p&gt;I have successfully used this model in Catara et al (2016) and Masin et al (2017). The self-starting function in R is &lt;code&gt;GRT.Exb()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tval &amp;lt;- c(2, 5, 10, 15, 20, 25, 30, 35, 40)
GR &amp;lt;- c(0, 0, 0.209, 0.435, 0.759, 0.821, 0.917, 0.445, 0)

modExb &amp;lt;- drm(GR ~ Tval, fct = GRT.Exb())
summary(modExb)
## 
## Model fitted: Exponential effect of temperature on GR50 (Type II - Masin et al., 2017)
## 
## Parameter estimates:
## 
##                     Estimate Std. Error t-value   p-value    
## Tb:(Intercept)      5.991778   1.205747  4.9693 0.0042141 ** 
## ThetaT:(Intercept) 19.254345   2.795867  6.8867 0.0009881 ***
## k:(Intercept)       0.181612   0.060564  2.9987 0.0301453 *  
## Tc:(Intercept)     36.933370   0.501666 73.6214 8.758e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.04049231 (5 degrees of freedom)
plot(modExb, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
     legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hyperbolic-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hyperbolic model&lt;/h1&gt;
&lt;p&gt;The following model was derived by the simple yield loss function devised by Kropff and van Laar (1993). It is very flexible, as it may depict different types of relationships between temperature and base water potential, according to the value taken by the parameter &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GR(g, T) = \frac{\max \left[T, T_b\right] - T_b}{\theta_T} \left( 1 - \frac{q \frac{\min \left[T, T_c\right] -T_b}{T_c- T_b} }{1 + (q-1) \frac{T-T_b}{T_c- T_b}}  \right) \quad \quad \quad (10)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In R, this model can be fitted by using the self-starter &lt;code&gt;GRT.YL()&lt;/code&gt; and we see that, with our dataset, the fit is very much like that of the exponential switch-off function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modYL &amp;lt;- drm(GR ~ Tval, fct = GRT.YL())
plot(modYL, log=&amp;quot;&amp;quot;, xlim = c(0, 40), ylim=c(0,1.2),
     legendPos = c(5, 1.0), xlab = &amp;quot;Temperature (°C)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HTT2step_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-warning-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A warning message&lt;/h1&gt;
&lt;p&gt;When we collect data about the response of germination rates to temperature and use them to parameterise nonlinear regression models by using nonlinear least squares, the basic assumption of homoscedasticity is rarely tenable. &lt;strong&gt;We should not forget this!&lt;/strong&gt; in the above example I used a robust variance-covariance sandwich estimator, although other techniques can be successfully used to deal with this problem.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Alvarado, V., Bradford, K.J., 2002. A hydrothermal time model explains the cardinal temperatures for seed germination. Plant, Cell and Environment 25, 1061–1069.&lt;/li&gt;
&lt;li&gt;Baty, F., Ritz, C., Charles, S., Brutsche, M., Flandrois, J. P., Delignette-Muller, M.-L., 2014. A toolbox for nonlinear regression in R: the package nlstools. Journal of Statistical Software, 65, 5, 1-21.&lt;/li&gt;
&lt;li&gt;Bradford, K.J., 2002. Applications of hydrothermal time to quantifying and modelling seed germination and dormancy. Weed Science 50, 248–260.&lt;/li&gt;
&lt;li&gt;Catara, S., Cristaudo, A., Gualtieri, A., Galesi, R., Impelluso, C., Onofri, A., 2016. Threshold temperatures for seed germination in nine species of Verbascum (Scrophulariaceae). Seed Science Research 26, 30–46.&lt;/li&gt;
&lt;li&gt;Garcia-Huidobro, J., Monteith, J.L., Squire, R., 1982. Time, temperature and germination of pearl millet (&lt;em&gt;Pennisetum typhoides&lt;/em&gt; S &amp;amp; H.). 1. Constant temperatures. Journal of Experimental Botany 33, 288–296.&lt;/li&gt;
&lt;li&gt;Kropff, M.J., van Laar, H.H. 1993. Modelling crop-weed interactions. CAB International, Books.&lt;/li&gt;
&lt;li&gt;Masin, R., Onofri, A., Gasparini, V., Zanin, G., 2017. Can alternating temperatures be used to estimate base temperature for seed germination? Weed Research 57, 390–398.&lt;/li&gt;
&lt;li&gt;Onofri A., 2019. DrcSeedGerm: Statistical approaches for data analyses in seed germination assays. R package version 0.98. &lt;a href=&#34;https://www.statforbiology.com&#34; class=&#34;uri&#34;&gt;https://www.statforbiology.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C., 2019. Dose-Response Analysis Using R. CRC Press&lt;/li&gt;
&lt;li&gt;Rowse, H.R., Finch-Savage, W.E., 2003. Hydrothermal threshold models can describe the germination response of carrot (Daucus carota) and onion (Allium cepa) seed populations across both sub- and supra-optimal temperatures. New Phytologist 158, 101–108.&lt;/li&gt;
&lt;li&gt;Zeileis, A., 2006. Object-oriented computation of sandwich estimators. Journal of Statistical Software, 16, 9, 1-16.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The R-squared and nonlinear regression: a difficult marriage?</title>
      <link>https://www.statforbiology.com/2021/stat_nls_r2/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_nls_r2/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Making sure that a fitted model gives a good description of the observed data is a fundamental step of every nonlinear regression analysis. To this aim we can (and should) use several techniques, either graphical or based on formal hypothesis testing methods. However, in the end, I must admit that I often feel the need of displaying a simple index, based on a single and largely understood value, that reassures the readers about the goodness of fit of my models.&lt;/p&gt;
&lt;p&gt;In linear regression, we already have such an index, that is known as the R&lt;sup&gt;2&lt;/sup&gt; or the &lt;em&gt;coefficient of determination&lt;/em&gt;. In words, this index represents the proportion of variance in the dependent variable that is explained by the regression effects. It ranges from 0 to 1 and, within this interval, the highest the value, the best the fit. The expression is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = \frac{SS_{reg}}{SS_{tot}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and it represents the ratio between the regression sum of squares (&lt;span class=&#34;math inline&#34;&gt;\(SS_{reg}\)&lt;/span&gt;) and total sum of squares (&lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt;), which is equal to the proportion of explained variance when we consider the population variance, that is obtained by dividing both &lt;span class=&#34;math inline&#34;&gt;\(SS_{reg}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt; by the number of observations (and not by the number of degrees of freedom). In the above expression, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SS_{reg} = \sum_{i = 1}^{n}{\left(\hat{y_i} - \bar{y} \right)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SS_{tot} = \sum_{i = 1}^{n}{\left(y_i - \bar{y} \right)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we also consider the squared residuals from the regression line, we can also define the residual sum of squares as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SS_{res} = \sum_{i = 1}^{n}{\left(y_i - \hat{y_i} \right)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where: &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is the i-th observation, &lt;span class=&#34;math inline&#34;&gt;\(\hat{y_i}\)&lt;/span&gt; is the i-th fitted value and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt; is the overall mean.&lt;/p&gt;
&lt;p&gt;In all linear models with an intercept term, the following equality holds:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SS_{tot} = SS_{reg} + SS_{res}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, it is always &lt;span class=&#34;math inline&#34;&gt;\(SS_{reg} \leq SS_{tot}\)&lt;/span&gt;, which implies that the R&lt;sup&gt;2&lt;/sup&gt; value may never be higher than 1 or lower than 0. Furthermore, we can write the alternative (and equivalent) definition:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R^2 = 1 - \frac{SS_{res}}{SS_{tot}}\]&lt;/span&gt;
Now, the question is:&lt;/p&gt;
&lt;div id=&#34;can-we-use-the-r-squared-in-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Can we use the R-squared in nonlinear regression?&lt;/h1&gt;
&lt;p&gt;Basically, we have two problems:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;nonlinear models do not have an intercept term, at least, not in the usual sense;&lt;/li&gt;
&lt;li&gt;the equality &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot} = SS_{reg} + SS_{res}\)&lt;/span&gt; may not hold.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For these reasons, most authors advocate against the use of the R&lt;sup&gt;2&lt;/sup&gt; in nonlinear regression analysis and recommend alternative measures, such as the Mean Squared Error (MSE; see Ratkowsky, 1990) or the AIC and BIC (see Spiess and Neumeyer, 2010). I would argue that the R&lt;sup&gt;2&lt;/sup&gt; may have a superior intuitive appeal as far as it is bound to 1 for a perfectly fitting model; with such a constraint, we can immediately see how good is the fit of our model.&lt;/p&gt;
&lt;p&gt;Schabenberger and Pierce (2002) recommend the following statistic, that is similar to the R&lt;sup&gt;2&lt;/sup&gt; for linear models:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{Pseudo-}R^2 = 1 - \frac{SS_{res}}{SS_{tot}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Why is it a ‘Pseudo-R&lt;sup&gt;2&lt;/sup&gt;’?. In contrast to what happens with linear models, this statistic:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;cannot exceed 1, but it may lower than 0;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;it cannot be interpreted as the proportion of variance explained by the model&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bearing these two limitations in mind, there is no reason why we should not use such a goodness-of-fit measure with nonlinear regression. In this line, the &lt;code&gt;R2.nls()&lt;/code&gt; function in the ‘aomisc’ package can be used to retrieve the R&lt;sup&gt;2&lt;/sup&gt; and Pseudo-R&lt;sup&gt;2&lt;/sup&gt; values from a nonlinear model fitted with the &lt;code&gt;nls()&lt;/code&gt; and &lt;code&gt;drm()&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
X &amp;lt;- c(0.1, 5, 7, 22, 28, 39, 46, 200)
Y &amp;lt;- c(1, 13.66, 14.11, 14.43, 14.78, 14.86, 14.78, 14.91)

#drm fit
model &amp;lt;- drm(Y ~ X, fct = MM.2())
R2nls(model)$PseudoR2
## [1] 0.9930399
#
# nls fit
model2 &amp;lt;- nls(Y ~ SSmicmen(X, Vm, K))
R2nls(model)$PseudoR2
## [1] 0.9930399&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Undoubtedly, the Pseudo-R&lt;sup&gt;2&lt;/sup&gt; gives, at first glance, a good feel for the quality of our regressions; but, please, do not abuse it. In particular, please, remember that a negative value might indicate a big problem with the fitted model. Above all, remember that the Pseudo-R&lt;sup&gt;2&lt;/sup&gt;, similarly to the R&lt;sup&gt;2&lt;/sup&gt; in multiple linear regression, should never be used as the basis to select and compare alternative regression model. Other statistics should be used to this aim.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Ratkowsky, D.A., 1990. Handbook of nonlinear regression models. Marcel Dekker Inc., Books.&lt;/li&gt;
&lt;li&gt;Schabenberger, O., Pierce, F.J., 2002. Contemporary statistical models for the plant and soil sciences. Taylor &amp;amp; Francis, CRC Press, Books.&lt;/li&gt;
&lt;li&gt;Spiess, A. N., &amp;amp; Neumeyer, N., 2010. An evaluation of &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; as an inadequate measure for nonlinear models in pharmacological and biochemical research: a Monte Carlo approach. BMC Pharmacology, 10, 6.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. Multienvironment diallel experiments</title>
      <link>https://www.statforbiology.com/2021/stat_met_diallelmet/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_met_diallelmet/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In recent times, a few colleagues at my Department and I have devoted some research effort to data management for diallel mating experiments, which we have summarised in a paper (&lt;a href=&#34;https://link.springer.com/article/10.1007/s00122-020-03716-8&#34;&gt;Onofri et al., 2020&lt;/a&gt;) and a series of five blog posts (&lt;a href=&#34;https://www.statforbiology.com/tags/diallel_models/&#34;&gt;see here&lt;/a&gt;). A final topic that remains to be covered relates to the frequent possibility that these diallel experiments are repeated across years and/or locations. How should the resulting dataset be analysed?&lt;/p&gt;
&lt;p&gt;We will start from a multi-environment full diallel experiment with 5 parental lines, in four blocks and 10 environments. The dataset is factitious and it was generated by Monte Carlo simulation, starting from the results reported in Zhang, et al. (2005; &lt;a href=&#34;https://acsess.onlinelibrary.wiley.com/doi/abs/10.2134/agronj2004.0260&#34;&gt;see here&lt;/a&gt;). The following box shows how we can load the data, after installing (if necessary) and loading the ‘lmDiallel’ package. In the same box, we use ‘dplyr’ to transform the explanatory variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
library(dplyr)

dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/diallelMET.csv&amp;quot;, header = T)
dataset &amp;lt;- dataset %&amp;gt;% 
  dplyr::mutate(across(c(Env, Block, Par1, Par2), .fns = factor))
head(dataset)
##   Env Block Par1 Par2 Yield
## 1   1     1    1    1 10.78
## 2   1     1    1    2 12.78
## 3   1     1    1    3 15.23
## 4   1     1    1    4 10.66
## 5   1     1    1    5 14.42
## 6   1     1    2    1 11.84&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fixed-model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fixed model fitting&lt;/h1&gt;
&lt;p&gt;For this full diallel experiment (withe selfs and reciprocals) we can fit the Griffing’s model 1, including the General combining Abilities (GCA), total Specific Combining Abilities (tSCA) and reciprocal effects (REC). We also include the environment effect, the block within environment effect and all interactions between genetical effects and the environment. If we regard all effects as fixed, we can code the final model either by using the &lt;code&gt;lm()&lt;/code&gt; function, or by using the &lt;code&gt;lm.diallel()&lt;/code&gt; wrapper in the ‘lmDiallel’ package, as shown in the box below. The two parameterisations are slightly different, although variance partitioning is totally equivalent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod &amp;lt;- lm(Yield ~ Env/Block + GCA(Par1, Par2) + tSCA(Par1, Par2) + 
            REC(Par1, Par2) + GCA(Par1, Par2):Env + 
             tSCA(Par1, Par2):Env + REC(Par1, Par2):Env,
           data = dataset)
# anova(dMod)

dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = dataset, fct = &amp;quot;GRIFFING1&amp;quot;,
                    Env = Env, Block = Block)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Yield
##                  Df Sum Sq Mean Sq  F value Pr(&amp;gt;F)    
## Environment       9   10.6    1.17   0.1550 0.9978    
## Env:Block        30 3195.1  106.50  14.0554 &amp;lt;2e-16 ***
## GCA               4 8673.7 2168.42 286.1657 &amp;lt;2e-16 ***
## GCA:Env          36  187.7    5.21   0.6882 0.9175    
## tSCA             10 3021.3  302.13  39.8725 &amp;lt;2e-16 ***
## tSCA:Env         90  108.8    1.21   0.1596 1.0000    
## Reciprocals      10 4379.7  437.97  57.7989 &amp;lt;2e-16 ***
## Reciprocals:Env  90  352.6    3.92   0.5170 0.9999    
## Residuals       720 5455.8    7.58                    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Considering the ‘diallel’ object ‘dMod2, the full list of genetical parameters (in each environment) is retrieved by using the &lt;code&gt;glht()&lt;/code&gt; function in the ’multcomp’ package. In the box below we show an excerpt of the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#                Estimate Std. Error t value Pr(&amp;gt;|t|)    
# g_1:1 == 0     -3.67200    0.38929  -9.432 7.06e-10 ***
# g_2:1 == 0     -0.63850    0.38929  -1.640 0.113019    
# g_3:1 == 0      1.28950    0.38929   3.312 0.002723 ** 
# g_4:1 == 0      0.03025    0.38929   0.078 0.938658    
# g_5:1 == 0      2.99075    0.38929   7.682 3.75e-08 ***
# ts_1:1:1 == 0   1.73500    1.10109   1.576 0.127184    
# ts_1:2:1 == 0   0.29150    0.80255   0.363 0.719379    
# ts_1:3:1 == 0  -1.43150    0.80255  -1.784 0.086153 .  
# ts_1:4:1 == 0  -2.10225    0.80255  -2.619 0.014504 *  
# ts_1:5:1 == 0   1.50725    0.80255   1.878 0.071631 .  
# ts_2:1:1 == 0   0.29150    0.80255   0.363 0.719379    
# ts_2:2:1 == 0   1.03050    1.10109   0.936 0.357942   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ANOVA table above shows that genetical effects did not significantly interact with the environment and, therefore, we might be interested in getting estimates of average genetical effects, which can be done by using the &lt;code&gt;glht()&lt;/code&gt; function and passing the &lt;code&gt;type = &#34;means&#34;&lt;/code&gt; argument in the &lt;code&gt;diallel.eff()&lt;/code&gt; call. An excerpt of the result is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh &amp;lt;- glht(linfct = diallel.eff(dMod2, type = &amp;quot;means&amp;quot;))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#               Estimate Std. Error t value Pr(&amp;gt;|t|)    
# g_1:1 == 0    -3.46259    0.12311 -28.127  &amp;lt; 2e-16 ***
# g_2:1 == 0    -0.55351    0.12311  -4.496 0.000127 ***
# g_3:1 == 0     0.64251    0.12311   5.219 1.89e-05 ***
# g_4:1 == 0     0.40521    0.12311   3.292 0.002868 ** 
# g_5:1 == 0     2.96838    0.12311  24.112  &amp;lt; 2e-16 ***
# ts_1:1:1 == 0  1.53104    0.34820   4.397 0.000165 ***
# ts_1:2:1 == 0  0.21247    0.25379   0.837 0.410125    
# ts_1:3:1 == 0 -1.39756    0.25379  -5.507 8.87e-06 ***
# ....
# ....
# r_1:2:1 == 0  -0.20075    0.30776  -0.652 0.519943    
# r_1:3:1 == 0   2.06700    0.30776   6.716 3.98e-07 ***
# r_1:4:1 == 0  -1.33550    0.30776  -4.339 0.000192 ***
# r_1:5:1 == 0  -3.97250    0.30776 -12.908 8.19e-13 ***
# ....
# ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mixed-model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mixed model fitting&lt;/h1&gt;
&lt;p&gt;In most cases we might be willing to regard the environment effect as random, so that all the interactions between genetical effects and the environment are random as well. A mixed model can be fitted by using the &lt;code&gt;mmer.diallel()&lt;/code&gt; wrapper, that is is available in the ‘lmDiallel’ package. The call is very similar to a &lt;code&gt;lm.diallel()&lt;/code&gt; call, as shown in the box above; we can use the ‘type = “environment”’ argument to specify that we want to include random environment effects. The fit can take quite a few seconds (or minutes, depending on your device…). Fixed effects and variance components can be easily retrieved from the model object, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod3 &amp;lt;- mmer.diallel(Yield ~ Par1 + Par2,
                    data = dataset, fct = &amp;quot;GRIFFING1&amp;quot;,
                    Env = Env, Block = Block, type = &amp;quot;environment&amp;quot;)
dMod3$beta #fixed effects
##    Trait                 Effect  Estimate Std.Error     t.value
## 1      Y            (Intercept) 16.272390 0.2800709  58.1009636
## 2      Y     GCA(Par1, Par2)g_1 -3.462590 0.1005471 -34.4374961
## 3      Y     GCA(Par1, Par2)g_2 -0.553515 0.1005471  -5.5050326
## 4      Y     GCA(Par1, Par2)g_3  0.642510 0.1005471   6.3901402
## 5      Y     GCA(Par1, Par2)g_4  0.405210 0.1005471   4.0300520
## 6      Y tSCA(Par1, Par2)ts_1:1  1.531040 0.3246046   4.7166305
## 7      Y tSCA(Par1, Par2)ts_1:2  0.212465 0.2365942   0.8980143
## 8      Y tSCA(Par1, Par2)ts_1:3 -1.397560 0.2365942  -5.9069910
## 9      Y tSCA(Par1, Par2)ts_1:4 -1.866010 0.2365942  -7.8869632
## 10     Y tSCA(Par1, Par2)ts_2:2  1.946890 0.3246046   5.9977275
## 11     Y tSCA(Par1, Par2)ts_2:3  0.896115 0.2365942   3.7875606
## 12     Y tSCA(Par1, Par2)ts_2:4 -3.674085 0.2365942 -15.5290556
## 13     Y tSCA(Par1, Par2)ts_3:3 -0.187160 0.3246046  -0.5765784
## 14     Y tSCA(Par1, Par2)ts_3:4  1.134515 0.2365942   4.7951930
## 15     Y tSCA(Par1, Par2)ts_4:4  4.228940 0.3246046  13.0279727
## 16     Y   REC(Par1, Par2)r_1:2 -0.200750 0.2869127  -0.6996903
## 17     Y   REC(Par1, Par2)r_1:3  2.067000 0.2869127   7.2042832
## 18     Y   REC(Par1, Par2)r_1:4 -1.335500 0.2869127  -4.6547268
## 19     Y   REC(Par1, Par2)r_1:5 -3.972500 0.2869127 -13.8456774
## 20     Y   REC(Par1, Par2)r_2:3  1.345250 0.2869127   4.6887092
## 21     Y   REC(Par1, Par2)r_2:4 -1.277750 0.2869127  -4.4534460
## 22     Y   REC(Par1, Par2)r_2:5 -2.285625 0.2869127  -7.9662747
## 23     Y   REC(Par1, Par2)r_3:4  4.424875 0.2869127  15.4223768
## 24     Y   REC(Par1, Par2)r_3:5 -0.038625 0.2869127  -0.1346229
## 25     Y   REC(Par1, Par2)r_4:5 -2.149875 0.2869127  -7.4931342
dMod3$varcomp #variance components
##               VarComp  VarCompSE
## Env        0.00000000 0.42671998
## Env:Block  2.99662037 0.84276821
## GCA:Env   -0.03826627 0.03707895
## tSCA:Env   0.00000000 0.14493517
## REC:Env    0.00000000 0.13004754
## Residuals  6.58550954 0.34464357&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random model fitting&lt;/h1&gt;
&lt;p&gt;In other instances, our aim is to estimate variance components for all genetical effects and, therefore, we might like to regard all effects as random. This is easily done by changing the call to &lt;code&gt;mmer.diallel()&lt;/code&gt; and replacing &lt;code&gt;type = &#34;environment&#34;&lt;/code&gt; with &lt;code&gt;type = &#34;all&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod4 &amp;lt;- mmer.diallel(Yield ~ Par1 + Par2,
                    data = dataset, fct = &amp;quot;GRIFFING1&amp;quot;,
                    Env = Env, Block = Block, type = &amp;quot;all&amp;quot;)
dMod4$beta #fixed effects
##   Trait      Effect Estimate Std.Error  t.value
## 1     Y (Intercept) 16.45538  1.920185 8.569682
dMod4$varcomp #variance components
##               VarComp  VarCompSE
## Env        0.00000000 0.42621427
## Env:Block  2.99608118 0.84185447
## GCA        4.11923254 3.41043097
## tSCA       4.68884763 2.14244233
## REC        5.39232126 2.44832836
## GCA:Env   -0.03828058 0.03706453
## tSCA:Env   0.00000000 0.14494177
## REC:Env    0.00000000 0.13005751
## Residuals  6.58560305 0.34466898&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, a similar procedure can be used to fit all other diallel models to multi-environment diallel experiments.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Luigi Russi&lt;br /&gt;
Niccolò Terzaroli&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0156744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0156744&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Griffing, B., 1956. Concept of general and specific combining ability in relation to diallel crossing systems. Australian Journal of Biological Science 9, 463–493.&lt;/li&gt;
&lt;li&gt;Hayman, B.I., 1954. The Analysis of Variance of Diallel Tables. Biometrics 10, 235. &lt;a href=&#34;https://doi.org/10.2307/3001877&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/3001877&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zhang, Y., Kang, M.S., Lamkey, K.R., 2005. DIALLEL-SAS05: A Comprehensive Program for Griffing’s and Gardner-Eberhart Analyses. Agronomy Journal 97, 1097–1106. &lt;a href=&#34;https://doi.org/10.2134/agronj2004.0260&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2134/agronj2004.0260&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Gardner-Eberhart models</title>
      <link>https://www.statforbiology.com/2021/stat_met_diallel_gardner/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_met_diallel_gardner/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Another post for this series about diallel mating experiments. So far, we have published a paper in Plant Breeding (&lt;a href=&#34;https://link.springer.com/article/10.1007/s00122-020-03716-8&#34;&gt;Onofri et al., 2020&lt;/a&gt;), where we presented &lt;code&gt;lmDiallel&lt;/code&gt;, a new R package to fit diallel models. We followed up this paper with a series of four blog posts, giving more detail about the package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see here&lt;/a&gt;), about the Hayman’s models type 1 (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;) and type 2 (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman2/&#34;&gt;see here&lt;/a&gt;) and about the Griffing’s family of models (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_met_diallel_griffing/&#34;&gt;see here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post we are going to talk about the Gardner-Eberarth models, which are particularly suitable to describe heterotic effects. The peculiar trait of these models is that they consider different means for crosses and selfed parents and, therefore, they are reserved for the mating designs 2 (selfed parents and crosses, but no reciprocals) or 1 (selfed parents, crosses and reciprocals). The first model is know as GE2 model and it is specified as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu_{\nu} + \gamma_k + 0.5 \, \left( v_i + v_j \right) + \bar{h} +  h_i + h_j + s_{ij} + \varepsilon_{ijk} \quad\quad\quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\gamma_k\)&lt;/span&gt; is the effect of block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\nu}\)&lt;/span&gt; is the overall mean for all selfed parents (not the overall mean, as in other diallel models). The parameters &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt;) represent the differences between the expected value for the selfed parents &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the mean for all selfed parents (&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\nu}\)&lt;/span&gt;). According to the authors, this would be the Variety Effect (VE); as a consequence, the expected value for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; selfed parent is &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\nu} + v_i\)&lt;/span&gt;, while the expected value for the cross &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;, in absence of any dominance/heterosis effects, would be &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\nu} + 0.5 \, \left( v_i + v_j \right)\)&lt;/span&gt;, that is the mean value of its parents. There is a close relationship between &lt;span class=&#34;math inline&#34;&gt;\(g_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_j\)&lt;/span&gt; in Griffing’s equations (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_met_diallel_griffing/&#34;&gt;see here&lt;/a&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(v_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt; in the GE2 equation (Eq. 1), that is: &lt;span class=&#34;math inline&#34;&gt;\(v_i = 2\, g_i + (n - 2) d_i\)&lt;/span&gt;; therefore, the sum of squares for the GCA and VE effects are the same, although the estimates are different.&lt;/p&gt;
&lt;p&gt;Since a cross does not necessarily respond according to the mean value of its parents, the parameter &lt;span class=&#34;math inline&#34;&gt;\(\bar{h}\)&lt;/span&gt; represents the average heterosis (H.BAR) contributed by the the whole set of genotypes used in crosses. In the balanced case, &lt;span class=&#34;math inline&#34;&gt;\(\bar{h}\)&lt;/span&gt; represents the difference between the overall mean for selfed parents and the overall mean for crosses, under the constraint that &lt;span class=&#34;math inline&#34;&gt;\(\bar{h} = 0\)&lt;/span&gt; for all selfed parents. Besides, the parameters &lt;span class=&#34;math inline&#34;&gt;\(h_i\)&lt;/span&gt; represent the average heterosis contributed by the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; parent in its crosses (Hi), while &lt;span class=&#34;math inline&#34;&gt;\(s_{ij}\)&lt;/span&gt; is the Specific Combining Ability (SCA) for the cross between the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents, that is totally equivalent to the corresponding parameter in Griffing’s models.&lt;/p&gt;
&lt;p&gt;It is clear that both the Hayman’s model type 2 and the GE2 model account for the heterosis effect, although they do it in a different way: in Hayman’s model type 2 the specific effect of heterosis is assessed with reference to the overall mean, while in GE2 it is assessed by comparing the mean of a cross with the means of its parents. Indeed, the sum of squares for the ‘MDD’ effect in Hayman’s model and ‘Hi’ effect in GE2 model are perfectly the same, although the parameters are different.&lt;/p&gt;
&lt;p&gt;Gardner and Eberhart proposed another model (GE3), which we have slightly modified to maintain a consistent notation in the frame of GLMs:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left\{ {\begin{array}{ll}
y_{ijk} = \mu_{\nu} + \gamma_k + \bar{h} + \textrm{gc}_i + \textrm{gc}_j + s_{ij} &amp;amp; \textrm{if} \quad i \neq j\\
y_{ijk} = \mu_{\nu} + \gamma_k + \textrm{sp}_i &amp;amp; \textrm{if} \quad i = j
\end{array}} \right. \quad\quad\quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Equation 2 is an array of two separate elements for crosses and selfed parents. For the crosses (equation above), the parameters &lt;span class=&#34;math inline&#34;&gt;\(\textrm{gc}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{gc}_j\)&lt;/span&gt; represent the GCA for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; parents in all their crosses (GCAC); it should be noted that GCA &lt;span class=&#34;math inline&#34;&gt;\(\neq\)&lt;/span&gt; GCAC, as this latter effect is estimated without considering the selfed parents. The parameters &lt;span class=&#34;math inline&#34;&gt;\(s{ij}\)&lt;/span&gt; are the same as in the previous models (Hayman’s and Griffing’s models: SCA effect), while &lt;span class=&#34;math inline&#34;&gt;\(\textrm{sp}_i\)&lt;/span&gt; represent the effects of selfed parents (SP): they are numerically equivalent to the corresponding effects in Equation 1, but the sum of squares are different (see Murray et al., 2003). Therefore, we use different names for these two effects (SP and Hi).&lt;/p&gt;
&lt;div id=&#34;example-1-a-half-diallel-no-reciprocals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1: a half-diallel (no reciprocals)&lt;/h1&gt;
&lt;p&gt;As an example of a diallel experiments with no reciprocals, we consider the data reported in Lonnquist and Gardner (1961) relating to the yield of 21 maize genotypes, obtained from six male and six female parentals. The dataset is available as &lt;code&gt;lonnquist61&lt;/code&gt; in the &lt;code&gt;lmDiallel&lt;/code&gt; package; in the box below we load the data, after installing (if necessary) and loading the ‘lmDiallel’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
library(multcomp)
data(&amp;quot;lonnquist61&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this complete diallel experiment we can fit equation 1 (model GE2), by including the functions &lt;code&gt;H.BAR()&lt;/code&gt;, &lt;code&gt;VEi()&lt;/code&gt;, &lt;code&gt;Hi()&lt;/code&gt; and &lt;code&gt;SCA()&lt;/code&gt;; we can use either the &lt;code&gt;lm()&lt;/code&gt; or the &lt;code&gt;lm.diallel()&lt;/code&gt; functions, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod &amp;lt;- lm(Yield ~ H.BAR(Par1, Par2) + VEi(Par1, Par2) + 
             Hi(Par1, Par2) + SCA(Par1, Par2),
           data = lonnquist61)
dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = lonnquist61, fct = &amp;quot;GE2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the dataset has no replicates and, therefore, we need to provide an estimate of the residual mean square and degrees of freedom. If we have fitted the model by using the &lt;code&gt;lm()&lt;/code&gt; function, the resulting ‘lm’ object can be explored by using the &lt;code&gt;summary.diallel()&lt;/code&gt; and &lt;code&gt;anova.diallel()&lt;/code&gt; functions. Otherwise, if we have fitted the model with the &lt;code&gt;lm.diallel()&lt;/code&gt; function, the resulting ‘diallel’ object can be explored by using the &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; methods. See the box below for an example: the results are, obviously, the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summary.diallel(dMod, MSE = 7.1, dfr = 60)
anova.diallel(dMod, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##                   Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## H.BAR(Par1, Par2)  1 115.440 115.440 16.2592 0.0001583 ***
## VEi(Par1, Par2)    5 234.230  46.846  6.5980 5.923e-05 ***
## Hi(Par1, Par2)     5  59.720  11.944  1.6823 0.1527246    
## SCA(Par1, Par2)    9  63.781   7.087  0.9981 0.4515416    
## Residuals         60           7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# summary(dMod2, MSE = 7.1, dfr = 60)
anova(dMod2, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## h.bar      1 115.440 115.440 16.2592 0.0001583 ***
## Variety    5 234.230  46.846  6.5980 5.923e-05 ***
## h.i        5  59.720  11.944  1.6823 0.1527246    
## SCA        9  63.781   7.087  0.9981 0.4515416    
## Residuals 60           7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also for the diallel object, we can retrieve the full list of genetical parameters with the &lt;code&gt;glht()&lt;/code&gt; function, by using the same syntax as shown above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gh &amp;lt;- glht(linfct = diallel.eff(dMod2, MSE = 7.1, dfr = 60))
summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0   92.450      1.088  84.987  &amp;lt; 2e-16 ***
# h.bar == 0        5.190      1.287   4.032  0.00043 ***
# v_B == 0          4.150      2.432   1.706  0.09991 .  
# v_G == 0         -4.550      2.432  -1.871  0.07270 .  
# v_H == 0         -0.750      2.432  -0.308  0.76028    
# v_K == 0         -1.150      2.432  -0.473  0.64031    
# v_K2 == 0         3.750      2.432   1.542  0.13524    
# v_M == 0         -1.450      2.432  -0.596  0.55625    
#...
#...
# s_K2:K == 0       0.585      2.064   0.283  0.77909    
# s_K2:M == 0      -1.115      2.064  -0.540  0.59364    
# s_M:B == 0       -1.040      2.064  -0.504  0.61859    
# s_M:G == 0       -2.290      2.064  -1.110  0.27737    
# s_M:H == 0        3.385      2.064   1.640  0.11304    
# s_M:K == 0        1.060      2.064   0.514  0.61189    
# s_M:K2 == 0      -1.115      2.064  -0.540  0.59364 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to fit Equation 2 (model GE3), we can follow a very similar approach, by using the functions &lt;code&gt;H.BAR()&lt;/code&gt;, &lt;code&gt;SP()&lt;/code&gt;, &lt;code&gt;GCAC()&lt;/code&gt; and &lt;code&gt;SCA()&lt;/code&gt;. The box below shows an example either with the &lt;code&gt;lm()&lt;/code&gt; or the with the &lt;code&gt;lm.diallel()&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod &amp;lt;- lm(Yield ~ H.BAR(Par1, Par2) + SP(Par1, Par2)
           + GCAC(Par1, Par2) + SCA(Par1, Par2),
           data = lonnquist61)
dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = lonnquist61, fct = &amp;quot;GE3&amp;quot;)

# summary.diallel(dMod, MSE = 7.1, dfr = 60)
anova.diallel(dMod, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##                   Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## H.BAR(Par1, Par2)  1 115.440 115.440 16.2592 0.0001583 ***
## SP(Par1, Par2)     5  55.975  11.195  1.5768 0.1804080    
## GCAC(Par1, Par2)   5 237.975  47.595  6.7035 5.069e-05 ***
## SCA(Par1, Par2)    9  63.781   7.087  0.9981 0.4515416    
## Residuals         60           7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# summary(dMod2, MSE = 7.1, dfr = 60)
anova(dMod2, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##             Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## h.bar        1 115.440 115.440 16.2592 0.0001583 ***
## Selfed par.  5  55.975  11.195  1.5768 0.1804080    
## Varieties    5 237.975  47.595  6.7035 5.069e-05 ***
## SCA          9  63.781   7.087  0.9981 0.4515416    
## Residuals   60           7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also for the diallel object, we can retrieve the full list of genetical parameters with the &lt;code&gt;glht()&lt;/code&gt; function, by using the same syntax as shown above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh &amp;lt;- glht(linfct = diallel.eff(dMod2, MSE = 7.1, dfr = 60))
summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept == 0   92.450      1.088  84.987  &amp;lt; 2e-16 ***
## h.bar == 0        5.190      1.287   4.032  0.00043 ***
## sp_B == 0         4.150      2.432   1.706  0.09991 .  
## sp_G == 0        -4.550      2.432  -1.871  0.07270 .  
## sp_H == 0        -0.750      2.432  -0.308  0.76028    
## sp_K == 0        -1.150      2.432  -0.473  0.64031    
## sp_K2 == 0        3.750      2.432   1.542  0.13524    
## sp_M == 0        -1.450      2.432  -0.596  0.55625    
## gc_B == 0         0.900      1.216   0.740  0.46593    
## gc_G == 0        -2.050      1.216  -1.686  0.10385    
## gc_H == 0        -0.025      1.216  -0.021  0.98376    
## gc_K == 0        -3.000      1.216  -2.467  0.02055 *  
## gc_K2 == 0        6.375      1.216   5.242 1.78e-05 ***
## gc_M == 0        -2.200      1.216  -1.809  0.08205 .  
## s_B:G == 0        4.810      2.064   2.330  0.02781 *  
## s_B:H == 0       -1.415      2.064  -0.686  0.49905    
## s_B:K == 0       -0.140      2.064  -0.068  0.94644    
## s_B:K2 == 0      -2.215      2.064  -1.073  0.29305    
## s_B:M == 0       -1.040      2.064  -0.504  0.61859    
## s_G:B == 0        4.810      2.064   2.330  0.02781 *  
## s_G:H == 0       -2.865      2.064  -1.388  0.17689    
## s_G:K == 0       -0.990      2.064  -0.480  0.63548    
## s_G:K2 == 0       1.335      2.064   0.647  0.52342    
## s_G:M == 0       -2.290      2.064  -1.110  0.27737    
## s_H:B == 0       -1.415      2.064  -0.686  0.49905    
## s_H:G == 0       -2.865      2.064  -1.388  0.17689    
## s_H:K == 0       -0.515      2.064  -0.250  0.80492    
## s_H:K2 == 0       1.410      2.064   0.683  0.50056    
## s_H:M == 0        3.385      2.064   1.640  0.11304    
## s_K:B == 0       -0.140      2.064  -0.068  0.94644    
## s_K:G == 0       -0.990      2.064  -0.480  0.63548    
## s_K:H == 0       -0.515      2.064  -0.250  0.80492    
## s_K:K2 == 0       0.585      2.064   0.283  0.77909    
## s_K:M == 0        1.060      2.064   0.514  0.61189    
## s_K2:B == 0      -2.215      2.064  -1.073  0.29305    
## s_K2:G == 0       1.335      2.064   0.647  0.52342    
## s_K2:H == 0       1.410      2.064   0.683  0.50056    
## s_K2:K == 0       0.585      2.064   0.283  0.77909    
## s_K2:M == 0      -1.115      2.064  -0.540  0.59364    
## s_M:B == 0       -1.040      2.064  -0.504  0.61859    
## s_M:G == 0       -2.290      2.064  -1.110  0.27737    
## s_M:H == 0        3.385      2.064   1.640  0.11304    
## s_M:K == 0        1.060      2.064   0.514  0.61189    
## s_M:K2 == 0      -1.115      2.064  -0.540  0.59364    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- none method)
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0   92.450      1.088  84.987  &amp;lt; 2e-16 ***
# h.bar == 0        5.190      1.287   4.032  0.00043 ***
# sp_B == 0         4.150      2.432   1.706  0.09991 .  
# sp_G == 0        -4.550      2.432  -1.871  0.07270 .  
# sp_H == 0        -0.750      2.432  -0.308  0.76028    
# sp_K == 0        -1.150      2.432  -0.473  0.64031    
# sp_K2 == 0        3.750      2.432   1.542  0.13524    
# ...
# ...
# s_K2:H == 0       1.410      2.064   0.683  0.50056    
# s_K2:K == 0       0.585      2.064   0.283  0.77909    
# s_K2:M == 0      -1.115      2.064  -0.540  0.59364    
# s_M:B == 0       -1.040      2.064  -0.504  0.61859    
# s_M:G == 0       -2.290      2.064  -1.110  0.27737    
# s_M:H == 0        3.385      2.064   1.640  0.11304    
# s_M:K == 0        1.060      2.064   0.514  0.61189    
# s_M:K2 == 0      -1.115      2.064  -0.540  0.59364   &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-a-full-diallel-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2: a full diallel experiment&lt;/h1&gt;
&lt;p&gt;If we have a full diallel experiment (with reciprocals), we can fit Equations 1 and 2, but we should also include the reciprocal effects, in order to avoid that the residual term is inflated and no longer provides a reliable estimate of the experimental error. We provide an example with the data in Hayman (1954), relating to a complete diallel experiment with eight parental lines, producing 64 combinations (8 selfs + 28 crosses with 2 reciprocals each). The R dataset is included in the ‘lmDiallel’ package and the models are fitted by using the same coding as above, apart from the fact that the function &lt;code&gt;REC()&lt;/code&gt; is included in the &lt;code&gt;lm()&lt;/code&gt; call and the arguments “GE2r” and “GE3r” are used instead of “GE2” and “GE3” in the &lt;code&gt;lm.diallel()&lt;/code&gt; call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;hayman54&amp;quot;)
contrasts(hayman54$Block) &amp;lt;- &amp;quot;contr.sum&amp;quot;
dMod &amp;lt;- lm(Ftime ~ Block + H.BAR(Par1, Par2) + VEi(Par1, Par2) + 
             Hi(Par1, Par2) + SCA(Par1, Par2) + REC(Par1, Par2),
           data = hayman54)
dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54, fct = &amp;quot;GE2r&amp;quot;)
# summary(dMod2)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block        1    142     142  0.3416   0.56100    
## h.bar        1  30797   30797 73.8840 3.259e-12 ***
## Variety      7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## h.i          7  34153    4879 11.7050 1.957e-09 ***
## SCA         20  37289    1864  4.4729 2.560e-06 ***
## Reciprocals 28  19112     683  1.6375   0.05369 .  
## Residuals   63  26260                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0  2.039e+02  5.104e+00  39.956  &amp;lt; 2e-16 ***
# h.bar == 0     -4.690e+01  5.456e+00  -8.596 4.48e-09 ***
# v_A == 0        8.506e+01  1.350e+01   6.299 1.14e-06 ***
# v_B == 0       -3.344e+01  1.350e+01  -2.476 0.020115 *  
# v_C == 0        1.841e+02  1.350e+01  13.630 2.37e-13 ***
# v_D == 0        3.706e+01  1.350e+01   2.745 0.010839 *  
# v_E == 0       -3.794e+01  1.350e+01  -2.809 0.009301 ** 
# v_F == 0       -3.394e+01  1.350e+01  -2.513 0.018499 *  
# v_G == 0       -1.509e+02  1.350e+01 -11.177 1.99e-11 ***
# v_H == 0       -4.994e+01  1.350e+01  -3.698 0.001023 ** 
# h_A == 0        4.885e+00  7.797e+00   0.627 0.536380    
# ...
# ...
# r_H:C == 0     -5.500e+00  1.021e+01  -0.539 0.594620    
# r_H:D == 0     -5.000e+00  1.021e+01  -0.490 0.628380    
# r_H:E == 0     -8.500e+00  1.021e+01  -0.833 0.412617    
# r_H:F == 0     -1.750e+01  1.021e+01  -1.714 0.098370 .  
# r_H:G == 0     -1.400e+01  1.021e+01  -1.371 0.181956    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for the GE3 model with reciprocal effects is shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod &amp;lt;- lm(Ftime ~ Block + H.BAR(Par1, Par2) + SP(Par1, Par2)
           + GCAC(Par1, Par2) + SCA(Par1, Par2) + REC(Par1, Par2),
           data = hayman54)
dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54, fct = &amp;quot;GE3r&amp;quot;)
# summary(dMod2)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block        1    142   142.4  0.3416   0.56100    
## h.bar        1  30797 30796.9 73.8840 3.259e-12 ***
## gcac         7 168923 24131.9 57.8941 &amp;lt; 2.2e-16 ***
## Selfed par.  7 142946 20420.9 48.9913 &amp;lt; 2.2e-16 ***
## SCA         20  37289  1864.4  4.4729 2.560e-06 ***
## Reciprocals 28  19112   682.6  1.6375   0.05369 .  
## Residuals   63  26260                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# anova(dMod)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;If we intend to regard the genetic effects as random and to estimate variance components, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a bunch of dummy variables. In order to make things simpler for routine experiments, we have coded the &lt;code&gt;mmer.diallel()&lt;/code&gt; wrapper using the same syntax as the &lt;code&gt;lm.diallel()&lt;/code&gt; function. The exemplary code is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random genetic effects
mod1m &amp;lt;- mmer.diallel(Yield ~ Par1 + Par2,
                      data = lonnquist61,
                      fct = &amp;quot;GE2&amp;quot;)
mod1m$varcomp
##          VarComp VarCompSE
## Variety 2.344044  2.333869
## h.i     5.172099  4.905691
## SCA     6.142047  2.789668
mod2m &amp;lt;- mmer.diallel(Yield ~ Par1 + Par2,
                      data = lonnquist61,
                      fct = &amp;quot;GE3&amp;quot;)
mod2m$varcomp
##               VarComp VarCompSE
## GCAC        10.125567  7.563026
## Selfed par.  4.107823  7.830039
## SCA          7.087220  3.342822
mod3m &amp;lt;- mmer.diallel(Ftime ~ Par1 + Par2,
                      data = hayman54,
                      fct = &amp;quot;GE2r&amp;quot;)
mod3m$varcomp
##                VarComp  VarCompSE
## Variety     2347.35935 1279.94018
## h.i          634.70067  408.56286
## SCA          362.24772  148.53288
## Reciprocals   66.78085   49.16288
## Residuals    415.44775   73.43871
mod4m &amp;lt;- mmer.diallel(Ftime ~ Par1 + Par2,
                      data = hayman54,
                      fct = &amp;quot;GE3r&amp;quot;)
mod4m$varcomp
##                 VarComp  VarCompSE
## GCAC          927.78740  537.89968
## Selfed par. 10003.93261 5456.47108
## SCA           362.96912  148.50097
## Reciprocals    67.50895   49.11942
## Residuals     412.54141   72.93144&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Luigi Russi&lt;br /&gt;
Niccolò Terzaroli&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0156744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0156744&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gardner, C.O., Eberhart, S.A., 1966. Analysis and Interpretation of the Variety Cross Diallel and Related Populations. Biometrics 22, 439. &lt;a href=&#34;https://doi.org/10.2307/2528181&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/2528181&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Griffing, B., 1956. Concept of general and specific combining ability in relation to diallel crossing systems. Australian Journal of Biological Science 9, 463–493.&lt;/li&gt;
&lt;li&gt;Hayman, B.I., 1954. The Analysis of Variance of Diallel Tables. Biometrics 10, 235. &lt;a href=&#34;https://doi.org/10.2307/3001877&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/3001877&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Split-plot designs: the transition to mixed models for a dinosaur</title>
      <link>https://www.statforbiology.com/2021/stat_lmm_splitplottransition/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_lmm_splitplottransition/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Those who long ago took courses in ‘analysis of variance’ or ‘experimental design’ … would have learned methods … based on observed and expected mean squares and methods of testing based on ‘error strata’ (if you weren’t forced to learn this, consider yourself lucky). (&lt;a href=&#34;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&#34;&gt;Douglas Bates, 2006&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In a previous post, I already mentioned that, due to my age, I see myself as a dinosaur within the R-users community. I already mentioned how difficult it is, for a dinosaur, to adjust to new concepts and paradigms in data analysis, after having done things differently for a long time ( &lt;a href=&#34;https://www.statforbiology.com/2020/stat_r_tidyverse_columnwise/&#34;&gt;see this post here&lt;/a&gt; ). Today, I decided to sit and write a second post, relating to data analyses for split-plot designs. Some years ago, when switching to R, this topic required some adjustments to my usual workflow, which gave me a few headaches.&lt;/p&gt;
&lt;p&gt;Let’s start from a real-life example.&lt;/p&gt;
&lt;div id=&#34;a-split-plot-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A split-plot experiment&lt;/h1&gt;
&lt;p&gt;The dataset ‘beet.csv’ is available in a web repository. It was obtained from a split-plot experiment with two experimental factors: three tillage methods (shallow ploughing, deep ploughing and minimum tillage) and two weed control methods (total and partial, meaning that the herbicide was sprayed broadcast or only along crop rows). Tillage methods were allocated to main-plots, while weed control methods were allocated to sub-plots and the experiment was designed in four complete blocks. A typical split-plot field experiment, indeed. The code below can be used to load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/beet.csv&amp;quot;
dataset &amp;lt;- read_csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Tillage, WeedControl, Block), .fns = factor))
dataset
## # A tibble: 24 x 4
##    Tillage WeedControl Block Yield
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
##  1 MIN     TOT         1     11.6 
##  2 MIN     TOT         2      9.28
##  3 MIN     TOT         3      7.02
##  4 MIN     TOT         4      8.02
##  5 MIN     PART        1      5.12
##  6 MIN     PART        2      4.31
##  7 MIN     PART        3      8.94
##  8 MIN     PART        4      5.62
##  9 SP      TOT         1     10.0 
## 10 SP      TOT         2      8.69
## # … with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-traditional-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The traditional approach&lt;/h1&gt;
&lt;p&gt;Split-plot designs are very commonly used in field experiments and they have been in fashion for (at least) eighty years, long before that the mixed model platform with REML estimation was largely available. Whoever has taken a course in ‘experimental design’ at the end of the 80s has studied how to perform a split-plot ANOVA by hand-calculations, based on the method of moments. For the youngest readers, it might be useful to give a few hints on what I used to do thirty years ago with the above dataset:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;calculate the overall mean and the means for the levels of blocks, tillage, weed control and for the combined levels of tillage and weed control.&lt;/li&gt;
&lt;li&gt;Calculate the means for the combined levels of blocks and tillage, which would correspond to the means for the twelve main-plots.&lt;/li&gt;
&lt;li&gt;With all those means, calculate the deviances for all effects and interactions, as the sums of squared residuals with respect to the overall mean.&lt;/li&gt;
&lt;li&gt;Derive the related variance, by using the appropriate number of degrees of freedom for each effect.&lt;/li&gt;
&lt;li&gt;Calculate F ratios, based on the appropriate error stratum, i.e. the mean square for the ‘blocks ⨉ tillage’ combinations (so called: error A) and the residual mean square.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most relevant aspect in the approach outlined above is the ‘block by tillage’ interaction; the mean square for this effect was used as the denominator in the F ratio, to test for the significance of the tillage main effect.&lt;/p&gt;
&lt;p&gt;The above process was simple to teach and simple to grasp and I used to see it as a totally correct approach to balanced (orthogonal) split-plot data. Those of you who are experienced with SAS should probably remember that, before the advent of PROC MIXED in 1992, split-plot designs were analysed with PROC GLM, using the very same approach as outlined above.&lt;/p&gt;
&lt;p&gt;Considering the above background, let’s see what I did when I switched to R?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-step-aov&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First step: ‘aov()’&lt;/h1&gt;
&lt;p&gt;Having the method of moments in mind, my first line of attack was to use the &lt;code&gt;aov()&lt;/code&gt; function, as suggested in Venables and Ripley (2002) at pag. 283. Those authors make use of the nesting operator in the expression &lt;code&gt;Error(Block/Tillage)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov &amp;lt;- aov(Yield ~ Tillage*WeedControl +
                 Error(Block/Tillage), data = dataset)
summary(mod.aov)
## 
## Error: Block
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)
## Residuals  3   3.66    1.22               
## 
## Error: Block:Tillage
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)   
## Tillage    2 23.656   11.83    19.4 0.0024 **
## Residuals  6  3.658    0.61                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Error: Within
##                     Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  
## WeedControl          1   3.32   3.320   1.225 0.2972  
## Tillage:WeedControl  2  19.46   9.732   3.589 0.0714 .
## Residuals            9  24.40   2.711                 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above definition, the block effect is regarded as random, while, in the traditional approach, it is often regarded as fixed. Indeed, still today, there is no consensus among agricultural scientists on whether the block effect should be regarded as random or fixed (see Dixon, 2016); for the sake of this exercise, let me regard it as fixed. After a few attempts, I discovered that I could move the effect of blocks from the &lt;code&gt;Error()&lt;/code&gt; definition to the fixed effect formula and use the expression &lt;code&gt;Error(Block:Tillage)&lt;/code&gt; to specify the uppermost error stratum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov2 &amp;lt;- aov(Yield ~ Block + Tillage*WeedControl +
                 Error(Block:Tillage), data = dataset)
## Warning in aov(Yield ~ Block + Tillage * WeedControl + Error(Block:Tillage), :
## Error() model is singular
summary(mod.aov2)
## 
## Error: Block:Tillage
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)   
## Block      3  3.660    1.22   2.001 0.2155   
## Tillage    2 23.656   11.83  19.399 0.0024 **
## Residuals  6  3.658    0.61                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Error: Within
##                     Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  
## WeedControl          1   3.32   3.320   1.225 0.2972  
## Tillage:WeedControl  2  19.46   9.732   3.589 0.0714 .
## Residuals            9  24.40   2.711                 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the above code produces a warning message, the result is totally the same as I would have obtained by hand-calculations.&lt;/p&gt;
&lt;p&gt;For me, the &lt;code&gt;aov()&lt;/code&gt; function represented a safe harbour, mainly because the result was very much like what I would expect, considering my experience with mean squares and error strata. Unfortunately, I had to realise that there were several limitations to this approach and, finally, I had to switch to the mixed model platform.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-step-the-mixed-model-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Second step: the mixed model framework&lt;/h1&gt;
&lt;p&gt;When making this switch to mixed models, I had the expectation that I should be able to reproduce the results obtained with the &lt;code&gt;aov()&lt;/code&gt; function and, formerly, by hand-calculations.&lt;/p&gt;
&lt;p&gt;I started with the &lt;code&gt;lme()&lt;/code&gt; function in the ‘nlme’ package (Pinheiro et al., 2018) and I had the idea that I could simply replace the &lt;code&gt;Error(Block:Tillage)&lt;/code&gt; statement with &lt;code&gt;random = ~1|Block:Tillage&lt;/code&gt;. Unfortunately, using the &lt;code&gt;:&lt;/code&gt; operator in the &lt;code&gt;lme()&lt;/code&gt; function is not possible and I had to resort to using the nesting operator &lt;code&gt;‘Block/Tillage’&lt;/code&gt;. Consequently, I noted that the F test for the block effect was wrong (of course: the specification was wrong…). I could have removed the block from the fixed effect model, but I was so stupidly determined to reproduce my hand-calculations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
## 
## Attaching package: &amp;#39;nlme&amp;#39;
## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     collapse
mod.lme &amp;lt;- lme(Yield ~ Block + Tillage*WeedControl,
               random = ~1|Block/Tillage, data = dataset)
anova(mod.lme)
## Warning in pf(Fval[i], nDF[i], dDF[i]): NaNs produced
##                     numDF denDF   F-value p-value
## (Intercept)             1     9 120.85864  &amp;lt;.0001
## Block                   3     0   0.08045     NaN
## Tillage                 2     6   6.32281  0.0333
## WeedControl             1     9   1.77497  0.2155
## Tillage:WeedControl     2     9   5.20229  0.0315&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, I tried to switch to the &lt;code&gt;lmer()&lt;/code&gt; function in the ‘lme4’ package (Bates et al., 2015). With this platform, it was possible to include the ‘block by tillage’ interaction as a random effect, according to my usual workflow. Still, the results did not match to those obtained with the &lt;code&gt;aov()&lt;/code&gt; function: an error message was raised and F ratios were totally different. Furthermore, p-levels were not even displayed (yes, now I know that we can use the ‘lmerTest’ package, but, please, wait a few seconds).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
mod.lmer.split &amp;lt;- lmer(Yield ~ Block + WeedControl*Tillage +
                     (1|Block:Tillage), 
                     data=dataset)
anova(mod.lmer.split)
## Analysis of Variance Table
##                     npar  Sum Sq Mean Sq F value
## Block                  3  3.6596  1.2199  0.6521
## WeedControl            1  3.3205  3.3205  1.7750
## Tillage                2 23.6565 11.8282  6.3228
## WeedControl:Tillage    2 19.4641  9.7321  5.2023&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What’s wrong with that? Why was I not able to reproduce my hand-calculations with the mixed model platform?&lt;/p&gt;
&lt;p&gt;I investigated this matter and I found a very enlightening post by Douglas Bates (the author of ‘nlme’ and ‘lme4’), which is available at &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&#34;&gt;this link&lt;/a&gt;. From there, it was clear to me that F ratios in mixed models are “&lt;em&gt;not based on expected mean squares and error strata&lt;/em&gt;”; further ahead, it is said that there is “&lt;em&gt;a problem with the assumption that the reference distribution for these F statistics should be an F distribution with a known numerator of degrees of freedom but a variable denominator degrees of freedom&lt;/em&gt;”. In the end, it was clear to me that, according to Douglas Bates, the traditional approach of calculating p-values from F ratios based on expected mean squares and error strata was not necessarily correct.&lt;/p&gt;
&lt;p&gt;I made some further research on this matter. Indeed, looking at the &lt;code&gt;aov()&lt;/code&gt; output above, I noted that the residual mean square was equal to 2.711, while the mean square for the ‘Block by Tillage’ interaction was 0.6097. My beloved method of moments brought me to a negative estimate of the variance component for the ‘block by tillage’ interaction, that is &lt;span class=&#34;math inline&#34;&gt;\((0.6097 - 2.711)/4 = -0.5254\)&lt;/span&gt;. I gasped: this was unreasonable and, at least, it would imply that the variance component for the ‘block by tillage’ random effect was not significantly different from zero. In other words, the mean square for the ‘block by tillage’ interaction and the mean square for the residuals were nothing but two separate estimates of the residual plot-to-plot error. I started being suspicious about my hand-calculations. Why did I use two estimates of the same amount as two different error strata?&lt;/p&gt;
&lt;p&gt;I tried a different line of attack: considering that the ‘block by tillage’ interaction was not significant, I removed it from the model. Afterwards I fitted a linear fixed effect model, where the two error strata had been pooled into the residual error term. I obtained the very same F ratios as those obtained from the ‘lmer’ fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.lm &amp;lt;- lm(Yield ~ Block + WeedControl*Tillage, data=dataset) 
anova(mod.lm)
## Analysis of Variance Table
## 
## Response: Yield
##                     Df  Sum Sq Mean Sq F value  Pr(&amp;gt;F)  
## Block                3  3.6596  1.2199  0.6521 0.59389  
## WeedControl          1  3.3205  3.3205  1.7750 0.20266  
## Tillage              2 23.6565 11.8282  6.3228 0.01020 *
## WeedControl:Tillage  2 19.4641  9.7321  5.2023 0.01922 *
## Residuals           15 28.0609  1.8707                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In that precise moment when I noted such a result, it was clear to me that, even with simple and orthogonal split-plot designs, hand-calculations do not necessarily produce correct results and should never, ever be used as the reference to assess the validity of a mixed model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suggestions-for-dinosaurs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Suggestions for dinosaurs&lt;/h1&gt;
&lt;p&gt;If you are one of those who have never taken a lesson about expected mean squares and error strata, well, believe me, you are lucky! For us dinosaurs, switching to the mixed model platform may be a daunting task. We need to free up our minds and change our workflow; a few suggestions are following.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-1-change-model-building-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rule 1: change model building process&lt;/h1&gt;
&lt;p&gt;In principle, do no insist on including the ‘block by tillage’ interaction in the model. With split-plot experiments, the main-plot is to be regarded as a &lt;em&gt;grouping structure&lt;/em&gt;, wherein we take repeated measures in different sub-plots. These measures are correlated, as they are more alike than measures taken in different sub-plots.&lt;/p&gt;
&lt;p&gt;Therefore, for this grouping structure (and for all grouping structures in general) we need to code a &lt;em&gt;grouping factor&lt;/em&gt;, to uniquely identify the repeated measures in each main-plot. This factor must be included in the model, otherwise we violate the basic assumption of independence of model residuals. Consider that the main-plot represent the randomisation units to which the tillage treatments were allocated; therefore, the main plot factor needs to be included in the model as a random effect. Please refer to the good paper of Piepho et al. (2003) for further information on this model building approach.&lt;/p&gt;
&lt;p&gt;In the box below I created the main-plot factor by using &lt;code&gt;dplyr()&lt;/code&gt; to combine the levels of blocks and tillage methods. The difference with the traditional approach of using the ‘block by tillage’ interaction in the model is subtle, but, in this case, the &lt;code&gt;lme()&lt;/code&gt; function returns no error. Please, note that, having no interest in the estimation of variance components, I have fitted this model by maximum likelihood estimation: it is confirmed that the main-plot random effect is zero (see the output of the &lt;code&gt;VarCorr()&lt;/code&gt; function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(mainPlots = factor(Block:Tillage))
mod.lme2 &amp;lt;- lme(Yield ~ Block + Tillage * WeedControl,
               random = ~1|mainPlots, data = dataset,
               method = &amp;quot;ML&amp;quot;)
VarCorr(mod.lme2)
## mainPlots = pdLogChol(1) 
##             Variance     StdDev      
## (Intercept) 4.462849e-10 2.112546e-05
## Residual    1.169203e+00 1.081297e+00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-2-change-the-approach-to-hypotheses-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rule 2: change the approach to hypotheses testing&lt;/h1&gt;
&lt;p&gt;In the agricultural sciences we have been very much familiar with ANOVA tables, showing all fixed effects along with their significance level. I am very much convinced that we should refrain from such a (possibly bad) habit. Indeed, there is no point in testing the significance of main effects before testing the significance of the ‘tillage by weed control’ interaction, as main effects are marginal to the interaction effect.&lt;/p&gt;
&lt;p&gt;At first, we need to concentrate on the interaction effect. According to maximum likelihood theory, it is very logic to think of a Likelihood Ratio Test (LRT), which consists of comparing the likelihoods of two alternative and nested models. In this case, the model above (‘mod.lme2’) can be compared with a ‘reduced’ model without the ‘tillage by weed control’ interaction term: if the two likelihoods are similar, that would be a sign that the interaction effect is not significant. The reduced model fit is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.lme3 &amp;lt;- lme(Yield ~ Block + Tillage + WeedControl,
               random = ~1|mainPlots, data = dataset,
               method = &amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logarithms of the two likelihoods show that the ‘full model’ (with the interaction term) is more ‘likely’ than the reduced model. The LRT is calculated as twice the difference between the two log-likelihoods (the logarithm of the ratio of two numbers is the difference of the logarithms, remember?).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll2 &amp;lt;- logLik(mod.lme2)
ll3 &amp;lt;- logLik(mod.lme3)
ll2; ll3
## &amp;#39;log Lik.&amp;#39; -35.93039 (df=11)
## &amp;#39;log Lik.&amp;#39; -42.25294 (df=9)
LRT &amp;lt;- - 2 * (as.numeric(ll3) - as.numeric(ll2))
LRT
## [1] 12.6451&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For large samples and under the null hypothesis that the two models are not significantly different, the LRT is distributed according to a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; with two degrees of freedom (i.e. the difference in the number of model parameters used by the two models). We could use such an assumption to obtain a p-level for the null, for example by way of the &lt;code&gt;anova()&lt;/code&gt; function, to which we pass the two model objects as arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.lme2, mod.lme3)
##          Model df       AIC      BIC    logLik   Test L.Ratio p-value
## mod.lme2     1 11  93.86078 106.8194 -35.93039                       
## mod.lme3     2  9 102.50589 113.1084 -42.25294 1 vs 2 12.6451  0.0018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, our experiment consists of only 24 observations and the large sample theory should not hold. Therefore, instead of relying on the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution, we can build an empirical sampling distribution for the LRT with Monte Carlo simulation (parametric bootstrap). The process is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;simulate a new dataset under the reduced model, using the fitted parameter estimates and assuming normality for the errors and random effects;&lt;/li&gt;
&lt;li&gt;fit to this dataset both the full and the reduced model;&lt;/li&gt;
&lt;li&gt;compute the LRT statistic;&lt;/li&gt;
&lt;li&gt;repeat steps 1 to 3 many times (e.g., 10000);&lt;/li&gt;
&lt;li&gt;examine the distribution of the bootstrapped LRT values and compute the proportion of those exceeding 12.6451 (empirical p-value).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To this aim, we can use the &lt;code&gt;simulate()&lt;/code&gt; function in the ‘nlme’ package. We pass the reduced model object as the first argument, the full model as the argument ‘m2’, the number of simulations and the seed (if we intend to obtain reproducible results). The fit may take quite a few minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- simulate(mod.lme3, nsim = 10000, m2 = mod.lme2, method=&amp;quot;ML&amp;quot;,
               set.seed = 1234)
lrtSimT &amp;lt;- as.numeric(2*(y$alt$ML[,2] - y$null$ML[,2]))
length(lrtSimT[lrtSimT &amp;gt; 12.6451])/length(lrtSimT)
## [1] 0.0211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the interaction is significant and we can go ahead with further analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;What is the take-home message for this post? When we have to analyse a dataset coming from a split-plot experiment, R forces us to use the mixed model platform. We should not necessarily expect to reproduce the approach and the results we were used to obtain when we made our hand-calculations based on least squares and the method of moments. On the contrary, we should adapt our model building and hypothesis testing process to such a very powerful platform, wherein the slit-plot is treated on equal footing to all other types of repeated measures designs.&lt;/p&gt;
&lt;p&gt;Hope this was fun! If you have any comments, drop me a line to the email below.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Douglas Bates, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. &lt;a href=&#34;doi:10.18637/jss.v067.i01&#34; class=&#34;uri&#34;&gt;doi:10.18637/jss.v067.i01&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2018). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-137, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Venables, W.N., Ripley, B.D., Venables, W.N., 2002. Modern applied statistics with S, 4th ed. ed, Statistics and computing. Springer, New York.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pairwise comparisons in nonlinear regression</title>
      <link>https://www.statforbiology.com/2021/stat_nls_paircomp/</link>
      <pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_nls_paircomp/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Pairwise comparisons are one of the most debated topic in agricultural research: they are very often used and, sometimes, abused, in literature. I have nothing against the appropriate use of this very useful technique and, for those who are interested, some colleagues and I have given a bunch of (hopefully) useful suggestions in a paper, a few years ago (&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/j.1365-3180.2009.00758.x&#34;&gt;follow this link here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Pairwise comparisons usually follow the application of some sort of linear or generalised linear model; in this setting, the ‘emmeans’ package (Lenth, 2020) is very handy, as it uses a very logical approach. However, we can find ourselves in the need of making pairwise comparisons between the elements of a vector, which does not came as the result of linear model fitting.&lt;/p&gt;
&lt;p&gt;For example, we may happen to have an old table of means with standard errors and have lost the original raw data. Or, we may happen to have a vector of parameters from a nonlinear regression model, fitted with the &lt;code&gt;nls()&lt;/code&gt; function. How do we make pairwise comparisons? Experienced users can make profit of the &lt;code&gt;glht()&lt;/code&gt; function in the ‘multcomp’ package, although this is not immediate and, at least for me, it takes always some attempts to recall the exact syntax.&lt;/p&gt;
&lt;p&gt;Therefore, I have built the &lt;code&gt;pairComp()&lt;/code&gt; wrapper, which is available within the ‘aomisc’ package, the accompanying package for this website. Let’s see how this function works by using a typical example.&lt;/p&gt;
&lt;div id=&#34;a-case-study&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A case-study&lt;/h1&gt;
&lt;p&gt;This is a real-life example, taken from a research published by Vischetti et al. in 1996 (we have used this example in other posts, before). That research considered three herbicides for weed control in sugar beet, i.e. metamitron (M), phenmedipham (P) and chloridazon (C). Four soil samples were contaminated, respectively with: (i) M alone, (ii) M + P (iii) M + C and (iv) M + P + C. The aim was to assess whether the degradation speed of metamitron in soil depended on the presence of co-applied herbicides. To reach this aim, the soil samples were incubated at 20°C and sub-samples were taken in different times after the beginning of the experiment. The concentration of metamitron in those sub-samples was measured by HPLC analyses, performed in triplicate. The resulting dataset is available within the ‘aomisc’ package.&lt;/p&gt;
&lt;p&gt;In the box below. we install the ‘aomisc’ package from gitHub (if necessary), load it and load the ‘metamitron’ dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
library(aomisc)
data(metamitron)
head(metamitron)
##   Time Herbicide   Conc
## 1    0         M  92.00
## 2    0         M 118.64
## 3    0         M  89.58
## 4    7         M  59.32
## 5    7         M  62.95
## 6    7         M  62.95
#...
#...
tail(metamitron)
##    Time Herbicide  Conc
## 91   55       MPC 35.75
## 92   55       MPC 37.83
## 93   55       MPC 27.41
## 94   67       MPC 23.38
## 95   67       MPC 28.41
## 96   67       MPC 18.92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first step we take is to fit a first-order degradation model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[C_{t, h} = A_h \, \exp \left(-k_h  \, t \right)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is the concentration at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; for metamitron in the &lt;span class=&#34;math inline&#34;&gt;\(h^{th}\)&lt;/span&gt; combination (M alone, M + P, M + C and M + P + C), &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the initial concentration for the metamitron in the &lt;span class=&#34;math inline&#34;&gt;\(h^{th}\)&lt;/span&gt; combination, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the degradation rate for metamitron in the &lt;span class=&#34;math inline&#34;&gt;\(h^{th}\)&lt;/span&gt; combination. This model is nonlinear and, therefore, we can use the &lt;code&gt;nls()&lt;/code&gt; function for nonlinear least squares regression. The code is given below: please, note that the two parameters are followed by the name of the factor variable in square brackets (i.e.: A[Herbicide] and k[Herbicide]). This is necessary, to fit a different parameter value for each level of the ‘Herbicide’ factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Fit nls grouped model
modNlin &amp;lt;- nls(Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time), 
               start=list(A=rep(100, 4), k=rep(0.06, 4)), 
               data=metamitron)
summary(modNlin)
## 
## Formula: Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## A1 9.483e+01  4.796e+00   19.77   &amp;lt;2e-16 ***
## A2 1.021e+02  4.316e+00   23.65   &amp;lt;2e-16 ***
## A3 9.959e+01  4.463e+00   22.31   &amp;lt;2e-16 ***
## A4 1.116e+02  4.184e+00   26.68   &amp;lt;2e-16 ***
## k1 4.260e-02  4.128e-03   10.32   &amp;lt;2e-16 ***
## k2 2.574e-02  2.285e-03   11.26   &amp;lt;2e-16 ***
## k3 3.034e-02  2.733e-03   11.10   &amp;lt;2e-16 ***
## k4 2.186e-02  1.822e-03   12.00   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 9.701 on 88 degrees of freedom
## 
## Number of iterations to convergence: 5 
## Achieved convergence tolerance: 7.136e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can retrieve the degradation rates for the four herbicides (&lt;em&gt;k1&lt;/em&gt;, &lt;em&gt;k2&lt;/em&gt;, &lt;em&gt;k3&lt;/em&gt; and &lt;em&gt;k4&lt;/em&gt;) together with standard errors and load them into two vectors, as shown in the box below. In order to make pairwise comparisons, we also need to retrieve an estimate of the residual degrees of freedom, which we can also extract from the model fit object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- summary(modNlin)
dRates &amp;lt;- tab$coef[5:8,1]
SEs &amp;lt;- tab$coef[5:8,2]
dfr = tab$df[2]
dRates
##         k1         k2         k3         k4 
## 0.04260044 0.02573512 0.03033803 0.02185935
SEs
##          k1          k2          k3          k4 
## 0.004128447 0.002284696 0.002733498 0.001822218
dfr
## [1] 88&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have one vector of estimates to be compared and one vector of standard errors. In this situation, we can make pairwise comparisons by using the &lt;code&gt;pairComp()&lt;/code&gt; function in the ‘aomisc’ package. We just have to pass the vector of model parameters, the vector of standard errors, and, optionally, the names of parameters (we do not need this, as ‘dRates’ is a named vector), the number of residual degrees of freedom (defaults to ‘Inf’) and the multiplicity adjustment method, as in the ‘multcomp’ package (defaults to “single-step”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cp &amp;lt;- pairComp(dRates, SEs, dfr = dfr, adjust = &amp;quot;holm&amp;quot;)
cp$pairs
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## k1-k2 == 0  0.016865   0.004718   3.574  0.00286 ** 
## k1-k3 == 0  0.012262   0.004951   2.477  0.04604 *  
## k1-k4 == 0  0.020741   0.004513   4.596 8.58e-05 ***
## k2-k3 == 0 -0.004603   0.003563  -1.292  0.37639    
## k2-k4 == 0  0.003876   0.002922   1.326  0.37639    
## k3-k4 == 0  0.008479   0.003285   2.581  0.04604 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- holm method)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also obtain a letter display, by taking the ‘Letters’ slot in the ‘cp’ object. In this case, we might like to change the yardstick protection level, by passing the ‘level’ argument in ‘pairComp()’, that defaults to 0.05.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cp$Letters
##          Mean          SE CLD
## k1 0.04260044 0.004128447   a
## k2 0.02573512 0.002284696  bc
## k3 0.03033803 0.002733498   b
## k4 0.02185935 0.001822218   c&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that the &lt;code&gt;pairComp()&lt;/code&gt; function can be flexibly used in every situation where we have a vector of estimates and a vector of standard errors. It yields correct results whenever the elements of the vector of estimates are uncorrelated. Hope this is useful. Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;
Blog &lt;a href=&#34;www.statforbiology.com&#34;&gt;www.statforbiology.com&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Torsten Hothorn, Frank Bretz and Peter Westfall (2008). Simultaneous Inference in General Parametric Models. Biometrical Journal 50(3), 346–363.&lt;/li&gt;
&lt;li&gt;Russell Lenth (2020). emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.5.0-5. &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Griffing&#39;s models (1956)</title>
      <link>https://www.statforbiology.com/2021/stat_met_diallel_griffing/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_met_diallel_griffing/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Diallel mating designs are often used by plant breeders to compare the possible crosses between a set of genotypes. In spite of such widespread usage, the process of data analysis in R is not yet strightforward and it is not clear which tool should be routinely used. We recently gave a small contribution by publishing a paper in Plant Breeding (&lt;a href=&#34;https://link.springer.com/article/10.1007/s00122-020-03716-8&#34;&gt;Onofri et al., 2020&lt;/a&gt; ), where we advocated the idea that models for diallel crosses are just a class of general linear models, that should be fit by Ordinary Least Squares (OLS) or REstricted Maximum Likelihood methods (REML).&lt;/p&gt;
&lt;p&gt;In that paper, we presented &lt;code&gt;lmDiallel&lt;/code&gt;, a new R package to fit diallel models, which we followed up with a series of three blog posts, giving more detail about the package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see here&lt;/a&gt;), about the Hayman’s models type 1 (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;) and type 2 (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_met_diallel_hayman2/&#34;&gt;see here&lt;/a&gt;). These latter models can be used to describe the data from full diallel experiments.&lt;/p&gt;
&lt;p&gt;In this fourth post we are going to talk about a very flexible family of models, that was introduced by Griffing in 1956 and it is still very used in plant breeding, to estimate General Combining Ability (GCA) and Specific Combining Ability (SCAs). The equations take different forms, to account for all possible mating schemes.&lt;/p&gt;
&lt;p&gt;With full diallel experiments (including selfs and reciprocals; &lt;strong&gt;mating scheme 1&lt;/strong&gt;), the model is very similar to Hayman’s model type 1, except that reciprocal effects are not parted into RGCA and RSCA (Reciprocal General Combining Ability and Reciprocal Specific Combining Ability). The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{ts}_{ij} + r_{ij} + \varepsilon_{ijk} \quad\quad\quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the expected value (the overall mean, in the balanced case) and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual random error term for the observation in the &lt;span class=&#34;math inline&#34;&gt;\(k^{th}\)&lt;/span&gt; block and with the parentals &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. All the other terms correspond to genetic effects, namely:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_j\)&lt;/span&gt; terms are the General Combining Abilities (GCAs) of the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(ts_{ij}\)&lt;/span&gt; term is the total Specific Combining Ability (SCA) for the combination &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(r_{ij}\)&lt;/span&gt; term is the reciprocal effect for a specific &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt; combination.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When the reciprocal crosses are not available (&lt;strong&gt;mating scheme 2&lt;/strong&gt;), the term &lt;span class=&#34;math inline&#34;&gt;\(\textrm{r}_{ij}\)&lt;/span&gt; needs to be dropped, so that the model reduces to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{ts}_{ij} + \varepsilon_{ijk} \quad\quad\quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When the reciprocals are available, but selfs are missing (&lt;strong&gt;mating scheme 3&lt;/strong&gt;), the model is similar to equation 1, but the term &lt;span class=&#34;math inline&#34;&gt;\(\textrm{ts}_{ij}\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\textrm{s}_{ij}\)&lt;/span&gt; (we use a different symbol, because the design matrix is slightly different and needs a different coding):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{s}_{ij} + r_{ij} + \varepsilon_{ijk} \quad\quad\quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Finally, when neither selfs nor reciprocals are available (&lt;strong&gt;mating scheme 4&lt;/strong&gt;), the equation reduces to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{s}_{ij} + \varepsilon_{ijk} \quad\quad\quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s see how to fit the above models by using a set of examples with different mating schemes.&lt;/p&gt;
&lt;div id=&#34;example-1-a-full-diallel-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1: a full diallel experiment&lt;/h1&gt;
&lt;p&gt;The example in Hayman (1954) relates to a complete diallel experiment with eight parental lines, producing 64 combinations (8 selfs + 28 crosses with 2 reciprocals each). The R dataset is included in the ‘lmDiallel’ package; in the box below we load the data, after installing (if necessary) and loading the ‘lmDiallel’ package (see box below). For brevity, some R commands are shown but not executed (they are commented out)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
data(&amp;quot;hayman54&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this complete diallel experiment we can fit equation 1, by including GCAs, tSCAs and reciprocal effects. Please, note that excluding any of these effects results in unreliable estimates of the residual mean square. We can use either the &lt;code&gt;lm()&lt;/code&gt; or the &lt;code&gt;lm.diallel()&lt;/code&gt; functions, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contrasts(hayman54$Block) &amp;lt;- &amp;quot;contr.sum&amp;quot;
dMod &amp;lt;- lm(Ftime ~ Block + GCA(Par1, Par2) + tSCA(Par1, Par2) +
             REC(Par1, Par2), data = hayman54)
dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54, fct = &amp;quot;GRIFFING1&amp;quot;)
# summary(dMod2)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block        1    142     142  0.3416   0.56100    
## GCA          7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## tSCA        28 102238    3651  8.7599 6.656e-13 ***
## Reciprocals 28  19112     683  1.6375   0.05369 .  
## Residuals   63  26260                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to obtain the full list of genetical parameters, we can use the &lt;code&gt;glht()&lt;/code&gt; function in the &lt;code&gt;multcomp&lt;/code&gt; package, together with the &lt;code&gt;diallel.eff()&lt;/code&gt; function in the ‘lmDiallel’ package. An excerpt of the results is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0  1.629e+02  1.805e+00  90.270  &amp;lt; 2e-16 ***
# g_A == 0        4.620e+01  3.376e+00  13.683 2.17e-13 ***
# g_B == 0       -2.459e+01  3.376e+00  -7.282 9.83e-08 ***
# g_C == 0        4.963e+01  3.376e+00  14.702 4.13e-14 ***
# g_D == 0        1.835e+01  3.376e+00   5.436 1.07e-05 ***
# g_E == 0       -2.093e+01  3.376e+00  -6.199 1.47e-06 ***
# g_F == 0        2.445e+00  3.376e+00   0.724 0.475340    
# g_G == 0       -4.471e+01  3.376e+00 -13.244 4.57e-13 ***
# g_H == 0       -2.640e+01  3.376e+00  -7.819 2.71e-08 ***
# ts_A:A == 0     3.371e+01  1.263e+01   2.669 0.012941 *  
# ts_A:B == 0    -3.151e+01  9.023e+00  -3.492 0.001731 ** 
# ...
# ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-no-reciprocals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2: no reciprocals&lt;/h1&gt;
&lt;p&gt;As an example of a diallel experiments with no reciprocals, we consider the data reported in Lonnquist and Gardner (1961) relating to the yield of 21 maize genotypes, obtained from six male and six female parentals. The dataset is available as &lt;code&gt;lonnquist61&lt;/code&gt; in the &lt;code&gt;lmDiallel&lt;/code&gt; package and the model fitting process is very similar to that shown before for the mating scheme 1, apart from the fact that we fit equation 2 instead of equation 1. In the ‘lm()’ call, we use the &lt;code&gt;GCA()&lt;/code&gt; and &lt;code&gt;tSCA()&lt;/code&gt; functions, while in the &lt;code&gt;lm.diallel()&lt;/code&gt; call, we set the argument ‘fct’ to “GRIFFING2”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
data(lonnquist61)
dMod &amp;lt;- lm(Yield ~ GCA(Par1, Par2) + tSCA(Par1, Par2), 
           data = lonnquist61)
dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = lonnquist61, fct = &amp;quot;GRIFFING2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the dataset has no replicates and, for the inferences, we need to provide an estimate of the residual mean square and degrees of freedom (see box below). If we have fitted the model by using the &lt;code&gt;lm()&lt;/code&gt; function, the resulting ‘lm’ object can be explored by using the &lt;code&gt;summary.diallel()&lt;/code&gt; and &lt;code&gt;anova.diallel()&lt;/code&gt; functions. Otherwise, if we have fitted the model with the &lt;code&gt;lm.diallel()&lt;/code&gt; function, the resulting ‘diallel’ object can be explored by using the &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; methods. See the box below for an example: the results are, obviously, the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summary.diallel(dMod, MSE = 7.1, dfr = 60)
anova.diallel(dMod, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##                  Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA(Par1, Par2)   5 234.23  46.846  6.5980 5.923e-05 ***
## tSCA(Par1, Par2) 15 238.94  15.929  2.2436   0.01411 *  
## Residuals        60          7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# summary(dMod2, MSE = 7.1, dfr = 60)
anova(dMod2, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##           Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA        5 234.23  46.846  6.5980 5.923e-05 ***
## tSCA      15 238.94  15.929  2.2436   0.01411 *  
## Residuals 60          7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also for the diallel object, we can retrieve the full list of genetical parameters with the &lt;code&gt;glht()&lt;/code&gt; function, by using the same syntax as shown above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh &amp;lt;- glht(linfct = diallel.eff(dMod2, MSE = 7.1, dfr = 60))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3-no-selfs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3: no selfs&lt;/h1&gt;
&lt;p&gt;When the experimental design includes the reciprocal crosses but not the selfs, we can fit Equation 3. As an example, we take the same dataset as before (‘hayman54’), but we remove the selfs by using ‘dplyr’. The fitting process is the same as shown above and only the model specification is changed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
data(hayman54)
hayman54b &amp;lt;- hayman54  %&amp;gt;% 
  filter(Par1 != Par2)

dMod &amp;lt;- lm(Ftime ~ Block + GCA(Par1, Par2) + 
             SCA.G3(Par1, Par2) + REC.G3(Par1, Par2), 
           data = hayman54b)
dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54b, fct = &amp;quot;GRIFFING3&amp;quot;)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block        1    329   329.1  0.8367   0.36432    
## GCA          7 168923 24131.9 61.3479 &amp;lt; 2.2e-16 ***
## tSCA        20  37289  1864.4  4.7398 2.318e-06 ***
## Reciprocals 28  19112   682.6  1.7352   0.04052 *  
## Residuals   55  21635                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4-no-reciprocals-no-selfs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 4: no reciprocals, no selfs&lt;/h1&gt;
&lt;p&gt;In this final example, we consider a mating scheme where neither the reciprocal crosses nor the selfs are included (mating scheme 4). The dataset is taken from the original Griffing’s paper (Griffing, 1956) and it is available as ‘Griffing56’ in the ‘lmDiallel’ package. The analysis proceeds in the very same fashion as above, apart from the fact that we fit Equation 4, instead of 3 and that we input the appropriate residual error term to obtain the correct inferences, as the original dataset does not contain the replicated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;griffing56&amp;quot;)

dMod &amp;lt;- lm(Yield ~ GCA(Par1, Par2) + SCA.G3(Par1, Par2), 
           data = griffing56)
anova.diallel(dMod, MSE = 21.05, dfr = 2558)
## Analysis of Variance Table
## 
## Response: Yield
##                      Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA(Par1, Par2)       8 18606.0 2325.75 110.487 &amp;lt; 2.2e-16 ***
## SCA.G3(Par1, Par2)   27  9164.9  339.44  16.125 &amp;lt; 2.2e-16 ***
## Residuals          2558           21.05                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = griffing56, fct = &amp;quot;GRIFFING4&amp;quot;)
anova(dMod2, MSE = 21.05, dfr = 2558)
## Analysis of Variance Table
## 
## Response: Yield
##             Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA          8 18606.0 2325.75 110.487 &amp;lt; 2.2e-16 ***
## tSCA        27  9164.9  339.44  16.125 &amp;lt; 2.2e-16 ***
## Residuals 2558           21.05                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# summary(dMod2, MSE = 21.05, dfr = 2558)

gh &amp;lt;- glht(linfct = diallel.eff(dMod2, MSE = 21.05, dfr = 2558))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;If we intend to regard the genetic effects as random and to estimate variance components, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a bunch of dummy variables. In order to make things simpler for routine experiments, we have coded the &lt;code&gt;mmer.diallel()&lt;/code&gt; wrapper using the same syntax as the &lt;code&gt;lm.diallel()&lt;/code&gt; function. The exemplary code is given in the box below, relating to Equation 2, although the other equations can be fitted in a similar manner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random genetic effects
mod1m &amp;lt;- mmer.diallel(Yield ~ Par1 + Par2,
                      data = lonnquist61,
                      fct = &amp;quot;GRIFFING2&amp;quot;)
mod1m$varcomp
##        VarComp VarCompSE
## GCA   3.863695  3.769373
## tSCA 15.930144  5.819217&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next post we will consider another important family of models, devised by Gardner and Eberarth in 1966, which accounts for heterotic effects. Please, stay tuned!&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Dr. Niccolò Terzaroli&lt;br /&gt;
Prof. Gino Russi&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744.&lt;/li&gt;
&lt;li&gt;Griffing, B., 1956. Concept of general and specific combining ability in relation to diallel crossing systems. Australian Journal of Biological Science 9, 463–493.&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Terzaroli, N., Russi, L., 2020. Linear models for diallel crosses: a review with R functions. Theor Appl Genet. &lt;a href=&#34;https://doi.org/10.1007/s00122-020-03716-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00122-020-03716-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Hayman&#39;s model (type 2)</title>
      <link>https://www.statforbiology.com/2021/stat_met_diallel_hayman2/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2021/stat_met_diallel_hayman2/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This posts follows two other previously published posts, where we presented our new ‘lmDiallel’ package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see here&lt;/a&gt;) and showed how we can use it to fit the Hayman’s model type 1, as proposed in Hayman (1954) (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;). In this post, we will give a further example relating to another very widespread model from the same author, the Hayman’s model type 2. We apologise for some overlapping with previous posts: we think this is necessary so that each post can be read on its own.&lt;/p&gt;
&lt;p&gt;The model we are going to talk about is used to describe the results of full (complete) diallel experiments, where we have crosses + reciprocals + selfs. If you are not sure what a diallel experiment is, we suggest you go back to one of our previous posts on this sequence, where we give some preliminary information for beginners. Otherwise, we can proceed to the motivating example.&lt;/p&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The example&lt;/h1&gt;
&lt;p&gt;In this post we will use the same example as provided in the original Hayman’s paper (Hayman, 1954), relating to a complete diallel experiment with eight parental lines. The R dataset is included in the ‘lmDiallel’ package; in the box below we load the data, after installing (if necessary) and loading the ‘lmDiallel’ package (see the box below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
data(&amp;quot;hayman54&amp;quot;)
head(hayman54)
##   Block Par1 Par2 Ftime
## 1     1    A    A   276
## 2     1    A    B   156
## 3     1    A    C   322
## 4     1    A    D   250
## 5     1    A    E   162
## 6     1    A    F   193&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-haymans-model-type-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Hayman’s model type 2&lt;/h1&gt;
&lt;p&gt;The Hayman’s model type 2 is derived from type 1 (see our previous post), by partitioning the tSCA effect in three additive components. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \left\{ {\begin{array}{ll}
\mu + \gamma_k + \textrm{g}_i + \textrm{g}_j + m + d_i + d_j + s_{ij} + rg^a_i + rg^b_j + rs_{ij} + \varepsilon_{ijk} &amp;amp; \textrm{for} \quad i \neq j\\
\mu + \gamma_k + 2\, \textrm{g}_i - (n - 1)m - (n - 2)d_i + \varepsilon_{ijk} &amp;amp; \textrm{for} \quad i = j \end{array}} \right.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the expected value (the overall mean, in the case of fully orthogonal designs), &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of parentals and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual random error term. All the other terms correspond to genetic effects, namely:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_j\)&lt;/span&gt; terms are the &lt;strong&gt;general combining abilities&lt;/strong&gt; (GCAs) of the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rg^a_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(rg^b_j\)&lt;/span&gt; terms are the &lt;strong&gt;reciprocal general combining abilities&lt;/strong&gt; (RGCAs) for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; term relates to the difference between the average value of all observations and the average values of crosses (&lt;strong&gt;Mean Dominance Deviation&lt;/strong&gt;; MDD).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(d_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_j\)&lt;/span&gt; terms relate to the differences between the yield of each selfed parent (&lt;span class=&#34;math inline&#34;&gt;\(Y_{ij}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(i = j\)&lt;/span&gt;) and the average yield of all selfed parents (&lt;strong&gt;dominance deviation&lt;/strong&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; parent; DD).&lt;/li&gt;
&lt;li&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(s_{ij}\)&lt;/span&gt; is the SCA effect for the combination &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rs_{ij}\)&lt;/span&gt; term is the &lt;strong&gt;reciprocal specific combining ability&lt;/strong&gt; (RSCA) for a specific &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt; combination, that is the discrepancy between the performances of the two reciprocals (e.g, A &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; B vs. B &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; A)(&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly to type 1, the Hayman’s model type 2 considers the genetical effects as differences with respect to the intercept &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is the mean of all observations (when the design is orthogonal). However, with respect to type 1, this latter model permits the estimation of a higher number of genetic effects (GCAs, RGCAs, MDD, DDs, SCAs and RSCAs) and provides an approach to quantify heterotic effects. We should consider that, due to unbalance (the number of crosses is never equal to the number of selfs), it is necessary to introduce some coefficients (i.e. &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n - 2\)&lt;/span&gt; in Equation 1), which do not have an obvious meaning. In future posts we will see that other diallel models were proposed, which account for heterotic effects in a different manner (Gardner and Eberhart, 1966).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting with R&lt;/h1&gt;
&lt;p&gt;Let’s assume that all effects are fixed, apart from the residual error effect. Consequently, Equation 1 is a specific parameterisation of a general linear model, which we can fit by the usual &lt;code&gt;lm()&lt;/code&gt; function and related methods. However, we need to exploit some of the facilities in our new ‘lmDiallel’ extension package, which consist of the &lt;code&gt;GCA()&lt;/code&gt;, &lt;code&gt;MDD()&lt;/code&gt;, &lt;code&gt;DD()&lt;/code&gt;, &lt;code&gt;SCA()&lt;/code&gt;, &lt;code&gt;RGCA()&lt;/code&gt; and &lt;code&gt;RSCA()&lt;/code&gt; functions (see the box below). The resulting &lt;code&gt;lm&lt;/code&gt; object can be explored by the usual R methods, such as &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; (the output of the &lt;code&gt;summary()&lt;/code&gt; method is partly hidden, for brevity)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contrasts(hayman54$Block) &amp;lt;- &amp;quot;contr.sum&amp;quot;
dMod &amp;lt;- lm(Ftime ~ Block + GCA(Par1, Par2) + MDD(Par1, Par2) +
             DD(Par1, Par2) + SCA(Par1, Par2) +
             RGCA(Par1, Par2) + RSCA(Par1, Par2), data = hayman54)
summary(dMod)$coef[1:6,]
##                      Estimate Std. Error    t value     Pr(&amp;gt;|t|)
## (Intercept)        162.898437   1.804567 90.2700843 2.381071e-68
## Block1              -1.054688   1.804567 -0.5844545 5.610017e-01
## GCA(Par1, Par2)g_A  46.195312   3.376036 13.6832990 1.558468e-20
## GCA(Par1, Par2)g_B -24.585938   3.376036 -7.2824864 6.421946e-10
## GCA(Par1, Par2)g_C  49.632812   3.376036 14.7015049 4.900927e-22
## GCA(Par1, Par2)g_D  18.351563   3.376036  5.4358311 9.415231e-07
# ...
# ...
anova(dMod)
## Analysis of Variance Table
## 
## Response: Ftime
##                  Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block             1    142     142  0.3416   0.56100    
## GCA(Par1, Par2)   7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## MDD(Par1, Par2)   1  30797   30797 73.8840 3.259e-12 ***
## DD(Par1, Par2)    7  34153    4879 11.7050 1.957e-09 ***
## SCA(Par1, Par2)  20  37289    1864  4.4729 2.560e-06 ***
## RGCA(Par1, Par2)  7   6739     963  2.3097   0.03671 *  
## RSCA(Par1, Par2) 21  12373     589  1.4135   0.14668    
## Residuals        63  26260     417                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simplicity, we also built a wrapper function named &lt;code&gt;lm.diallel()&lt;/code&gt;, which can be used in the very same fashion as &lt;code&gt;lm()&lt;/code&gt;. The syntax is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm.diallel(formula, Block, Env, data, fct)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘formula’ specifies the response variable and the two variables for parentals (e.g., Yield ~ Par1 + Par2) and the two arguments ‘Block’ and ‘Env’ are used to specify optional variables, coding for blocks and environments, respectively. The argument ‘data’ is a ‘dataframe’ where to look for the explanatory variables and, finally, ‘fct’ is a string variable coding for the selected model (“HAYMAN2”, for this example; see below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54, fct = &amp;quot;HAYMAN2&amp;quot;)
# summary(dMod2)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##           Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block      1    142     142  0.3416   0.56100    
## MDD        1  30797   30797 73.8840 3.259e-12 ***
## GCA        7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## DD         7  34153    4879 11.7050 1.957e-09 ***
## SCA       20  37289    1864  4.4729 2.560e-06 ***
## RGCA       7   6739     963  2.3097   0.03671 *  
## RSCA      21  12373     589  1.4135   0.14668    
## Residuals 63  26260                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function works very much like the &lt;code&gt;lm()&lt;/code&gt; function and makes use of the general purpose linear model solver &lt;code&gt;lm.fit()&lt;/code&gt;. Apart from simplicity, another advantage is that the call to &lt;code&gt;lm.diallel()&lt;/code&gt; returns an object of both ‘lm’ and ‘diallel’ classes. For this latter class, we built several specific S3 methods, such as the usual &lt;code&gt;anova()&lt;/code&gt;, &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;model.matrix()&lt;/code&gt; methods, partly shown in the box above.&lt;/p&gt;
&lt;p&gt;Considering that diallel models are usually fitted to determine genetical parameters, we also built the &lt;code&gt;glht.diallelMod()&lt;/code&gt; method and the &lt;code&gt;diallel.eff()&lt;/code&gt; function, which can be used with the ‘multcomp’ package, to retrieve the complete list of genetical parameters. An excerpt is shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0 162.8984     1.8046  90.270  &amp;lt; 2e-16 ***
# m == 0          -5.8627     0.6821  -8.596 4.48e-09 ***
# g_A == 0        46.1953     3.3760  13.683 2.17e-13 ***
# g_B == 0       -24.5859     3.3760  -7.282 9.83e-08 ***
# g_C == 0        49.6328     3.3760  14.702 4.13e-14 ***
# g_D == 0        18.3516     3.3760   5.436 1.07e-05 ***
# g_E == 0       -20.9297     3.3760  -6.199 1.47e-06 ***
# g_F == 0         2.4453     3.3760   0.724 0.475340    
# g_G == 0       -44.7109     3.3760 -13.244 4.57e-13 ***
# g_H == 0       -26.3984     3.3760  -7.819 2.71e-08 ***
# d_A == 0         1.2213     1.9492   0.627 0.536380    
# d_B == 0        -2.6224     1.9492  -1.345 0.190113    
# ...
# ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a previous post (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;) we have shown that, when diallel models are fitted to the genotype means (and thus we have no replicates), an appropriate estimate of residual mean square and degrees of freedom can be passed as arguments to the &lt;code&gt;summary()&lt;/code&gt;, &lt;code&gt;anova()&lt;/code&gt; and &lt;code&gt;diallel.eff()&lt;/code&gt; methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;If we intend to regard the genetic effects as random and to estimate variance components, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a bunch of dummy variables. In order to make things simpler for routine experiments, we have coded the &lt;code&gt;mmer.diallel()&lt;/code&gt; wrapper using the same syntax as the &lt;code&gt;lm.diallel()&lt;/code&gt; function. The exemplary code is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random genetic effects
mod1m &amp;lt;- mmer.diallel(Ftime ~ Par1 + Par2, Block = Block, 
                      data = hayman54,
                      fct = &amp;quot;HAYMAN2&amp;quot;)
mod1m$varcomp
##              VarComp   VarCompSE
## Block        0.00000    9.188298
## MDD       1783.96081 3118.893561
## GCA       1005.92052  574.893353
## RGCA        17.97898   19.920016
## DD         659.53567  468.205470
## SCA        351.74035  144.688653
## RSCA        32.02325   46.361581
## Residuals  412.54051   73.506382&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s all about the Hayman’s models; you may have noted that both models (type 1 and 2) were devised for full diallel experiments, which are not so widespread in ‘genetical’ literature. A few years later, in 1956, the Australian scientist B. Griffing made the relevant effort of creating a comprehensive set of models which can be fitted to all types of diallel experiments. We will talk about these models in a future post.&lt;/p&gt;
&lt;p&gt;Thanks for reading (and happy 2021!)&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Prof. Luigi Russi&lt;br /&gt;
Dr. Niccolò Terzaroli&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0156744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0156744&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hayman, B.I., 1954. The Analysis of Variance of Diallel Tables. Biometrics 10, 235. &lt;a href=&#34;https://doi.org/10.2307/3001877&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/3001877&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Terzaroli, N., Russi, L., 2020. Linear models for diallel crosses: a review with R functions. Theor Appl Genet. &lt;a href=&#34;https://doi.org/10.1007/s00122-020-03716-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00122-020-03716-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>General code to fit ANOVA models with JAGS and &#39;rjags&#39;</title>
      <link>https://www.statforbiology.com/2020/stat_bayesian_anovamodels/</link>
      <pubDate>Wed, 23 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_bayesian_anovamodels/</guid>
      <description>


&lt;p&gt;One of the reasons why I like BUGS and all related dialects has been put nicely in a very good book, i.e. “Introduction to WinBUGS for ecologists” (Kery, 2010); at page 11, the author says: &lt;em&gt;“WinBUGS helps free the modeler in you”&lt;/em&gt;. Ultimately, that statement is true: when I have fully understood a model with all its components (and thus I have become a modeler), I can very logically translate it to BUGS code. The drawback is that, very often, the final coding appears to be rather ‘problem-specific’ and difficult to be reused in other situations, without an extensive editing work.&lt;/p&gt;
&lt;p&gt;For example, consider the ‘ANOVA models’ with all their ‘flavors’: one-way, two-ways with interactions, nested, and so on. These models are rather common in agricultural research and they are relatively easy to code in BUGS, following the suggestions provided in Kery’s book. However, passing from one model to the others requires some editing, which is often prone to errors. And errors in BUGS are difficult to spot in short time… Therefore, I have been wondering: “&lt;em&gt;Can I write general BUGS code, which can be used for all ANOVA models with no substantial editing?&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;Finally, I have found a solution and, as it took me awhile to sort things out, I thought I might share it, for the benefit of those who would like to fit ANOVA models in the Bayesian framework. It works with JAGS (JUST ANOTHER GIBBS SAMPLER), a BUGS dialect running also in Mac OS and developed by Marty Plummer. JAGS can be used from R, thanks to the ‘rjags’ package (Plummer, 2019), which I will use in this post.&lt;/p&gt;
&lt;p&gt;Let’s start from a working example.&lt;/p&gt;
&lt;div id=&#34;a-genotype-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A genotype experiment&lt;/h1&gt;
&lt;p&gt;The yields of seven wheat genotypes were compared in one experiment laid down in three randomised complete blocks. The data is available in an external repository as a ‘csv’ file and it can be loaded by using the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat2002.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName, header = T)
head(dataset)
##   Plot Block Genotype Yield
## 1   57     A COLOSSEO  4.31
## 2   61     B COLOSSEO  4.73
## 3   11     C COLOSSEO  5.64
## 4   60     A    CRESO  3.99
## 5   10     B    CRESO  4.82
## 6   42     C    CRESO  4.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the typical situation were we might be interested in fitting an ANOVA model, with the yield as the response variable and the blocks and genotypes as the explanatory factors. The ultimate aim is to estimate genotype means and credible intervals, which calls for the Bayesian approach.&lt;/p&gt;
&lt;p&gt;For the sake of simplicity, let’s take both the block and genotype effects as fixed; in matrix notation, a general linear fixed effects model can be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = X \, \beta + \varepsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the vector of the observed response, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the vector of estimated parameters, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the design matrix and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the vector of residuals, which we assume as gaussian and homoscedastic (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon \sim N(0, \sigma^2 I)\)&lt;/span&gt;; N is the multivariate gaussian distribution). The same model can also be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim N \left( X \, \beta, \sigma^2 I \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In JAGS (and maybe also in other BUGS dialects), we can code every linear model using the above specification, as long as we can provide the correct design matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Luckily, we will see that this is a rather simple task; but… let’s do it one step at a time!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specification-of-a-jags-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Specification of a JAGS model&lt;/h1&gt;
&lt;p&gt;First of all, we open R and code a general linear JAGS model, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coding a JAGS model
modelSpec &amp;lt;- &amp;quot;
data {
n &amp;lt;- length(Y)
np &amp;lt;- dim(X)
nk &amp;lt;- dim(K)
}

model {
# Model 
for (i in 1:n) {
   expected[i] &amp;lt;- inprod(X[i,], beta)
   Y[i] ~ dnorm(expected[i], tau)
  }

# Priors
beta[1] ~ dunif(0, 1000000)
for (i in 2:np[2]){
  beta[i] ~ dnorm(0, 0.000001)
  }
sigma ~ dunif(0, 100)

# Derived quantities (model specific)
tau &amp;lt;- 1 / ( sigma * sigma)

# Contrasts of interest
for (i in 1:nk[1]) {
   mu[i] &amp;lt;- inprod(K[i,], beta)
  }
}&amp;quot;
writeLines(modelSpec, con=&amp;quot;ModelAOV.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see some more detail; you may notice that the code above consists of two fundamental parts, surrounded by curly brackets:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a ‘data’ part&lt;/li&gt;
&lt;li&gt;a ‘model’ part&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the data part, we create three variables, i.e. the number of data (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;), the number of estimated parameters (&lt;span class=&#34;math inline&#34;&gt;\(np\)&lt;/span&gt;) and the number of contrasts (see later). All variables are used in successive model steps and they are obtained, respectively, by counting the number of observations in the vector &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, the number of columns in the design matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the number of rows in the contrast matrix &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the model part we have three further components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the model specification&lt;/li&gt;
&lt;li&gt;the priors&lt;/li&gt;
&lt;li&gt;the derived quantities&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model specification contains ‘deterministic’ and ‘stochastic’ statements (nodes). The ‘deterministic’ node returns the expected values for all observations, based on multiplying the design matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by the vector of estimated parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. In practice, we use a ‘for()’ loop and, for each i&lt;sup&gt;th&lt;/sup&gt; observation, we sum the products of all element in the i&lt;sup&gt;th&lt;/sup&gt; row of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by the corresponding elements in the vector of estimated parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. This sum of products is accomplished by using the function &lt;code&gt;inprod(X[i,], beta)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the ‘stochastic’ node we specify that the observed values in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are sampled from a gaussian distribution (‘dnorm’), with mean equal to the expected value and precision equal to ‘tau’. In JAGS, WinBUGS and all related software, the normal distribution is parameterised by using the precision &lt;span class=&#34;math inline&#34;&gt;\(\tau = 1/\sigma^2\)&lt;/span&gt;, instead of standard deviation.&lt;/p&gt;
&lt;p&gt;Next, we have to define the priors for all the estimands. For those who are not very much into Bayesian inference, I will only say that priors represent our expectations about model parameters before looking at the data; in this example, we use very vague priors, meaning that, before looking at the data, we have no idea about the values of these unknown quantities. In detail, for the intercept we specify a uniform distribution from 0 to 10000 (&lt;code&gt;beta[1] ~ dunif(0, 1000000)&lt;/code&gt;), meaning that the overall mean might be included between 0 and 10000 and we have no preference for any values within that range (a very vague prior, indeed). For all other effects in the vector &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, our prior expectation is that they are normally distributed with a mean of 0 and very low precision (&lt;code&gt;beta[i] ~ dnorm(0, 0.000001)&lt;/code&gt;). For the residual standard deviation, we expect that it is uniformly distributed from 0 to 100. The selection of priors is central to Bayesian inference and, in other circumstances, you may like to adopt more informative priors. We do not discuss this important item here.&lt;/p&gt;
&lt;p&gt;In the end, we also specify some quantities that should be derived from estimated parameters. As we have put a prior on standard deviation, we need to derive the precision (&lt;code&gt;tau &amp;lt;- 1 / ( sigma * sigma)&lt;/code&gt;), that is necessary for the stochastic node in the specification of our linear model. Afterwards, we add a set of contrasts, which are specified by way of a matrix of contrast coefficients (&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;; one row per each contrast). This is useful to calculate, e.g., the means of treatment levels or pairwise differences between means as linear combinations of model parameters.&lt;/p&gt;
&lt;p&gt;The model definition in the box above is assigned to a text string (&lt;code&gt;modelSpec&lt;/code&gt;) and it is finally written to an external text file (‘modelAOV.txt’), using the function &lt;code&gt;writeLines()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I conclude this part by saying that, based on our model specification, JAGS requires three input ingredients: the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; vector of responses, the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix and the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; contrast matrix. Furthermore, JAGS requires initial values for all estimands, i.e. for all quantities for which we have specified our prior expectations (the ‘beta’ vector and the ‘sigma’ scalar).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-jags-model-from-within-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting the JAGS model from within R&lt;/h1&gt;
&lt;p&gt;JAGS models can be fitted from R by using the &lt;code&gt;rjags&lt;/code&gt; package (Plummer, 2019). However, we have some preliminary steps to accomplish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;loading the dataset (see the first box above);&lt;/li&gt;
&lt;li&gt;creating the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; vector of responses&lt;/li&gt;
&lt;li&gt;creating the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix&lt;/li&gt;
&lt;li&gt;creating the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrix&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first two steps are obvious. The third step can be accomplished by using the &lt;code&gt;model.matrix()&lt;/code&gt; function: the call is very similar to an ‘lm()’ call, although we do not need to explicitly indicate the response variable (see the box below). In order to create the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrix of contrasts, we might prefer to work with the sum-to-zero parameterisation (&lt;code&gt;options(contrasts=c(&amp;quot;contr.sum&amp;quot;, &amp;quot;contr.poly&amp;quot;))&lt;/code&gt;), so that the intercept represents the overall mean (for balanced designs) and the effects of blocks and genotypes represent differences with respect to the overall mean. In the box below we specify a set of eight contrasts returning the means for all genotypes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(contrasts=c(&amp;quot;contr.sum&amp;quot;, &amp;quot;contr.poly&amp;quot;))
Y &amp;lt;- dataset$Yield  
X &amp;lt;- model.matrix( ~ Block + Genotype, data = dataset)

k1 &amp;lt;- c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0)
k2 &amp;lt;- c(1, 0, 0, 0, 1, 0, 0, 0, 0, 0)
k3 &amp;lt;- c(1, 0, 0, 0, 0, 1, 0, 0, 0, 0)
k4 &amp;lt;- c(1, 0, 0, 0, 0, 0, 1, 0, 0, 0)
k5 &amp;lt;- c(1, 0, 0, 0, 0, 0, 0, 1, 0, 0)
k6 &amp;lt;- c(1, 0, 0, 0, 0, 0, 0, 0, 1, 0)
k7 &amp;lt;- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 1)
k8 &amp;lt;- c(1, 0, 0,-1,-1,-1,-1,-1,-1,-1)
K &amp;lt;- rbind(k1, k2, k3, k4, k5, k6, k7, k8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need further explanation about the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrices and their role in the analysis, I have added an appendix below. Otherwise, we are ready to fit the model. To this aim, we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;load the &lt;code&gt;rjags&lt;/code&gt; library;&lt;/li&gt;
&lt;li&gt;create two lists: a list of all the data needed for the analysis (&lt;code&gt;dataList&lt;/code&gt;) and a list of the initial values for the parameters to be estimated (&lt;code&gt;initList&lt;/code&gt;). Initial values need not be particularly precise;&lt;/li&gt;
&lt;li&gt;send the model specification and the other data to JAGS, using the function &lt;code&gt;jags.model()&lt;/code&gt; from the &lt;code&gt;rjags&lt;/code&gt; package;&lt;/li&gt;
&lt;li&gt;start the sampler, using the &lt;code&gt;coda.samples()&lt;/code&gt; function. In this step, we specify which parameters we want to obtain estimates for and the number of samples we want to draw (&lt;code&gt;n.iter&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;obtain the number of required samples, using the &lt;code&gt;window()&lt;/code&gt; function. In this step, we specify how many samples should be discarded as &lt;code&gt;burn.in&lt;/code&gt;. These samples might have been produced before reaching the convergence, so they might not come from the correct posterior distribution and we need to get rid of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the posterior, we obtain the mean and median as measures of central tendency, the standard deviation as a measure of uncertainty and credible intervals, which are the Bayesian analog to confidence intervals. Due to our vague priors, the results are very similar to those obtained with a traditional frequentist analysis (see the appendix below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rjags)

# Create lists
dataList &amp;lt;- list(Y = Y, X = X, K = K)
initList &amp;lt;- list(beta = c(4.3, rep(0, 9)), sigma = 0.33)

# Start sampler
mcmc &amp;lt;- jags.model(&amp;quot;modelAOV.txt&amp;quot;, 
                   data = dataList, inits = initList, 
                   n.chains = 4, n.adapt = 100)
## Compiling data graph
##    Resolving undeclared variables
##    Allocating nodes
##    Initializing
##    Reading data back into data table
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 24
##    Unobserved stochastic nodes: 11
##    Total graph size: 451
## 
## Initializing model
# Get samples
res &amp;lt;- coda.samples(mcmc, variable.names = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;mu&amp;quot;),
                    n.iter = 1000)

out &amp;lt;- summary(window(res, start = 110))
res &amp;lt;- cbind(out$statistics[,1:2], out$quantiles[,c(1,5)])
res
##                 Mean         SD        2.5%       97.5%
## beta[1]   4.42459380 0.07450192  4.27331180  4.56808970
## beta[2]  -0.21830958 0.10451119 -0.42114931 -0.01073556
## beta[3]  -0.01067826 0.10489729 -0.22122939  0.19833336
## beta[4]   0.46390007 0.19802745  0.04693158  0.83344935
## beta[5]  -0.09827277 0.19450502 -0.48923642  0.27830973
## beta[6]  -0.18856790 0.19460922 -0.58070525  0.19744961
## beta[7]  -0.07897598 0.20104587 -0.46988505  0.32557805
## beta[8]   0.54737970 0.19813294  0.15200081  0.93771826
## beta[9]   0.07786131 0.19946009 -0.31222840  0.48406509
## beta[10] -1.09035526 0.19353015 -1.47151174 -0.70790010
## mu[1]     4.88849386 0.21213313  4.44865961  5.29774010
## mu[2]     4.32632102 0.20825090  3.91707525  4.71763694
## mu[3]     4.23602590 0.20766216  3.83381632  4.64905719
## mu[4]     4.34561782 0.21404163  3.93107938  4.77594175
## mu[5]     4.97197349 0.21222639  4.54813702  5.38971184
## mu[6]     4.50245511 0.21323351  4.09000675  4.93210416
## mu[7]     3.33423854 0.20600397  2.92628945  3.73812600
## mu[8]     4.79162462 0.21207498  4.36009497  5.21086914
## sigma     0.35632989 0.08036709  0.24276414  0.55078418&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reusing-the-code-for-a-multi-environment-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reusing the code for a multi-environment experiment&lt;/h1&gt;
&lt;p&gt;The JAGS model above is very general and can be easily reused for other situations. For example, if the above genotype experiment is replicated across years, we might like to fit an ANOVA model by considering the blocks (within years), the genotypes, the years and the ‘year by genotype’ interaction. The dataset is available in the same external repository, as the ‘WinterWheat.csv’ file.&lt;/p&gt;
&lt;p&gt;The JAGS specification for this multienvironment model does not change, we only need to update the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrices, as shown in the box below. In order to obtain the contrast matrix for the means of the ‘genotype x environment’ combinations we need to write some cumbersome code, as shown below (but, perhaps, some of you could suggest better alternatives…).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

# Loading the data
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat.csv&amp;quot;
dataset &amp;lt;- read_csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Block, Year, Genotype), .fns = factor))
dataset
## # A tibble: 168 x 5
##     Plot Block Genotype Yield Year 
##    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;
##  1     2 1     COLOSSEO  6.73 1996 
##  2   110 2     COLOSSEO  6.96 1996 
##  3   181 3     COLOSSEO  5.35 1996 
##  4     2 1     COLOSSEO  6.26 1997 
##  5   110 2     COLOSSEO  7.01 1997 
##  6   181 3     COLOSSEO  6.11 1997 
##  7    17 1     COLOSSEO  6.75 1998 
##  8   110 2     COLOSSEO  6.82 1998 
##  9   256 3     COLOSSEO  6.52 1998 
## 10    18 1     COLOSSEO  7.18 1999 
## # … with 158 more rows
# Create input matrices
Y &amp;lt;- dataset$Yield 
X &amp;lt;- model.matrix(~ Genotype*Year +  Block:Year, data = dataset)

# Workaround to get K matrix
asgn &amp;lt;- attr(X, &amp;quot;assign&amp;quot;)
tmp1 &amp;lt;- expand.grid(Genotype = unique(dataset$Genotype),
                    Year = unique(dataset$Year))
K1 &amp;lt;- model.matrix(~ Genotype*Year, data = tmp1)
K2 &amp;lt;- matrix(0, nrow = nrow(K1), ncol = length(asgn[asgn==4]))
colnames(K2) &amp;lt;- colnames(X)[asgn==4]
K &amp;lt;- cbind(K1, K2)
row.names(K) &amp;lt;- with(tmp1, interaction(Genotype:Year))
# K

# Create lists
dataList &amp;lt;- list(Y = Y, X = X, K = K)
initList &amp;lt;- list(beta = c(4.3, rep(0, length(X[1,])-1)), sigma = 0.33)

# Start sampler
mcmc &amp;lt;- jags.model(&amp;quot;modelAOV.txt&amp;quot;, 
                   data = dataList, inits = initList, 
                   n.chains = 4, n.adapt = 100)
## Compiling data graph
##    Resolving undeclared variables
##    Allocating nodes
##    Initializing
##    Reading data back into data table
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 168
##    Unobserved stochastic nodes: 71
##    Total graph size: 16519
## 
## Initializing model
# Get samples
res &amp;lt;- coda.samples(mcmc, variable.names = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;mu&amp;quot;),
                    n.iter = 1000)

out &amp;lt;- summary(window(res, start = 110))
res &amp;lt;- cbind(out$statistics[,1:2], &amp;#39;50%&amp;#39;=out$quantiles[,3],
             out$quantiles[,c(1, 5)])
head(res)
##               Mean         SD        50%         2.5%      97.5%
## beta[1]  6.2677640 0.03043537  6.2673982  6.208390004  6.3300080
## beta[2]  0.1464792 0.07854643  0.1461239 -0.009216115  0.2979934
## beta[3] -0.2947195 0.07764813 -0.2955634 -0.446178189 -0.1401161
## beta[4]  0.3248028 0.07746617  0.3246501  0.174748219  0.4733587
## beta[5] -0.1843623 0.08221827 -0.1854063 -0.349242266 -0.0245657
## beta[6]  0.4269969 0.07989058  0.4260559  0.272364897  0.5842687
#....
tail(res)
##             Mean         SD       50%      2.5%    97.5%
## mu[52] 4.3419716 0.22876493 4.3422616 3.8980334 4.792529
## mu[53] 4.9626900 0.22571023 4.9634626 4.5132893 5.392722
## mu[54] 4.5076303 0.22951179 4.5094695 4.0518930 4.955052
## mu[55] 3.3378495 0.22618643 3.3342110 2.9010714 3.769008
## mu[56] 4.7907886 0.22765164 4.7854894 4.3385675 5.241458
## sigma  0.3915724 0.02904505 0.3893148 0.3428075 0.454776&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The discovery of the &lt;code&gt;inprod()&lt;/code&gt; function was a very big hit for me: the above approach is very flexible and lend itself to a lot of potential uses, including fitting mixed models. I will show some examples in future posts.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy 2021! Let’s hope we finally get back to normality!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Kery, M., 2010. Introduction to WinBUGS for ecologists. A Bayesian approach to regression, ANOVA, mixed models and related analyses. Academic Press, Burlington, MA (USA).&lt;/li&gt;
&lt;li&gt;Plummer M. (2019). rjags: Bayesian Graphical Models using MCMC. R package version 4-10. &lt;a href=&#34;https://CRAN.R-project.org/package=rjags&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rjags&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;I feel that it may be useful to take a look at the results of traditional model fitting with the &lt;code&gt;lm()&lt;/code&gt; function and to explore the role of the matrices &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. Let’s go back to the first example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat2002.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName, header = T)
mod.aov &amp;lt;- lm(Yield ~ Block + Genotype, data = dataset)
summary(mod.aov)
## 
## Call:
## lm(formula = Yield ~ Block + Genotype, data = dataset)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.38458 -0.12854 -0.08271  0.20396  0.51875 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  4.424583   0.065697  67.348  &amp;lt; 2e-16 ***
## Block1      -0.218333   0.092910  -2.350  0.03397 *  
## Block2      -0.009583   0.092910  -0.103  0.91931    
## Genotype1    0.468750   0.173818   2.697  0.01737 *  
## Genotype2   -0.097917   0.173818  -0.563  0.58212    
## Genotype3   -0.184583   0.173818  -1.062  0.30624    
## Genotype4   -0.084583   0.173818  -0.487  0.63406    
## Genotype5    0.538750   0.173818   3.100  0.00784 ** 
## Genotype6    0.078750   0.173818   0.453  0.65745    
## Genotype7   -1.084583   0.173818  -6.240 2.16e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3218 on 14 degrees of freedom
## Multiple R-squared:  0.8159, Adjusted R-squared:  0.6976 
## F-statistic: 6.895 on 9 and 14 DF,  p-value: 0.0007881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s also look at the first row of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix, which I can also retrieve from the fitted model object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.matrix(mod.aov)[1,]
## (Intercept)      Block1      Block2   Genotype1   Genotype2   Genotype3 
##           1           1           0           1           0           0 
##   Genotype4   Genotype5   Genotype6   Genotype7 
##           0           0           0           0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the vector of estimated parameters and the first row in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; have 10 elements: if we multiply them and sum, we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 1 \cdot 4.425 + 1 \cdot -0.218 + 0 \cdot -0.0096 + 1 \cdot 0.469 + 0 \cdot ... = 4.675\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is exactly the first fitted value (first genotype in first block):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(mod.aov)[1]
##     1 
## 4.675&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we do this for all rows in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, we obtain all fitted values and such an operation is most quickly done by using matrix multiplication.&lt;/p&gt;
&lt;p&gt;Likewise, if we multiply the elements in ‘beta’ for the corresponding elements in the first row of the ‘K’ matrix and sum, we get the mean for the first genotype (COLOSSEO) and if we do so for all rows in &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; we get all the genotype means.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 1 \cdot 4.425 + 0 \cdot -0.218 + 0 \cdot -0.0096 + 1 \cdot 0.469 + 0 \cdot ... = 4.893\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emmeans::emmeans(mod.aov, ~Genotype) 
##  Genotype emmean    SE df lower.CL upper.CL
##  COLOSSEO   4.89 0.186 14     4.49     5.29
##  CRESO      4.33 0.186 14     3.93     4.73
##  DUILIO     4.24 0.186 14     3.84     4.64
##  GRAZIA     4.34 0.186 14     3.94     4.74
##  IRIDE      4.96 0.186 14     4.56     5.36
##  SANCARLO   4.50 0.186 14     4.10     4.90
##  SIMETO     3.34 0.186 14     2.94     3.74
##  SOLEX      4.79 0.186 14     4.39     5.19
## 
## Results are averaged over the levels of: Block 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this is useful!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>From &#39;&#39;for()&#39;&#39; loops to the &#39;&#39;split-apply-combine&#39;&#39; paradigm for column-wise tasks: the transition for a dinosaur</title>
      <link>https://www.statforbiology.com/2020/stat_r_tidyverse_columnwise/</link>
      <pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_r_tidyverse_columnwise/</guid>
      <description>


&lt;p&gt;I have been involved with data crunching for 30 years, and, due to my age, I see myself as a dinosaur within the R-users community. I must admit, I’m rather slow to incorporate new paradigms in my programming workflow … I’m pretty busy and the time I save today is often more important than the time I could save in the future, by picking up new techniques. However, resisting to progress is not necessarily a good idea and, from time to time, also a dinosaur feels like living more dangerously and exploring new ideas and views.&lt;/p&gt;
&lt;p&gt;Today, I want to talk about column-wise tasks in relatively big datasets. These tasks are rather common with agricultural and biological experiments, where we have several subjects in different treatment groups and we record, for each subject, a high number of traits. In these conditions, the very first step to data analyses is to calculate several descriptive stats for each group of subjects and each variable. What is the best method to write simple and reusable code? As I will show later, the most natural approach to me may not necessarily be the most fashionable one.&lt;/p&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The example&lt;/h1&gt;
&lt;p&gt;A few days ago, I met a colleague from the plant pathology group, who asked my co-operation for an experiment where he studied the content in 62 mycotoxins in wheat caryopses, as recorded in 70 samples coming from different Italian regions. The dataset was structured as 70 x 64 table, as shown in the scheme below: there is one row for each wheat sample, while the first column represents the sample id, the second column represents the region from where that sample was collected and the other columns represent the concentrations of the 62 toxins (one column per toxin).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_general_tidyverse.png&#34; width=&#34;75%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That colleague wanted me to calculate, for each toxin and for each region, the following stats:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;number of collected samples&lt;/li&gt;
&lt;li&gt;mean concentration value&lt;/li&gt;
&lt;li&gt;maximum concentration value&lt;/li&gt;
&lt;li&gt;standard error&lt;/li&gt;
&lt;li&gt;number of contaminated samples (concentration higher than 0)&lt;/li&gt;
&lt;li&gt;percentage of contaminated samples&lt;/li&gt;
&lt;li&gt;mean value for contaminated samples&lt;/li&gt;
&lt;li&gt;standard error for contaminated samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;He also wanted me to return to him a list of 62 tables (one per toxin), with all the regions along with their descriptive stats.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-have-already-done-something-like-this&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;I have already done something like this!&lt;/h1&gt;
&lt;p&gt;When facing this task, my first reaction was to search in my ‘computer’s attic’, looking for previously written codes to accomplish the very same task. As I said, this is a fairly common task in my everyday work and I could find several examples of old codes. Looking at those, I realised that, when the number of variables is high, I have so far made an extensive use of &lt;code&gt;for&lt;/code&gt; loops to repeat the same task across columns. This is indeed no wonder, as I came across R in 2000, after an extensive usage of general purpose languages, such as Quick Basic and Visual Basic.&lt;/p&gt;
&lt;p&gt;In practise, most of my earliest R codes were based on the &lt;code&gt;tapply()&lt;/code&gt; function to calculate statistics for grouped data and &lt;code&gt;for&lt;/code&gt; loops to iterate over columns. In the box below I show an example of such an approach, by using a factitious dataset, with the very same structure as my colleague’s dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
dataset &amp;lt;- read.csv(&amp;quot;https://casaonofri.it/_datasets/Mycotoxins.csv&amp;quot;, header = T)
# str(dataset)
# &amp;#39;data.frame&amp;#39;: 70 obs. of  64 variables:
#  $ Sample : int  1 2 3 4 5 6 7 8 9 10 ...
#  $ Region : chr  &amp;quot;Lombardy&amp;quot; &amp;quot;Lombardy&amp;quot; &amp;quot;Lombardy&amp;quot; &amp;quot;Lombardy&amp;quot; ...
#  $ DON    : num  8.62 16.2 18.19 27.08 10.97 ...
#  $ DON3G  : num  21.7 28.4 34.7 26.9 26.4 ...
#  $ NIV    : num  23.5 25.3 22.6 27.8 29.4 ...
#  $ NIVG   : num  38.6 26.2 13.5 21.4 15.4 ...
#  $ T2     : num  23.9 23.6 19.7 25.7 21.7 ...
#  $ HT2    : num  19 37.6 32.1 22.3 25.6 ...
#  $ HT2G   : num  13.7 25.7 25.8 33.4 32.7 ...
#  $ NEO    : num  29.06 7.56 27.52 32.91 24.49 ...

returnList &amp;lt;- list()
for(i in 1:(length(dataset[1,]) - 3)){
  y &amp;lt;- as.numeric( unlist(dataset[, i + 3]) )
  Count &amp;lt;- tapply(y, dataset$Region, length)
  Mean &amp;lt;- tapply(y, dataset$Region, mean)
  Max &amp;lt;- tapply(y, dataset$Region, max)
  SE &amp;lt;- tapply(y, dataset$Region, sd)/sqrt(Count)
  nPos &amp;lt;- tapply(y != 0, dataset$Region, sum)
  PercPos &amp;lt;- tapply(y != 0, dataset$Region, mean)*100
  muPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA), dataset$Region, mean, na.rm = T)
  muPos[is.na(muPos)] &amp;lt;- 0
  sdPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA), dataset$Region, sd, na.rm = T)
  SEpos &amp;lt;- sdPos/sqrt(nPos)
  returnList[[i]] &amp;lt;- data.frame(cbind(Count, Mean, Max, SE, nPos, PercPos, muPos, SEpos))
  names(returnList)[[i]] &amp;lt;- colnames(dataset)[i + 3]
}
print(returnList$CRV, digits = 2)
##                 Count Mean Max   SE nPos PercPos muPos SEpos
## Abruzzo             4   28  30 0.85    4     100    28  0.85
## Apulia              9   25  40 2.67    9     100    25  2.67
## Campania            2   20  21 0.74    2     100    20  0.74
## Emilia Romagna      8   23  33 2.80    8     100    23  2.80
## Latium              7   20  33 2.76    7     100    20  2.76
## Lombardy            4   25  32 5.12    4     100    25  5.12
## Molise              1   18  18   NA    1     100    18    NA
## Sardinia            6   25  38 3.32    6     100    25  3.32
## Sicily              6   21  30 2.94    6     100    21  2.94
## The Marche          5   19  23 1.58    5     100    19  1.58
## Tuscany             5   30  34 1.22    5     100    30  1.22
## Umbria              9   21  32 2.83    9     100    21  2.83
## Veneto              4   23  28 1.99    4     100    23  1.99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I must admit that the above code is ugly: first, &lt;code&gt;for&lt;/code&gt; loops in R are not very efficient and, second, reusing that code requires quite a bit of editing and it is prone to errors. Therefore, I asked myself how I could write more efficient code …&lt;/p&gt;
&lt;p&gt;First of all, I thought I should use &lt;code&gt;apply()&lt;/code&gt; instead of the &lt;code&gt;for&lt;/code&gt; loop. Thus I wrote a function to calculate the required stats for each column and &lt;code&gt;apply()-ed&lt;/code&gt; that function to all columns of my data-frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funBec &amp;lt;- function(y, group){
  # y &amp;lt;- as.numeric( unlist(dataset[, i + 3]) )
  Count &amp;lt;- tapply(y, group, length)
  Mean &amp;lt;- tapply(y, group, mean)
  Max &amp;lt;- tapply(y, group, max)
  SE &amp;lt;- tapply(y, group, sd)/sqrt(Count)
  nPos &amp;lt;- tapply(y != 0, group, sum)
  PercPos &amp;lt;- tapply(y != 0, group, mean)*100
  muPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA),group, mean, na.rm = T)
  muPos[is.na(muPos)] &amp;lt;- 0
  sdPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA), group, sd, na.rm = T)
  SEpos &amp;lt;- sdPos/sqrt(nPos)
  data.frame(cbind(Count, Mean, Max, SE, nPos, PercPos, muPos, SEpos))
}
returnList2 &amp;lt;- apply(dataset[3:length(dataset[1,])], 2,
                     function(col) funBec(col, dataset$Region))
# kable(returnList2$CRV, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am pretty happy with the above approach; it feels ‘natural’, to me and the coding appears to be pretty clear and easily reusable. However, it makes only use of the &lt;code&gt;base&lt;/code&gt; R implementation and, therefore, it looks a bit out-fashioned… a dinosaur’s code…&lt;/p&gt;
&lt;p&gt;What could I possibly do to write more ‘modern’ code? A brief survey of posts from the R world suggested the ultimate solution: “I must use the ‘tidyverse’!”. Therefore, I tried to perform my column-wise task by using &lt;code&gt;dplyr&lt;/code&gt; and related packages and I must admit that I could not immediately find an acceptable solution. After some careful thinking, it was clear that the tidyverse requires a strong change in programming habits, which is not always simple for those who are over a certain age. Or, at least, that is not simple for me…&lt;/p&gt;
&lt;p&gt;Eventually, I thought I should switch from a &lt;code&gt;for&lt;/code&gt; attitude to a &lt;code&gt;split-apply-combine&lt;/code&gt; attitude; indeed, I discovered that, if I ‘melted’ the dataset so that the variables were stacked one above the other, I could, consequently:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;split the dataset in several groups, consisting of one toxin and one region,&lt;/li&gt;
&lt;li&gt;calculate the stats for each group, and&lt;/li&gt;
&lt;li&gt;combine the results for all groups.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’d like to share the final code: I used the &lt;code&gt;pivot_longer()&lt;/code&gt; function to melt the dataset, the &lt;code&gt;group_by()&lt;/code&gt; function to create an ‘internal’ grouping structure by regions and toxins, the &lt;code&gt;summarise()&lt;/code&gt; and &lt;code&gt;across()&lt;/code&gt; functions to calculate the required stats for each group. In the end, I obtained a tibble, that I split into a list of tibbles, by using the &lt;code&gt;group_split()&lt;/code&gt; function. The code is shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
returnList6 &amp;lt;- dataset %&amp;gt;%
  select(-Sample) %&amp;gt;% 
  pivot_longer(names_to = &amp;quot;Toxin&amp;quot;, values_to = &amp;quot;Conc&amp;quot;,
               cols = c(3:length(dataset[1,]) - 1)) %&amp;gt;% 
  group_by(Toxin, Region) %&amp;gt;% 
  summarise(across(&amp;quot;Conc&amp;quot;, .fns =
               list(Mean = mean,
                    Max = max,
                    SE = function(df) sd(df)/sqrt(length(df)),
                    nPos = function(df) length(df[df &amp;gt; 0]),
                    percPos = function(df) length(df[df &amp;gt; 0])/length(df)*100,
                    muPos =  function(df) mean(df[df &amp;gt; 0]),
                    SEpos =  function(df) sd(df[df &amp;gt; 0])/sqrt(length(df[df &amp;gt; 0]))
                    ))) %&amp;gt;% 
  ungroup() %&amp;gt;%
  group_split(Toxin, .keep = F)
names(returnList6) &amp;lt;- names(dataset)[3:64]
# returnList6$CRV&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, I can say that with the above code I have, at least, tried not to be a dinosaur… but, I am sure that many young scientists out there can suggest better solutions. If so, please drop me a line at the email address below. Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;[UPDATE: 13/12/2020]&lt;/p&gt;
&lt;p&gt;Bryan Hutchinson from the UK proposed an alternative solution based on the ‘data.table’ and ‘DescTools’ libraries. I have never used these libraries before, but the code looks very efficient and elegant. At the end, some examples of how the dataset can be filtered are given. Thanks, Bryan!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(DescTools)
 
dataset &amp;lt;- fread(&amp;quot;https://casaonofri.it/_datasets/Mycotoxins.csv&amp;quot;, header = T)
tnames &amp;lt;- names(dataset[,-c(1:2)])
 
sum_stats &amp;lt;- function(var) {
  # convert integer to double
  var &amp;lt;- as.numeric(var)
  list(
    mean = mean(var, na.rm = TRUE),
    max = max(var, na.rm = TRUE),
    se = sd(var, na.rm = TRUE) / sqrt(length(var)),
    nPos = length(var &amp;gt;0),
    percPos = length(var &amp;gt;0)/length(var)*100,
    muPos =  mean(var &amp;gt; 0),
    SEpos =  sd(var &amp;gt; 0)/sqrt(length(var &amp;gt; 0))
  )
}
 
df_long &amp;lt;- melt(dataset,
                measure.vars = list(tnames),
                variable.name = &amp;quot;Toxin&amp;quot;,
                value.name = &amp;quot;Conc&amp;quot;)
 
 
# df_long[, sum_stats(Conc), .(Toxin, Region)]
# df_long[Toxin == &amp;quot;FLAG&amp;quot;, sum_stats(Conc), .(Toxin, Region)]
# df_long[Region == &amp;quot;Lombardy&amp;quot;, sum_stats(Conc), .(Region, Toxin)]&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;[UPDATE: 14/12/2020]&lt;/p&gt;
&lt;p&gt;A younger collegue (Renzo Bonifazi) suggested some changes to the code, which I am happy to share here. Setting the range of columns at the beginning seems to be wise and removing the ‘ungroup()’ function is correct. I did not know about the ‘setNames()’ function, which was used at the end. Thanks, Renzo!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define here the range of cols to apply upon the code
range_cols &amp;lt;- c(3:length(dataset[1,]))

results &amp;lt;- dataset %&amp;gt;%
  select(-Sample) %&amp;gt;% 
  pivot_longer(names_to = &amp;quot;Toxin&amp;quot;, values_to = &amp;quot;Conc&amp;quot;,
               cols = range_cols - 1) %&amp;gt;% # use user defined range of cols
  group_by(Toxin, Region) %&amp;gt;% 
  summarise(across(&amp;quot;Conc&amp;quot;, .fns =
                     list(Mean = mean,
                          Max = max,
                          SE = function(df) sd(df)/sqrt(length(df)),
                          nPos = function(df) length(df[df &amp;gt; 0]),
                          percPos = function(df) length(df[df &amp;gt; 0])/length(df)*100,
                          muPos =  function(df) mean(df[df &amp;gt; 0]),
                          SEpos =  function(df) sd(df[df &amp;gt; 0])/sqrt(length(df[df &amp;gt; 0]))
                     ))) %&amp;gt;%
   group_split(.keep = F) %&amp;gt;% # This fun by default seems to split based on the first defined group var, i.e. &amp;quot;Toxin&amp;quot; in this case
setNames(names(dataset)[range_cols]) # define the names internally&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for the experimental design in linear/nonlinear regression analyses</title>
      <link>https://www.statforbiology.com/2020/stat_nlmm_designconstraints/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_nlmm_designconstraints/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this post, I am going to talk about an issue that is often overlooked by agronomists and biologists. The point is that field experiments are very often laid down in blocks, using split-plot designs, strip-plot designs or other types of designs with grouping factors (blocks, main-plots, sub-plots). We know that these grouping factors should be appropriately accounted for in data analyses: ‘analyze them as you have randomized them’ is a common saying attributed to Ronald Fisher. Indeed, observations in the same group are correlated, as they are more alike than observations in different groups. What happens if we neglect the grouping factors? We break the independence assumption and our inferences are invalid (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;To my experience, field scientists are totally aware of this issue when they deal with ANOVA-type models (e.g., see Jensen et al., 2018). However, a brief survey of literature shows that there is not the same awareness, when field scientists deal with linear/nonlinear regression models. Therefore, I decided to sit down and write this post, in the hope that it may be useful to obtain more reliable data analyses.&lt;/p&gt;
&lt;div id=&#34;an-example-with-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with linear regression&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the ‘yieldDensity.csv’ dataset, that is available on gitHub. It represents an experiment where sunflower was tested with increasing weed densities (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 10 blocks. a swift plot shows that yield is linearly related to weed density, which calls for linear regression analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(lattice)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/yieldDensityB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yield
## 1     1       0 29.90
## 2     2       0 34.23
## 3     3       0 37.12
## 4     4       0 26.37
## 5     5       0 34.48
## 6     6       0 33.70
plot(yield ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We might be tempted to neglect the block effect and run a linear regression analysis of yield against density. This is clearly wrong (I am violating the independence assumption) and inefficient, as any block-to-block variability goes into the residual error term, which is, therefore, inflated.&lt;/p&gt;
&lt;p&gt;Some of my collegues would take the means for densities and use those to fit a linear regression model (two-steps analysis). By doing so, block-to-block variability is cancelled out and the analysis becomes more efficient. However, such a solution is not general, as it is not feasible, e.g., when we have unbalanced designs and heteroscedastic data. With the appropriate approach, sound analyses can also be made in two-steps (Damesa et al., 2017). From my point of view, it is reasonable to search for more general solutions to deal with one-step analyses.&lt;/p&gt;
&lt;p&gt;Based on our experience with traditional ANOVA models, we might think of taking the block effect as fixed and fit it as and additive term. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg &amp;lt;- lm(yield ~ block + density, data=dataset)
summary(mod.reg)
## 
## Call:
## lm(formula = yield ~ block + density, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6062 -0.8242 -0.3315  0.7505  4.6244 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 29.10462    0.57750  50.397  &amp;lt; 2e-16 ***
## block2       4.57750    0.74668   6.130 4.81e-08 ***
## block3       7.05875    0.74668   9.453 4.49e-14 ***
## block4      -3.98000    0.74668  -5.330 1.17e-06 ***
## block5       6.17625    0.74668   8.272 6.37e-12 ***
## block6       5.92750    0.74668   7.938 2.59e-11 ***
## block7       1.23750    0.74668   1.657  0.10199    
## block8       1.25500    0.74668   1.681  0.09733 .  
## block9       2.34875    0.74668   3.146  0.00245 ** 
## block10      2.25125    0.74668   3.015  0.00359 ** 
## density     -0.26744    0.00701 -38.149  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.493 on 69 degrees of freedom
## Multiple R-squared:  0.9635, Adjusted R-squared:  0.9582 
## F-statistic: 181.9 on 10 and 69 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With regression, this solution is not convincing. Indeed, the above model assumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected. Is this a reasonable assumption? I vote no.&lt;/p&gt;
&lt;p&gt;Let’s check this by fitting a different regression model per block (ten different slopes + ten different intercepts):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg2 &amp;lt;- lm(yield ~ block/density + block, data=dataset)
anova(mod.reg, mod.reg2)
## Analysis of Variance Table
## 
## Model 1: yield ~ block + density
## Model 2: yield ~ block/density + block
##   Res.Df    RSS Df Sum of Sq      F  Pr(&amp;gt;F)  
## 1     69 153.88                              
## 2     60 115.75  9    38.135 2.1965 0.03465 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-level confirms that the block had a significant effect both on the intercept and on the slope. To describe such an effect we need 20 parameters in the model, which is not very parsimonious. And above all: which regression line do we use for predictions? Taking the block effect as fixed is clearly sub-optimal with regression models.&lt;/p&gt;
&lt;p&gt;The question is: can we fit a simpler and clearer model? The answer is: yes. Why don’t we take the block effect as random? This is perfectly reasonable. Let’s do it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = ~ density|block, data=dataset)
summary(modMix.1)
## Linear mixed-effects model fit by REML
##   Data: dataset 
##        AIC      BIC    logLik
##   340.9166 355.0569 -164.4583
## 
## Random effects:
##  Formula: ~density | block
##  Structure: General positive-definite, Log-Cholesky parametrization
##             StdDev     Corr  
## (Intercept) 3.16871858 (Intr)
## density     0.02255249 0.09  
## Residual    1.38891957       
## 
## Fixed effects:  yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0370844 69  30.65311       0
## density     -0.26744 0.0096629 69 -27.67704       0
##  Correlation: 
##         (Intr)
## density -0.078
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9923722 -0.5657555 -0.1997103  0.4961675  2.6699060 
## 
## Number of Observations: 80
## Number of Groups: 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above fit shows that the random effects (slope and intercept) are sligthly correlated (r = 0.091). We might like to try a simpler model, where random effects are independent. To do so, we need to consider that the above model is equivalent to the following model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s just two different ways to code the very same model. However, this latter coding, based on a ‘pdMat’ structure, can be easily modified to remove the correlation. Indeed, ‘pdSymm’ specifies a totally unstructured variance-covariance matrix for random effects and it can be replaced by ‘pdDiag’, which specifies a diagonal matrix, where covariances (off-diagonal terms) are constrained to 0. The coding is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.2 &amp;lt;- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)
summary(modMix.2)
## Linear mixed-effects model fit by REML
##   Data: dataset 
##       AIC      BIC   logLik
##   338.952 350.7355 -164.476
## 
## Random effects:
##  Formula: ~density | block
##  Structure: Diagonal
##         (Intercept)    density Residual
## StdDev:    3.198267 0.02293222 1.387148
## 
## Fixed effects:  yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0460282 69  30.39102       0
## density     -0.26744 0.0097463 69 -27.44020       0
##  Correlation: 
##         (Intr)
## density -0.139
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9991174 -0.5451478 -0.1970267  0.4925092  2.6700388 
## 
## Number of Observations: 80
## Number of Groups: 10
anova(modMix.1, modMix.2)
##          Model df      AIC      BIC    logLik   Test    L.Ratio p-value
## modMix.1     1  6 340.9166 355.0569 -164.4583                          
## modMix.2     2  5 338.9520 350.7355 -164.4760 1 vs 2 0.03535079  0.8509&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model could be further simplified. For example, the code below shows how we could fit models with either random intercept or random slope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Model with only random intercept
modMix.3 &amp;lt;- lme(yield ~ density, random = list(block = ~1), data=dataset)

#Alternative
#random = ~ 1|block

#Model with only random slope
modMix.4 &amp;lt;- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)

#Alternative
#random = ~density - 1 | block&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with nonlinear regression&lt;/h1&gt;
&lt;p&gt;The problem may become trickier if we have a nonlinear relationship. Let’s have a look at another similar dataset (‘YieldLossB.csv’), that is also available on gitHub. It represents another experiment where sunflower was grown with the same increasing densities of another weed (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 8 blocks. In this case, the yield loss was recorded and analysed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/YieldLossB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yieldLoss
## 1     1       0     1.532
## 2     2       0    -0.661
## 3     3       0    -0.986
## 4     4       0    -0.697
## 5     5       0    -2.264
## 6     6       0    -1.623
plot(yieldLoss ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A swift plot shows that the relationship between density and yield loss is not linear. Literature references (Cousens, 1985) show that this could be modelled by using a rectangular hyperbola:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[YL = \frac{i \, D}{1 + \frac{i \, D}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(YL\)&lt;/span&gt; is the yield loss, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is weed density, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the slope at the origin of axes and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum asymptotic yield loss. This function, together with self-starters, is available in the ‘NLS.YL()’ function in the ‘aomisc’ package, which is the accompanying package for this blog. If you do not have this package, please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this link&lt;/a&gt; to download it.&lt;/p&gt;
&lt;p&gt;The problem is the very same as above: the block effect may produce random fluctuations for both model parameters. The only difference is that we need to use the ‘nlme()’ function instead of ‘lme()’. With nonlinear mixed models, I strongly suggest you use a ‘groupedData’ object, which permits to avoid several problems. The second line below shows how to turn a data frame into a ‘groupedData’ object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
datasetG &amp;lt;- groupedData(yieldLoss ~ 1|block, dataset)
nlin.mix &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                        fixed = list(i ~ 1, A ~ 1),
            random = i + A ~ 1|block)
summary(nlin.mix)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   474.8228 491.5478 -231.4114
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite, Log-Cholesky parametrization
##          StdDev    Corr 
## i        0.1112839 i    
## A        4.0444538 0.195
## Residual 1.4142272      
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23238 0.0382246 104 32.24038       0
## A 68.52305 1.9449745 104 35.23082       0
##  Correlation: 
##   i     
## A -0.408
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4416770 -0.7049388 -0.1805690  0.3385458  2.8788981 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly to linear mixed models, the above coding implies correlated random effects (r = 0.194). Alternatively, the above model can be coded by using a ’pdMat construct, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix2 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdSymm(list(i ~ 1, A ~ 1)))
summary(nlin.mix2)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   474.8225 491.5475 -231.4113
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite
##          StdDev    Corr 
## i        0.1112839 i    
## A        4.0466971 0.194
## Residual 1.4142009      
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23242  0.038225 104 32.24107       0
## A 68.52068  1.945173 104 35.22600       0
##  Correlation: 
##   i     
## A -0.409
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4414051 -0.7049356 -0.1805322  0.3385275  2.8787362 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can try to simplify the model, for example by excluding the correlation between random effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix3 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdDiag(list(i ~ 1, A ~ 1)))
summary(nlin.mix3)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##   Data: datasetG 
##        AIC      BIC    logLik
##   472.9076 486.8451 -231.4538
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: Diagonal
##                 i        A Residual
## StdDev: 0.1172791 4.389173 1.408963
## 
## Fixed effects:  list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23243 0.0393514 104 31.31852       0
## A 68.57655 1.9905549 104 34.45097       0
##  Correlation: 
##   i     
## A -0.459
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.3577291 -0.6849962 -0.1785860  0.3255925  2.8592764 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a little fantasy, we can easily code several alternative models to represent alternative hypotheses about the observed data. Obviously, the very same method can be used (and SHOULD be used) to account for other grouping factors, such as main-plots in split-plot designs or plots in repeated measure designs.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Cousens, R., 1985. A simple model relating yield loss to weed density. Annals of Applied Biology 107, 239–252. &lt;a href=&#34;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jensen, S.M., Schaarschmidt, F., Onofri, A., Ritz, C., 2018. Experimental design matters for statistical analysis: how to handle blocking: Experimental design matters for statistical analysis. Pest Management Science 74, 523–534. &lt;a href=&#34;https://doi.org/10.1002/ps.4773&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1002/ps.4773&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Hayman&#39;s model (type 1)</title>
      <link>https://www.statforbiology.com/2020/stat_met_diallel_hayman1/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_met_diallel_hayman1/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In a previous post we have presented our new ‘lmDiallel’ package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see this link here&lt;/a&gt; and see also the original paper in &lt;a href=&#34;https://rdcu.be/caxZh&#34;&gt;Theoretical and Applied Genetics&lt;/a&gt;). This package provides an extensions to fit a class of linear models of interest for plant breeders or geneticists, the so-called diallel models. In this post and other future posts we would like to present some examples of how to use this package: please, sit back and relax and, if you have comments, let us know, using the email link at the bottom of this post.&lt;/p&gt;
&lt;div id=&#34;but-what-is-a-diallel&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;But… what is a ‘diallel’?&lt;/h1&gt;
&lt;p&gt;If you are not a plant breeder or a geneticist in general, you may be asking this question. From the ancient Greek language, the ‘diallel’ word means ‘reciprocating’ and a diallel cross is a set of several possible crosses and selfs between some parental lines. For example, if we take the male lines A, B and C together with the same female lines A, B and C and we imagine to cross those lines with one another, we obtain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the selfs A&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;A, B&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;B and C&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;C,&lt;/li&gt;
&lt;li&gt;the crosses A&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;B, A&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;C and B&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;C,&lt;/li&gt;
&lt;li&gt;and, in some instances, the reciprocals B&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;A, C&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;A and C&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;B (where the father and mother are swapped).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The performances of crosses and/or selfs and/or reciprocals can be compared by planning field experiments, usually known as &lt;strong&gt;diallel experiments&lt;/strong&gt; and designed as randomised complete blocks with 3-4 replicates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The example&lt;/h1&gt;
&lt;p&gt;Depending on how the experiment is planned, we can have four experimental methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Crosses + reciprocals + selfs (complete diallel)&lt;/li&gt;
&lt;li&gt;Crosses and reciprocals (no selfs)&lt;/li&gt;
&lt;li&gt;Crosses and selfs (no reciprocals)&lt;/li&gt;
&lt;li&gt;Only crosses (no selfs, no reciprocals)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post we will concentrate on the first design (complete diallel) and we will use a simple example with three parental lines (A, B and C). The csv file (‘diallel1.csv’) is available in an external repository; in the box below we load the data and we use the &lt;code&gt;group_by()&lt;/code&gt; function in the ‘dplyr’ package to obtain the means for all crosses and selfs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
rm(list = ls())
df &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/diallel1.csv&amp;quot;)
df$Block &amp;lt;- factor(df$Block)
dfM &amp;lt;- df %&amp;gt;% 
  group_by(Par1, Par2) %&amp;gt;% 
  summarise(YieldM = mean(Yield), SEs = sd(Yield/sqrt(4)))
dfM
## # A tibble: 9 x 4
## # Groups:   Par1 [3]
##   Par1  Par2  YieldM   SEs
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 A     A         12 0.740
## 2 A     B         13 0.600
## 3 A     C         14 0.498
## 4 B     A         11 1.00 
## 5 B     B         15 0.332
## 6 B     C         21 0.273
## 7 C     A         17 0.295
## 8 C     B         16 0.166
## 9 C     C         19 1.90&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-model-do-we-use&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What model do we use?&lt;/h1&gt;
&lt;p&gt;In order to describe the above dataset, we might think of a two-way ANOVA model, where the ‘father’ and ‘mother’ lines (the ‘Par1’ and ‘Par2’ variables, respectively) are used as the explanatory factors.&lt;/p&gt;
&lt;p&gt;This is a very tempting solution, but we should resist: a two way ANOVA model regards the ‘father’ and ‘mother’ effects as two completely different series of treatments, neglecting the fact that they are, indeed, the same genotypes in different combinations. That is exactly why we need specific &lt;strong&gt;diallel models&lt;/strong&gt; to describe the results of diallel experiments!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-haymans-model-type-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Hayman’s model type 1&lt;/h1&gt;
&lt;p&gt;The first diallel model was proposed by Hayman (1954) and it was devised for complete diallel experiments, where reciprocals are available. Neglecting the design effects (blocks and/or environments), the Hayman’s model is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{ts}_{ij} + \textrm{rg}^a_{i} + \textrm{rg}^b_{j} + rs_{ij} + \varepsilon_{ijk} \quad\quad\quad (Eq. 1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is expected value (the overall mean, in the balanced case) and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual random error terms for the observation in the &lt;span class=&#34;math inline&#34;&gt;\(k^{th}\)&lt;/span&gt; block and with the parentals &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. All the other terms correspond to genetic effects, namely:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_j\)&lt;/span&gt; terms are the &lt;strong&gt;general combining abilities&lt;/strong&gt; (GCAs) of the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents. Each term relates to the average performances of a parental line in all its hybrid combination, under the sum-to-zero constraint (i.e. the sum of &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; values for all parentals must be zero). For example, with our balanced experiment, the overall mean is &lt;span class=&#34;math inline&#34;&gt;\(\mu = 15.33\)&lt;/span&gt;, while the mean for the A parent when used as the ‘father’ is &lt;span class=&#34;math inline&#34;&gt;\(\mu_{A.} = 13\)&lt;/span&gt; and the mean for the same parent A when used as the ‘mother’ is &lt;span class=&#34;math inline&#34;&gt;\(\mu_{.A} = 13.33\)&lt;/span&gt;. Consequently:
&lt;span class=&#34;math display&#34;&gt;\[g_A = \left(13 + 13.33 \right)/2 - 15.33 = -2.167\]&lt;/span&gt; Analogously, it is &lt;span class=&#34;math inline&#34;&gt;\(g_B = -0.167\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rg^a_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(rg^b_j\)&lt;/span&gt; terms are the &lt;strong&gt;reciprocal general combining abilities&lt;/strong&gt; (RGCAs) for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents. Each term relates to the discrepancy between the effect of a parent when it is used as father/mother and its average effect in all its combinations. For example, considering the parent A, the term &lt;span class=&#34;math inline&#34;&gt;\(rg^a_A\)&lt;/span&gt; is: &lt;span class=&#34;math display&#34;&gt;\[rg^a_A = \mu_{A.} - \frac{\mu_{A.} + \mu_{.A}}{2} = 13 - 13.167 = -0.167\]&lt;/span&gt; Obviously, it must be &lt;span class=&#34;math inline&#34;&gt;\(rg^a_A = - rg^b_B\)&lt;/span&gt; and it must also be that the sum of all &lt;span class=&#34;math inline&#34;&gt;\(rg^a\)&lt;/span&gt; terms is zero (sum-to-zero constraint).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\textrm{ts}_{ij}\)&lt;/span&gt; term is the total &lt;strong&gt;specific combining ability&lt;/strong&gt; (tSCA) for the combination between the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents. It relates to the discrepancy from additivity for a specific combination of two parentals. For example, considering the ‘A &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; B’ cross, the expected yield under additivity would be: &lt;span class=&#34;math display&#34;&gt;\[\mu_{A:B} = \mu + \textrm{g}_A + \textrm{g}_B +\textrm{rg}^a_{A} + \textrm{rg}^b_{B} =\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[ = 15.33 - 2.167 - 0.167 - 0.167 - 0.5 = 12.333\]&lt;/span&gt; while the observed yield is 13, with a with a difference of &lt;span class=&#34;math inline&#34;&gt;\(-0.667\)&lt;/span&gt;. On the other hand, considering the ‘B &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; A’ reciprocal cross, the expected yield under additivity would be: &lt;span class=&#34;math display&#34;&gt;\[\mu_{A:B} = \mu + \textrm{g}_A + \textrm{g}_B +\textrm{rg}^a_{B} + \textrm{rg}^b_{A} =\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= 15.33 - 2.167 - 0.167 + 0.167 + 0.5 = 13.667\]&lt;/span&gt; while the observed yield is 11, with a difference of &lt;span class=&#34;math inline&#34;&gt;\(2.667\)&lt;/span&gt;. The tSCA for the cross between A and B (regardless of the reciprocal) is the average difference, that is &lt;span class=&#34;math inline&#34;&gt;\(\textrm{ts}_{AB} = (-0.667 + 2.667)/2 = 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rs_{ij}\)&lt;/span&gt; term is the &lt;strong&gt;reciprocal specific combining ability&lt;/strong&gt; (RSCA) for a specific &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt; combination, that is the discrepancy between the performances of the two reciprocals (e.g, A &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; B vs. B &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; A). For example, the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{rs}_{AB}\)&lt;/span&gt; term is equal to &lt;span class=&#34;math inline&#34;&gt;\(-0.667 - 1 = -1.667\)&lt;/span&gt;, that is the opposite of &lt;span class=&#34;math inline&#34;&gt;\(\textrm{rs}_{BA}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting with R&lt;/h1&gt;
&lt;p&gt;Hands-calculations based on means may be useful to understand the meaning of genetical effects, although they are biased with unbalanced designs and, above all, they are totally uninteresting from a practical point of view: we’d rather fit the model by using a statistical software.&lt;/p&gt;
&lt;p&gt;Let’s assume that all effects are fixed, apart from the residual standard error. This is a reasonable assumption, as we have a very low number of parentals, which would make the estimation of variance components totally unreliable. We clearly see that the Hayman’s model above is a specific parameterisation of a general linear model and we should be able to fit it by the usual &lt;code&gt;lm()&lt;/code&gt; function and related methods. We can, indeed, do so by using our ‘lmDiallel’ extension package, that provides the facilities to generate the correct design matrices for the Hayman’s model (and for other diallel models, as we will show in future posts).&lt;/p&gt;
&lt;p&gt;At the beginning, we have to install (if necessary) and load the ‘lmDiallel’ package (see box below). Model fitting can be performed by using the &lt;code&gt;GCA()&lt;/code&gt;, &lt;code&gt;tSCA()&lt;/code&gt;, &lt;code&gt;RGCA()&lt;/code&gt; and &lt;code&gt;RSCA()&lt;/code&gt; functions as shown in the box below: the resulting &lt;code&gt;lm&lt;/code&gt; object can be explored by the usual R methods, such as &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
dMod &amp;lt;- lm(Yield ~ Block + GCA(Par1, Par2) + tSCA(Par1, Par2) +
              RGCA(Par1, Par2) + RSCA(Par1, Par2), data = df)
summary(dMod)
## 
## Call:
## lm(formula = Yield ~ Block + GCA(Par1, Par2) + tSCA(Par1, Par2) + 
##     RGCA(Par1, Par2) + RSCA(Par1, Par2), data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.3500 -0.5644  0.0606  0.4722  2.7911 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)             1.558e+01  5.780e-01  26.962  &amp;lt; 2e-16 ***
## Block2                 -3.772e-01  8.174e-01  -0.461 0.648613    
## Block3                 -3.011e-01  8.174e-01  -0.368 0.715830    
## Block4                 -3.261e-01  8.174e-01  -0.399 0.693458    
## GCA(Par1, Par2)g_A     -2.167e+00  2.890e-01  -7.497 9.77e-08 ***
## GCA(Par1, Par2)g_B     -1.667e-01  2.890e-01  -0.577 0.569516    
## tSCA(Par1, Par2)ts_A:A  1.000e+00  5.780e-01   1.730 0.096457 .  
## tSCA(Par1, Par2)ts_A:B -1.000e+00  4.570e-01  -2.188 0.038609 *  
## tSCA(Par1, Par2)ts_B:B  1.634e-16  5.780e-01   0.000 1.000000    
## RGCA(Par1, Par2)rg_A   -1.667e-01  2.890e-01  -0.577 0.569516    
## RGCA(Par1, Par2)rg_B    1.333e+00  3.389e-01   3.934 0.000622 ***
## RSCA(Par1, Par2)        2.500e+00  5.309e-01   4.709 8.71e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.734 on 24 degrees of freedom
## Multiple R-squared:  0.8269, Adjusted R-squared:  0.7476 
## F-statistic: 10.42 on 11 and 24 DF,  p-value: 1.129e-06
anova(dMod)
## Analysis of Variance Table
## 
## Response: Yield
##                  Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block             3   0.784   0.261  0.0869    0.9665    
## GCA(Par1, Par2)   2 244.000 122.000 40.5743 1.999e-08 ***
## tSCA(Par1, Par2)  3  24.000   8.000  2.6606    0.0710 .  
## RGCA(Par1, Par2)  2   9.333   4.667  1.5520    0.2323    
## RSCA(Par1, Par2)  1  66.667  66.667 22.1717 8.710e-05 ***
## Residuals        24  72.164   3.007                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simplicity, we also built a wrapper function named &lt;code&gt;lm.diallel()&lt;/code&gt;, which can be used in the very same fashion as &lt;code&gt;lm()&lt;/code&gt;. The syntax is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm.diallel(formula, Block, Env, data, fct)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘formula’ specifies the response variable and the two variables for parentals (e.g., Yield ~ Par1 + Par2) and the two arguments ‘Block’ and ‘Env’ are used to specify optional variables, coding for blocks and environments, respectively. The argument ‘data’ is a ‘dataframe’ where to look for the explanatory variables and, finally, ‘fct’ is a string variable coding for the selected model (“HAYMAN1”, for this example; see below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2, Block = Block,
                    data = df, fct = &amp;quot;HAYMAN1&amp;quot;)
summary(dMod2)
## 
## Call:
## lm.diallel(formula = Yield ~ Par1 + Par2, Block = Block, fct = &amp;quot;HAYMAN1&amp;quot;, 
##     data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.3500 -0.5644  0.0606  0.4722  2.7911 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept  1.533e+01  2.890e-01  53.056  &amp;lt; 2e-16 ***
## Block1     2.511e-01  5.006e-01   0.502 0.620484    
## Block2    -1.261e-01  5.006e-01  -0.252 0.803236    
## Block3    -5.000e-02  5.006e-01  -0.100 0.921264    
## g_A       -2.167e+00  2.890e-01  -7.497 9.77e-08 ***
## g_B       -1.667e-01  2.890e-01  -0.577 0.569516    
## ts_A:A     1.000e+00  5.780e-01   1.730 0.096457 .  
## ts_A:B    -1.000e+00  4.570e-01  -2.188 0.038609 *  
## ts_B:B     7.155e-16  5.780e-01   0.000 1.000000    
## rg_A      -1.667e-01  2.890e-01  -0.577 0.569516    
## rg_B       1.333e+00  3.389e-01   3.934 0.000622 ***
## rs_A:B     2.500e+00  5.309e-01   4.709 8.71e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.734 on 24 degrees of freedom
## Multiple R-squared:  0.8269, Adjusted R-squared:  0.7476 
## F-statistic: 10.42 on 11 and 24 DF,  p-value: 1.129e-06
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block      3   0.784   0.261  0.0869    0.9665    
## GCA        2 244.000 122.000 40.5743 1.999e-08 ***
## tSCA       3  24.000   8.000  2.6606    0.0710 .  
## RGCA       2   9.333   4.667  1.5520    0.2323    
## RSCA       1  66.667  66.667 22.1717 8.710e-05 ***
## Residuals 24  72.164                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function works very much like the &lt;code&gt;lm()&lt;/code&gt; function and makes use of the general purpose linear model solver &lt;code&gt;lm.fit()&lt;/code&gt;. Apart from simplicity, another advantage is that the call to &lt;code&gt;lm.diallel()&lt;/code&gt; returns an object of both ‘lm’ and ‘diallel’ classes. For this latter class, we built several specific S3 methods, such as the usual &lt;code&gt;anova()&lt;/code&gt;, &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;model.matrix()&lt;/code&gt; methods, partly shown in the box above.&lt;/p&gt;
&lt;p&gt;Considering that diallel models are usually fitted to determine genetical parameters, we also built the &lt;code&gt;glht.diallelMod()&lt;/code&gt; method and the &lt;code&gt;diallel.eff()&lt;/code&gt; function, which can be used with the ‘multcomp’ package, to retrieve the complete list of genetical parameters, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;)) 
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept == 0  1.533e+01  2.890e-01  53.056  &amp;lt; 2e-16 ***
## g_A == 0       -2.167e+00  2.890e-01  -7.497 5.85e-08 ***
## g_B == 0       -1.667e-01  2.890e-01  -0.577 0.569106    
## g_C == 0        2.333e+00  2.890e-01   8.074 1.49e-08 ***
## ts_A:A == 0     1.000e+00  5.780e-01   1.730 0.095471 .  
## ts_A:B == 0    -1.000e+00  4.570e-01  -2.188 0.037819 *  
## ts_A:C == 0     9.992e-16  4.570e-01   0.000 1.000000    
## ts_B:A == 0    -1.000e+00  4.570e-01  -2.188 0.037819 *  
## ts_B:B == 0     7.155e-16  5.780e-01   0.000 1.000000    
## ts_B:C == 0     1.000e+00  4.570e-01   2.188 0.037819 *  
## ts_C:A == 0     9.992e-16  4.570e-01   0.000 1.000000    
## ts_C:B == 0     1.000e+00  4.570e-01   2.188 0.037819 *  
## ts_C:C == 0    -1.000e+00  5.780e-01  -1.730 0.095471 .  
## rg_A == 0      -1.667e-01  2.890e-01  -0.577 0.569106    
## rg_B == 0       1.333e+00  3.389e-01   3.934 0.000555 ***
## rg_C == 0      -1.167e+00  3.389e-01  -3.443 0.001962 ** 
## rs_A:B == 0     2.500e+00  5.309e-01   4.709 7.25e-05 ***
## rs_A:C == 0    -2.500e+00  5.309e-01  -4.709 7.25e-05 ***
## rs_B:A == 0    -2.500e+00  5.309e-01  -4.709 7.25e-05 ***
## rs_C:A == 0     2.500e+00  5.309e-01   4.709 7.25e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- none method)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-in-two-steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting in two steps&lt;/h1&gt;
&lt;p&gt;In some cases, the analysis is performed in two steps and a diallel model is fitted to the means of selfs and crosses, which are calculated in the first step. Under the assumption of variance homogeneity and equal number of replicates, we can fit the Hayman’s model by using the &lt;code&gt;lm.diallel()&lt;/code&gt; function without the ‘Block’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod3 &amp;lt;- lm.diallel(YieldM ~ Par1 + Par2, 
                    data = dfM, fct = &amp;quot;HAYMAN1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we have no reliable estimate of residual error, but the &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; methods have been enhanced to give us the possibility of passing some information from the first step, i.e. an appropriate estimate of the residual mean square and degrees of freedom; the residual mean square from the first step needs to be appropriately weighted for the number of replicates (i.e., for this example, MSE = 3.007/4 with 24 degrees of freedom).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(dMod3, MSE = 3.007/4, dfr = 24)
## Analysis of Variance Table
## 
## Response: YieldM
##           Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA        2 61.000 30.5000 40.5720 2.000e-08 ***
## tSCA       3  6.000  2.0000  2.6605   0.07101 .  
## RGCA       2  2.333  1.1667  1.5519   0.23236    
## RSCA       1 16.667 16.6667 22.1705 8.713e-05 ***
## Residuals 24         0.7518                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
summary(dMod3, MSE = 3.007/4, dfr = 24)
##                Estimate        SE       t value     Pr(&amp;gt;|t|)
## Intercept  1.533333e+01 0.2890117  5.305436e+01 2.157713e-26
## g_A       -2.166667e+00 0.2890117 -7.496812e+00 9.771159e-08
## g_B       -1.666667e-01 0.2890117 -5.766779e-01 5.695269e-01
## ts_A:A     1.000000e+00 0.5780235  1.730034e+00 9.646589e-02
## ts_A:B    -1.000000e+00 0.4569677 -2.188339e+00 3.861373e-02
## ts_B:B     2.294461e-15 0.5780235  3.969495e-15 1.000000e+00
## rg_A      -1.666667e-01 0.2890117 -5.766779e-01 5.695269e-01
## rg_B       1.333333e+00 0.3388963  3.934340e+00 6.219023e-04
## rs_A:B     2.500000e+00 0.5309484  4.708555e+00 8.712864e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The genetical parameters can be obtained by using the &lt;code&gt;glht()&lt;/code&gt; function and passing the information from the first step within the call to the &lt;code&gt;diallel.eff()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh2 &amp;lt;- glht(linfct = diallel.eff(dMod3, MSE = 3.007/4, dfr = 24))
summary(gh2, test = adjusted(type = &amp;quot;none&amp;quot;)) 
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept == 0  1.533e+01  2.890e-01  53.054  &amp;lt; 2e-16 ***
## g_A == 0       -2.167e+00  2.890e-01  -7.497 5.85e-08 ***
## g_B == 0       -1.667e-01  2.890e-01  -0.577 0.569117    
## g_C == 0        2.333e+00  2.890e-01   8.073 1.49e-08 ***
## ts_A:A == 0     1.000e+00  5.780e-01   1.730 0.095480 .  
## ts_A:B == 0    -1.000e+00  4.570e-01  -2.188 0.037824 *  
## ts_A:C == 0    -1.110e-15  4.570e-01   0.000 1.000000    
## ts_B:A == 0    -1.000e+00  4.570e-01  -2.188 0.037824 *  
## ts_B:B == 0     2.294e-15  5.780e-01   0.000 1.000000    
## ts_B:C == 0     1.000e+00  4.570e-01   2.188 0.037824 *  
## ts_C:A == 0    -1.110e-15  4.570e-01   0.000 1.000000    
## ts_C:B == 0     1.000e+00  4.570e-01   2.188 0.037824 *  
## ts_C:C == 0    -1.000e+00  5.780e-01  -1.730 0.095480 .  
## rg_A == 0      -1.667e-01  2.890e-01  -0.577 0.569117    
## rg_B == 0       1.333e+00  3.389e-01   3.934 0.000555 ***
## rg_C == 0      -1.167e+00  3.389e-01  -3.443 0.001962 ** 
## rs_A:B == 0     2.500e+00  5.309e-01   4.709 7.25e-05 ***
## rs_A:C == 0    -2.500e+00  5.309e-01  -4.709 7.25e-05 ***
## rs_B:A == 0    -2.500e+00  5.309e-01  -4.709 7.25e-05 ***
## rs_C:A == 0     2.500e+00  5.309e-01   4.709 7.25e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- none method)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;In some cases, genetic effects are regarded as random and the aim is to estimate variance components. For this, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a few dummy variables, which may make the task difficult for practitioners. Therefore, we coded a wrapper for the &lt;code&gt;mmer()&lt;/code&gt; function (&lt;code&gt;mmer.diallel()&lt;/code&gt;)that uses the same syntax as &lt;code&gt;lm.diallel()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It would make no sense to estimate the variance components for genetic effects with a diallel experiment based on three parentals and, therefore, we give an example based on the ‘hayman54’ dataset, as available in the ‘lmDiallel’ package and relating to a complete diallel experiment with eight parentals (Hayman, 1954).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
data(hayman54)
mod.ran &amp;lt;- mmer.diallel(Ftime ~ Par1 + Par2, Block = Block,
                        data = hayman54, fct = &amp;quot;HAYMAN1&amp;quot;)
mod.ran$varcomp
##              VarComp  VarCompSE
## Block        0.00000   9.321698
## GCA       1276.73142 750.174164
## RGCA        17.97647  19.909911
## tSCA      1110.99398 330.172943
## RSCA        30.53937  46.467163
## Residuals  418.47875  74.563526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do hope that you enjoyed this post; if you are interested in diallel models, please, stay tuned: we have other examples on the way.&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Prof. Luigi Russi&lt;br /&gt;
Dr. Niccolò Terzaroli&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0156744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0156744&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hayman, B.I., 1954. The Analysis of Variance of Diallel Tables. Biometrics 10, 235. &lt;a href=&#34;https://doi.org/10.2307/3001877&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/3001877&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Terzaroli, N., Russi, L., 2020. Linear models for diallel crosses: a review with R functions. Theoretical Applied Genetics, &lt;a href=&#34;https://doi.org/10.1007/s00122-020-03716-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00122-020-03716-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. Introduction</title>
      <link>https://www.statforbiology.com/2020/stat_met_diallel1/</link>
      <pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_met_diallel1/</guid>
      <description>


&lt;p&gt;Together with some colleagues from the plant breeding group, we have just published a new paper, where we presented a bunch of R functions to analyse the data from diallel experiments. The paper is titled ‘&lt;em&gt;Linear models for diallel crosses: a review with R functions&lt;/em&gt;’ and it is published in the ‘&lt;em&gt;Theoretical and Applied Genetics&lt;/em&gt;’ Journal. If you are interested, you can take a look &lt;a href=&#34;https://rdcu.be/caxZh&#34;&gt;here at this link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Diallel experiments are based on a set of possible crosses between some homozygous (inbred) lines. For example, if we have the male lines A, B and C and the female lines A, B and C (same lines used, alternatively, as male and female), we would have the following selfed parents: AA, BB and CC and the following crosses: AB, AC, BC. In some instances, we might also have the reciprocals BA, CA and CB. Selfed parents and crosses are compared on a Randomised Complete Block Design, usually replicated across seasons and/or locations.&lt;/p&gt;
&lt;p&gt;For these diallel experiments, six main diallel models are available in literature, to quantify genetic effects, such as general combining ability (GCA), specific combining ability (SCA), reciprocal (maternal) effects and heterosis. If you are an expert in plant breeding, you do not need any other explanation; if you are not an expert, well… you are like me: we only need to know that these effects are determined as linear combinations of means for crosses, means for selfed parents and reciprocals. However, as I recently discovered, fitting diallel models to experimental data from diallel experiments is a relevant task for plant breeders.&lt;/p&gt;
&lt;p&gt;When I started dealing with diallel models, I was very surprised by the fact that they are often presented as separate entities, to be fitted by using specialised software; indeed, to the eyes of a biostatistician, it would appear that all diallel models are only different parameterisations of the same general linear model (Mohring et al., 2011). Therefore, it seemed to me very strange that we could not fit diallel models by simply using the &lt;code&gt;lm()&lt;/code&gt; function in R and related platform.&lt;/p&gt;
&lt;p&gt;A deeper diving in this subject showed me that the main implementation problem was that certain effects, such as the GCA effect, require the definition of unconventional design matrices, which were not yet available in R. Indeed, the packages ‘asreml-R’ and ‘sommer’ permit, e.g., the overlay of design matrices (function &lt;code&gt;and()&lt;/code&gt; in ‘asreml’ and &lt;code&gt;overlay()&lt;/code&gt; in ‘sommer’), which is useful to code GCA effects, but none of the two packages played well with the &lt;code&gt;lm()&lt;/code&gt; function in R. Therefore, together with Niccolò and Luigi, we decided to enhance the &lt;code&gt;model.matrix()&lt;/code&gt; function in R, building a handful of new R functions, aimed at producing the correct design matrices for all types of diallel models. All these functions are available within the ‘lmDiallel’ package, which is available on gitHub; it can be installed by using the ‘install_github()’ function, as available in the ‘devtools’ package. Therefore, if necessary, install this package first. The code is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;) # Only at first instance
library(devtools)
install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The core functions for ‘lmDiallel’ are named after the corresponding genetic effects, i.e.: &lt;code&gt;GCA()&lt;/code&gt; (general combining ability), &lt;code&gt;tSCA()&lt;/code&gt; (total Specific Combining Ability), &lt;code&gt;RGCA()&lt;/code&gt; (reciprocal general combining ability), &lt;code&gt;RSCA()&lt;/code&gt; (reciprocal specific combining ability), &lt;code&gt;REC()&lt;/code&gt; (RECiprocal effects = RGCA + RSCA), &lt;code&gt;DD()&lt;/code&gt; (Dominance Deviation), &lt;code&gt;MDD()&lt;/code&gt; (Mean Dominance Deviation), &lt;code&gt;H.BAR()&lt;/code&gt; (Average Heterosis), &lt;code&gt;Hi()&lt;/code&gt; (Average hetorosis for one parent), &lt;code&gt;VEi()&lt;/code&gt; (Variety Effect), &lt;code&gt;SP()&lt;/code&gt; (effect of Selfed Parents) and &lt;code&gt;GCAC()&lt;/code&gt; (GCA for parents in their crosses). The usage of these functions is very simple. For example, let’s assume that we have the two variables ‘Par1’ and ‘Par2’ in a dataset, to represent the two parental lines (father and mother); the GCA effect is coded as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GCA(Par1, Par2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the SCA effect is coded as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SCA(Par1, Par2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using these R functions as building blocks, we can fit all diallel models inside the &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;lme()&lt;/code&gt; functions. For example, the following line of code fits a diallel model containing the GCA and SCA effects, to the data contained in the ‘df’ dataframe:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm(yield ~ GCA(Par1, Par2) + SCA(Par1, Par2), data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the effect of reciprocals and random blocks can be introduced by the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lme(yield ~ GCA(Par1, Par2) + SCA(Par1, Par2) +
            REC(Par1, Par2),
            random = ~1|Block, data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model building process outlined above is clearly rooted in the frame of general linear models, although we recognise that plant breeders usually refer to certain relevant parameterisations of diallel models by using the name of the authors. In this respect, it is very common to use the terms “HAYMAN1”, “GRIFFING1”, “GRIFFING2”, “HAYMAN2”, “GE2” and “GE3” to refer to the main six diallel models available in literature (see Hayman, 1954; Griffing, 1956; Gardner and Eberhart, 1966). Although these models can be built and fit by using the above method, we thought it might be useful to simplify the whole process. For this reason, we also built a wrapper function named &lt;code&gt;lm.diallel()&lt;/code&gt;, which can be used in the very same fashion as &lt;code&gt;lm()&lt;/code&gt;. The syntax is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm.diallel(formula, Block, Env, data, fct)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘formula’ uses the regular R syntax to specify the response variable and the two variables for parentals (e.g., Yield ~ Par1 + Par2). The two arguments ‘Block’ and ‘Env’ are used to specify optional variables, coding for blocks and environments, respectively. The argument ‘data’ is a ‘dataframe’ where to look for explanatory variables. Finally, ‘fct’ is a string variable coding for the selected model, i.e. “HAYMAN1”, “GRIFFING1”, “GRIFFING2”, “HAYMAN2”, “GE2”, “GE3”, according to the existing literature.&lt;/p&gt;
&lt;p&gt;We have also built the &lt;code&gt;summary()&lt;/code&gt;, &lt;code&gt;vcov(),&lt;/code&gt; &lt;code&gt;anova()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; methods for ‘lm.diallel’ objects, in order to obey to some peculiar aspects of diallel models.&lt;/p&gt;
&lt;p&gt;In our paper (&lt;a href=&#34;https://rdcu.be/caxZh&#34;&gt;‘Linear models for diallel crosses: a review with R functions’&lt;/a&gt;) we have reviewed diallel models and gave examples on how they can be fitted with our new package ‘lmDiallel’. We have also shown how the facilities we provide can be used to fit random effects diallel models with ‘jags’. We intend to provide a more lengthy documentation for our package in a coming series of posts; thus, if you are interested, please, stay tuned.&lt;/p&gt;
&lt;p&gt;I believe that increasing the usability of existing packages that have gained a wide popularity may be an advantageous programming strategy, compared to the usual strategy of building brand new platforms. From the point of view of the developer, it is efficient, as it requires a minor programming effort. From the point of view of the users (professionals, technicians and students), it is handy to be put in the conditions of making statistical analyses, without the need of learning new softwares and/or languages and/or syntaxes. Due to its open-source nature, the R environment is often overwhelming for users, that are confused by the extremely wide availability of alternative methods to perform the same task. In this regard, a programming strategy aimed at supporting some existing reference platforms might help build a more comfortable environment for statistical analyses.&lt;/p&gt;
&lt;p&gt;Thanks for reading and, please, stay tuned! If you have comments, please, drop me a line at the email address below. Best wishes,&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Covarrubias-Pazaran G (2016) Genome-assisted prediction of quantitative traits using the R package sommer. PLoS ONE 11:e0156744.&lt;/li&gt;
&lt;li&gt;Gardner CO, Eberhart SA (1966) Analysis and interpretation of the variety cross diallel and related populations. Biometrics 22:439–452.&lt;/li&gt;
&lt;li&gt;Gilmoure A, Gogel BJ, Cullis BR, Whelam SJ, Thompson R (2015) ASReml user guide release 4.1 structural specification. VSN International Ltd, Hemel Hempstead, HP1 1ES, UK&lt;/li&gt;
&lt;li&gt;Griffing B (1956) Concept of general and specific combining ability in relation to diallel crossing systems. Aust J Biol Sci 9:463–493&lt;/li&gt;
&lt;li&gt;Möhring J, Melchinger AE, Piepho HP (2011) REML-based diallel analysis. Crop Sci 51:470–478.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>QQ-plots and Box-Whisker plots: where do they come from?</title>
      <link>https://www.statforbiology.com/2020/stat_general_percentiles/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_general_percentiles/</guid>
      <description>


&lt;div id=&#34;for-the-most-curious-students&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;For the most curious students&lt;/h1&gt;
&lt;p&gt;QQ-plots and Box-Whisker plots usually become part of the statistical toolbox for the students attending my course of ‘Experimental methods in agriculture’. Most of them learn that the QQ-plot can be used to check for the basic assumption of gaussian residuals in linear models and that the Box-Whisker plot can be used to describe the experimental groups, when their size is big enough and we do not want to assume a gaussian distribution. Furthermore, most students learn to use the &lt;code&gt;plot()&lt;/code&gt; method on an ‘lm’ object and the &lt;code&gt;boxplot()&lt;/code&gt; function in the base ‘graphic’ package and concentrate on the interpretation of the R output. To me, in practical terms, this is enough; however, there is at least a couple of students per year who think that this is not enough and they want to know more. What is the math behind those useful plots? Can we draw them by hand?&lt;/p&gt;
&lt;p&gt;If I were to give further detail about these two types of graphs, I should give a more detailed explanation of percentiles, at the very beginning of my course. I usually present the concept, which is rather easy to grasp, for students and I also give an example of how to calculate the 50th percentile (i.e. the median). So far, so good. But, what about the 25th and 75th percentiles, which are needed to build the Box-Whisker plot? As a teacher, I must admit I usually skip this aspect; I resort to showing the use of the &lt;code&gt;quantile()&lt;/code&gt; function and the students are happy, apart the same couple per year, who asks: “what is this function doing in the background?”.&lt;/p&gt;
&lt;p&gt;Usually, there is no time to satisfy the above thirst for knowledge. First of all, I need time to introduce other important concepts for agricultural student; second, giving more detail would imply a high risk of being ‘beaten’ by all the other, less eager, students. Therefore, I decided to put my explanation here, to the benefit of the most curious of my students.&lt;/p&gt;
&lt;p&gt;As an example, we will use 11 observations, randomly selected from a uniform distribution. First of all, we sort them out in increasing order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
y &amp;lt;- sort( runif(11))
y
##  [1] 0.009495756 0.113703411 0.232550506 0.514251141 0.609274733 0.622299405
##  [7] 0.623379442 0.640310605 0.666083758 0.693591292 0.860915384&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-p-values-of-observations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The P-values of observations&lt;/h1&gt;
&lt;p&gt;Let’s try to imagine that our eleven observations are percentiles, although we do not know what their percentage point is. Well, we know something, at least; we have an odd number of observations and we know that the central value (0.622299405) is the median, i.e. the 50th percentile. But, what about the other values? In other words, we are looking for the P-values associated to each observation (probability points).&lt;/p&gt;
&lt;p&gt;In order to determine the P-values, we divide the whole scale from 0 to 100% into as many intervals as there are values in our sample, that is eleven intervals. Each interval contains &lt;span class=&#34;math inline&#34;&gt;\(1 / 11 \times 100 = 9.09\%\)&lt;/span&gt; of the whole scale: the first interval goes from 0% to 9.09%, the second goes from 9.09% to 18.18% and so on, until the 11th interval, that goes from 90.91% to 100%.&lt;/p&gt;
&lt;p&gt;Now, each value in the ordered sample is associated to one probability interval. The problem is: where do we put the value, within each interval? A possible line of attack, that is the default in R with &lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; 10\)&lt;/span&gt;, is to put the value in the middle of the interval, as shown in the Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.casaonofri.it/_Figures/PercentagePoints.png&#34; style=&#34;width:95.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Consequently, each point corresponds to the following P-values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1:11 - 0.5)/11
##  [1] 0.04545455 0.13636364 0.22727273 0.31818182 0.40909091 0.50000000
##  [7] 0.59090909 0.68181818 0.77272727 0.86363636 0.95454545&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{i - 0.5}{n} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of values and &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the index of each value in the sorted vector. In other words, the first value is the 4.5th percentile, the second value is the 13.64th percentile and so on, until the 11th value that is the 95.45th percentile.&lt;/p&gt;
&lt;p&gt;Unfortunately, the calculation of P-values is not unique and there are other ways to locate the values within each interval. A second possibility, is to put the value close to the beginning of each interval. In this case, the corresponding P-values can be calculated as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1:11 - 1)/(11 - 1)
##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{i - 1}{n - 1} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;According to this definition, the first value is the 0th percentile, the second is the 10th percentile and so on.&lt;/p&gt;
&lt;p&gt;A third possibility, is to put the value at the end of each interval. In this case, each point corresponds to the following P-values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:11/11
##  [1] 0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455
##  [7] 0.63636364 0.72727273 0.81818182 0.90909091 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{i}{n} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although it is not the default in R, this third approach is, perhaps, the most understandable, as it is more closely related to the definition of percentiles. It is clear that there is 9.09% subjects equal to or lower than the first one and that there is 18.18% of subjects equal to or lower than the second one.&lt;/p&gt;
&lt;p&gt;Several other possibilities exist, but we will not explore them here. However, the most common function in R to calculate probability points is the &lt;code&gt;ppoints()&lt;/code&gt; function, which gives different results, according to the selection of the ‘a’ argument. In detail, if &lt;span class=&#34;math inline&#34;&gt;\(a = 0.5\)&lt;/span&gt; (the default) we obtain the first series of P-values (where each of the original values is put in the middle of each interval) while, if &lt;span class=&#34;math inline&#34;&gt;\(a = 1\)&lt;/span&gt;, we obtain the second series of P-values (where each of the original values is put near to the beginning of each interval).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ppoints(y)
##  [1] 0.04545455 0.13636364 0.22727273 0.31818182 0.40909091 0.50000000
##  [7] 0.59090909 0.68181818 0.77272727 0.86363636 0.95454545
ppoints(y, a = 1)
##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;from-ppoints-to-the-qq-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;From &lt;code&gt;ppoints()&lt;/code&gt; to the QQ-plot&lt;/h1&gt;
&lt;p&gt;P-values are used to draw quantile-quantile plots. Let’s imagine that we want to know whether the eleven values in the vector ‘y’ can be regarded as a sample from a gaussian distribution. To this aim, we can standardise them and plot them against the corresponding percentiles of a gaussian distribution, which we derive from the above determined P-values, by the &lt;code&gt;pnorm()&lt;/code&gt; function. The output is exactly the same as that produced by a call to the &lt;code&gt;qqnorm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.coord &amp;lt;- qnorm(ppoints(y))
y.coord &amp;lt;- scale(y, scale = T) 
plot(y.coord ~ x.coord, main = &amp;quot;Manual QQ-plot&amp;quot;,
     xlab = &amp;quot;Theoretical quantiles&amp;quot;, ylab = &amp;quot;Standardised values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_General_percentiles_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;percentiles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Percentiles&lt;/h1&gt;
&lt;p&gt;Percentiles can be calculated by solving the three equations above for &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. For example, from the first equation, with simple math, we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[i = n p_i + 0.5\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which gives the index for the required percentile. If we look for the 30th percentile, the index is &lt;span class=&#34;math inline&#34;&gt;\(11 \times 0.30 + 0.5 = 3.8\)&lt;/span&gt;, i.e. this percentile is between the 3rd and the 4th value, that are, respectively, 0.232550506 and 0.514251141. We can find the exact percentile by a sort of ‘weighted average’ of these two values, considering the decimal part of the index (0.8):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1 - 0.8) * y[3] + 0.8 * y[4]
## [1] 0.457911&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the same result, by using the &lt;code&gt;quantile()&lt;/code&gt; function in R and setting the ‘type = 5’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(y, 0.3, type = 5)
##      30% 
## 0.457911&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we start from the second equation, we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[i = p \times (n - 1) + 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the 30th percentile, the index is &lt;span class=&#34;math inline&#34;&gt;\(0.3 \times (11 - 1) + 1 = 4\)&lt;/span&gt;. Thus, the required percentile is exactly in 4th position (0.5142511). With R, we can use the same &lt;code&gt;quantile()&lt;/code&gt; function, but we have to set the ‘type = 7’ argument, that is the default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(y, 0.3)
##       30% 
## 0.5142511&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take the third equation above, we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[i = n \times p\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and thus the index for the 30th percentile is: &lt;span class=&#34;math inline&#34;&gt;\(11 \times 0.3 = 3.3\)&lt;/span&gt;. We use the same kind of ‘weighted average’ to retrieve the exact percentile:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1 - 0.3) * y[3] + 0.3 * y[4]
## [1] 0.3170607&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(y, 0.3, type = 4)
##       30% 
## 0.3170607&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the above math to derive the 25th, the 50th and the 75th percentile, we are ready to draw our boxplot by hand. Please, consider that the &lt;code&gt;boxplot()&lt;/code&gt; function in R uses the default quantiles (‘type = 7’).&lt;/p&gt;
&lt;p&gt;It’s all, for today. If you have any comments, please, drop me a note at &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;. Best wishes,&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building ANOVA-models for long-term experiments in agriculture</title>
      <link>https://www.statforbiology.com/2020/stat_lte_modelbuilding/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_lte_modelbuilding/</guid>
      <description>


&lt;p&gt;This is the follow-up of a manuscript that we (some colleagues and I) have published in 2016 in the European Journal of Agronomy (Onofri et al., 2016). I thought that it might be a good idea to rework some concepts to make them less formal, simpler to follow and more closely related to the implementation with R. Please, be patient: this lesson may be longer than usual.&lt;/p&gt;
&lt;div id=&#34;what-are-long-term-experiments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are long-term experiments?&lt;/h1&gt;
&lt;p&gt;Agricultural experiments have to deal with long-term effects of cropping practices. Think about fertilisation: certain types of organic fertilisers may give effects on soil fertility, which are only observed after a relatively high number of years (say: 10-15). In order to observe those long-term effects, we need to plan Long Term Experiments (LTEs), wherein each plot is regarded as a small cropping system, with the selected combination of rotation, fertilisation, weed control and other cropping practices. Due to the fact that yield and other relevant variables are repeatedly recorded over time, LTEs represent a particular class of multi-environment experiments with repeated measures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-main-problem-with-ltes-lack-of-independence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The main problem with LTEs: lack of independence&lt;/h1&gt;
&lt;p&gt;We know that, with linear models, once the effects of experimental factors have been accounted for, the residuals must be independent. Otherwise, inferences are invalid.&lt;/p&gt;
&lt;p&gt;With LTEs, observations are repeatedly taken on the same plot and, therefore, the residuals cannot be independent. Indeed, all measurements taken on one specific plot will be affected by the peculiar characteristics of that plot and they will be more alike than measurements taken in different plots. Thus, there will be a ‘plot’ effect, which will induce a within-plot correlation. The problem is: how do we restore the necessary independence of residuals?&lt;/p&gt;
&lt;p&gt;At the basic level, the main way to account for the ‘plot’ effect is by including a random term in the model; in this way, we recognise that there is a plot-to-plot variability, following a gaussian distribution, with mean equal to 0 and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B\)&lt;/span&gt; (‘plot’ error). This plot-to-plot variability is additional to the usual residual variability (within-plot error), that is also gaussian with mean equal to 0 and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As the result, if we take one observation, the variance will be equal to the sum &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B + \sigma^2\)&lt;/span&gt;. If we take two observations in different plots, they will have different random ‘plot’ effects and, thus, they will be independent. Otherwise, if we take two observations in the same plot, they will share the same random plot effect and they will ‘co-vary’, showing a positive covariance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B\)&lt;/span&gt;. The correlation among observations in the same plot will be quantified by the ratio &lt;span class=&#34;math inline&#34;&gt;\(\rho = \sigma^2_B / (\sigma^2_B + \sigma^2)\)&lt;/span&gt; (intra-class correlation).&lt;/p&gt;
&lt;p&gt;In simple words, adding a random plot effect to the model accounts for the fact that observations in the same plot are correlated. This would be similar to a split-plot design (indeed, we talk about split-plot in time) with the important difference that the sub-plot factor (year) is not randomised. The correlation of observations in one plot will always be the same, independent from the year, which is known as Compound Symmetry (CS) correlation structure. Be careful: &lt;strong&gt;it may be more reasonable to assume that observations close in time are more correlated than observations distant in time&lt;/strong&gt;, but we will address this point elsewhere.&lt;/p&gt;
&lt;p&gt;It is necessary to remember that, apart from plots, the experimental design may be characterised by other grouping structures, such as blocks or main plots. All these grouping structures must be appropriately referenced in the model, to account for intra-group correlation. I’ll be back into this in a few moments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-problem-with-ltes-rotation-treatments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Another problem with LTEs: rotation treatments&lt;/h1&gt;
&lt;p&gt;In many instances, the aim of LTEs is to compare different cropping systems, which are allocated to different plots, possibly in different blocks. When the different cropping systems involve rotations, we need to consider a very important rule, that was pointed out by W.G. Cochran in a seminal paper dating back to 1937: “&lt;em&gt;The most important rule about rotation experiments is that each crop in the rotation must be grown every year&lt;/em&gt;”. If we do not follow this rule, “&lt;em&gt;the experiment has to last longer to obtain equal information on the long-term effects of the treatments&lt;/em&gt;” and “&lt;em&gt;the effects of the treatments on the separate crops are obtained under different seasonal conditions, so that a compact summary of the results of the experiment as a whole is made exceedingly difficult&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;The above rule has important consequences: first of all, with, e.g., a three year rotation, we need three plots per treatment and per block in each year, which increases the size of the experiment (that’s why some LTEs are designed without within-year replicates; Patterson, 1964). Secondly, if we want to consider only one crop in the rotation, the experiment becomes unbalanced, as not all plots contribute useful data in each one year. Last, but not least, in long rotations the test crop may return onto the same plot after a relatively long period of time, which may create a totally different correlation structure, compared to short rotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-build-anova-like-models-for-ltes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How do we build ‘ANOVA-like’ models for LTEs?&lt;/h1&gt;
&lt;p&gt;For the reasons explained above, building ANOVA-like models for data analyses may be a daunting task and it is useful to follow a structured procedure. First of all, we need to remember that ANOVA models are based on classification variables, commonly known as factors. There are three types of factors:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;treatment factors, which are randomly allocated to randomisation units (e.g. rotations, fertilisations, management of crop residues);&lt;/li&gt;
&lt;li&gt;block factors, which group the observations according to some ‘innate’ (not randomly allocated) criterion (e.g. by position), such as the blocks, the locations, the main-plots, the sub-plots and so on. Block factors may represent the randomisation units, to which treatments are randomly allocated;&lt;/li&gt;
&lt;li&gt;repeated factors, which relate to time and thus cannot be randomised (e.g. years, cycles …).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to build a model, the starting point is to list all factors (treatment, grouping and repeated) and their relationships. We can follow the general method proposed by Piepho et al. (2003), which we have slightly modified, to make it more ‘R-centric’. The relationships among factors can be specified by using the following operators:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the ‘colon’ operator denotes an interaction of crossed effects (e.g. A:B means that A and B are crossed factors);&lt;/li&gt;
&lt;li&gt;the ‘nesting’ operator denotes nested effects (e.g. A/B means that B is nested within A and it is equal to A + A:B);&lt;/li&gt;
&lt;li&gt;the ‘crossing’ operator denotes the full factorial model for two terms (A*B = A + B + A:B).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When building models we need to pay attention to properly code interactions. Let’s have a look at a simple two-way ANOVA, with the ‘JohnsonGrass.csv’ dataset. In this case we have the two crossed effects Length and Timing and we could build a model as: ‘RIZOMEWEIGHT ~ LENGTH + TIMING + LENGTH:TIMING’ that is shortened as: ’RIZOMEWEIGHT ~ LENGTH*TIMING’. However, if we build our model as: ‘RIZOMEWEIGHT ~ LENGTH:TIMING’, the two main effects are ‘absorbed’ by the term ‘LENGTH:TIMING’, which is no longer an interaction. The code below may clear up what I mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- readr::read_csv(&amp;quot;https://www.casaonofri.it/_datasets/JohnsonGrass.csv&amp;quot;)

mod1 &amp;lt;- lm(RizomeWeight ~ Length + Timing + Length:Timing, data = dataset)
anova(mod1)
## Analysis of Variance Table
## 
## Response: RizomeWeight
##               Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Length         2  1762.2   881.1  9.4795 0.0002961 ***
## Timing         5 16927.9  3385.6 36.4241 3.896e-16 ***
## Length:Timing 10   952.7    95.3  1.0250 0.4354263    
## Residuals     54  5019.2    92.9                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
mod2 &amp;lt;- lm(RizomeWeight ~ Length:Timing, data = dataset)
anova(mod2)
## Analysis of Variance Table
## 
## Response: RizomeWeight
##               Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Length:Timing 17 19642.8 1155.46  12.431 4.766e-13 ***
## Residuals     54  5019.2   92.95                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-to-model-building&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Steps to model building&lt;/h1&gt;
&lt;p&gt;The steps to model building may be summarised as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Select the repeated factor.&lt;/li&gt;
&lt;li&gt;Consider one fixed level of the repeated factor and build a treatment model for the randomized treatment factors.&lt;/li&gt;
&lt;li&gt;Consider one fixed level of the repeated factor and build a block model for block factors.&lt;/li&gt;
&lt;li&gt;Check whether randomised treatment factors might interact with block effects: if such an interaction is to be expected it should be added to the model.&lt;/li&gt;
&lt;li&gt;Include the unrandomised repeated factor into the model.&lt;/li&gt;
&lt;li&gt;Combine treatment model and repeated factor model, by crossing or nesting as appropriate.&lt;/li&gt;
&lt;li&gt;Consider which effects in the block model reference randomisation units, i.e. those units which receive the levels of a factor or factor combination by a randomisation process. It should be clear that randomisation units can be seen as randomly selected from a wider population. Therefore, the corresponding terms should be assigned a separate random effect, as explicitly recommended in Piepho (2004).&lt;/li&gt;
&lt;li&gt;Excluding the terms for randomisation units, nest the repeated factor in all the other terms in the block model.&lt;/li&gt;
&lt;li&gt;Combine random effects for randomisation units with the repeated factor, by using the colon operator, in order to derive the correct error terms to accommodate correlation structures.&lt;/li&gt;
&lt;li&gt;Apart from randomisation units (see #7), decide which factors are random and which are fixed. In our examples, the random model will include all random terms for randomisation units (terms at steps 7 and 9), while the fixed model will include all the other terms. Several extensions/changes to this basic approach are possible.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key idea for the above approach is that for a properly designed experiment, valid analyses should be possible for the data at each single level of the repeated factor. Such a basic requirement should never be taken for granted, but it should be carefully checked before the beginning of the model building process (see later for Example 3).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-further-definitions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some further definitions&lt;/h1&gt;
&lt;p&gt;It is perhaps important to clear up some definitions, which we will use afterwards. Each crop component in a rotation is usually known as a phase; e.g., in the rotation Maize-Wheat-Wheat, Maize is phase 1, Wheat is phase 2 and Wheat is phase 3. The number of phases defines the period (duration) of the rotation. All phases need to be contemporarily present in any one year and, therefore, we can define the so-called sequences: i.e. each of the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; possible arrangements for a rotation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; years, having the same crop ordering, but different initial phases (e.g Maize - Wheat - Wheat, Wheat - Wheat - Maize and Wheat - Maize - Wheat). Each sequences is uniquely identified by its starting phase, which needs to be randomly allocated to each plot at the start of the experiment.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;In order to give a practical demonstration, we have selected five exemplary datasets, relating to LTEs with different designs. If you are in a hurry, you can follow the links below to jump directly to the example that is most relevant for you.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Example 1. &lt;a href=&#34;#example-1-ltes-with-monocultures-or-perennial-crops&#34;&gt;LTEs to compare monocultures or perennial crops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 2. &lt;a href=&#34;#example-2.-ltes-with-different-rotations-of-the-same-length-and-one-test-crop-per-rotation-cycle&#34;&gt;LTEs to compare rotations of the same length and one test crop per rotation cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 3. &lt;a href=&#34;#example-3.-ltes-with-a-fixed-rotation-one-test-crop-per-cycle-and-different-treatments&#34;&gt;LTEs with a fixed rotation (one test crop per cycle) and different treatments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 4. &lt;a href=&#34;#example-4.-lte-with-a-fixed-rotation-different-treatments-and-more-than-one-phase-per-crop-and-cycle&#34;&gt;LTE with a fixed rotation, different treatments and more than one phase per crop and cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 5. &lt;a href=&#34;#example-5-ltes-with-several-rotations-of-different-lengths-and-different-number-of-phases-per-crop-and-rotation-cycle&#34;&gt;LTEs with several rotations of different lengths and different number of phases per crop and rotation cycle&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will analyse all examples, using the ‘tidyverse’ (Wickham, 2019) for data management and the ‘nlme’ package to fit random effect models (Pinheiro et al., 2019). We will also use the ‘asreml-R’ (Butler, 2019) package, for those of you who own a licence. Let’s load those packages in the R environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
# library(asreml)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;example-1-ltes-with-monocultures-or-perennial-crops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: LTEs with monocultures or perennial crops&lt;/h2&gt;
&lt;p&gt;Wheat is grown in continuous cropping from 1983 to 2012, with three fertilisation levels (150, 200 and 250 kg N ha&lt;span class=&#34;math inline&#34;&gt;\(^{-1}\)&lt;/span&gt;), randomly assigned to three plots in each of three blocks. In all, there are nine plots with yearly sampling, with a total of 270 wheat yield observations in 30 years. The following figure shows the design for one block: the spatial split for each plot represents the actual temporal split.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this example, the repeated factor is the year (YEAR). In one year, the treatment factor is nitrogen fertilisation (N) and there are two block factors, i.e. the blocks (BLOCK) and the plots within each block (PLOT). Therefore, the block model is BLOCK + BLOCK:PLOT.&lt;/p&gt;
&lt;p&gt;We now introduce the repeated factor YEAR and combine it with the treatment model, by including N*YEAR = N + YEAR + N:YEAR. The term BLOCK:PLOT references the randomisation units and receives a random effect. As the year might interact with the block, we add the term BLOCK:YEAR. We also combine the year with the random effect for plots (BLOCK:PLOT:YEAR), although this residual term does not need to be explicitly coded when implementing the model. The final model is (the operator ~ means ‘is modelled as’):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ N + BLOCK + YEAR + N:YEAR + BLOCK:YEAR
RANDOM = BLOCK:PLOT + BLOCK:PLOT:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the above notation in R, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE1.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Block, Plot, Year, N), factor))
head(dataset)
## # A tibble: 6 x 5
##   Plot  N     Year  Block Yield
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
## 1 33    fn150 1983  1      4.54
## 2 98    fn150 1983  3      4.18
## 3 162   fn150 1983  2      3.7 
## 4 33    fn150 1984  1      4.57
## 5 98    fn150 1984  3      5.04
## 6 162   fn150 1984  2      5.06
# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Year + N + N:Year + Block:Year,
           random = ~ 1|Plot, data = dataset)
anova(mod)
##             numDF denDF  F-value p-value
## (Intercept)     1   116 5434.427  &amp;lt;.0001
## Block           2     4    0.300  0.7564
## Year           29   116   36.496  &amp;lt;.0001
## N               2     4    1.304  0.3664
## Year:N         58   116    1.663  0.0105
## Block:Year     58   116    2.545  &amp;lt;.0001
# Implementation with asreml (the residual statement is unnecessary, here)
# Need to sort the data according to the residual statement
# datasetS &amp;lt;- dataset %&amp;gt;% 
#  arrange(Plot, Year)
# mod &amp;lt;- asreml(Yield ~ Block + Year + N + N:Year + Block:Year,
#           random = ~ Plot, 
#           residual = ~ Plot:Year, data = datasetS)
# wald(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is worth to notice that the same model may be fitted in an alternative way, i.e. by dropping the BLOCK:PLOT random effects and using the residual term BLOCK:PLOT:YEAR to accommodate the CS structure into the model. This may be done very intuitively with ‘asreml’, by using the following notation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Code not run
# mod &amp;lt;- asreml(Yield ~ Block + Year + N + N:Year + Block:Year,
#           residual = ~ Plot:cor(Year), data = datasetS)
# summary(mod)$varcomp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the final command returns the correlation between observations in the same plot. With ‘lme’ package, the notation is different, as we have to switch from the ‘lme()’ to the ‘gls()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- gls(Yield ~ Block + Year + N + N:Year + Block:Year,
           correlation = corCompSymm(form = ~1|Plot), data = dataset)
mod$modelStruct$corStruct
## Correlation structure of class corCompSymm representing
##       Rho 
## 0.1269864&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This alternative coding can be used to implement different correlation structures for the cases when a simple CS correlation structure is not satisfactory. For example, when the observations close in time are more correlated than those distant in time, we can implement a serial correlation structure by appropriately changing the ‘residual’ argument in ‘asreml()’ or the ‘correlation’ argument in ‘lme()’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2.-ltes-with-different-rotations-of-the-same-length-and-one-test-crop-per-rotation-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2. LTEs with different rotations of the same length and one test crop per rotation cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in five types of two-year rotations, with either pea (&lt;em&gt;Pisum sativum&lt;/em&gt; L), grain sorghum (&lt;em&gt;Sorghum bicolor&lt;/em&gt; (L.) Moench), sugar beet (&lt;em&gt;Beta vulgaris&lt;/em&gt; L. subsp. &lt;em&gt;saccharifera&lt;/em&gt;), sunflower (&lt;em&gt;Helianthus annuus&lt;/em&gt; L.) and faba bean (&lt;em&gt;Vicia faba&lt;/em&gt; L. subsp. &lt;em&gt;minor&lt;/em&gt;). For each rotation, there are two possible sequences (wheat in odd years and wheat in even years) and the ten combinations (five rotations by two sequences) are completely randomised to ten plots per each of three blocks (Figure 2). Therefore, five wheat plots out of the available ten plots are used from each block and year, for a total of 450 observations, from 1983 to 2012. The experimental design for one block is shown in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure2.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example we have two crops in a rotation and both crops are grown in different plots in the same year. Thus, for each rotation, we have two sequences in time (e.g., maize-sunflower and sunflower-maize). If we consider only one of the two crops, the main difference with respect to Example 1 is that the data obtained in two consecutive years for the same treatment and block are independent, in the sense that they are obtained in different plots. Otherwise, data obtained in a two-year interval (on different rotation cycles) on the same block are correlated, as they originate from the same plot.&lt;/p&gt;
&lt;p&gt;Which is the repeated factor? Indeed, if we look only at one phase in the rotation (in this case wheat), we note that observations are repeated every second year on the same plot (Figure above), according to the sequence they belong to. In other words, observations are repeated on each rotation cycle (CYC; two years) in the same plot, while there is neither a within-cycle repetition nor a within-cycle phase difference: we have only one observation per plot per cycle. Therefore, it is natural to take the rotation cycle as the repeated factor (CYC instead of YEAR).&lt;/p&gt;
&lt;p&gt;As the next step, we should look at what happens in one fixed level of CYC: what did we randomize to the ten plots in a two-years time slot? It is clear that, considering only wheat, we randomised each combination of rotation (ROT) and positioning in the sequence (SEQ; i.e. wheat as the first crop of the sequence and wheat as the second crop of the sequence). Therefore, the treatment factors are ROT and SEQ. Now we can cross the repeated factor with the treatment factors. Accordingly, The model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ SEQ*ROT + BLOCK + CYC + ROT:CYC + BLOCK:CYC
RANDOM: BLOCK:PLOT + BLOCK:PLOT:CYC &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code below shows how to fit the model with R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE2.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor))
head(dataset)
## # A tibble: 6 x 8
##   Block Main  Plot  Rot   Year  Sequence Cycle Yield
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
## 1 1     1_1   4     SBW   1983  1        1      5.1 
## 2 3     3_1   70    SBW   1983  1        1      4.5 
## 3 2     2_1   135   SBW   1983  1        1      4.53
## 4 1     1_0   27    SBW   1984  0        1      5.83
## 5 3     3_0   95    SBW   1984  0        1      6.26
## 6 2     2_0   160   SBW   1984  0        1      6.22
# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Rot*Sequence + Block:Sequence +
             Cycle + Cycle:Sequence + Rot:Cycle + 
             Rot:Sequence:Cycle + Cycle:Block +
             Cycle:Block:Sequence,
             random = ~1|Plot,
           data = dataset)
anova(mod)
##                      numDF denDF   F-value p-value
## (Intercept)              1   224 115809.71  &amp;lt;.0001
## Block                    2    16     42.68  &amp;lt;.0001
## Rot                      4    16      4.19  0.0165
## Sequence                 1    16     26.14  0.0001
## Cycle                   14   224    133.43  &amp;lt;.0001
## Rot:Sequence             4    16      3.60  0.0283
## Block:Sequence           2    16      2.24  0.1384
## Sequence:Cycle          14   224    119.86  &amp;lt;.0001
## Rot:Cycle               56   224      1.74  0.0025
## Block:Cycle             28   224      8.77  &amp;lt;.0001
## Rot:Sequence:Cycle      56   224      1.61  0.0081
## Block:Sequence:Cycle    28   224      6.85  &amp;lt;.0001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach is commonly suggested in literature (see Yates, 1954) and it is convenient, mainly because the resulting model is orthogonal and may be fitted by ordinary least squares. Indeed, for Dataset 2 (and similar experiments), there is only one observation for each block, treatment, cycle, sequence and no missing data (in our case: 3 blocks x 5 rotations x 2 sequences x 15 cycles = 450 observations). The phase should not enter into this model, as we are looking only at one of the two crops (only one phase).&lt;/p&gt;
&lt;p&gt;However, the drawback is that such an approach cannot be immediately extended to the other more complex examples (e.g. rotations with different lengths and/or with a different number of test-crops). Furthermore, the effect of years is partitioned into three components, i.e. ‘cycles’, ‘sequences’ and ‘cycle x sequences’, which might make modeling possible ‘fertility’ trends over time less immediate. In this respect, we should note that possible differences between sequences for a given cycle (wheat as the first crop of the sequence and wheat as the second crop of the sequence, i.e. wheat in even years and wheat in odd years) do not carry any meaning that helps understand the behaviour of rotations.&lt;/p&gt;
&lt;p&gt;An alternative and more natural approach is to take the year as the repeated factor; indeed, for Example 2, the year effect is totally confounded with the factorial combination of ‘cycle’ and ‘sequence’ (15 cycles x 2 sequences = 30 years). If we consider the YEAR as the repeated factor, in one year the treatment model is composed only by the rotation (ROT), that is allocated to plots (PLOT), within blocks. The block model for one year is BLOCK/PLOT = BLOCK + BLOCK:PLOT. We can combine the treatment model with the repeated factor (ROT:YEAR) and add the term BLOCK:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ ROT + BLOCK + YEAR + ROT:YEAR + BLOCK:YEAR
RANDOM: BLOCK:PLOT + BLOCK:PLOT:YEAR &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from random effects for randomisation units, this model is totally similar to the one used for multi-environment genotype experiments and, represents a convenient and clear platform for the analyses of LTE data. We remind the reader that the residual term (BLOCK:PLOT:YEAR) does not need to be explicitly coded when implementing the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Rot + Year + Rot:Year + Block:Year,
             random = ~1|Plot,
           data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us a good common modelling platform for all datasets, although it may be argued that the models are no longer orthogonal, as not all plots produce data in all years. It should be recognised, however, that the lack of orthogonality can easily be accommodated within mixed models.&lt;/p&gt;
&lt;p&gt;The situation is totally different if we look at both the phases of the rotation (e.g. wheat and sunflower): in this case, we have a phase difference within each cycle and, considering one level of the repeated factor year, the treatment model should contain both the rotation and the phase, together with their interaction. When we introduce the year (steps 5 and 6 above), we also introduce the interactions ‘year x rotation’, ‘year x phase’ and ‘year x rotation x phase’, which are all meaningful when studying the behaviour of rotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3.-ltes-with-a-fixed-rotation-one-test-crop-per-cycle-and-different-treatments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3. LTEs with a fixed rotation (one test crop per cycle) and different treatments&lt;/h2&gt;
&lt;p&gt;Durum wheat (&lt;em&gt;Triticum durum&lt;/em&gt; L.) is grown in a two-year rotation with a spring crop and nine cropping systems, consisting of the factorial combination of three soil tillage methods (CT: conventional 40 cm deep ploughing; M: scarification at 25 cm; S: sod seeding with chemical desiccation and chopping) and three N-fertilisation levels (N0, N90 and N180, corresponding to 0, 90 and 180 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;). The two possible rotation sequences (wheat-spring crop and spring crop-wheat) are arranged in two adjacent fields, which therefore host the two different crops of the rotation in the same year. Within the two fields, there are two independent randomisations, each with two blocks, tillage levels randomised to main-plots (1500 &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;) and N levels randomised to sub-plots (500 &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), according to a split-plot design with two replicates. The design is taken from Seddaiu et al. (2016), while the data have been simulated by using Monte Carlo methods.&lt;/p&gt;
&lt;p&gt;This type of LTE is very similar to the previous one, though we have a different experimental layout:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;there are two experimental treatments, laid out in a split-plot design;&lt;/li&gt;
&lt;li&gt;the two sequences are accommodated in two fields.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The experimental design for Dataset 3, for each of two fields in one year is reported in the figure below. The position of wheat and spring crop is exchanged in the following year (CT: conventional ploughing; M: scarification; S: sod seeding; 0: no nitrogen fertilisation; 1: 90 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;; 2: 180 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure3.jpg&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before proceeding to model building for Example 3, we need to discuss whether valid analyses are possible at each single level of the repeated factor. Indeed, this is clearly true if we take the year as the repeated factor and consider only one of the two crops in the rotation (wheat, in this case). However, if we intended to consider both crops and compare e.g. their yields, the crop effect would be confounded with the field effect within a single year and, therefore, valid within-year analyses would not be possible. In this case, we should resort to taking the rotation cycle as the repeated factor.&lt;/p&gt;
&lt;p&gt;Dealing only with wheat, we can therefore take the YEAR as the repeated factor and consider that, in one year, the randomised treatment factors are tillage (T) and nitrogen fertilisation (N) and the treatment model is T + N + T:N.&lt;/p&gt;
&lt;p&gt;The block factors are the FIELDS, the BLOCKS within fields, the MAIN plots within blocks and the subplots (SUB) within main plots. The block model (for one year) is FIELD + FIELD:BLOCK + FIELD:BLOCK:MAIN + FIELD:BLOCK:MAIN:SUB.&lt;/p&gt;
&lt;p&gt;The treatment and repeated model can be combined as: (T + N + T:N)*YEAR = T + N + T:N + YEAR + T:YEAR + N:YEAR + T:N:YEAR.&lt;/p&gt;
&lt;p&gt;At this stage, the FIELD main effect needs to be removed, as it is totally confounded with the years. We assign a random effect to the other randomisation units, i.e. FIELD:BLOCK, FIELD:BLOCK:MAIN and FIELD:BLOCK:MAIN:SUB and combine these random terms with the repeated factor YEAR, by using the colon operator, which leads us to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ T + N + T:N + YEAR + T:YEAR + N:YEAR + T:N:YEAR
RANDOM: FIELD:BLOCK + FIELD:BLOCK:MAIN + FIELD:BLOCK:MAIN:SUB + FIELD:BLOCK:YEAR + FIELD:BLOCK:MAIN:YEAR + FIELD:BLOCK:MAIN:SUB:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual, the last term (residual) does not need to be explicitly included when implementing the model, but it can be used, together with the two previous ones (FIELD:BLOCK:YEAR + FIELD:BLOCK:MAIN:YEAR) to accommodate possible serial correlation structures into the model, by allowing year-specifity of all design effects and the residuals. For this types of models with several crossed random effects, the coding of ‘lme()’ is not straightforward and it does not always lead to a flexible implementation of correlation structures.&lt;/p&gt;
&lt;p&gt;In the code below we need to build dummy variables for all random effects (five variables, excluding the residual error term, which is not needed). Afterwards, we have to code the random effects as a list; R expects that the element of such list are nested and, therefore, we need to work around this by coding an additional variable, which takes the value of ‘1’ for all subjects, so that the nesting structure is only artificial. We use the ‘pdIdent’ construct to say that each random effect is homoscedastic and uncorrelated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE3.csv&amp;quot;)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:9), factor)) %&amp;gt;% 
  mutate(FB = factor(Block:Field),
         FBM = factor(FB:Main),
         FBMS = factor(FBM:Sub),
         FBY = factor(FB:Year),
         FBMY = factor(FBM:Year),
         one = 1L)

mod &amp;lt;- lme(Yield ~ T + N + N:T + 
                 Year + Year:T + Year:N + Year:N:T,
                 random = list(one = pdIdent(~FB - 1),
                               one = pdIdent(~FBM - 1),
                               one = pdIdent(~FBMS - 1),
                               one = pdIdent(~FBY - 1),
                               one = pdIdent(~FBMY - 1)),
               data = dataset, na.action = na.omit)

# Fixed effects tested by using LRT
library(car)
Anova(mod)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##             Chisq Df Pr(&amp;gt;Chisq)    
## T          5.1169  2    0.07743 .  
## N        804.1717  2  &amp;lt; 2.2e-16 ***
## Year     446.6794 17  &amp;lt; 2.2e-16 ***
## T:N        6.2105  4    0.18397    
## T:Year   142.5132 34  3.065e-15 ***
## N:Year   246.5584 34  &amp;lt; 2.2e-16 ***
## T:N:Year 159.3230 68  2.705e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fit takes a long time and it is not easy to manipulate the model to introduce correlation structures for the random effects. However, it is not difficult to introduce the correlation of residuals, by using the ‘correlation’ argument (see above).&lt;/p&gt;
&lt;p&gt;Coding the same model with ‘asreml()’ is easier and so is to introduce patterned correlations structures. However, the design has to be balanced and, therefore, we need to introduce NAs for missing observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Asreml fit (not run)
# datasetS &amp;lt;- dataset %&amp;gt;% 
#  arrange(Sub, Year)
# mod2 &amp;lt;- asreml(Yield ~ T + N + N:T + 
#                 Year + Year:T + Year:N + Year:N:T,
#                 random = ~FB + FB:Main + FB:Main:Sub + 
#                 FB:Year + FB:Main:Year,
#               residual = ~ Sub:Year,
#               data = datasetS)
# summary(mod2)$varcomp&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4.-lte-with-a-fixed-rotation-different-treatments-and-more-than-one-phase-per-crop-and-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4. LTE with a fixed rotation, different treatments and more than one phase per crop and cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in a three-year rotation maize-wheat-wheat, under two types of management of crop residues (burial and removal), which are randomised to main plots, while the three possible rotation sequences are randomised to subplots. This experiment has 18 plots (three sequences x two treatment levels x three blocks) and, in every year, 12 of those are cropped with wheat and six with maize.&lt;/p&gt;
&lt;p&gt;Also in this case, the response variable is wheat yield from 1983 to 2012, i.e. twelve observations per year and 360 observations in total. Data obtained in the same plot in different years belong to two different phases (wheat after maize and wheat after wheat; the experimental design for one block is shown in the figure below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/_Figures/Stat_lte_ModelBuildingFigure4.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With respect to Example 3, the situation becomes more complex, because we have two distinct phases in each rotation cycle (phase difference: wheat after maize and wheat after wheat). As usual, we start by regarding the YEAR as the repeated factor. In one year, the treatment factors are the management of soil residues (RES, that is randomly allocated to main-plots) and the phases (P; randomly allocated to subplots); the treatment model is indeed: RES*P.&lt;/p&gt;
&lt;p&gt;In one year, the block model is: BLOCK/MAIN/SUB = BLOCK + BLOCK:MAIN + BLOCK:MAIN:SUB. Introducing the YEAR as repeated factor, we can combine the treatment model with the repeated model as: RES + P + &lt;a href=&#34;RES:P&#34; class=&#34;uri&#34;&gt;RES:P&lt;/a&gt; + YEAR + &lt;a href=&#34;RES:YEAR&#34; class=&#34;uri&#34;&gt;RES:YEAR&lt;/a&gt; + P:YEAR + &lt;a href=&#34;RES:P:YEAR&#34; class=&#34;uri&#34;&gt;RES:P:YEAR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The terms BLOCK:MAIN and BLOCK:MAIN:SUB reference randomisation units and should receive random effects. The blocks may interact with the years (BLOCK:YEAR), while the random effects for randomisation units can be made year-specific by adding BLOCK:MAIN:YEAR and the residual term BLOCK:MAIN:SUB:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ RES + P + RES:P + YEAR + RES:YEAR + P:YEAR + RES:P:YEAR + BLOCK + BLOCK:YEAR
RANDOM: BLOCK:MAIN + BLOCK:MAIN:SUB + BLOCK:MAIN:YEAR + BLOCK:MAIN:SUB:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE4.csv&amp;quot;)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor)) %&amp;gt;% 
  mutate(Main = factor(Block:Res),
         Sub = factor(Main:Sub))

mod &amp;lt;- lme(Yield ~ Block + Res*P + 
              Year + Year:Res + Year:P + Year:Res:P +
              Block:Year, 
           random = list(Main = pdIdent(~1),
                         Main = pdIdent(~Sub - 1),
                         Main = pdIdent(~Year - 1)),
           data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-5-ltes-with-several-rotations-of-different-lengths-and-different-number-of-phases-per-crop-and-rotation-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 5: LTEs with several rotations of different lengths and different number of phases per crop and rotation cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in five maize (M) - wheat (W) rotations of different lengths, i.e. M-W, M-W-W, M-W-W-W, M-W-W-W-W, M-W-W-W-W-W. For all rotations, all phases are contemporarily present in each year, for a total of 20 plots (one for each of the possible sequences, i.e. 2 + 3 + 4 + 5 + 6 = 20) in each of three blocks. Considering wheat yield as the response variable, we find that only 15 observations are obtained in each year, for a total of 1350 data, from 1983 to 2012.&lt;/p&gt;
&lt;p&gt;Experiments of this type represent a high degree of complexity. Indeed, in contrast to all other examples, after 30 years there are plots with: (i) a different number of observations for the same test crop; (ii) a different number of cycles (in some cases the last cycle is also incomplete); (iii) a different number of phases for wheat.&lt;/p&gt;
&lt;p&gt;In some cases, it is necessary to compare several rotations with different characteristics (e.g. a different duration and/or a different number of tests crops and /or a different number of phases per crop), which may create a complex design with some degree of non-orthogonality.&lt;/p&gt;
&lt;p&gt;The repeated factor is again the YEAR. In one year, the treatment factors are the rotation system (ROT) and the rotation phase (P), which are randomly allocated to plots. As there is a different number of phases for each rotation, we nest the phase within the rotation, leading to the following treatment model: ROT/P = ROT + P:ROT.&lt;/p&gt;
&lt;p&gt;In one year, the block model is BLOCK/PLOT = BLOCK + BLOCK:PLOT. The repeated factor is included and combined with the treatment model, by introducing YEAR + ROT:YEAR + ROT:P:YEAR.&lt;/p&gt;
&lt;p&gt;The term BLOCK:PLOT references randomisation units and needs to receive a random effect, while the term YEAR:BLOCK can be added to the model, together with the residual term BLOCK:PLOT:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ ROT + P:ROT + YEAR + ROT:YEAR + ROT:P:YEAR
RANDOM: BLOCK:PLOT + BLOCK:PLOT:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the implementation below we did not succeed reaching convergence with ‘lme()’, but we were successful ‘with asreml()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE5.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor))

# mod &amp;lt;- asreml(Yield ~ Block + Rot + Rot:P + 
#                 Year + Year:Rot + Year:Rot:P + Block:Year,
#               random = ~Plot,
#               data = dataset)

# mod &amp;lt;- lme(Yield ~ Block + Rot + Rot:P + 
#                Year + Year:Rot + Year:Rot:P + Block:Year, 
#                random=~1|Plot,
#              data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Warning!&lt;/h1&gt;
&lt;p&gt;Models 1 to 5 are fairly similar and they are very closely related to those used for multi-environment experiments. However, there are some peculiar aspects which needs to be taken under consideration.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Apart from Dataset 1, there is always a certain degree of unbalance, as plots do not produce data every year. Testing for fixed effects requires great care.&lt;/li&gt;
&lt;li&gt;In all cases, the model formulations shown above induce a compound symmetry correlation structure for observations taken in the same plot over time (‘split-plot in time’). This is seldom appropriate and, thus, more complex correlations structures should be considered.&lt;/li&gt;
&lt;li&gt;In the above formulations, only randomisation units have been given a random effect, while all the other effects have been regarded as fixed. Obviously, depending on the aims of the analyses, it might be convenient and appropriate to regard some of these effects (e.g., the year main effect and interactions) as random.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It’s all, thanks for reading this far. If you have any comments, please, drop me a note at &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;. Best wishes,&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Brien, C.J., Demetrio, C.G.B., 2009. Formulating mixed models for experiments, including longitudinal experiments. Journal of Agricultural, Biological and Environmental Statistics 14, 253–280.&lt;/li&gt;
&lt;li&gt;David Butler (2019). asreml: Fits the Linear Mixed Model. R package version 4.1.0.110. www.vsni.co.uk&lt;/li&gt;
&lt;li&gt;Cochran, W.G., 1939. Long-Term Agricultural Experiments. Supplement to the Journal of the Royal Statistical Society 6, 104–148.&lt;/li&gt;
&lt;li&gt;Onofri, A., Seddaiu, G., Piepho, H.-P., 2016. Long-Term Experiments with cropping systems: Case studies on data analysis. European Journal of Agronomy 77, 223–235.&lt;/li&gt;
&lt;li&gt;Patterson, H.D., 1964. Theory of Cyclic Rotation Experiments. Journal of the Royal Statistical Society. Series B (Methodological) 26, 1–45.&lt;/li&gt;
&lt;li&gt;Payne, R.W., 2015. The design and analysis of long-term rotation experiments. Agronomy Journal 107, 772–785.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Richter, C., 2004. A Mixed Modelling Approach for Randomized Experiments with Repeated Measures. Journal of Agronomy and Crop Science 190, 230–247.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2019). &lt;em&gt;nlme: Linear and Nonlinear Mixed Effects Models&lt;/em&gt;. R package version 3.1-142, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Seddaiu, G., Iocola, I., Farina, R., Orsini, R., Iezzi, G., Roggero, P.P., 2016. Long term effects of tillage practices and N fertilization in rainfed Mediterranean cropping systems: durum wheat, sunflower and maize grain yield. European Journal of Agronomy 77, 166–178. &lt;a href=&#34;https://doi.org/10.1016/j.eja.2016.02.008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.eja.2016.02.008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34; class=&#34;uri&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting complex mixed models with nlme. Example #5</title>
      <link>https://www.statforbiology.com/2020/stat_met_jointreg/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_met_jointreg/</guid>
      <description>


&lt;div id=&#34;a-joint-regression-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Joint Regression model&lt;/h1&gt;
&lt;p&gt;Let’s talk about a very old, but, nonetheless, useful technique. It is widely known that the yield of a genotype in different environments depends on environmental covariates, such as the amount of rainfall in some critical periods of time. Apart from rain, also temperature, wind, solar radiation, air humidity and soil characteristics may concur to characterise a certain environment as good or bad and, ultimately, to determine yield potential.&lt;/p&gt;
&lt;p&gt;Early in the 60s, several authors proposed that the yield of genotypes is expressed as a function of an environmental index &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt;, measuring the yield potential of each environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (Finlay and Wilkinson, 1963; Eberhart and Russel, 1966; Perkins and Jinks, 1968). For example, for a genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, we could write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu_i + \beta_i e_j\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the yield &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in a certain environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is expressed as a linear function of the environmental index &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is the slope, which expresses how the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; responds to the environment.&lt;/p&gt;
&lt;p&gt;A graphical example may be useful; in the figure below we have two genotypes tested in 10 environments. The yield of the first genotype (red) increases as the environmental index increases, with slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 0.81\)&lt;/span&gt;. On the other hand, the yield of the second genotype (blue) does not change much with the environment (&lt;span class=&#34;math inline&#34;&gt;\(\beta_2 = -0.08)\)&lt;/span&gt;. Clearly, a high value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; demonstrates that the genotype is responsive to the environment and makes profit of favorable conditions. Otherwise, a low &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; value (close to 0) demonstrates that the genotype is not responsive and tends to give more or less the same yield in all environments (static stability; Wood, 1976).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_met_JointReg_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By now, it should be clear that &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a relevant measure of stability. Now, the problem is: how do we determine such value from a multi-environment genotype experiment? As usual, let’s start from a meaningful example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-multi-environment-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A multi-environment experiment&lt;/h1&gt;
&lt;p&gt;Let’s take the data in Sharma (2006; Statistical And Biometrical Techniques In Plant Breeding, New Age International ltd. New Delhi, India). They refer to a multi-environment experiment with 7 genotypes, 6 environments and 3 blocks; let’s load the data in the dataframe ‘dataFull’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(emmeans)
## Welcome to emmeans.
## NOTE -- Important change from versions &amp;lt;= 1.41:
##     Indicator predictors are now treated as 2-level factors by default.
##     To revert to old behavior, use emm_options(cov.keep = character(0))
Block &amp;lt;- factor(rep(c(1:3), 42))
Var &amp;lt;- factor(rep(LETTERS[1:7],each=18))
Loc &amp;lt;- factor(rep(rep(letters[1:6], each=3), 7))
P1 &amp;lt;- factor(Loc:Block)
Yield &amp;lt;- c(60,65,60,80,65,75,70,75,70,72,82,90,48,45,50,50,40,40,
           80,90,83,70,60,60,85,90,90,70,85,80,40,40,40,38,40,50,
           25,28,30,40,35,35,35,30,30,40,35,35,35,25,20,35,30,30,
           50,65,50,40,40,40,48,50,52,45,45,50,50,50,45,40,48,40,
           52,50,55,55,54,50,40,40,60,48,38,45,38,30,40,35,40,35,
           22,25,25,30,28,32,28,25,30,26,28,28,45,50,45,50,50,50,
           30,30,25,28,34,35,40,45,35,30,32,35,45,35,38,44,45,40)
dataFull &amp;lt;- data.frame(Block, Var, Loc, Yield)
rm(Block, Var, Loc, P1, Yield)
head(dataFull)
##   Block Var Loc Yield
## 1     1   A   a    60
## 2     2   A   a    65
## 3     3   A   a    60
## 4     1   A   b    80
## 5     2   A   b    65
## 6     3   A   b    75&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-an-environmental-index&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is an environmental index?&lt;/h1&gt;
&lt;p&gt;First of all, we need to define an environmental index, which can describe the yield potential in each of the seven environments. Yates and Cochran (1937) proposed that we use the mean of all observations in each environment, expressed as the difference between the environmental mean yield &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and the overall mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (i.e. &lt;span class=&#34;math inline&#34;&gt;\(e_j = \mu_j - \mu\)&lt;/span&gt;). Let’s do it; in the box below we use the package ‘dplyr’ to augment the dataset with a new variable, representing the environmental indices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
dataFull &amp;lt;- dataFull %&amp;gt;%
  group_by(Loc) %&amp;gt;% 
  mutate(ej = mean(Yield) - mean(dataFull$Yield))
head(dataFull)
## # A tibble: 6 x 5
## # Groups:   Loc [2]
##   Block Var   Loc   Yield    ej
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 1     A     a        60 1.45 
## 2 2     A     a        65 1.45 
## 3 3     A     a        60 1.45 
## 4 1     A     b        80 0.786
## 5 2     A     b        65 0.786
## 6 3     A     b        75 0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This step is ok with balanced data and it is clear that a high environmental index identifies the favorable environments, while a low (negative) environmental index identifies unfavorable environments. It is necessary to keep in mind that we have unwillingly put a constraint on &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; values, that have to sum up to zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-model-definition-equation-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Full model definition (Equation 1)&lt;/h1&gt;
&lt;p&gt;Now, it is possible to regress the yield data for each genotype against the environmental indices, according to the following joint regression model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \gamma_{jk} + \mu_i + \beta_i e_j + d_{ij} + \varepsilon_{ijk} \quad\quad\quad \textrm{(Equation 1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where: &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; is the yield for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of blocks within environments and &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the average yield for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. As we have seen in the figure above, the average yield of a genotype in each environment cannot be exactly described by the regression against the environmental indices (in other words: the observed means do not lie along the regression line). As the consequence, we need the random term &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; to represent the deviation from the regression line for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Finally, the random elements &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; represent the deviations between the replicates for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (within-trial errors). As I said, &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; are random, with variances equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;According to Finlay-Wilkinson, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; is assumed to be equal for all genotypes. Otherwise, according to Eberarth-Russel, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d}\)&lt;/span&gt; may assume a different value for each genotype (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d(i)}\)&lt;/span&gt;) and may become a further measure of stability: if this is small, a genotype does not show relevant variability of yield, apart from that due to the regression against the environmental indices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;We can start the analyses by fitting a traditional ANOVA model, to keep as a reference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov &amp;lt;- lm(Yield ~ Loc/Block + Var*Loc, data = dataFull)
anova(mod.aov)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Loc        5  1856.0   371.2  17.9749 1.575e-11 ***
## Var        6 20599.2  3433.2 166.2504 &amp;lt; 2.2e-16 ***
## Loc:Block 12   309.8    25.8   1.2502    0.2673    
## Loc:Var   30 12063.6   402.1  19.4724 &amp;lt; 2.2e-16 ***
## Residuals 72  1486.9    20.7                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said, Equation 1 is a mixed model, which calls for the use of the ‘lme()’ function. For better understanding, it is useful to start by augmenting the previous ANOVA model with the regression term (‘Var/ej’). We use the nesting operator, to have different regression lines for each level of ‘Var’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Augmented ANOVA model
mod.aov2 &amp;lt;- lm(Yield ~ Loc/Block + Var/ej + Loc:Var, data=dataFull)
anova(mod.aov2)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Loc        5  1856.0   371.2  17.9749 1.575e-11 ***
## Var        6 20599.2  3433.2 166.2504 &amp;lt; 2.2e-16 ***
## Loc:Block 12   309.8    25.8   1.2502    0.2673    
## Var:ej     6  9181.2  1530.2  74.0985 &amp;lt; 2.2e-16 ***
## Loc:Var   24  2882.5   120.1   5.8159 2.960e-09 ***
## Residuals 72  1486.9    20.7                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the GE interaction in the ANOVA model has been decomposed into two parts: the regression term (‘Var/ej’) and the deviation from regression (‘Loc:Var’), with 6 and 24 degrees of freedom, respectively. This second term corresponds to &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; in Equation 1 (please, note that the two terms ‘Var/ej’ and ‘Loc:Var’ are partly confounded).&lt;/p&gt;
&lt;p&gt;The above analysis is only useful for teaching purposes, but it is unsatisfactory, because the &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; terms have been regarded as fixed, which is pretty illogical. Therefore, we change the fixed effect model into a mixed model, where we include the random ‘genotype by environment’ interaction. We also change the fixed block effect into a random effect and remove the intercept, to more strictly adhere to the parameterisation of Equation 1. The two random effects ‘Loc:Block’ and ‘Loc:Var’ are not nested into each other and we need to code them by using ‘pdMat’ constructs, which are not straightforward. You can use the code in the box below as a guidance to fit a Finlay-Wilkinson model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finlay-Wilkinson model
modFull1 &amp;lt;- lme(Yield ~ Var/ej - 1, 
                random = list(Loc = pdIdent(~ Var - 1),
                              Loc = pdIdent(~ Block - 1)), 
                data=dataFull)
summary(modFull1)$tTable
##              Value Std.Error  DF    t-value      p-value
## VarA    63.1666667 2.4017164 107 26.3006350 1.624334e-48
## VarB    66.1666667 2.4017164 107 27.5497417 2.135264e-50
## VarC    31.8333333 2.4017164 107 13.2544097 2.599693e-24
## VarD    47.1111111 2.4017164 107 19.6156012 3.170228e-37
## VarE    44.7222222 2.4017164 107 18.6209421 2.378452e-35
## VarF    34.2777778 2.4017164 107 14.2722004 1.614127e-26
## VarG    35.8888889 2.4017164 107 14.9430169 6.028635e-28
## VarA:ej  3.2249875 0.6257787 107  5.1535588 1.176645e-06
## VarB:ej  4.7936139 0.6257787 107  7.6602379 8.827229e-12
## VarC:ej  0.4771074 0.6257787 107  0.7624219 4.474857e-01
## VarD:ej  0.3653064 0.6257787 107  0.5837629 5.606084e-01
## VarE:ej  1.2369950 0.6257787 107  1.9767291 5.064533e-02
## VarF:ej -2.4316943 0.6257787 107 -3.8858692 1.770611e-04
## VarG:ej -0.6663160 0.6257787 107 -1.0647790 2.893729e-01
VarCorr(modFull1)
##          Variance           StdDev   
## Loc =    pdIdent(Var - 1)            
## VarA     27.5007919         5.2441197
## VarB     27.5007919         5.2441197
## VarC     27.5007919         5.2441197
## VarD     27.5007919         5.2441197
## VarE     27.5007919         5.2441197
## VarF     27.5007919         5.2441197
## VarG     27.5007919         5.2441197
## Loc =    pdIdent(Block - 1)          
## Block1    0.4478291         0.6692003
## Block2    0.4478291         0.6692003
## Block3    0.4478291         0.6692003
## Residual 20.8781458         4.5692610&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the variance component &lt;span class=&#34;math inline&#34;&gt;\(\sigma_d\)&lt;/span&gt; (27.50) is the same for all genotypes; if we want to let a different value for each genotype (Eberarth-Russel model), we need to change the ‘pdMat’ construct for the ‘Loc:Var’ effect, turning from ‘pdIdent’ to ‘pdDiag’, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eberhart-Russel model
modFull2 &amp;lt;- lme(Yield ~ Var/ej - 1, 
                random = list(Loc = pdDiag(~ Var - 1),
                              Loc = pdIdent(~ Block - 1)), 
                data=dataFull)
summary(modFull2)$tTable
##              Value Std.Error  DF    t-value      p-value
## VarA    63.1666667 3.0507629 107 20.7052032 3.221930e-39
## VarB    66.1666667 2.7818326 107 23.7852798 1.604422e-44
## VarC    31.8333333 1.7240721 107 18.4640387 4.753742e-35
## VarD    47.1111111 2.3526521 107 20.0246824 5.564350e-38
## VarE    44.7222222 2.4054296 107 18.5921974 2.699536e-35
## VarF    34.2777778 1.9814442 107 17.2993906 8.947485e-33
## VarG    35.8888889 2.2617501 107 15.8677515 7.076551e-30
## VarA:ej  3.2249875 0.7948909 107  4.0571447 9.466174e-05
## VarB:ej  4.7936139 0.7248198 107  6.6135249 1.522848e-09
## VarC:ej  0.4771074 0.4492152 107  1.0620909 2.905857e-01
## VarD:ej  0.3653064 0.6129948 107  0.5959372 5.524757e-01
## VarE:ej  1.2369950 0.6267462 107  1.9736777 5.099652e-02
## VarF:ej -2.4316943 0.5162748 107 -4.7100774 7.473942e-06
## VarG:ej -0.6663160 0.5893098 107 -1.1306718 2.607213e-01
VarCorr(modFull2)
##          Variance           StdDev   
## Loc =    pdDiag(Var - 1)             
## VarA     48.7341240         6.9809830
## VarB     39.3227526         6.2707856
## VarC     10.7257438         3.2750181
## VarD     26.1010286         5.1089166
## VarE     27.6077467         5.2543074
## VarF     16.4479246         4.0556041
## VarG     23.5842788         4.8563648
## Loc =    pdIdent(Block - 1)          
## Block1    0.4520678         0.6723599
## Block2    0.4520678         0.6723599
## Block3    0.4520678         0.6723599
## Residual 20.8743411         4.5688446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the regression slopes we see that the genotypes A and B are the most responsive to the environment (&lt;span class=&#34;math inline&#34;&gt;\(\beta_A = 3.22\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_B = 4.79\)&lt;/span&gt;, respectively), while the genotypes C and D are stable in a static sense, although their average yield is pretty low.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-joint-regression-model-in-two-steps-equation-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a joint regression model in two-steps (Equation 2)&lt;/h1&gt;
&lt;p&gt;In the previous analyses we used the plot data to fit a joint regression model. In order to reduce the computational burden, it may be useful to split the analyses in two-steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;we analyse the plot data, to retrieve the means for the ‘genotype by environment’ combinations;&lt;/li&gt;
&lt;li&gt;we fit the joint regression model to those means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results of the two approaches are not necessarily the same, as some information in the first step is lost in the second. Several weighing schemes have been proposed to make two-steps fitting more reliable (Möhring and Piepho, 2009); in this example, I will show an unweighted two-steps analyses, which is simple, but not necessarily the best way to go.&lt;/p&gt;
&lt;p&gt;A model for the second step is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu_i + \beta_i e_j + f_{ij} \quad\quad\quad \textrm{(Equation 2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the residual random component &lt;span class=&#34;math inline&#34;&gt;\(f_{ij}\)&lt;/span&gt; is assumed as normally distributed, with mean equal to zero and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f\)&lt;/span&gt;. In general, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f &amp;gt; \sigma^2_d\)&lt;/span&gt;, as the residual sum of squares from Model 2 also contains a component for within trial errors. Indeed, for a balanced experiment, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_{f} = \sigma^2_d + \frac{\sigma^2}{k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the within-trial error, which needs to be obtained from the first step. In the previous analyses we have already fitted an anova model to the whole dataset (‘mod.aov’). In the box below, we make use of the ‘emmeans’ package to retrieve the least squares means for the seven genotypes in all locations. Subsequently, the environmental means are calculated and centered, by subtracting the overall mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emmeans)
muGE &amp;lt;- as.data.frame( emmeans(mod.aov, ~Var:Loc) )[,1:3]
names(muGE) &amp;lt;- c(&amp;quot;Var&amp;quot;, &amp;quot;Loc&amp;quot;, &amp;quot;Yield&amp;quot;)
muGE &amp;lt;- muGE %&amp;gt;% 
  group_by(Loc) %&amp;gt;% 
  mutate(ej = mean(Yield) - mean(muGE$Yield))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we fit Equation 2 to the means. In the code below we assume homoscedasticity and, thus, we are fitting the Finlay-Wilkinson model. The variance component &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; is obtained by subtracting a fraction of the residual variance from the first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finlay-Wilkinson model
modFinlay &amp;lt;- lm(Yield ~ Var/ej - 1, data=muGE)
summary(modFinlay)
## 
## Call:
## lm(formula = Yield ~ Var/ej - 1, data = muGE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.3981 -3.5314 -0.8864  3.7791 11.2045 
## 
## Coefficients:
##         Estimate Std. Error t value Pr(&amp;gt;|t|)    
## VarA     63.1667     2.3915  26.413  &amp;lt; 2e-16 ***
## VarB     66.1667     2.3915  27.668  &amp;lt; 2e-16 ***
## VarC     31.8333     2.3915  13.311 1.24e-13 ***
## VarD     47.1111     2.3915  19.699  &amp;lt; 2e-16 ***
## VarE     44.7222     2.3915  18.701  &amp;lt; 2e-16 ***
## VarF     34.2778     2.3915  14.333 2.02e-14 ***
## VarG     35.8889     2.3915  15.007 6.45e-15 ***
## VarA:ej   3.2250     0.6231   5.176 1.72e-05 ***
## VarB:ej   4.7936     0.6231   7.693 2.22e-08 ***
## VarC:ej   0.4771     0.6231   0.766 0.450272    
## VarD:ej   0.3653     0.6231   0.586 0.562398    
## VarE:ej   1.2370     0.6231   1.985 0.056998 .  
## VarF:ej  -2.4317     0.6231  -3.902 0.000545 ***
## VarG:ej  -0.6663     0.6231  -1.069 0.294052    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 5.858 on 28 degrees of freedom
## Multiple R-squared:  0.9905, Adjusted R-squared:  0.9857 
## F-statistic: 208.3 on 14 and 28 DF,  p-value: &amp;lt; 2.2e-16
sigmaf &amp;lt;- summary(modFinlay)$sigma^2 
sigma2 &amp;lt;- summary(mod.aov)$sigma^2 
sigmaf - sigma2/3 #sigma2_d
## [1] 27.43169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the box below, we allow for different variances for each genotype and, therefore, we fit the Eberarth-Russel model. As before, we can retrieve the variance components &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d(i)}\)&lt;/span&gt; from the fitted model object, by subtracting the within-trial error obtained in the first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eberarth-Russel model
modEberarth &amp;lt;- gls(Yield ~ Var/ej - 1, 
              weights=varIdent(form=~1|Var), data=muGE)
coefs &amp;lt;- summary(modEberarth)$tTable
coefs
##              Value Std.Error    t-value      p-value
## VarA    63.1666667 3.0434527 20.7549360 1.531581e-18
## VarB    66.1666667 2.7653537 23.9270177 3.508778e-20
## VarC    31.8333333 1.7165377 18.5450822 2.912238e-17
## VarD    47.1111111 2.3344802 20.1805574 3.204306e-18
## VarE    44.7222222 2.3899219 18.7128381 2.304763e-17
## VarF    34.2777778 1.9783684 17.3262868 1.685683e-16
## VarG    35.8888889 2.2589244 15.8876005 1.537133e-15
## VarA:ej  3.2249875 0.7929862  4.0668898 3.511248e-04
## VarB:ej  4.7936139 0.7205262  6.6529352 3.218756e-07
## VarC:ej  0.4771074 0.4472521  1.0667527 2.951955e-01
## VarD:ej  0.3653064 0.6082600  0.6005761 5.529531e-01
## VarE:ej  1.2369950 0.6227056  1.9864844 5.684599e-02
## VarF:ej -2.4316943 0.5154734 -4.7174004 6.004832e-05
## VarG:ej -0.6663160 0.5885736 -1.1320862 2.672006e-01
sigma &amp;lt;- summary(modEberarth)$sigma
sigma2fi &amp;lt;- (c(1, coef(modEberarth$modelStruct$varStruct, uncons = FALSE)) * sigma)^2
names(sigma2fi)[1] &amp;lt;- &amp;quot;A&amp;quot;
sigma2fi - sigma2/3 #sigma2_di
##        A        B        C        D        E        F        G 
## 48.69203 38.99949 10.79541 25.81519 27.38676 16.60005 23.73284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting in two steps we obtain the very same result as with fitting in one step, but it ain’t necessarily so.&lt;/p&gt;
&lt;p&gt;I would like to conclude by saying that a joint regression model, the way I have fitted it here, is simple and intuitively appealing, although it has been criticized for a number of reasons. In particular, it has been noted that the environmental indices &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; are estimated from the observed data and, therefore, they are not precisely known. On the contrary, linear regression makes the assumption that the levels of explanatory variables are precisely known and not sampled. As the consequence, our estimates of the slopes &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; may be biased. Furthermore, in our construction we have put some arbitrary constraints on the environmental indices (&lt;span class=&#34;math inline&#34;&gt;\(\sum{e_j} = 0\)&lt;/span&gt;) and on the regression slopes (&lt;span class=&#34;math inline&#34;&gt;\(\sum({\beta_i})/G = 1\)&lt;/span&gt;; where G is the number of genotypes), which are not necessarily reasonable.&lt;/p&gt;
&lt;p&gt;Alternative methods of fitting joint regression models have been proposed (see Piepho, 1998), but they are slightly more complex and I will deal with them in a future post.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Eberhart, S.A., Russel, W.A., 1966. Stability parameters for comparing verieties. Crop Science 6, 36–40.&lt;/li&gt;
&lt;li&gt;Finlay, K.W., Wilkinson, G.N., 1963. The analysis of adaptation in a plant-breeding programme. Australian Journal of Agricultural Research 14, 742–754.&lt;/li&gt;
&lt;li&gt;Möhring, J., Piepho, H.-P., 2009. Comparison of Weighting in Two-Stage Analysis of Plant Breeding Trials. Crop Science 49, 1977–1988.&lt;/li&gt;
&lt;li&gt;Perkins, J.M., Jinks, J.L., 1968. Environmental gentype-environmental components of variability. III. Multiple lines and crosses. Heredity 23, 339–356.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1998. Methods for comparing the yield stability of cropping systems - A review. Journal of Agronomy and Crop Science 180, 193–213.&lt;/li&gt;
&lt;li&gt;Wood, J., 1976. The use of environmental variables in the interpretation of genotype-environment interaction. Heredity 37, 1–7.&lt;/li&gt;
&lt;li&gt;Yates, F., and Cochran G., 1938. The analysis of groups of experiments. Journal of Agricultural Sciences, 28, 556—580.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>AMMI analyses for GE interactions</title>
      <link>https://www.statforbiology.com/2020/stat_met_ammi/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_met_ammi/</guid>
      <description>


&lt;p&gt;The CoViD-19 situation in Italy is little by little improving and I feel a bit more optimistic. It’s time for a new post! I will go back to a subject that is rather important for most agronomists, i.e. the selection of crop varieties.&lt;/p&gt;
&lt;p&gt;All farmers are perfectly aware that crop performances are affected both by the genotype and by the environment. These two effects are not purely additive and they often show a significant interaction. By this word, we mean that a genotype can give particularly good/bad performances in some specific environmental situations, which we may not expect, considering its average behaviour in other environmental conditions. The Genotype by Environment (GE) interaction may cause changes in the ranking of genotypes, depending on the environment and may play a key role in varietal recommendation, for a given mega-environment.&lt;/p&gt;
&lt;p&gt;GE interactions are usually studied by way of Multi-Environment Trials (MET), where experiments are repeated across several years, locations or any combinations of those. Traditional techniques of data analyses, such as two-way ANOVA, give very little insight on the stability/reliability of genotypes across environments and, thus, other specialised techniques are necessary to shed light on interaction effects. I have already talked about stability analyses in other posts, such as &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/&#34;&gt;in this post about the stability variance&lt;/a&gt; or in this &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_environmentalvariance/&#34;&gt;other post about the environmental variance&lt;/a&gt;. Now, I would like to propose some simple explanations about AMMI analysis. AMMI stands for: &lt;strong&gt;Additive Main effect Multiplicative Interaction&lt;/strong&gt; and it has become very much in fashion in the last 20-25 years.&lt;/p&gt;
&lt;p&gt;Let’s start with a real MET example.&lt;/p&gt;
&lt;div id=&#34;a-met-with-faba-bean&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A MET with faba bean&lt;/h1&gt;
&lt;p&gt;This experiment consists of 12 faba bean genotypes (well, it was, indeed, 6 genotypes in two sowing dates; but, let’s disregard this detail from now on) in four blocks, two locations and three years (six environments, in all). The dataset is online available as ‘fabaBean.csv’. It has been published by Stagnari et al. (2007).&lt;/p&gt;
&lt;p&gt;First of all, let’s load the dataset and transform the block variable into a factor. Let’s also inspect the two-way table of means, together with the marginal means for genotypes and environments, which will be useful later. In this post, we will make use of the packages ‘dplyr’ (Wickham &lt;em&gt;et al&lt;/em&gt;., 2020), ‘emmeans’ (Lenth, 2020) and ‘aomisc’; this latter is the companion package for this website and must have been installed as detailed in this &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;page here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# options(width = 70)

rm(list=ls())
# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
library(reshape)
library(emmeans)
library(aomisc)

fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/fabaBean.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName, header=T)
dataset &amp;lt;- transform(dataset, Block = factor(Block),
                     Genotype = factor(Genotype),
                     Environment = factor(Environment))
head(dataset)
##      Genotype Block Environment Yield
## 1    Chiaro_A     1       bad_1  4.36
## 2    Chiaro_P     1       bad_1  2.76
## 3 Collameno_A     1       bad_1  3.01
## 4 Collameno_P     1       bad_1  2.50
## 5    Palomb_A     1       bad_1  3.85
## 6    Palomb_P     1       bad_1  2.21
#
# Two-ways table of means
GEmedie &amp;lt;- cast(Genotype ~ Environment, data = dataset,
                value = &amp;quot;Yield&amp;quot;, fun=mean)
GEmedie
##       Genotype  bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## 1     Chiaro_A 4.1050 2.3400 4.1250 4.6325 2.4100 3.8500
## 2     Chiaro_P 2.5075 1.3325 4.2025 3.3225 1.4050 4.3175
## 3  Collameno_A 3.2500 2.1150 4.3825 3.8475 2.2325 4.0700
## 4  Collameno_P 1.9075 0.8475 3.8650 2.5200 0.9850 4.0525
## 5     Palomb_A 3.8400 2.0750 4.2050 5.0525 2.6850 4.6675
## 6     Palomb_P 2.2500 0.9725 3.2575 3.2700 0.8825 4.0125
## 7      Scuro_A 4.3700 2.1050 4.1525 4.8625 2.1275 4.2050
## 8      Scuro_P 3.0500 1.6375 3.9300 3.7200 1.7475 4.5125
## 9    Sicania_A 3.8300 1.9450 4.5050 3.9550 2.2350 4.2350
## 10   Sicania_P 3.2700 0.9900 3.7300 4.0475 0.8225 3.8950
## 11   Vesuvio_A 4.1375 2.0175 4.0275 4.5025 2.2650 4.3225
## 12   Vesuvio_P 2.1225 1.1800 3.5250 3.0950 0.9375 3.6275
#
# Marginal means for genotypes
apply(GEmedie, 1, mean)
##    Chiaro_A    Chiaro_P Collameno_A Collameno_P    Palomb_A 
##    3.577083    2.847917    3.316250    2.362917    3.754167 
##    Palomb_P     Scuro_A     Scuro_P   Sicania_A   Sicania_P 
##    2.440833    3.637083    3.099583    3.450833    2.792500 
##   Vesuvio_A   Vesuvio_P 
##    3.545417    2.414583
#
# Marginal means for environments
apply(GEmedie, 2, mean)
##    bad_1    bad_2    bad_3    pap_1    pap_2    pap_3 
## 3.220000 1.629792 3.992292 3.902292 1.727917 4.147292
#
# Overall mean
mean(as.matrix(GEmedie))
## [1] 3.103264&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What model could we possibly fit to the above data? The basic two-way ANOVA model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{ijk} = \mu + \gamma_{jk} + g_i + e_j + ge_{ij} + \varepsilon_{ijk} \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the yield &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for given block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is described as a function of the effects of blocks within environments (&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;), genotypes (&lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;), environments (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and GE interaction (&lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt;). The residual error term &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is assumed to be normal and homoscedastic, with standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Let’s also assume that both the genotype and environment effects are fixed: this is useful for teaching purposes and it is reasonable, as we intend to study the behaviour of specific genotypes in several specific environments.&lt;/p&gt;
&lt;p&gt;The interaction effect &lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt;, under some important assumptions (i.e. balanced data, no missing cells and homoscedastic errors), is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ge_{ij} = Y_{ij.} - \left( \mu + g_i + e_j \right) = Y_{ij.} - Y_{i..} - Y_{.j.} + \mu \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij.}\)&lt;/span&gt; is the mean of the combination between the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y_{i..}\)&lt;/span&gt; is the mean for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{.j.}\)&lt;/span&gt; is the mean for the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. For example, for the genotype ‘Chiaro_A’ in the environment ‘bad_1’, the interaction effect was:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;4.1050 - 3.577 - 3.22 + 3.103
## [1] 0.411&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the interaction was positive, in the sense that ‘Chiaro_A’, gave 0.411 tons per hectare more than we could have expected, considering its average performances across environments and the average performances of all genotypes in ‘bad_1’.&lt;/p&gt;
&lt;p&gt;More generally, the two-way table of interaction effects can be obtained by doubly centring the matrix of means, as shown in the following box.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GE &amp;lt;- as.data.frame(t(scale( t(scale(GEmedie, center=T,
 scale=F)), center=T, scale=F)))
print(round(GE, 3))
##              bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## Chiaro_A     0.411  0.236 -0.341  0.256  0.208 -0.771
## Chiaro_P    -0.457 -0.042  0.466 -0.324 -0.068  0.426
## Collameno_A -0.183  0.272  0.177 -0.268  0.292 -0.290
## Collameno_P -0.572 -0.042  0.613 -0.642 -0.003  0.646
## Palomb_A    -0.031 -0.206 -0.438  0.499  0.306 -0.131
## Palomb_P    -0.308  0.005 -0.072  0.030 -0.183  0.528
## Scuro_A      0.616 -0.059 -0.374  0.426 -0.134 -0.476
## Scuro_P     -0.166  0.011 -0.059 -0.179  0.023  0.369
## Sicania_A    0.262 -0.032  0.165 -0.295  0.160 -0.260
## Sicania_P    0.361 -0.329  0.048  0.456 -0.595  0.058
## Vesuvio_A    0.475 -0.054 -0.407  0.158  0.095 -0.267
## Vesuvio_P   -0.409  0.239  0.221 -0.119 -0.102  0.169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that the overall mean for all elements in ‘GE’ is zero and the sum of squares is equal to a fraction of the interaction sum of squares in ANOVA (that is &lt;span class=&#34;math inline&#34;&gt;\(RMSE/r\)&lt;/span&gt;; where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the number of blocks).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(unlist(GE))
## [1] 6.914424e-18
sum(GE^2)
## [1] 7.742996
mod &amp;lt;- lm(Yield ~ Environment/Block + Genotype*Environment, data = dataset)
anova(mod)
## Analysis of Variance Table
## 
## Response: Yield
##                       Df Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Environment            5 316.57  63.313 580.9181 &amp;lt; 2.2e-16 ***
## Genotype              11  70.03   6.366  58.4111 &amp;lt; 2.2e-16 ***
## Environment:Block     18   6.76   0.375   3.4450 8.724e-06 ***
## Environment:Genotype  55  30.97   0.563   5.1669 &amp;lt; 2.2e-16 ***
## Residuals            198  21.58   0.109                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
30.97/4
## [1] 7.7425&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;decomposing-the-ge-matrix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Decomposing the GE matrix&lt;/h1&gt;
&lt;p&gt;It would be nice to be able to give a graphical summary of the GE matrix; in this regard, we could think of using Principal Component Analysis (PCA) via Singular Value Decomposition (SVD). This has been shown by Zobel &lt;em&gt;et al&lt;/em&gt;. (1988) and, formerly, by Gollob (1968). May I just remind you a few things about PCA and SVD? No overwhelming math detail, I promise!&lt;/p&gt;
&lt;p&gt;Most matrices (and our GE matrix) can be decomposed as the product of three matrices, according to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = U D V^T \quad \quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the matrix to be decomposed, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the matrix of the first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is the matrix of the first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(X^T X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the diagonal matrix of the first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; singular values of &lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(X^T X\)&lt;/span&gt;; it does not matter, they are the same).&lt;/p&gt;
&lt;p&gt;Indeed, if we want to decompose our GE matrix, it is more clever (and more useful to our purposes), to write the following matrices:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_g = U D^{1/2} \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_e = V D^{1/2} \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GE = S_g \, S_e^T \quad \quad (6)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; is the matrix of row-scores (genotype scores) and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt; is the matrix of column scores (environment scores). Let me give you an empirical proof, in the box below. In order to find &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;, I will use a mathematical operation that is known as Singular Value Decomposition (SVD):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;U &amp;lt;- svd(GE)$u
V &amp;lt;- svd(GE)$v
D &amp;lt;- diag(svd(GE)$d)
Sg &amp;lt;- U %*% sqrt(D)
Se &amp;lt;- V %*% sqrt(D)
row.names(Sg) &amp;lt;- levels(dataset$Genotype)
row.names(Se) &amp;lt;- levels(dataset$Environment)
colnames(Sg) &amp;lt;- colnames(Se) &amp;lt;- paste(&amp;quot;PC&amp;quot;, 1:6, sep =&amp;quot;&amp;quot;)
round(Sg %*% t(Se), 3)
##              bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## Chiaro_A     0.411  0.236 -0.341  0.256  0.208 -0.771
## Chiaro_P    -0.457 -0.042  0.466 -0.324 -0.068  0.426
## Collameno_A -0.183  0.272  0.177 -0.268  0.292 -0.290
## Collameno_P -0.572 -0.042  0.613 -0.642 -0.003  0.646
## Palomb_A    -0.031 -0.206 -0.438  0.499  0.306 -0.131
## Palomb_P    -0.308  0.005 -0.072  0.030 -0.183  0.528
## Scuro_A      0.616 -0.059 -0.374  0.426 -0.134 -0.476
## Scuro_P     -0.166  0.011 -0.059 -0.179  0.023  0.369
## Sicania_A    0.262 -0.032  0.165 -0.295  0.160 -0.260
## Sicania_P    0.361 -0.329  0.048  0.456 -0.595  0.058
## Vesuvio_A    0.475 -0.054 -0.407  0.158  0.095 -0.267
## Vesuvio_P   -0.409  0.239  0.221 -0.119 -0.102  0.169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;: they are two interesting entities. I will round up a little to make them smaller, and less scaring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(Sg, 3)
##                PC1    PC2    PC3    PC4    PC5 PC6
## Chiaro_A    -0.607 -0.384  0.001  0.208 -0.063   0
## Chiaro_P     0.552  0.027 -0.081  0.045  0.164   0
## Collameno_A  0.084 -0.542 -0.006  0.176  0.057   0
## Collameno_P  0.807 -0.066 -0.132 -0.172  0.079   0
## Palomb_A    -0.321  0.110  0.591 -0.083  0.389   0
## Palomb_P     0.281  0.346  0.282  0.042 -0.253   0
## Scuro_A     -0.626  0.139 -0.163  0.017 -0.080   0
## Scuro_P      0.230  0.077  0.182 -0.207 -0.242   0
## Sicania_A   -0.063 -0.324 -0.355 -0.280  0.090   0
## Sicania_P   -0.214  0.683 -0.402  0.148  0.151   0
## Vesuvio_A   -0.438 -0.008  0.020 -0.300 -0.177   0
## Vesuvio_P    0.316 -0.058  0.063  0.405 -0.114   0
round(Se, 3)
##          PC1    PC2    PC3    PC4    PC5 PC6
## bad_1 -0.831  0.095 -0.467 -0.317 -0.151   0
## bad_2  0.044 -0.418  0.070  0.371 -0.403   0
## bad_3  0.670 -0.130 -0.525  0.171  0.298   0
## pap_1 -0.661  0.513  0.289  0.314  0.221   0
## pap_2 -0.069 -0.627  0.420 -0.294  0.208   0
## pap_3  0.846  0.567  0.213 -0.244 -0.173   0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both matrices have 6 columns. Why six, are you asking? I promised I would not go into math detail; it’s enough to know that the number of columns is always equal to the minimum value between the number of genotypes and the number of environments. The final column is irrelevant (all elements are 0). &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; has 12 rows, one per genotype; these are the so called genotype scores: each genotype has six scores. &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt; has six rows, one per environment (environment scores).&lt;/p&gt;
&lt;p&gt;You may have some ‘rusty’ memories about matrix multiplication; however, what we have discovered in the code box above is that the GE interaction for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; genotype and the &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; environment can be obtained as the product of genotype scores and environments scores. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ge_{ij} = \sum_{z = 1}^n \left[ S_g(iz) \cdot S_e(jz) \right] \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of columns (number of principal components). An example is in order, at this point; again, let’s consider the first genotype and the first environment. The genotype and environments scores are in the first columns of &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;; if we multiply the elements in the same positioning (1st with 1st, 2nd with 2nd, and so on) and sum up, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.607 * -0.831 +
-0.384 *  0.095 +
 0.001 * -0.467 +
 0.208 * -0.317 + 
-0.063 * -0.151 +
     0 * 0
## [1] 0.411047&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s done: we have transformed the interaction effect into the sum of multiplicative terms. If we replace Equation 7 into the ANOVA model above (Equation 1), we obtain an &lt;em&gt;Additive Main effects Multiplicative Interaction&lt;/em&gt; model, i.e. an AMMI model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reducing-the-rank&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reducing the rank&lt;/h1&gt;
&lt;p&gt;In this case we took all available columns in &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;. For the sake of simplicity, we could have taken only a subset of those columns. The Eckart-Young (1936) theorem says that, if we take &lt;span class=&#34;math inline&#34;&gt;\(m &amp;lt; 6\)&lt;/span&gt; columns, we obtain the best possible approximation of GE in reduced rank space. For example, let’s use the first two columns of &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt; (the first two principal component scores):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PC &amp;lt;- 2
Sg2 &amp;lt;- Sg[,1:PC]
Se2 &amp;lt;- Se[,1:PC]
GE2 &amp;lt;- Sg2 %*% t(Se2)
print ( round(GE2, 3) )
##              bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## Chiaro_A     0.468  0.134 -0.357  0.205  0.282 -0.732
## Chiaro_P    -0.456  0.013  0.367 -0.351 -0.055  0.482
## Collameno_A -0.122  0.230  0.127 -0.334  0.334 -0.236
## Collameno_P -0.676  0.063  0.549 -0.567 -0.014  0.645
## Palomb_A     0.277 -0.060 -0.230  0.269 -0.047 -0.209
## Palomb_P    -0.201 -0.132  0.144 -0.009 -0.236  0.434
## Scuro_A      0.534 -0.086 -0.438  0.486 -0.044 -0.451
## Scuro_P     -0.184 -0.022  0.144 -0.113 -0.064  0.238
## Sicania_A    0.022  0.133  0.000 -0.124  0.207 -0.237
## Sicania_P    0.243 -0.295 -0.232  0.492 -0.414  0.206
## Vesuvio_A    0.363 -0.016 -0.293  0.286  0.035 -0.375
## Vesuvio_P   -0.268  0.038  0.219 -0.239  0.015  0.234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;GE2 is not equal to GE, but it is a close approximation. A close approximation in what sense?… you may wonder. Well, the sum of squared elements in GE2 is as close as possible (with &lt;span class=&#34;math inline&#34;&gt;\(n = 2\)&lt;/span&gt;) to the sum of squared elements in GE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(GE2^2)
## [1] 6.678985&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the sum of squares in GE2 is 86% of the sum of squares in GE. A very good approximation, isn’t it? It means that the variability of yield across environments is described well enough by using a relatively low number of parameters (scores). However, the multiplicative part of our AMMI model needs to be modified:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ge_{ij} = \sum_{z = 1}^m \left[ s_{g(iz)} \cdot s_{e(jz)} \right] + \xi_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed, a residual term &lt;span class=&#34;math inline&#34;&gt;\(\xi_{ij}\)&lt;/span&gt; is necessary, to account for the fact that the sum of multiplicative terms is not able to fully recover the original matrix GE. Another example? For the first genotype and the first environment the multiplicative interaction is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.607 * -0.831 + -0.384 * 0.095
## [1] 0.467937&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the residual term &lt;span class=&#34;math inline&#34;&gt;\(\xi_{11}\)&lt;/span&gt; is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.41118056 -0.607 * -0.831 + -0.384 * 0.095
## [1] 0.8791176&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, the residual terms need to be small enough to be negligible, otherwise the approximation in reduced rank space is not good enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-this-useful&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why is this useful?&lt;/h1&gt;
&lt;p&gt;Did you get lost? Hope you didn’t, but let’s make a stop and see where we are standing now. We started from the interaction matrix GE and found a way to decompose it as the product of two matrices, i.e. &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;, a matrix of genotype scores and a matrix of environment scores. We discovered that we could obtain a good approximation of GE by working in reduced rank space and we only used two genotypic scores and two environment scores, in place of the available six.&lt;/p&gt;
&lt;p&gt;This is great! Now we have the ability of drawing a biplot, i.e. we can plot both genotypic scores and environmental scores in a dispersion graph (biplot: two plots in one), as we see below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(Sg[,1:2], Se[,1:2], xlim = c(-1, 1), ylim = c(-1, 1),
       xlab = &amp;quot;PC 1&amp;quot;, ylab = &amp;quot;PC 2&amp;quot;)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_met_AMMI_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph provides a very effective description of GE interaction effects. I will not go into detail, here. Just a few simple comments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;genotypes and environments lying close to the origin of the axes do not interact with each other (the product of scores would be close to 0)&lt;/li&gt;
&lt;li&gt;genotype and environments lying far away from the origin of axes show very big interaction and, therefore, high yield instability. Someone says that the euclidean distance from the origin should be taken as a measure of instability&lt;/li&gt;
&lt;li&gt;the interaction is positive, when genotypes and environments are close to each other. If two objects are close, their scores (co-ordinates) will have the same signs and thus their product will be positive.&lt;/li&gt;
&lt;li&gt;the interaction is negative, when genotypes and environments are far away from each other. If two objects are distant, their scores (co-ordinates) will have opposte signs and thus their product will be negative.&lt;/li&gt;
&lt;li&gt;For instance, ‘Palomb_P’, ‘Scuro_P’, ‘Chiaro_P’ and ‘Collameno_P’ gave particularly good yields in the environments ‘pap_3’ and ‘bad_3’, while ‘Scuro_A’, ‘Palomb_A’ and ‘Vesuvio_A’ gave particularly good yields (compared to their average) in the environments ‘pap_1’ and ‘bad_1’. ‘Sicania_A’ and ‘Collameno_A’ gave good yields in ‘bad_2’ and ‘pap_2’.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;how-many-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How many components?&lt;/h2&gt;
&lt;p&gt;In my opinion, AMMI analysis is mainly a visualisation method. Therefore, we should select as many components (columns in &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;) as necessary to describe a main part of the interaction sum of squares. In our example, two components are enough, as they represent 86% of the interaction sum of squares.&lt;/p&gt;
&lt;p&gt;However, many people (and reviewers) are still very concerned with formal hypothesis testing. Therefore, we could proceed in a sequential fashion, and introduce the components one by one.&lt;/p&gt;
&lt;p&gt;The first component has a sum of squares equal to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PC &amp;lt;- 1
Sg2 &amp;lt;- Sg[,1:PC]
Se2 &amp;lt;- Se[,1:PC]
GE2 &amp;lt;- Sg2 %*% t(Se2)
sum(GE2^2)
## [1] 5.290174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have seen that the second component has an additional sum of squares equal to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6.678985 - 5.290174
## [1] 1.388811&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can go further ahead and get the sum of squares for all components. According to Zobel (1988), the degrees of freedom for each component are equal to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ df_n = i + j - 1 - 2m \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the number of genotypes, &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is the number of environments, and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of the selected components. In our case, the first PC has 15 DF, the second one has 13 DF and so on.&lt;/p&gt;
&lt;p&gt;If we can have a reliable estimate of the pure error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; (see above), we can test the significance of each component by using F tests (although some authors argue that this is too a liberal approach; see Cornelius, 1993).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-ammi-analysis-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple AMMI analysis with R&lt;/h1&gt;
&lt;p&gt;We have seen that AMMI analysis, under the hood, is a sort of PCA. Therefore, it could be performed, in R by using one of the available packages for PCA. For the sake of simplicity, I have coded a couple of functions, i.e. ‘AMMI()’ and ‘AMMImeans()’ and they are both available in the ‘aomisc’ package. I have described the first one a few years ago in an R news paper (&lt;a href=&#34;https://www.researchgate.net/publication/289419258_Using_R_to_perform_the_AMMI_analysis_on_agriculture_variety_trials&#34;&gt;Onofri and Ciriciofolo, 2007&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Here, I will describe the second one, which permits to handle a small degree of unbalance (a few plots, missing at random). The analysis proceeds in two steps.&lt;/p&gt;
&lt;div id=&#34;first-step-on-raw-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First step on raw data&lt;/h2&gt;
&lt;p&gt;During the first step we need to obtain a reliable matrix of means for the ‘genotype x environment’ combinations. If the environment is fixed, we can use least squares means, which are unbiased, also when some observations are missing. If the environment effect is random, we could use the BLUPs, but we will not consider such an option here.&lt;/p&gt;
&lt;p&gt;In the box below we take the ‘mod’ object from a two way ANOVA fit and derive the residual mean square (RMSE), which we divide by the number of blocks. This will be our error term to test the significance of components. Later, we pass the ‘mod’ object to the ‘emmeans()’ function, to retrieve the expected marginal means for the ‘genotype by environment’ combinations and proceed to the second step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMSE &amp;lt;- summary(mod)$sigma^2 / 4
dfr &amp;lt;- mod$df.residual
ge.lsm &amp;lt;- emmeans(mod, ~Genotype:Environment)
ge.lsm &amp;lt;- data.frame(ge.lsm)[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;second-step-on-least-square-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second step on least square means&lt;/h2&gt;
&lt;p&gt;This second step assumes that the residual variances for all environments are homogeneous. If so (we’d better check this), we can take the expected marginal means (‘ge.lsm’) and submit them to AMMI analysis, by using the ‘AMMImeans()’ function. The syntax is fairly obvious; we also pass to it the RMSE and its degrees of freedom. The resulting object can be explored, by using the appropriate slots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AMMIobj &amp;lt;- AMMImeans(yield = ge.lsm$emmean, 
                     genotype = ge.lsm$Genotype, 
                     environment = ge.lsm$Environment, 
                     MSE = RMSE, dfr = dfr)
#
AMMIobj$genotype_scores
##                     PC1          PC2
## Chiaro_A    -0.60710888 -0.383732821
## Chiaro_P     0.55192742  0.026531045
## Collameno_A  0.08444877 -0.542185666
## Collameno_P  0.80677055 -0.065752971
## Palomb_A    -0.32130513  0.110117240
## Palomb_P     0.28104959  0.345909298
## Scuro_A     -0.62638795  0.139185954
## Scuro_P      0.22961347  0.076555540
## Sicania_A   -0.06286803 -0.323857285
## Sicania_P   -0.21433211  0.683296898
## Vesuvio_A   -0.43786742 -0.007914342
## Vesuvio_P    0.31605973 -0.058152890
#
AMMIobj$environment_scores
##               PC1         PC2
## bad_1 -0.83078550  0.09477362
## bad_2  0.04401963 -0.41801637
## bad_3  0.67043214 -0.12977423
## pap_1 -0.66137357  0.51268429
## pap_2 -0.06863235 -0.62703224
## pap_3  0.84633965  0.56736492
#
round(AMMIobj$summary, 4)
##   PC Singular_value  PC_SS Perc_of_Total_SS cum_perc
## 1  1         2.3000 5.2902          68.3220  68.3220
## 2  2         1.1785 1.3888          17.9364  86.2584
## 3  3         0.8035 0.6456           8.3375  94.5959
## 4  4         0.5119 0.2621           3.3846  97.9806
## 5  5         0.3954 0.1564           2.0194 100.0000
## 6  6         0.0000 0.0000           0.0000 100.0000
#
round(AMMIobj$anova, 4)
##   PC     SS DF     MS       F P.value
## 1  1 5.2902 15 0.3527 12.9437  0.0000
## 2  2 1.3888 13 0.1068  3.9208  0.0000
## 3  3 0.6456 11 0.0587  2.1539  0.0184
## 4  4 0.2621  9 0.0291  1.0687  0.3876
## 5  5 0.1564  7 0.0223  0.8198  0.5718
## 6  6 0.0000  5 0.0000  0.0000  1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In detail, we can retrieve the genotype and environment scores, the proportion of the GE variance explained by each component and the significance of PCs.&lt;/p&gt;
&lt;p&gt;Just to show you, the box below reports the code for AMMI analysis on raw data. Please, note that this only works with balanced data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AMMIobj2 &amp;lt;- AMMI(yield = dataset$Yield, 
                 genotype = dataset$Genotype,
                 environment = dataset$Environment, 
                 block = dataset$Block)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I agree, these functions are not very ambitious. However, they are simple enough to be usable and give reliable results, as long as the basic assumptions for the method are respected. You may also consider to explore other more comprehensive R packages, such as ‘agricolae’ (de Mendiburu, 2020).&lt;/p&gt;
&lt;p&gt;Thank you for reading, so far, and… happy coding!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;literature-references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Literature references&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Annichiarico, P. (1997). Additive main effects and multiplicative interaction (AMMI) analysis of genotype-location interaction in variety trials repeated over years. Theoretical applied genetics, 94, 1072-1077.&lt;/li&gt;
&lt;li&gt;Ariyo, O. J. (1998). Use of additive main effects and multiplicative interaction model to analyse multilocation soybean varietal trials. J. Genet. and Breed, 129-134.&lt;/li&gt;
&lt;li&gt;Cornelius, P. L. (1993). Statistical tests and retention of terms in the Additive Main Effects and Multiplicative interaction model for cultivar trials. Crop Science, 33,1186-1193.&lt;/li&gt;
&lt;li&gt;Crossa, J. (1990). Statistical Analyses of multilocation trials. Advances in Agronomy, 44, 55-85.&lt;/li&gt;
&lt;li&gt;Gollob, H. F. (1968). A statistical model which combines features of factor analytic and analysis of variance techniques. Psychometrika, 33, 73-114.&lt;/li&gt;
&lt;li&gt;Lenth R., 2020. emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.4.6. &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;de Mendiburu F., 2020. agricolae: Statistical Procedures for Agricultural Research. R package version 1.3-2. &lt;a href=&#34;https://CRAN.R-project.org/package=agricolae&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=agricolae&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Onofri, A., Ciriciofolo, E., 2007. Using R to perform the AMMI analysis on agriculture variety trials. R NEWS 7, 14–19.&lt;/li&gt;
&lt;li&gt;Stagnari F., Onofri A., Jemison J., Monotti M. (2006). Multivariate analyses to discriminate the behaviour of faba bean (Vicia faba L. var. minor) varieties as affected by sowing time in cool, low rainfall Mediterranean environments. Agronomy For Sustainable Development, 27, 387–397.&lt;/li&gt;
&lt;li&gt;Hadley Wickham, Romain François, Lionel Henry and Kirill Müller, 2020. dplyr: A Grammar of Data Manipulation. R package version 0.8.5. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Zobel, R. W., Wright, M.J., and Gauch, H. G. (1988). Statistical analysis of a yield trial. Agronomy Journal, 388-393.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Seed germination: fitting hydro-time models with R</title>
      <link>https://www.statforbiology.com/2020/stat_seedgermination_ht1step/</link>
      <pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_seedgermination_ht1step/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;THE CODE IN THIS POST WAS UPDATED ON JANUARY 2022&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I am locked at home, due to the COVID-19 emergency in Italy. Luckily I am healthy, but there is not much to do, inside. I thought it might be nice to spend some time to talk about seed germination models and the connections with survival analysis.&lt;/p&gt;
&lt;p&gt;We all know that seeds need water to germinate. Indeed, the absorption of water activates the hydrolytic enzymes, which break down food resources stored in seeds and provide energy for germination. As the consequence, there is a very close relationship between water content in the substrate and germination velocity: the higher the water content the quickest the germination, as long as the availability of oxygen does not become a problem (well, water and oxygen in soil may compete for space and a high water content may result in oxygen shortage).&lt;/p&gt;
&lt;p&gt;Indeed, it is relevant to build germination models, linking the proportion of germinated seeds to water availability in the substrate; these models are usually known as hydro-time (HT) models. The starting point is the famous equation of Bradford (1992), where the germination rate (GR) for the &lt;span class=&#34;math inline&#34;&gt;\(i-th\)&lt;/span&gt; seed in the lot is expressed as a linear function of water potential in the substrate (&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ GR_i = \textrm{max} \left( \frac{\Psi - \Psi_{b(i)}}{\theta_H}; 0 \right) \quad \quad \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In that equation, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b(i)}\)&lt;/span&gt; is the base water potential for the &lt;span class=&#34;math inline&#34;&gt;\(i-th\)&lt;/span&gt; seed and &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt; is the hydro-time constant, expressed as &lt;em&gt;MPa day&lt;/em&gt; or &lt;em&gt;MPa hour&lt;/em&gt;. The concept is relatively simple: we just need to remember that the water can only move from a position with a higher water potential to a position with a lower water potential. Therefore, a seed cannot germinate when its base water potential is higher than the water potential in the substrate.&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(\Psi &amp;gt; \Psi_b(i)\)&lt;/span&gt;, the germination rate of the &lt;span class=&#34;math inline&#34;&gt;\(i-th\)&lt;/span&gt; seed is linearly related to &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;: the higher this latter value, the higher the germination rate. Now we should consider that the germination rate is the inverse of the germination time (&lt;span class=&#34;math inline&#34;&gt;\(GR = 1/t\)&lt;/span&gt;); thus, the higher the GR, the shortest the germination time. Germination is achieved at the time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; when:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ t \, \left( \Psi - \Psi_{b(i)} \right) = \theta_H \quad \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2021/stat_seedgermination_htt2step/&#34;&gt;Elsewhere in this website&lt;/a&gt;, I show that Equation 1 can be fitted to germination data in a two-steps fashion. In this page we will see how we can embed Equation 1 into a time-to-event model, to predict the proportion of germinated seeds, depending on time and water content in the substrate. As usual, let’s start from a practical example.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;the-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The dataset&lt;/h1&gt;
&lt;p&gt;The germination of rapeseed (&lt;em&gt;Brassica napus&lt;/em&gt; L. var. &lt;em&gt;oleifera&lt;/em&gt;, cv. Excalibur) was tested at fourteen different water potentials (0, -0.03, -0.15, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1, -1.1, -1.2, -1.5 MPa), which were created by using a polyethylene glycol solution (PEG 6000). For each water potential level, three replicated Petri dishes with 50 seeds were incubated at 20°C. Germinated seeds were counted and removed every 2-3 days for 14 days.&lt;/p&gt;
&lt;p&gt;The dataset was published by Pace et al. (2012) and it is available as &lt;code&gt;rape&lt;/code&gt; in the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, which needs to be installed from github (see below). Furthermore, the package &lt;code&gt;drcte&lt;/code&gt; is necessary to fit time-to-event models and it should also be installed from gitHub. Please, make sure you have the most updated version for both packages.&lt;/p&gt;
&lt;p&gt;The following code loads the necessary packages, loads the dataset &lt;code&gt;rape&lt;/code&gt; and shows the first six lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)
# install_github(&amp;quot;OnofriAndreaPG/drcte&amp;quot;)
library(drcSeedGerm)
library(drcte)
data(rape)
head(rape)
##   Psi Dish timeBef timeAf nSeeds nCum propCum
## 1   0    1       0      3     49   49    0.98
## 2   0    1       3      4      0   49    0.98
## 3   0    1       4      5      0   49    0.98
## 4   0    1       5      7      0   49    0.98
## 5   0    1       7     10      0   49    0.98
## 6   0    1      10     14      0   49    0.98&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data are grouped by assessment interval: ‘timeAf’ represents the moment when germinated seeds were counted, while ’timeBef’ represents the previous inspection time (or the beginning of the assay). The column ’nSeeds’ is the number of seeds that germinated during the time interval between ‘timeBef’ and ‘timeAf. The ’propCum’ column contains the cumulative proportions of germinated seeds and it is not necessary for time-to-event models.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;building-hydro-time-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building hydro-time models&lt;/h1&gt;
&lt;div id=&#34;models-based-on-the-distribution-of-germination-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models based on the distribution of germination time&lt;/h2&gt;
&lt;p&gt;How can we rework Equation 1 to predict the proportion of germinated seeds, as a function of time and water potential? One line of attack follows the proposal we made in a relatively recent paper (Onofri at al., 2018). We start from the idea that the time course of the proportion of germinated seeds (&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;) is expected to increase over time, according to a S-shaped curve, such as the usual log-logistic cumulative probability function (other cumulative distribution functions can be used; see our original paper):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{ d }{1 + \exp \left\{ b \left[ \log(t) - \log( e ) \right] \right\} } \quad \quad \quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum germinated proportion. Considering that the germination rate is the inverse of germination time, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{ d }{1 + \exp \left\{ b \left[ \log(t) - \log(1 / GR_{50} ) \right] \right\} } \quad \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt; is the median germination rate in the population. We can now express &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as linear/nonlinear functions of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; (temperature and other environmental variables can be included as well. See our original paper):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t, \Psi) = \frac{ h_1(\Psi) }{1 + \exp \left\{ b \left[ \log(t) - \log(1 / \left[ GR_{50}(\Psi) \right] ) \right] \right\} } \quad \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our paper, for &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt;, we slightly reparameterised Equation 1 above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ GR_{50}(\Psi) = \textrm{max} \left( \frac{\Psi - \Psi_{b(50)}}{\theta_H}; 0 \right) \quad \quad \quad \quad (6)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b(50)}\)&lt;/span&gt; is the median base water potential in the population.&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, we used a shifted exponential distribution, which implies that germination capability is fully determined by the distribution of base water potential within the population and no germination occurs at &lt;span class=&#34;math inline&#34;&gt;\(\Psi \leq \Psi_b\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = h_1(\Psi ) = \textrm{max} \left\{ G \, \left[ 1 - \exp \left( \frac{ \Psi - \Psi_{b(50)} }{\sigma_{\Psi_b}} \right) \right]; 0 \right\} \quad \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above equation, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b}\)&lt;/span&gt; represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population, which determines the steepness of the increase in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; increases. &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the germinable fraction, accounting for the fact that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; may not reach 1, regardless of time and water potential.&lt;/p&gt;
&lt;p&gt;The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; was assumed to be constant and independent on &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;. In the end, our hydro-time model is composed by four sub-models:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a cumulative probability function (log-logistic, in our example), based on the three parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = 1/GR_{50}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, although, this was indeed a simple identity model &lt;span class=&#34;math inline&#34;&gt;\(b(\Psi) = b\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This hydro-time model was implemented in R as the &lt;code&gt;HTE1()&lt;/code&gt; function, and it is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, together with the appropriate self-starting routine. It can be fitting by using the &lt;code&gt;drmte()&lt;/code&gt; function in the &lt;code&gt;drcte&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modHTE &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi, 
                data = rape, fct = HTE1())
summary(modHTE)
## 
## Model fitted: Hydro-time model with shifted exponential for Pmax and linear model for GR50
## 
## Robust estimation: no 
## 
## Parameter estimates:
## 
##                         Estimate Std. Error  t-value   p-value    
## G:(Intercept)          0.9577918  0.0063667  150.438 &amp;lt; 2.2e-16 ***
## Psib:(Intercept)      -1.0397239  0.0047017 -221.138 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.1108891  0.0087598   12.659 &amp;lt; 2.2e-16 ***
## thetaH:(Intercept)     0.9061385  0.0301594   30.045 &amp;lt; 2.2e-16 ***
## b:(Intercept)          4.0273963  0.1960845   20.539 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As seeds are clustered in Petri dishes, in order not to violate the independence assumption, it is preferable to get cluster robust standard errors. One possibility is to use the grouped version of the sandwich estimator, as available in the ‘sandwich’ package (Berger, 2017). The function &lt;code&gt;coeftest()&lt;/code&gt; is available in the ‘lmtest’ package (Zeileis, 2002) and its usage is shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(modHTE, vcov = vcovCL, cluster = rape$Dish)
## 
## t test of coefficients:
## 
##                         Estimate Std. Error   t value  Pr(&amp;gt;|t|)    
## G:(Intercept)          0.9577918  0.0080923  118.3591 &amp;lt; 2.2e-16 ***
## Psib:(Intercept)      -1.0397239  0.0047063 -220.9206 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.1108891  0.0121879    9.0983 &amp;lt; 2.2e-16 ***
## thetaH:(Intercept)     0.9061385  0.0410425   22.0781 &amp;lt; 2.2e-16 ***
## b:(Intercept)          4.0273963  0.1934513   20.8187 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the model is fitted, we may be interested in using the resulting curve to retrieve some biologically relevant information. For example, it may be interesting to retrieve the germination rates for some selected percentiles (e.g., the 30th, 20th and 10th percentiles). This is possible using the &lt;code&gt;GRate()&lt;/code&gt; function, that is a wrapper for the original &lt;code&gt;ED()&lt;/code&gt; function in the package &lt;code&gt;drc&lt;/code&gt;. It reverses the behavior of the &lt;code&gt;ED()&lt;/code&gt; function, in the sense that it considers, by default, the percentiles for the whole population, including the ungerminated fraction, which is, in our opinion, the most widespread interpretation of germination rates in seed science. The &lt;code&gt;GRate()&lt;/code&gt; function works very much like the &lt;code&gt;ED()&lt;/code&gt; function, although additional variables, such as the selected &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; level can be specified by using the argument &lt;code&gt;x2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Naive standard errors
GRate(modHTE, x2 = -1, respLev = c(30, 20, 10))
##           Estimate          SE
## GR:1:30 0.00000000 0.000000000
## GR:1:20 0.03579236 0.005808732
## GR:1:10 0.05130356 0.006221881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we see that the &lt;span class=&#34;math inline&#34;&gt;\(GR_{30}\)&lt;/span&gt; cannot be calculated, as the germination capacity did not reach 30% at the selected water potential level (&lt;span class=&#34;math inline&#34;&gt;\(-1 \,\, MPa\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;As we said, cluster robust standard errors are recommended. The &lt;code&gt;GRate()&lt;/code&gt; function allows entering a user-defined variance-covariance matrix, that is obtained by using the &lt;code&gt;vcovCL()&lt;/code&gt; function in the &lt;code&gt;sandwich&lt;/code&gt; package. If necessary, germination times can be obtained in a similar way, by using the &lt;code&gt;GTime()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Cluster robust standard errors
sc &amp;lt;- vcovCL(modHTE, cluster = rape$Dish)
GRate(modHTE, x2 = -1, respLev = c(30, 20, 10), vcov.=sc)
##           Estimate          SE
## GR:1:30 0.00000000 0.000000000
## GR:1:20 0.03579236 0.005450975
## GR:1:10 0.05130356 0.005869140
#Germination times
GTime(modHTE, x2 = -1, respLev = c(30, 20, 10), vcov.=sc)
##        Estimate      SE
## T:1:30      Inf      NA
## T:1:20 27.93892 4.25494
## T:1:10 19.49183 2.22987&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Last, but not least, we can predict the proportion of germinated seeds at given time and water potential level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictSG(modHTE, se.fit=T, vcov. = vcovCL,
        newdata = data.frame(Time=c(10, 10, 10), 
                             Psi=c(-1.5, -0.75, 0))
        )
##      Prediction         SE
## [1,]  0.0000000 0.00000000
## [2,]  0.8794025 0.03907487
## [3,]  0.9576590 0.01493134&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;models-based-on-the-distribution-of-psi_b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models based on the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Another type of hydro-time model was proposed by Bradford (2002) and later extended by Mesgaran et al (2013). This approach starts always from Equation 1; from that equation, considering that the germination time is the inverse of the GR, we can easily get to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Psi_b = \Psi - \frac{\theta_H}{t} \quad \quad \quad (8)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the germination time. What does this equation tell us? Let’s assume that the hydro-time to germination is 10 &lt;span class=&#34;math inline&#34;&gt;\(MPa \, d\)&lt;/span&gt; and the environmental water potential is -1 &lt;span class=&#34;math inline&#34;&gt;\(MPa\)&lt;/span&gt;. A single seed germinates in exactly one day, if its base water potential is &lt;span class=&#34;math inline&#34;&gt;\(-1 - 10/1 = -11\)&lt;/span&gt;. If the base water potential is higher, germination will take more than one day; if it is lower, germination will take less than one day. But now, the following questions come: how many seeds in a population will be able to germinate in one day? And in two days? And in &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; days?&lt;/p&gt;
&lt;p&gt;We know that the seeds within a population do not germinate altogether in the same moment, as they have different individual values of base water potential. If the population is big enough, we can describe the variation of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population by using some density function, possibly parameterised by way of a location (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and a scale (&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;) parameter:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \Psi_b \sim \phi \left( \frac{\Psi_b - \mu}{\sigma} \right) \quad \quad \quad (9)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is easier to understand if we make a specific example. Let’s assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; values within the population is gaussian, with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = -9\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1\)&lt;/span&gt;. Let’s also assume that the hydro-time parameter (&lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt;) is constant within the population. We have the situation depicted in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HT1step_files/figure-html/distribPsi-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The red left tail represents the proportion of seeds that germinate during the first day, as they have base water potentials equal to or lower than -11. By using the gaussian cumulative distribution function we can easily see that that proportion is 0.228:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(-1 - 10/1, mean = -9, sd = 1)
## [1] 0.02275013&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More generally, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \Phi \left\{ \frac{\Psi - (\theta_H / t) -\mu }{\sigma} \right\} \quad \quad \quad (10)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is the selected cumulative distribution function. The above model returns the proportion of germinated seeds (G), as a function of time and water potential in the substrate. According to Bradford (2002), &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is cumulative gaussian.&lt;/p&gt;
&lt;p&gt;Let’s think more deeply about Equation 9 (Bradford, 2002). This function was built to represent the cumulative distribution function of base water potential within the population. However, &lt;strong&gt;it can be as well taken to represent the cumulative distribution function of germination time within the population&lt;/strong&gt;. Obviously, while the first distribution is gaussian, the second one is not: indeed, the germination time appears at the denominator of the expression &lt;span class=&#34;math inline&#34;&gt;\(\theta_H / t\)&lt;/span&gt;. It doesn’t matter: every cumulative distribution function for germination time can be fit by using time-to-event methods!&lt;/p&gt;
&lt;p&gt;We implemented this model in R as the function &lt;code&gt;HTnorm()&lt;/code&gt; that is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package and it is meant to be used with the &lt;code&gt;drm()&lt;/code&gt; function, in the &lt;code&gt;drc&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Mesgaran et al (2013) suggested that the distribution of base water potential within the population may not be gaussian and proposed several alternatives, which we have all implemented within the package. In all, &lt;code&gt;drcSeedGerm&lt;/code&gt; contains six possible distributions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;gaussian distribution (function &lt;code&gt;HTnorm()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;logistic distribution (function &lt;code&gt;HTL()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Gumbel (function &lt;code&gt;HTG()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;log-logistic (function &lt;code&gt;HTLL()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Weibull (Type I) (function &lt;code&gt;HTW1()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Weibull (Type II) (function &lt;code&gt;HTW2()&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The equations are given at the end of this page; for gaussian, logistic and log-logistic distributions, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b(50)}\)&lt;/span&gt; is the median base water potential within the population. For the gaussian distribution, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi b}\)&lt;/span&gt; corresponds to the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population.&lt;/p&gt;
&lt;p&gt;Distributions based on logarithms (the log-logistic and all other distributions thereafter) are only defined for positive amounts. On the contrary, we know that base water potential is mostly negative. Therefore, shifted distributions need to be used, by introducing a shifting parameter &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; which ‘moves’ the distribution to the left, along the x-axis, so that negative values are possible (see Mesgaran et al., 2013).&lt;/p&gt;
&lt;p&gt;Let’s fit the above functions to the ‘rape’ dataset. But, before, let me highlight that providing starting values is not necessary, as self-starting routines are already implemented for all models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi, 
              data = rape, fct = HTnorm())
mod2 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
              data = rape, fct = HTL())
mod3 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
              data = rape, fct = HTG())
mod4 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTLL())
mod5 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTW1())
mod6 &amp;lt;- drmte(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTW2())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the best model for this dataset? Let’s use the Akaike’s Information Criterion (AIC) to decide:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod1, mod2, mod3, mod4, mod5, mod6, modHTE)
##         df      AIC
## mod1   291 3516.914
## mod2   291 3300.824
## mod3   291 3097.775
## mod4   290 2886.608
## mod5   290 2889.306
## mod6   290 3009.023
## modHTE 289 2832.481&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first model &lt;code&gt;modHTE&lt;/code&gt; considers explicitly the distribution of germination times and it is the best fitting of all. The other models consider explicitly the distribution of base water potential, while the distribution of germination times is indirectly included. Among these models, the gaussian is the worse fitting, while the log-logistic is the best one (&lt;code&gt;mod4&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;For this latter model, we take a look at the value of estimated parameters, with cluster robust standard errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(mod4, vcov = vcovCL, cluster = rape$Dish)
## 
## t test of coefficients:
## 
##                     Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## thetaH:(Intercept)  0.676804   0.074160   9.1263   &amp;lt;2e-16 ***
## delta:(Intercept)   1.136671   0.101804  11.1652   &amp;lt;2e-16 ***
## Psib50:(Intercept) -0.947798   0.020575 -46.0647   &amp;lt;2e-16 ***
## sigma:(Intercept)   0.371971   0.172825   2.1523   0.0322 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Germination rates and times for a certain percentile (e.g. GR50, GR30), can be obtained by using the &lt;code&gt;GRate()&lt;/code&gt; and &lt;code&gt;GTime()&lt;/code&gt; function in &lt;code&gt;drcSeedGerm&lt;/code&gt;. Again, the use of cluster-robust standard errors is highly recommended.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GRate(mod4, respLev=c(30, 50, 70), x2 = 0, vcov. = vcovCL)
##          Estimate         SE
## GR:1:30 0.6775790 0.08365887
## GR:1:50 0.7140812 0.08509008
## GR:1:70 0.7710049 0.09376307
GTime(mod4, respLev=c(30, 50, 70), x2 = 0, vcov. = vcovCL)
##        Estimate        SE
## T:1:30 1.475843 0.1822184
## T:1:50 1.400401 0.1668721
## T:1:70 1.297009 0.1577312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also make predictions about the germinated proportion for a certain time and water potential level. The code below returns the maximum germinated proportions at -1.5, -0.75, and 0 MPa.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictSG(mod4, se.fit=T, vcov. = vcovCL,
        newdata = data.frame(Time=c(10, 10, 10), 
                             Psi=c(-1.5, -0.75, 0))
        )
##        Prediction           SE
## [1,] 6.541125e-15 1.102220e-13
## [2,] 8.035981e-01 5.617769e-02
## [3,] 9.906224e-01 9.435188e-03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use the &lt;code&gt;predictSG()&lt;/code&gt; function to plot the ‘modHTE’ and ‘mod4’ objects together in the same graph.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_SeedGermination_HT1step_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Berger, S., Graham, N., Zeileis, A., 2017. Various versatile variances: An object-oriented implementation of clustered covariances in R. Faculty of Economics and Statistics, University of Innsbruck, Innsbruck.&lt;/li&gt;
&lt;li&gt;Bradford, K.J., 2002. Applications of hydrothermal time to quantifying and modeling seed germination and dormancy. Weed Science 50, 248–260.&lt;/li&gt;
&lt;li&gt;Mesgaran, M.B., Mashhadi, H.R., Alizadeh, H., Hunt, J., Young, K.R., Cousens, R.D., 2013. Importance of distribution function selection for hydrothermal time models of seed germination. Weed Research 53, 89–101. &lt;a href=&#34;https://doi.org/10.1111/wre.12008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Neve, P., Cousens, R.D., 2014. Experimental design and parameter estimation for threshold models in seed germination. Weed Research 54, 425–435. &lt;a href=&#34;https://doi.org/10.1111/wre.12095&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12095&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pace, R., Benincasa, P., Ghanem, M.E., Quinet, M., Lutts, S., 2012. Germination of untreated and primed seeds in rapeseed (brassica napus var oleifera del.) under salinity and low matric potential. Experimental Agriculture 48, 238–251.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C. (2019) Dose-Response Analysis Using R CRC Press.
Achim Zeileis, Torsten Hothorn (2002). Diagnostic Checking in Regression Relationships. R News 2(3), 7-10. URL: &lt;a href=&#34;https://CRAN.R-project.org/doc/Rnews/&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/doc/Rnews/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;some-further-detail&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some further detail&lt;/h1&gt;
&lt;p&gt;Let us conclude this page by giving some detail on all equations.&lt;/p&gt;
&lt;p&gt;The equation for the model &lt;code&gt;HTnorm()&lt;/code&gt;. Here, we show all other equations, as implemented in our package.&lt;/p&gt;
&lt;div id=&#34;htl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTL()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \frac{1}{1 + exp \left[ - \frac{  \Psi  - \left( \theta _H/t \right) - \Psi_{b(50)} } {\sigma}  \right] }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htg&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTG()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \exp \left\{ { - \exp \left[ { - \left( {\frac{{\Psi - (\theta _H / t ) - \mu }}{\sigma }} \right)} \right]} \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htll&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTLL()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = \frac{1}{1 + \exp \left\{ \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b50} + \delta)  }{\sigma}\right\} }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htw1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTW1()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = exp \left\{ - \exp \left[ - \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b50} + \delta)  }{\sigma}\right] \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htw2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTW2()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(t, \Psi) = 1 - exp \left\{ - \exp \left[ \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b50} + \delta)  }{\sigma}\right] \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A collection of self-starters for nonlinear regression in R</title>
      <link>https://www.statforbiology.com/2020/stat_nls_usefulfunctions/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_nls_usefulfunctions/</guid>
      <description>


&lt;p&gt;Usually, the first step of every nonlinear regression analysis is to select the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, which best describes the phenomenon under study. The next step is to fit this function to the observed data, possibly by using some sort of nonlinear least squares algorithms. These algorithms are iterative, in the sense that they start from some initial values of model parameters and repeat a sequence of operations, which continuously improve the initial guesses, until the least squares solution is approximately reached.&lt;/p&gt;
&lt;p&gt;This is the main problem: we need to provide initial values for all model parameters! It is not irrelevant; indeed, if our guesses are not close enough to least squares estimates, the algorithm may freeze during the estimation process and may not reach convergence. Unfortunately, guessing good initial values for model parameters is not always easy, especially for students and practitioners. This is where self-starters come in handy.&lt;/p&gt;
&lt;p&gt;Self-starter functions can automatically calculate initial values for any given dataset and, therefore, they can make nonlinear regression analysis as smooth as linear regression analysis. From a teaching perspective, this means that the transition from linear to nonlinear models is immediate and hassle-free!&lt;/p&gt;
&lt;p&gt;In a recent post (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_nls_selfstarting/&#34;&gt;see here&lt;/a&gt;) I gave detail on how self-starters can be built, both for the ‘nls()’ function in the ‘stats’ package and for the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2019). Both ‘nls()’ and ‘drm()’ can be used to fit nonlinear regression models in R and the respective packages already contain several robust self-starting functions. I am a long-time user of both ‘nls()’ and ‘drm()’ and I have little-by-little built a rather wide knowledge base of self-starters for both. I’ll describe them in this post; they are available within the package ‘aomisc’, that is the accompanying package for this blog.&lt;/p&gt;
&lt;p&gt;First of all, we need to install this package (if necessary) and load it, by using the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#installing package, if not yet available
# library(devtools)
# install_github(&amp;quot;onofriandreapg/aomisc&amp;quot;)

# loading package
library(aomisc)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;functions-and-curve-shapes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functions and curve shapes&lt;/h1&gt;
&lt;p&gt;Nonlinear regression functions are usually classified according to the shape they show when they are plotted in a XY-graph. Such an approach is taken, e.g., in the great book of Ratkowsky (1990). The following classification is heavily based on that book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomials
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#straight-line-function&#34;&gt;Straight line function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-polynomial-function&#34;&gt;Quadratic polynomial function&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Concave/Convex curves (no inflection)
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#exponential-function&#34;&gt;Exponential function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#asymptotic-function&#34;&gt;Asymptotic function / Negative exponential function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#power-function&#34;&gt;Power curve function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logarithmic-function&#34;&gt;Logarithmic function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rectangular-hyperbola&#34;&gt;Rectangular hyperbola&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Sigmoidal curves
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-function&#34;&gt;Logistic function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gompertz-function&#34;&gt;Gompertz function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modified-gompertz-function&#34;&gt;Modified Gompertz function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#log-logistic-function&#34;&gt;Log-logistic function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weibull-function-type-1&#34;&gt;Weibull (type 1) function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weibull-function-type-2&#34;&gt;Weibull (type 2) function&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Curves with maxima/minima
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#bragg-function&#34;&gt;Bragg function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lorentz-function&#34;&gt;Lorentz function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beta-function&#34;&gt;Beta function&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s go through these functions and see how we can fit them both with ‘nls()’ and ‘drm()’, by using the appropriate self-starters. There are many functions and, therefore, the post is rather long… however, you can look at the graph below to spot the function you are interested in and use the link above to reach the relevant part in this web page.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-2-1.png&#34; alt=&#34;The shapes of the most important functions&#34; width=&#34;95%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The shapes of the most important functions
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomials&lt;/h1&gt;
&lt;p&gt;Polynomials are a group on their own, as they are characterized by very flexible shapes, which can be used to describe several different biological processes. They are simple and, although they may be curvilinear, they are always linear in the parameters and can be fitted by using least squares methods. One disadvantage is that they cannot describe asymptotic processes, which are very common in biology. Furthermore, polynomials are prone to overfitting, as we may be tempted to add terms to improve the fit, with little care for biological realism.&lt;/p&gt;
&lt;p&gt;Nowadays, thanks to the wide availability of nonlinear regression algorithms, the use of polynomials has sensibly decreased; linear or quadratic polynomials are mainly used when we want to approximate the observed response within a narrow range of a quantitative predictor. On the other hand, higher order polynomials are very rarely seen, in practice.&lt;/p&gt;
&lt;div id=&#34;straight-line-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Straight line function&lt;/h2&gt;
&lt;p&gt;Among the polynomials, we should cite the straight line. Obviously, this is not a curve, although it deserves to be mentioned here. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1 \, X \quad \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is the slope, i.e. the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The Y increases as X increases when &lt;span class=&#34;math inline&#34;&gt;\(b_1 &amp;gt; 0\)&lt;/span&gt;, otherwise it decreases. Straight lines are the most common functions for regression and they are most often used to approximate biological phenomena within a small range for the predictor.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-polynomial-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic polynomial function&lt;/h2&gt;
&lt;p&gt;The quadratic polynomial is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1\, X + b_2 \, X^2 \quad \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt;, taken separately, lack a clear biological meaning. However, it is interesting to consider the first derivative, which measures the rate at which the value &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; changes with respect to the change of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Calculating a derivative may be tricky for biologists; however, we can make use of the available facilities in R, represented by the D() function, which requires an expression as the argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a + b*X + c*X^2), &amp;quot;X&amp;quot;)
## b + c * (2 * X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the first derivative is not constant, but it changes according to the level of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The stationary point, i.e. the point at which the derivative is zero, is &lt;span class=&#34;math inline&#34;&gt;\(X_m = - b_1 / 2 b_2\)&lt;/span&gt;; this point is a maximum when &lt;span class=&#34;math inline&#34;&gt;\(b_2 &amp;gt; 0\)&lt;/span&gt;, otherwise it is a minimum.&lt;/p&gt;
&lt;p&gt;The maximum/minimum value is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_m = \frac{4\,b_0\,b_2 - b_1^2}{4\,b_2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-fitting-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polynomial fitting in R&lt;/h2&gt;
&lt;p&gt;In R, polynomials are fitted by using ‘lm()’. In a couple of cases, I found myself in the need of fitting a polynomial by using nonlinear regression with ‘nls()’ o ‘drm()’. I know, this is not efficient…, but some methods for ‘drc’ objects are rather handy. For these unusual cases, we can use the &lt;code&gt;NLS.linear()&lt;/code&gt;, &lt;code&gt;NLS.poly2()&lt;/code&gt;, &lt;code&gt;DRC.linear()&lt;/code&gt; and &lt;code&gt;DRC.poly2()&lt;/code&gt; self-starting functions, as available in the ‘aomisc’ package. An example of usage is given below: in this case, the polynomial has been used to describe a concave increasing trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- seq(5, 50, 5)
Y &amp;lt;- c(12.6, 74.1, 157.6, 225.5, 303.4, 462.8, 
       669.9, 805.3, 964.2, 1169)

# nls fit
model &amp;lt;- nls(Y ~ NLS.poly2(X, a, b, c))

#drc fit
model &amp;lt;- drm(Y ~ X, fct = DRC.poly2())
summary(model)
## 
## Model fitted: Second Order Polynomial (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value  p-value    
## a:(Intercept) -23.515000  31.175139 -0.7543  0.47528    
## b:(Intercept)   5.466470   2.604011  2.0993  0.07395 .  
## c:(Intercept)   0.371561   0.046141  8.0527 8.74e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  26.50605 (7 degrees of freedom)
plot(model, log = &amp;quot;&amp;quot;, main = &amp;quot;2nd order polynomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concaveconvex-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concave/Convex curves&lt;/h1&gt;
&lt;div id=&#34;exponential-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential function&lt;/h2&gt;
&lt;p&gt;The exponential function describes an increasing/decreasing trend, with constant relative rate. The most common equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  e^{k X} \quad \quad \quad (3) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Two additional and equivalent parameterisations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  b^X  =  e^{d + k X} \quad \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Equations 3 and the two equations 4 are equivalent, as proved by setting &lt;span class=&#34;math inline&#34;&gt;\(b = e^k\)&lt;/span&gt; e &lt;span class=&#34;math inline&#34;&gt;\(a = e^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  b^X  = a  (e^k)^{X} =  a  e^{kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  e^{kX} = e^d \cdot e^{kX} =  e^{d + kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The meaning of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is clear: this is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;. In order to understand the meaning of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, we can calculate the first derivative of the exponential function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * exp(k * X)), &amp;quot;X&amp;quot;)
## a * (exp(k * X) * k)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we see that the ratio of increase/decrease of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{dY}{dX} = k \, a \, e^{k \, X} = k \, Y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, the relative ratio of increase/decrease is constant and equal to &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{dY}{dX} \frac{1}{Y} = k\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases if &lt;span class=&#34;math inline&#34;&gt;\(k &amp;gt; 0\)&lt;/span&gt; (exponential growth), while it decreases when &lt;span class=&#34;math inline&#34;&gt;\(k &amp;lt; 0\)&lt;/span&gt; (exponential decay). This curve is used to describe the growth of populations in non-limiting environmental conditions, or to describe the degradation of xenobiotics in the environment (first-order degradation kinetic).&lt;/p&gt;
&lt;p&gt;Another slightly different parameterisation exists, which is common in bioassay work and it is mainly used as an exponential decay model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \exp(-x/e) \quad \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the same as &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in the model above and &lt;span class=&#34;math inline&#34;&gt;\(e = - 1/k\)&lt;/span&gt;. For all the aforementioned exponential decay equations &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. In the above curve, a lower asymptote &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt; can also be included, for those situations where the phenomenon under study does not approach 0 when the independent variable approaches infinity:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d -c) \exp(-x/e) \quad \quad \quad (6)\]&lt;/span&gt;
The exponential function is nonlinear in &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and needs to be fitted by using ‘nls()’ or ‘drm()’. Several self-starting functions are available: in the package &lt;code&gt;aomisc&lt;/code&gt; you can find &lt;code&gt;NLS.expoGrowth()&lt;/code&gt;, &lt;code&gt;NLS.expoDecay()&lt;/code&gt;, &lt;code&gt;DRC.expoGrowth()&lt;/code&gt; and &lt;code&gt;DRC.expoDecay()&lt;/code&gt;, which can be used to fit the Equation 3, respectively with ‘nls()’ and ‘drm()’. The ‘drc’ package also contains the functions &lt;code&gt;EXD.2()&lt;/code&gt; and &lt;code&gt;EXD.3()&lt;/code&gt;, which can be used to fit, respectively, the Equations 5 and 6. Examples of usage are given below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(degradation)

# nls fit
model &amp;lt;- nls(Conc ~ NLS.expoDecay(Time, a, k),
             data = degradation)

# drm fit
model &amp;lt;- drm(Conc ~ Time, fct = DRC.expoDecay(),
             data = degradation)
summary(model)
## 
## Model fitted: Exponential Decay Model (2 parms)
## 
## Parameter estimates:
## 
##                    Estimate Std. Error t-value   p-value    
## init:(Intercept) 99.6349312  1.4646680  68.026 &amp;lt; 2.2e-16 ***
## k:(Intercept)     0.0670391  0.0019089  35.120 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.621386 (22 degrees of freedom)
plot(model, log = &amp;quot;&amp;quot;, main = &amp;quot;Exponential decay&amp;quot;, ylim = c(0, 110))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotic function&lt;/h2&gt;
&lt;p&gt;The asymptotic function can be used to model the growth of a population/individual, where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; approaches an horizontal asymptote as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; tends to infinity. This function is used in several different parameterisations and it is also known as the monomolecular growth function, the Mitscherlich law or the von Bertalanffy law. Due to its biological meaning, the most widespread parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a - (a - b) \, \exp (- c  X) \quad \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum attainable value for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (plateau), &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the initial &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; value (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is proportional to the relative rate of increase for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. Indeed, we can see that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a - (a - b) * exp (- c * X)), &amp;quot;X&amp;quot;)
## (a - b) * (exp(-c * X) * c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;that is, the absolute ratio of increase of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at a given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is not constant, but depends on the attained value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{dY}{dX} = c \, (a - Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the relative rate of increase of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{dY}{dX} \frac{1}{Y} = c \, \frac{(a - Y)}{Y}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It means that the relative rate of increase of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is maximum at the beginning and approaches 0 when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; approaches the plateau &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In order to fit the asymptotic function, the ‘aomisc’ package contains the self-starting routines &lt;code&gt;NLS.asymReg()&lt;/code&gt; and &lt;code&gt;DRC.asymReg()&lt;/code&gt;, which can be used, respectively, with ‘nls()’ and ‘drm()’. The ‘drc’ package contains the function &lt;code&gt;AR.3()&lt;/code&gt;, that is a similar parameterisation where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named &lt;code&gt;SSasymp()&lt;/code&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(1, 3, 5, 7, 9, 11, 13, 20)
Y &amp;lt;- c(8.22, 14.0, 17.2, 16.9, 19.2, 19.6, 19.4, 19.6)

# nls fit
model &amp;lt;- nls(Y ~ NLS.asymReg(X, init, m, plateau) )

# drm fit
model &amp;lt;- drm(Y ~ X, fct = DRC.asymReg())
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Asymptotic regression&amp;quot;, 
     ylim = c(0,25), xlim = c(0,20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If we take the asymptotic function and set &lt;span class=&#34;math inline&#34;&gt;\(b = 0\)&lt;/span&gt;, we get the negative exponential function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a [1 -  \exp (- c  X) ] \quad \quad \quad (8)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function shows a similar shape as the asymptotic function, but &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is 0 when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is 0 (the curve passes through the origin). It is often used to model the absorbed Photosintetically Active Radiation (&lt;span class=&#34;math inline&#34;&gt;\(Y = PAR_a\)&lt;/span&gt;) as a function of Leaf Area Index (X = LAI). In this case, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the incident PAR (&lt;span class=&#34;math inline&#34;&gt;\(a = PAR_i\)&lt;/span&gt;), and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; represents the extinction coefficient.&lt;/p&gt;
&lt;p&gt;In order to fit the Equation 8, we can use the self-starters &lt;code&gt;NLS.negExp()&lt;/code&gt; with ‘nls()’ and &lt;code&gt;DRC.negExp()&lt;/code&gt; with ‘drm()’; both self-starters are available within the ‘aomisc’ package. The ‘drc’ package contains the function &lt;code&gt;AR.2()&lt;/code&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named &lt;code&gt;SSasympOrig()&lt;/code&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;power-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power function&lt;/h2&gt;
&lt;p&gt;The power function is also known as Freundlich function or allometric function and the most common parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a \, X^b \quad \quad \quad (9)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is perfectly equivalent to an exponential curve on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a\,X^b  = a\, e^{\log( X^b )}  = a\,e^{b \, \log(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve does not have an asymptote for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The slope (first derivative) of the curve is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * X^b), &amp;quot;X&amp;quot;)
## a * (X^(b - 1) * b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both parameters relate to the ‘slope’ of the curve and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates its shape. If &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; b &amp;lt; 1\)&lt;/span&gt;, the response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases and the curve is convex up. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. Otherwise, if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 1\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. The three curve types are shown in Figure 1, above.&lt;/p&gt;
&lt;p&gt;The power function (Freundlich equation) is often used in agricultural chemistry, e.g. to model the sorption of xenobiotics in soil. It is also used to model the number of plant species as a function of sampling area (Muller-Dumbois method). The following example uses the &lt;code&gt;DRC.powerCurve()&lt;/code&gt; and &lt;code&gt;NLS.powerCurve()&lt;/code&gt; self starters in the ‘aomisc’ package to fit a species-area curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(speciesArea)

#nls fit
model &amp;lt;- nls(numSpecies ~ NLS.powerCurve(Area, a, b),
             data = speciesArea)

# drm fit
model &amp;lt;- drm(numSpecies ~ Area, fct = DRC.powerCurve(),
             data = speciesArea)
summary(model)
## 
## Model fitted: Power curve (Freundlich equation) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 4.348404   0.337197  12.896 3.917e-06 ***
## b:(Intercept) 0.329770   0.016723  19.719 2.155e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.9588598 (7 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;logarithmic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logarithmic function&lt;/h2&gt;
&lt;p&gt;This is indeed a linear model on log-transformed &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = a + b \, \log(X) \quad \quad \quad (10)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to the logarithmic function, it must be &lt;span class=&#34;math inline&#34;&gt;\(X &amp;gt; 0\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates the shape: if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 0\)&lt;/span&gt;, the curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, as shown in Figure 1 above.&lt;/p&gt;
&lt;p&gt;The logarithmic equation can be fit by using ‘lm()’. If necessary, it can also be fit by using ‘nls()’ and ‘drm()’: the self-starting functions &lt;code&gt;NLS.logCurve()&lt;/code&gt; and &lt;code&gt;DRC.logCurve()&lt;/code&gt; are available within the ‘aomisc’ package. We show an example of their usage in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(1,2,4,5,7,12)
Y &amp;lt;- c(1.97, 2.32, 2.67, 2.71, 2.86, 3.09)

# lm fit
model &amp;lt;- lm(Y ~ log(X) )

# nls fit
model &amp;lt;- nls(Y ~ NLS.logCurve(X, a, b) )

# drm fit
model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;rectangular-hyperbola&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rectangular hyperbola&lt;/h2&gt;
&lt;p&gt;This function is also known as the Michaelis-Menten function and it is often parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X} \quad \quad \quad (11)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, up to a plateau level. The parameter &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the higher asymptote (for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;), while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the X value giving a response equal to &lt;span class=&#34;math inline&#34;&gt;\(a/2\)&lt;/span&gt;. Indeed, it is easily shown that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a}{2} = \frac{a\,X_{50} } {b + X_{50} } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which leads to &lt;span class=&#34;math inline&#34;&gt;\(b = x_{50}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The slope (first derivative) is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression( (a*X) / (b + X) ), &amp;quot;X&amp;quot;)
## a/(b + X) - (a * X)/(b + X)^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we can see that the initial slope (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) is $i = a/b $. The equation 11 is not defined for &lt;span class=&#34;math inline&#34;&gt;\(X = b\)&lt;/span&gt; and it makes no biological sense for &lt;span class=&#34;math inline&#34;&gt;\(X &amp;lt; b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An alternative parameterisation is obtained considering that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X} = \frac{a}{ \frac{b}{X} + \frac{X}{X}} = \frac{a}{1 + \frac{b}{X}} \quad \quad \quad (12)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This parameterisation is sometimes used in bioassays and it is parameterised with &lt;span class=&#34;math inline&#34;&gt;\(d = a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = b\)&lt;/span&gt;. From a strict mathematical point of view, the equation 12 is not defined for &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, although it approaches 0 when &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Michaelis-Menten function is used in pesticide chemistry, enzyme kinetics and in weed competition studies, to model yield losses as a function of weed density. In R, this model can be fit by using ‘nls()’ and the self-starting functions &lt;code&gt;SSmicmen()&lt;/code&gt;, within the package ‘nlme’. If you prefer a ‘drm()’ fit, you can use the &lt;code&gt;MM.2()&lt;/code&gt; function in the ‘drc’ package, which uses the parameterisation in Equation 12.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(0, 5, 7, 22, 28, 39, 46, 200)
Y &amp;lt;- c(12.74, 13.66, 14.11, 14.43, 14.78, 14.86, 14.78, 14.91)

#drm fit
model &amp;lt;- drm(Y ~ X, fct = MM.2())
summary(model)
## 
## Model fitted: Michaelis-Menten (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value  p-value   
## d:(Intercept) 14.93974    2.86695  5.2110 0.001993 **
## e:(Intercept)  0.45439    2.24339  0.2025 0.846183   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  5.202137 (6 degrees of freedom)
plot(model, log = &amp;quot;&amp;quot;, main = &amp;quot;Michaelis-Menten function&amp;quot;, 
     ylim = c(12, 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sigmoidal-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sigmoidal curves&lt;/h1&gt;
&lt;p&gt;Sigmoidal curves are S-shaped and they may be increasing, decreasing, symmetric or non-symmetric around the inflection point. They are parameterised in countless ways, which may often be confusing. I will show some widespread parameterisations, that are very useful for bioassays or growth studies. All curves can be turned from increasing to decreasing (and vice-versa) by reversing the sign of the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;div id=&#34;logistic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic function&lt;/h2&gt;
&lt;p&gt;The logistic curve derives from the cumulative logistic distribution function; the curve is symmetric around the inflection point and it may be parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + exp(- b (X - e))} \quad \quad \quad (13)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the higher asymptote, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the lower asymptote, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value at the inflection point, while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point. As the curve is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; represents also the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value producing a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; (usually known as the ED50, in biological assay). The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be positive or negative and, consequently, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may increase or decrease as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The above function is known as the four-parameter logistic. If necessary, constraints can be put on parameter values, i.e. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; can be constrained to 0 (three-parameter logistic) and, additionally, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be constrained to 1 (two-parameter logistic).&lt;/p&gt;
&lt;p&gt;In the ‘aomisc’ package, the three logistic curves (four-, three- and two-parameters) are available as &lt;code&gt;NLS.L4()&lt;/code&gt;, &lt;code&gt;NLS.L3()&lt;/code&gt; and &lt;code&gt;NLS.L2()&lt;/code&gt;, respectively. With ‘drm()’, we can use the self-starting functions &lt;code&gt;L.4()&lt;/code&gt; and &lt;code&gt;L.3()&lt;/code&gt; in the package ‘drc’, while the &lt;code&gt;L.2()&lt;/code&gt; function for the two-parameter logistic has been included in the ‘aomisc’ package. The only difference between the self-starters for ‘drm()’ and the self-starters for ‘nls()’ is that, in the former, the sign for the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter is reversed, i.e. it is negative for increasing curves and positive for decreasing curves.&lt;/p&gt;
&lt;p&gt;The four- and three-parameter logistic curves can also be fit with ‘nls()’, respectively with the self-starting functions &lt;code&gt;SSfpl()&lt;/code&gt; and &lt;code&gt;SSlogis()&lt;/code&gt;, in the ‘nlme’ package. In these functions, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(scal = -1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the box below, I show an example, regarding the growth of a sugar-beet crop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(beetGrowth)

# nls fit
model &amp;lt;- nls(weightInf ~ NLS.L3(DAE, b, d, e), data = beetGrowth)
model.2 &amp;lt;- nls(weightInf ~ NLS.L4(DAE, b, c, d, e), data = beetGrowth)
model.3 &amp;lt;- nls(weightInf/max(weightInf) ~ NLS.L2(DAE, b, e), data = beetGrowth)

# drm fit
model &amp;lt;- drm(weightInf ~ DAE, fct = L.3(), data = beetGrowth)
model.2 &amp;lt;- drm(weightInf ~ DAE, fct = L.4(), data = beetGrowth)
model.3 &amp;lt;- drm(weightInf/max(weightInf) ~ DAE, fct = L.2(), data = beetGrowth)
summary(model)
## 
## Model fitted: Logistic (ED50 as parameter) with lower limit fixed at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.118771   0.018319 -6.4835 1.032e-05 ***
## d:(Intercept) 25.118357   1.279417 19.6327 4.127e-12 ***
## e:(Intercept) 58.029764   1.834414 31.6340 3.786e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.219389 (15 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Logistic function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;gompertz-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gompertz function&lt;/h2&gt;
&lt;p&gt;The Gompertz curve is parameterised in very many ways. I tend to prefer a parameterisation that resembles the one used for the logistic function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ - b \, (X - e) \right] \right\} \quad \quad \quad (14)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difference between the logistic and Gompertz functions is that this latter is not symmetric around the inflection point: it shows a longer lag at the beginning, but raises steadily afterwards. The parameters have the same meaning as those in the logistic function, apart from the fact that &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, i.e. the abscissa of the inflection point, does not give a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. As for the logistic, we can have four-, three- and two-parameter Gompertz functions, which can be used to describe plant growth or several other biological processes.&lt;/p&gt;
&lt;p&gt;In R, the Gompertz equation can be fit by using ‘drm()’ and, respectively the &lt;code&gt;G.4()&lt;/code&gt;, &lt;code&gt;G.3()&lt;/code&gt; and &lt;code&gt;G.2()&lt;/code&gt; self-starters. With ‘nls’, the ‘aomisc’ package contains the corresponding functions &lt;code&gt;NLS.G4()&lt;/code&gt;, &lt;code&gt;NLS.G3()&lt;/code&gt; and &lt;code&gt;NLS.G2()&lt;/code&gt;. As for the logistic, the only difference between the self starters for ‘drm()’ and the self starters for ‘nls()’ is that, in the former, the sign for the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter is reversed, i.e. it is negative for increasing curves and positive for decreasing curves.&lt;/p&gt;
&lt;p&gt;The three-parameter Gompertz can also be fit with ‘nls()’, by using the &lt;code&gt;SSGompertz()&lt;/code&gt; self-starter in the ‘nlme’ package, although this is a different parameterisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# nls fit
model &amp;lt;- nls(weightFree ~ NLS.G3(DAE, b, d, e), data = beetGrowth)
model.2 &amp;lt;- nls(weightFree ~ NLS.G4(DAE, b, c, d, e), data = beetGrowth)
model.3 &amp;lt;- nls(weightFree/max(weightFree) ~ NLS.G2(DAE, b, e), data = beetGrowth)

# drm fit
model &amp;lt;- drm(weightFree ~ DAE, fct = G.3(), data = beetGrowth)
model.2 &amp;lt;- drm(weightFree ~ DAE, fct = G.4(), data = beetGrowth)
model.3 &amp;lt;- drm(weightFree/max(weightFree) ~ DAE, fct = G.2(), data = beetGrowth)
summary(model)
## 
## Model fitted: Gompertz with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.122255   0.029938 -4.0836 0.0009783 ***
## d:(Intercept) 35.078529   1.668665 21.0219 1.531e-12 ***
## e:(Intercept) 49.008075   1.165191 42.0601 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.995873 (15 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Gompertz function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;modified-gompertz-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modified Gompertz function&lt;/h2&gt;
&lt;p&gt;We have seen that the logistic curve is symmetric around the inflection point, while the Gompertz shows a longer lag at the beginning and raises steadily afterwards. We can describe a different pattern by modifying the Gompertz function as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \right\} \quad \quad \quad (15)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The resulting curve increases steadily at the beginning, but slows down later on. Also for this function, we can put constraints on &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and/or &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;, so that we have four-parameter, three-parameter and two-parameter modified Gompertz equations; these models can be fit by using ‘nls()’ and the self starters &lt;code&gt;NLS.E4()&lt;/code&gt;, &lt;code&gt;NLS.E3()&lt;/code&gt; and &lt;code&gt;NLS.E2()&lt;/code&gt; in the ‘aomisc’ package. Likewise, the modified Gompertz equations can be fit with ‘drm()’ and the self starters &lt;code&gt;E.4()&lt;/code&gt;, &lt;code&gt;E.3()&lt;/code&gt; and &lt;code&gt;E.2()&lt;/code&gt;, also available in the ‘aomisc’ package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# nls fit
model &amp;lt;- nls(weightInf ~ NLS.E3(DAE, b, d, e), data = beetGrowth)
model.2 &amp;lt;- nls(weightFree ~ NLS.E4(DAE, b, c, d, e), data = beetGrowth)
model.3 &amp;lt;- nls(I(weightFree/max(weightFree)) ~ NLS.E2(DAE, b, e), data = beetGrowth)


# drm fit
model &amp;lt;- drm(weightInf ~ DAE, fct = E.3(), data = beetGrowth)
model.2 &amp;lt;- drm(weightFree ~ DAE, fct = E.4(), data = beetGrowth)
model.3 &amp;lt;- drm(weightFree/max(weightFree) ~ DAE, fct = E.2(), data = beetGrowth)
summary(model)
## 
## Model fitted: Modified Gompertz equation (3 parameters) (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept)  0.092508   0.013231  6.9917 4.340e-06 ***
## d:(Intercept) 25.107004   1.304379 19.2482 5.493e-12 ***
## e:(Intercept) 63.004111   1.747087 36.0624 4.945e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.253878 (15 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Modified Gompertz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;log-logistic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log-logistic function&lt;/h2&gt;
&lt;p&gt;The log-logistic curve is symmetric on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (a log-normal curve would be practically equivalent, but it is used far less often). For example, in biologic assays (but also in germination assays), the log-logistic curve is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \exp \left\{ - b \left[ \log(X) - \log(e) \right] \right\} } \quad \quad \quad (16)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters have the same meaning as the logistic equation given above. In particular, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; represents the X-level which gives a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (ED50). It is easy to see that the above equation is equivalent to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \left( \frac{X}{e} \right)^{-b}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Log-logistic functions are used for crop growth, seed germination and bioassay work and they can have the same constraints as the logistic function (four- three- and two-parameter log-logistic). They can be fit by ‘drm()’ and the self starters &lt;code&gt;LL.4()&lt;/code&gt; (four-parameter log-logistic), &lt;code&gt;LL.3()&lt;/code&gt; (three-parameter log-logistic, with &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;) and &lt;code&gt;LL.2()&lt;/code&gt; (two-parameter log-logistic, with &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;), as available in the ‘drc’ package. With respect to Equation 16, in these self-starters the sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is reversed, i.e. it is negative for the increasing log-logistic curve and positive for the decreasing curve. In the package ‘aomisc’, the corresponding self starters for ‘nls()’ are &lt;code&gt;NLS.LL4()&lt;/code&gt;, &lt;code&gt;NLS.LL3()&lt;/code&gt; and &lt;code&gt;NLS.LL2()&lt;/code&gt;, which are all derived from Equation 15 (i.e. the sign of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is positive for increasing curves and negative for decreasing curves).&lt;/p&gt;
&lt;p&gt;We show an example of a log-logistic fit, relating to a bioassay with &lt;em&gt;Brassica rapa&lt;/em&gt; treated at increasing dosages of an herbicide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)

model &amp;lt;- nls(FW ~ NLS.LL4(Dose, b, c, d, e), data = brassica)
model &amp;lt;- nls(FW ~ NLS.LL3(Dose, b, d, e), data = brassica)
model &amp;lt;- nls(FW/max(FW) ~ NLS.LL2(Dose, b, e), data = brassica)

model &amp;lt;- drm(FW ~ Dose, fct = LL.4(), data = brassica)
summary(model)
## 
## Model fitted: Log-logistic (ED50 as parameter) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.45113    0.24113  6.0181 1.743e-06 ***
## c:(Intercept)  0.34948    0.18580  1.8810   0.07041 .  
## d:(Intercept)  4.53636    0.20514 22.1140 &amp;lt; 2.2e-16 ***
## e:(Intercept)  2.46557    0.35111  7.0221 1.228e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4067837 (28 degrees of freedom)
plot(model, ylim = c(0,6), main = &amp;quot;Log-logistic function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-function-type-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull function (type 1)&lt;/h2&gt;
&lt;p&gt;The type 1 Weibull function corresponds to the Gompertz, but it is based on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ - b \, (\log(X) - \log(e)) \right] \right\} \quad \quad \quad (17)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters have the same meaning as the other sigmoidal curves given above, apart from the fact that &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the abscissa of the inflection point, but it is not the ED50.&lt;/p&gt;
&lt;p&gt;The four-parameters, three-parameters and two-parameters Weibull functions can be fit by using ‘drm()’ and the self-starters available within the ‘drc’ package, i.e. &lt;code&gt;W1.4()&lt;/code&gt;, &lt;code&gt;W1.3()&lt;/code&gt; and &lt;code&gt;W1.2()&lt;/code&gt;. The ‘aomisc’ package contains the corresponding self-starters &lt;code&gt;NLS.W1.4()&lt;/code&gt;, &lt;code&gt;NLS.W1.3()&lt;/code&gt; and &lt;code&gt;NLS.W1.2()&lt;/code&gt;, which can be used with ‘nls()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)

model &amp;lt;- nls(FW ~ NLS.W1.4(Dose, b, c, d, e), data = brassica)
model.2 &amp;lt;- nls(FW ~ NLS.W1.3(Dose, b, d, e), data = brassica)
model.3 &amp;lt;- nls(FW/max(FW) ~ NLS.W1.2(Dose, b, e), data = brassica)

model &amp;lt;- drm(FW ~ Dose, fct = W1.4(), data = brassica)
model.2 &amp;lt;- drm(FW ~ Dose, fct = W1.3(), data = brassica)
model.3 &amp;lt;- drm(FW/max(FW) ~ Dose, fct = W1.2(), data = brassica)
summary(model)
## 
## Model fitted: Weibull (type 1) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.01252    0.15080  6.7143 2.740e-07 ***
## c:(Intercept)  0.50418    0.14199  3.5507  0.001381 ** 
## d:(Intercept)  4.56137    0.19846 22.9841 &amp;lt; 2.2e-16 ***
## e:(Intercept)  3.55327    0.45039  7.8894 1.359e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.3986775 (28 degrees of freedom)
plot(model, ylim = c(0,6), main = &amp;quot;Weibull type 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-function-type-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull function (type 2)&lt;/h2&gt;
&lt;p&gt;The type 2 Weibull is similar to the type 1 Weibull, but describes a different type of asymmetry, corresponding to the modified Gompertz function above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (\log(X) - \log(e)) \right] \right\} \right\} \quad \quad \quad (18)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As for fitting, the ‘drc’ package contains the self-starting functions &lt;code&gt;W2.4()&lt;/code&gt;, &lt;code&gt;W2.3()&lt;/code&gt; and &lt;code&gt;W2.2()&lt;/code&gt;, which can be used with ‘drm()’ (be careful: the sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is reversed, with respect to Equation 18). The ‘aomisc’ package contains the corresponding self-starters &lt;code&gt;NLS.W2.4()&lt;/code&gt;, &lt;code&gt;NLS.W2.3()&lt;/code&gt; and &lt;code&gt;NLS.W2.2()&lt;/code&gt;, which can be used with ‘nls()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)

model &amp;lt;- nls(FW ~ NLS.W2.4(Dose, b, c, d, e), data = brassica)
model.1 &amp;lt;- nls(FW ~ NLS.W2.3(Dose, b, d, e), data = brassica)
model.2 &amp;lt;- nls(FW/max(FW) ~ NLS.W2.2(Dose, b, e), data = brassica)

model &amp;lt;- drm(FW ~ Dose, fct = W2.4(), data = brassica)
summary(model)
## 
## Model fitted: Weibull (type 2) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.96191    0.17684 -5.4395 8.353e-06 ***
## c:(Intercept)  0.18068    0.25191  0.7173    0.4792    
## d:(Intercept)  4.53804    0.21576 21.0328 &amp;lt; 2.2e-16 ***
## e:(Intercept)  1.66342    0.25240  6.5906 3.793e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4215551 (28 degrees of freedom)
plot(model, ylim = c(0,6), main = &amp;quot;Weibull type 2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;curves-with-maximaminima&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Curves with maxima/minima&lt;/h1&gt;
&lt;p&gt;It is sometimes necessary to describe phenomena where the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; variable reaches a maximum value at a certain level of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; variable, and drops afterwords. For example, growth or germination rates are higher at optimal temperature levels and lower at supra-optimal or sub-optimal temperature levels. Another example relates to bioassays: in some cases, low doses of toxic substances induce a stimulation of growth (hormesis), which needs to be described by an appropriate model. Let’s see some functions which may turn out useful in these circumstances.&lt;/p&gt;
&lt;div id=&#34;bragg-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bragg function&lt;/h2&gt;
&lt;p&gt;This function is connected to the normal (Gaussian) distribution and has a symmetric shape with a maximum equal to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, that is reached when &lt;span class=&#34;math inline&#34;&gt;\(X = e\)&lt;/span&gt; and two inflection points. In this model, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; relates to the slope at the inflection points; the response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; approaches 0 when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; approaches &lt;span class=&#34;math inline&#34;&gt;\(\pm \infty\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \, \exp \left[ - b (X - e)^2 \right] \quad \quad \quad (19)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we would like to have lower asymptotes different from 0, we should add the parameter &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \, \exp \left[ - b (X - e)^2 \right] \quad \quad \quad (20)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two Bragg curves have been used in applications relating to the science of carbon materials. We can fit them with ‘drm()’, by using the self starters &lt;code&gt;DRC.bragg.3()&lt;/code&gt; (Equation 19) and &lt;code&gt;DRC.bragg.4&lt;/code&gt; (Equation 20), in the ‘aomisc’ package. With ‘nls()’, you can use the corresponding self-starters &lt;code&gt;NLS.bragg.3()&lt;/code&gt; and &lt;code&gt;NLS.bragg.4&lt;/code&gt;, also in the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
Y1 &amp;lt;- c(0.1, 2, 5.7, 9.3, 19.7, 28.4, 20.3, 6.6, 1.3, 0.1)
Y2 &amp;lt;- Y1 + 2

# nls fit
mod.nls &amp;lt;- nls(Y1 ~ NLS.bragg.3(X, b, d, e) )
mod.nls2 &amp;lt;- nls(Y2 ~ NLS.bragg.4(X, b, c, d, e) )

# drm fit
mod.drc &amp;lt;- drm(Y1 ~ X, fct = DRC.bragg.3() )
mod.drc2 &amp;lt;- drm(Y2 ~ X, fct = DRC.bragg.4() )
plot(mod.drc, ylim = c(0, 30), log = &amp;quot;&amp;quot;) 
plot(mod.drc2, add = T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lorentz-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lorentz function&lt;/h2&gt;
&lt;p&gt;The Lorentz function is similar to the Bragg function, although it has worse statistical properties (Ratkowsky, 1990). The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = \frac{d} { 1 + b (X - e)^2 } \quad \quad \quad (21)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also allow for lower asymptotes different from 0, by adding a further parameter:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + \frac{d - c} { 1 + b (X - e)^2 } \quad \quad \quad (22)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two Lorentz functions can be fit with ‘drm()’, by using the self-starters &lt;code&gt;DRC.lorentz.3()&lt;/code&gt; (Equation 21) and &lt;code&gt;DRC.lorentz.4&lt;/code&gt; (Equation 22), in the ‘aomisc’ package. With ‘nls()’, you can use the corresponding self-starters &lt;code&gt;NLS.lorentz.3()&lt;/code&gt; and &lt;code&gt;NLS.lorentz.4&lt;/code&gt;, also in the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
Y1 &amp;lt;- c(0.1, 2, 5.7, 9.3, 19.7, 28.4, 20.3, 6.6, 1.3, 0.1)
Y2 &amp;lt;- Y1 + 2

# nls fit
mod.nls &amp;lt;- nls(Y1 ~ NLS.lorentz.3(X, b, d, e) )
mod.nls2 &amp;lt;- nls(Y2 ~ NLS.lorentz.4(X, b, c, d, e) )

# drm fit
mod.drc &amp;lt;- drm(Y1 ~ X, fct = DRC.lorentz.3() )
mod.drc2 &amp;lt;- drm(Y2 ~ X, fct = DRC.lorentz.4() )
plot(mod.drc, ylim = c(0, 30), log = &amp;quot;&amp;quot;) 
plot(mod.drc2, add = T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta function&lt;/h2&gt;
&lt;p&gt;The beta function derives from the beta density function and it has been adapted to describe phenomena taking place only within a minimum and a maximum threshold value (threshold model). One typical example is seed germination, where the germination rate (GR, i.e. the inverse of germination time) is 0 below the base temperature level and above the cutoff temperature level. Between these two extremes, the GR increases with temperature up to a maximum level, that is reached at the optimal temperature level.&lt;/p&gt;
&lt;p&gt;The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \,\left\{  \left( \frac{X - X_b}{X_o - X_b} \right) \left( \frac{X_c - X}{X_c - X_o} \right) ^ {\frac{X_c - X_o}{X_o - X_b}} \right\}^b \quad \quad \quad (23)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum level for the expected response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_c\)&lt;/span&gt; are, respectively, the minumum and maximum threshold levels, &lt;span class=&#34;math inline&#34;&gt;\(X_o\)&lt;/span&gt; is the abscissa at the maximum expected response level and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is a shape parameter. The above function is only defined for &lt;span class=&#34;math inline&#34;&gt;\(X_b &amp;lt; X &amp;lt; X_c\)&lt;/span&gt; and it returns 0 elsewhere.&lt;/p&gt;
&lt;p&gt;In R, the beta function can be fitted either with ‘drm()’ and the self-starter &lt;code&gt;DRC.beta()&lt;/code&gt;, or with ‘nls()’ and the self-starter &lt;code&gt;NLS.beta()&lt;/code&gt;. Both the self-starters are available within the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
Y &amp;lt;- c(0, 0, 0, 7.7, 12.3, 19.7, 22.4, 20.3, 6.6, 0, 0)

model &amp;lt;- nls(Y ~ NLS.beta(X, b, d, Xb, Xo, Xc))
model &amp;lt;- drm(Y ~ X, fct = DRC.beta())
summary(model)
## 
## Model fitted: Beta function
## 
## Parameter estimates:
## 
##                Estimate Std. Error  t-value   p-value    
## b:(Intercept)   1.29834    0.26171   4.9609 0.0025498 ** 
## d:(Intercept)  22.30117    0.53645  41.5715 1.296e-08 ***
## Xb:(Intercept)  9.41770    1.15826   8.1309 0.0001859 ***
## Xo:(Intercept) 31.14068    0.58044  53.6504 2.815e-09 ***
## Xc:(Intercept) 40.47294    0.33000 122.6455 1.981e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.7251948 (6 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Beta function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are; I have discussed more than 20 models, which are commonly used for nonlinear regression. These models can be found in several different parameterisations and flavours; they can also be modified and combined to suit the needs of several disciplines in biology, chemistry and so on. However, this will require another post.&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-readings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further readings&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Miguez, F., Archontoulis, S., Dokoohaki, H., Glaz, B., Yeater, K.M., 2018. Chapter 15: Nonlinear Regression Models and Applications, in: ACSESS Publications. American Society of Agronomy, Crop Science Society of America, and Soil Science Society of America, Inc.&lt;/li&gt;
&lt;li&gt;Ratkowsky, D.A., 1990. Handbook of nonlinear regression models. Marcel Dekker Inc., New York, USA.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2019&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dose-Response Analysis Using R. CRC Press&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-starting routines for nonlinear regression models</title>
      <link>https://www.statforbiology.com/2020/stat_nls_selfstarting/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_nls_selfstarting/</guid>
      <description>


&lt;p&gt;In R, the &lt;code&gt;drc&lt;/code&gt; package represents one of the main solutions for nonlinear regression and dose-response analyses (Ritz et al., 2015). It comes with a lot of nonlinear models, which are useful to describe several biological processes, from plant growth to bioassays, from herbicide degradation to seed germination. These models are provided with self-starting functions, which free the user from the hassle of providing initial guesses for model parameters. Indeed, getting these guesses may be a tricky task, both for students and for practitioners.&lt;/p&gt;
&lt;p&gt;Obviously, we should not expect that all possible models and parameterisations are included in the ‘drc’ package; therefore, sooner or later, we may all find ourselves in the need of building a user-defined function, for some peculiar tasks of nonlinear regression analysis. I found myself in that position several times in the past and it took me awhile to figure out a solution.&lt;/p&gt;
&lt;p&gt;In this post, I’ll describe how we can simply build self-starters for our nonlinear regression analyses, to be used in connection with the ‘drm()’ function in the ‘drc’ package. In the end, I will also extend the approach to work with the ‘nls()’ function in the ‘stats’ package.&lt;/p&gt;
&lt;p&gt;Let’s consider the following dataset, depicting the relationship between temperature and growth rate. We may note that the response reaches a maximum value around 30°C, while it is lower below and above such an optimal value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
Temp &amp;lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
RGR &amp;lt;- c(0.1, 2, 5.7, 9.3, 19.7, 28.4, 20.3, 6.6, 1.3, 0.1)
plot(RGR ~ Temp, xlim = c(5, 50), 
     xlab = &amp;quot;Temperature&amp;quot;, ylab = &amp;quot;Growth rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_selfStarting_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Bragg equation can be a good candidate model for such a situation. It is characterized by a bell-like shape:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \, \exp \left[- b \, (X - e)^2 \right] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the observed growth rate, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the temperature, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum level for the expected response, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the abscissa at which such maximum occurs and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection points (the curve is bell-shaped and shows two inflection points around the maximum value). Unfortunately, such an equation is not already available within the &lt;code&gt;drc&lt;/code&gt; package. What should we do, then?&lt;/p&gt;
&lt;p&gt;First of all, let’s write this function in the usual R code. In my opinion, this is more convenient than writing it directly within the &lt;code&gt;drc&lt;/code&gt; framework; indeed, the usual R coding is not specific to any packages and can be used with all other nonlinear regression and plotting facilities, such as &lt;code&gt;nls()&lt;/code&gt;, or &lt;code&gt;nlme()&lt;/code&gt;. Let’s call this new function &lt;code&gt;bragg.3.fun()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Definition of Bragg function
bragg.3.fun &amp;lt;- function(X, b, d, e){
  d * exp(- b * (X - e)^2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to transport ‘bragg.3.fun()’ into the &lt;code&gt;drc&lt;/code&gt; platform, we need to code a function returning a list of (at least) three components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a response function (fct)&lt;/li&gt;
&lt;li&gt;a self-starting routine (ssfct)&lt;/li&gt;
&lt;li&gt;a vector with the names of parameters (names)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Optionally, we can also include a descriptive text, the derivatives and other useful information. This is the skeleton code, which I use as the template.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MyNewDRCfun &amp;lt;- function(){

  fct &amp;lt;- function(x, parm) {
      # function code here
  }
  ssfct &amp;lt;- function(data){
     # Self-starting code here
  }
  names &amp;lt;- c()
  text &amp;lt;- &amp;quot;Descriptive text&amp;quot;
    
  ## Returning the function with self starter and names
  returnList &amp;lt;- list(fct = fct, ssfct = ssfct, names = names, text = text)
  class(returnList) &amp;lt;- &amp;quot;drcMean&amp;quot;
  invisible(returnList)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two functions &lt;code&gt;fct()&lt;/code&gt; and &lt;code&gt;ssfct()&lt;/code&gt; are called internally by the &lt;code&gt;drm()&lt;/code&gt; function and, therefore, the list of arguments must be defined exactly as shown above. In detail, &lt;code&gt;fct()&lt;/code&gt; receives two arguments as inputs: the predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the dataframe of parameters, with one row and as many columns as there are parameters in the model. The predictor and parameters are used to return the vector of responses; in the code below, I am calling the function &lt;code&gt;bragg.3.fun()&lt;/code&gt; from within the function &lt;code&gt;fct()&lt;/code&gt;. Alternatively, the Bragg function can be directly coded within &lt;code&gt;fct()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fct &amp;lt;- function(x, parm) {
  bragg.3.fun(x, parm[,1], parm[,2], parm[,3])
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;ssfct()&lt;/code&gt; receives one argument as input, that is a dataframe with the predictor in the first column and the observed response in the second. These two variables can be used to calculate the starting values for model parameters. In order to get a starting value for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, we could take the maximum value for the observed response, by using the function &lt;code&gt;max()&lt;/code&gt;. Likewise, to get a starting value for &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, we could take the positioning of the maximum value in the observed response and use it to index the predictor. Once we have obtained a starting value for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, we can note that, from the Bragg equation, with simple math, we can derive the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \log \left( \frac{Y}{d} \right) = - b \left( X - e\right)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if we transform the observed response and the predictor as above, we can use polynomial regression to estimate a starting value for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. In the end, this self starting routine can be coded as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ssftc &amp;lt;- function(data){
  # Get the data     
  x &amp;lt;- data[, 1]
  y &amp;lt;- data[, 2]
  
  d &amp;lt;- max(y)
  e &amp;lt;- x[which.max(y)]
  
  ## Linear regression on pseudo-y and pseudo-x
  pseudoY &amp;lt;- log( y / d )
  pseudoX &amp;lt;- (x - e)^2
  coefs &amp;lt;- coef( lm(pseudoY ~ pseudoX - 1) )
  b &amp;lt;- coefs[1]
  return( c(b, d, e) )
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It may be worth to state that the self-starting function may be simply skipped by specifying starting values for model parameters, right inside &lt;code&gt;ssfct()&lt;/code&gt; (see Kniss et al., 2011).&lt;/p&gt;
&lt;p&gt;Now, let’s ‘encapsulate’ all components within the skeleton function given above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DRC.bragg.3 &amp;lt;- function(){
  fct &amp;lt;- function(x, parm) {
    bragg.3.fun(x, parm[,1], parm[,2], parm[,3])
  }
  ssfct &amp;lt;- function(data){
    # Get the data     
    x &amp;lt;- data[, 1]
    y &amp;lt;- data[, 2]
    
    d &amp;lt;- max(y)
    e &amp;lt;- x[which.max(y)]
    
    ## Linear regression on pseudo-y and pseudo-x
    pseudoY &amp;lt;- log( y / d )
    pseudoX &amp;lt;- (x - e)^2
    coefs &amp;lt;- coef( lm(pseudoY ~ pseudoX - 1) )
    b &amp;lt;- - coefs[1]
    start &amp;lt;- c(b, d, e)
    return( start )
  }
  names &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;)
  text &amp;lt;- &amp;quot;Bragg equation&amp;quot;
    
  ## Returning the function with self starter and names
  returnList &amp;lt;- list(fct = fct, ssfct = ssfct, names = names, text = text)
  class(returnList) &amp;lt;- &amp;quot;drcMean&amp;quot;
  invisible(returnList)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the &lt;code&gt;DRC.bragg.3()&lt;/code&gt; function is ready, it can be used as the value for the argument &lt;code&gt;fct&lt;/code&gt; in the &lt;code&gt;drm()&lt;/code&gt; function call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- drm(RGR ~ Temp, fct = DRC.bragg.3())
summary(mod)
## 
## Model fitted: Bragg equation
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## b:(Intercept)  0.0115272  0.0014506  7.9466 9.513e-05 ***
## d:(Intercept) 27.4122086  1.4192874 19.3141 2.486e-07 ***
## e:(Intercept) 29.6392304  0.3872418 76.5393 1.710e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  1.71838 (7 degrees of freedom)
plot(mod, log = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_selfStarting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;and-what-about-nls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And… what about nls()?&lt;/h1&gt;
&lt;p&gt;Yes, I know, some of you may prefer using the function &lt;code&gt;nls()&lt;/code&gt;, within the &lt;code&gt;stats&lt;/code&gt; package. In that platform, we can directly use &lt;code&gt;bragg.3.fun()&lt;/code&gt; as the response model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.nls &amp;lt;- nls(RGR ~ bragg.3.fun(Temp, b, d, e),
               start = list (b = 0.01, d = 27, e = 30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we are forced to provide starting values for all estimands, which might be a tricky task, unless we build a self-starting routine, as we did before for the &lt;code&gt;drc&lt;/code&gt; platform. This is not an impossible task and we can also re-use part of the code we have already written above. We have to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;build a self-starting function by using the appropriate coding (see below). In this step we should be careful to the command &lt;code&gt;sortedXyData(mCall[[&amp;quot;X&amp;quot;]], LHS, data)&lt;/code&gt;. The part in quotation marks (“X”) should correspond to the name of the predictor in the &lt;code&gt;bragg.3.fun()&lt;/code&gt; function definition.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;selfStart()&lt;/code&gt; function to combine the function with its self-starting routine.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bragg.3.init &amp;lt;- function(mCall, LHS, data) {
    xy &amp;lt;- sortedXyData(mCall[[&amp;quot;X&amp;quot;]], LHS, data)
    x &amp;lt;-  xy[, &amp;quot;x&amp;quot;]; y &amp;lt;- xy[, &amp;quot;y&amp;quot;]
    
    d &amp;lt;- max(y)
    e &amp;lt;- x[which.max(y)]

    ## Linear regression on pseudo-y and pseudo-x
    pseudoY &amp;lt;- log( y / d )
    pseudoX &amp;lt;- (x - e)^2
    coefs &amp;lt;- coef( lm(pseudoY ~ pseudoX - 1) )
    b &amp;lt;- - coefs[1]
    start &amp;lt;- c(b, d, e)
    names(start) &amp;lt;- mCall[c(&amp;quot;b&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;)]
    start
}

NLS.bragg.3 &amp;lt;- selfStart(bragg.3.fun, bragg.3.init, parameters=c(&amp;quot;b&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can use the &lt;code&gt;NLS.bragg.3()&lt;/code&gt; function in the &lt;code&gt;nls()&lt;/code&gt; call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.nls &amp;lt;- nls(RGR ~ NLS.bragg.3(Temp, b, d, e) )
summary(mod.nls)
## 
## Formula: RGR ~ NLS.bragg.3(Temp, b, d, e)
## 
## Parameters:
##    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## b  0.011527   0.001338   8.618 5.65e-05 ***
## d 27.411715   1.377361  19.902 2.02e-07 ***
## e 29.638976   0.382131  77.562 1.56e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.718 on 7 degrees of freedom
## 
## Number of iterations to convergence: 8 
## Achieved convergence tolerance: 5.203e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have been building a lot of self-starters, both for &lt;code&gt;drm()&lt;/code&gt; and for &lt;code&gt;nls()&lt;/code&gt; and I have shared them within my &lt;code&gt;aomisc&lt;/code&gt; package. Therefore, should you need to fit some unusual nonlinear regression model, it may be worth to take a look at that package, to see whether you find something suitable.&lt;/p&gt;
&lt;p&gt;That’s it, thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Kniss, A.R., Vassios, J.D., Nissen, S.J., Ritz, C., 2011. Nonlinear Regression Analysis of Herbicide Absorption Studies. Weed Science 59, 601–610. &lt;a href=&#34;https://doi.org/10.1614/WS-D-11-00034.1&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1614/WS-D-11-00034.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J. C., Gerhard, D. (2015) Dose-Response Analysis Using R PLOS ONE, 10(12), e0146021&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some everyday data tasks: a few hints with R (revisited)</title>
      <link>https://www.statforbiology.com/2020/stat_r_shapingdata2/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_r_shapingdata2/</guid>
      <description>


&lt;p&gt;One year ago, I published a post titled ‘Some everyday data tasks: a few hints with R’. In that post, I considered four data tasks, that we all need to accomplish daily, i.e.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;subsetting&lt;/li&gt;
&lt;li&gt;sorting&lt;/li&gt;
&lt;li&gt;casting&lt;/li&gt;
&lt;li&gt;melting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In that post, I used the methods I was more familiar with. And, as a long-time R user, I have mainly incorporated in my workflow all the functions from the base R implementation.&lt;/p&gt;
&lt;p&gt;But now, the tidyverse is with us! Well, as far as I know, the tidyverse has been around long before my post. However, for a long time, I did not want to surrender to such a new paradygm. I am no longer a young scientist and, therefore, picking up new techniques is becoming more difficult: why should I abandon my effective workflow in favour of new techniques, which I am not familiar with? Yes I know, the young scientists are thinking that I am just an old dinosaur, who is trying to resist to progress by all means… It is a good point! I see that reading the code produced by my younger collegues is becoming difficult, due to the massive use of the tidyverse and the pipes. I still have a few years to go, before retirement and I do not yet fell like being set aside. Therefore, a few weeks ago I finally surrendered and ‘embraced’ the tidyverse. Here is how I revisited my previous post.&lt;/p&gt;
&lt;div id=&#34;subsetting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subsetting the data&lt;/h1&gt;
&lt;p&gt;Subsetting means selecting the records (rows) or the variables (columns) which satisfy certain criteria. Let’s take the ‘students.csv’ dataset, which is available on one of my repositories. It is a database of student’s marks in a series of exams for different subjects and, obviously, I will use the ‘readr’ package to read it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
library(tidyr)
students &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/students.csv&amp;quot;)
students
## # A tibble: 232 x 6
##       Id Subject  Date        Mark  Year HighSchool 
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      
##  1     1 AGRONOMY 10/06/2002    30  2001 HUMANITIES 
##  2     2 AGRONOMY 08/07/2002    24  2001 AGRICULTURE
##  3     3 AGRONOMY 24/06/2002    30  2001 AGRICULTURE
##  4     4 AGRONOMY 24/06/2002    26  2001 HUMANITIES 
##  5     5 AGRONOMY 23/01/2003    30  2001 HUMANITIES 
##  6     6 AGRONOMY 09/09/2002    28  2001 AGRICULTURE
##  7     7 AGRONOMY 24/02/2003    26  2001 HUMANITIES 
##  8     8 AGRONOMY 09/09/2002    26  2001 SCIENTIFIC 
##  9     9 AGRONOMY 09/09/2002    23  2001 ACCOUNTING 
## 10    10 AGRONOMY 08/07/2002    27  2001 HUMANITIES 
## # … with 222 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the usual &lt;code&gt;read.csv&lt;/code&gt; function I saved some typing, as I did not need to specify the ‘header = T’ argument. Furthermore, printing the tibble only shows the first ten rows, which makes the ‘head()’ function no longer needed.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and try to subset this tibble: we want to extract the good students, with marks higher than 28. In my previous post, I used the ‘subset()’ function. Now, I will use the ‘filter()’ function in the ‘dplyr’ package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subData &amp;lt;- subset(students, Mark &amp;gt;= 28)
subData &amp;lt;- filter(students, Mark &amp;gt;= 28)
subData
## # A tibble: 87 x 6
##       Id Subject  Date        Mark  Year HighSchool  
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1     1 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  2     3 AGRONOMY 24/06/2002    30  2001 AGRICULTURE 
##  3     5 AGRONOMY 23/01/2003    30  2001 HUMANITIES  
##  4     6 AGRONOMY 09/09/2002    28  2001 AGRICULTURE 
##  5    11 AGRONOMY 09/09/2002    28  2001 SCIENTIFIC  
##  6    17 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  7    18 AGRONOMY 10/06/2002    30  2001 AGRICULTURE 
##  8    19 AGRONOMY 09/09/2002    30  2001 AGRICULTURE 
##  9    20 AGRONOMY 09/09/2002    30  2001 OTHER SCHOOL
## 10    22 AGRONOMY 23/01/2003    30  2001 ACCOUNTING  
## # … with 77 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have noted that all other subsetting examples in my previous post can be solved by simply replacing ‘subset()’ with ‘filter()’, with no other changes. However, differences appear when I try to select the columns. Indeed, ‘dplyr’ has a specific function ‘select()’, which should be used for this purpose. Therefore, in the case that I want to select the students with marks ranging from 26 to 28 in Maths or Chemistry and, at the same time, I want to report only the three columns ‘Subject’, ‘Mark’ and ‘Date’, I need to split the process in two steps (filter and, then, select):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
#                     Subject == &amp;quot;MATHS&amp;quot; | 
#                     Subject == &amp;quot;CHEMISTRY&amp;quot;,
#                   select = c(Subject, Mark, HighSchool))
subData1 &amp;lt;- filter(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;)
subData &amp;lt;- select(subData1, c(Subject, Mark, HighSchool))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the above two-steps process I could easily understand the meaning of the pipe operator: it simply replaces the word ‘then’ between the two steps (&lt;code&gt;filter&lt;/code&gt; and then &lt;code&gt;select&lt;/code&gt; is translated into &lt;code&gt;filter %&amp;gt;% select&lt;/code&gt;). Here is the resulting code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students %&amp;gt;%
  filter(Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;) %&amp;gt;%
  select(c(Subject, Mark, HighSchool))
subData
## # A tibble: 50 x 3
##    Subject    Mark HighSchool  
##    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1 CHEMISTRY    20 AGRICULTURE 
##  2 CHEMISTRY    21 HUMANITIES  
##  3 CHEMISTRY    21 HUMANITIES  
##  4 CHEMISTRY    18 AGRICULTURE 
##  5 CHEMISTRY    28 OTHER SCHOOL
##  6 CHEMISTRY    23 ACCOUNTING  
##  7 CHEMISTRY    26 ACCOUNTING  
##  8 CHEMISTRY    27 AGRICULTURE 
##  9 CHEMISTRY    27 SCIENTIFIC  
## 10 CHEMISTRY    23 ACCOUNTING  
## # … with 40 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the end: there is not much difference between ‘subset()’ and ‘filter()’. However, I must admit I am seduced by the ‘pipe’ operator… my younger collegues may be right: it should be possible to chain several useful data management steps, producing highly readable code. But… how about debugging?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sorting the data&lt;/h1&gt;
&lt;p&gt;In my previous post I showed how to sort a data frame by using the ‘order()’ function. Now, I can use the ‘arrange()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sortedData &amp;lt;- students[order(-students$Mark, students$Subject), ]
# head(sortedData)
sortedData &amp;lt;- arrange(students, desc(Mark), Subject)
sortedData
## # A tibble: 232 x 6
##       Id Subject  Date        Mark  Year HighSchool  
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1     1 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  2     3 AGRONOMY 24/06/2002    30  2001 AGRICULTURE 
##  3     5 AGRONOMY 23/01/2003    30  2001 HUMANITIES  
##  4    17 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  5    18 AGRONOMY 10/06/2002    30  2001 AGRICULTURE 
##  6    19 AGRONOMY 09/09/2002    30  2001 AGRICULTURE 
##  7    20 AGRONOMY 09/09/2002    30  2001 OTHER SCHOOL
##  8    22 AGRONOMY 23/01/2003    30  2001 ACCOUNTING  
##  9    38 BIOLOGY  28/02/2002    30  2001 AGRICULTURE 
## 10    42 BIOLOGY  28/02/2002    30  2001 ACCOUNTING  
## # … with 222 more rows
# sortedData &amp;lt;- students[order(-students$Mark, -xtfrm(students$Subject)), ]
# head(sortedData)
sortedData &amp;lt;- arrange(students, desc(Mark), desc(Subject))
sortedData
## # A tibble: 232 x 6
##       Id Subject Date        Mark  Year HighSchool  
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1   116 MATHS   01/07/2002    30  2001 OTHER SCHOOL
##  2   117 MATHS   18/06/2002    30  2001 ACCOUNTING  
##  3   118 MATHS   09/07/2002    30  2001 AGRICULTURE 
##  4   121 MATHS   18/06/2002    30  2001 ACCOUNTING  
##  5   123 MATHS   09/07/2002    30  2001 HUMANITIES  
##  6   130 MATHS   07/02/2002    30  2001 SCIENTIFIC  
##  7   131 MATHS   09/07/2002    30  2001 AGRICULTURE 
##  8   134 MATHS   26/02/2002    30  2001 AGRICULTURE 
##  9   135 MATHS   11/02/2002    30  2001 AGRICULTURE 
## 10   143 MATHS   04/02/2002    30  2001 ACCOUNTING  
## # … with 222 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for sorting, there is no competition! The ‘arrange()’ function, together with the ‘desc()’ function for descending order, represents a much clearer way to sort the data, with respect to the traditional ‘order()’ function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;casting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Casting the data&lt;/h1&gt;
&lt;p&gt;When we have a dataset in the LONG format, we might be interested in reshaping it into the WIDE format. This is the same as what the ‘pivot table’ function in Excel does. For example, take the ‘rimsulfuron.csv’ dataset in my repository. This contains the results of a randomised block experiment, where we have 16 herbicides in four blocks. The dataset is in the LONG format, with one row per plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rimsulfuron &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/rimsulfuron.csv&amp;quot;)
## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Herbicide = col_character(),
##   Plot = col_double(),
##   Code = col_double(),
##   Block = col_double(),
##   Box = col_double(),
##   WeedCover = col_double(),
##   Yield = col_double()
## )
rimsulfuron
## # A tibble: 64 x 7
##    Herbicide                              Plot  Code Block   Box WeedCover Yield
##    &amp;lt;chr&amp;gt;                                 &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Rimsulfuron (40)                          1     1     1     1      27.8  85.9
##  2 Rimsulfuron (45)                          2     2     1     1      27.8  93.0
##  3 Rimsulfuron (50)                          3     3     1     1      23    86.9
##  4 Rimsulfuron (60)                          4     4     1     1      42.8  53.0
##  5 Rimsulfuron (50+30 split)                 5     5     1     2      15.1  71.4
##  6 Rimsulfuron + thyfensulfuron              6     6     1     2      22.9  75.3
##  7 Rimsulfuron + hoeing                      7     7     1     2      17.7  73.2
##  8 Pendimethalin (pre) + rimsulfuron (p…     8     8     1     2      10.2  65.5
##  9 Pendimethalin (post) + rimsuulfuron …     9     9     1     1       5.4  94.8
## 10 Rimsulfuron + Atred                      10    10     1     1      40.3  94.1
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s put this data frame in the WIDE format, with one row per herbicide and one column per block. In my previous post, I used to the ‘cast()’ function in the ‘reshape’ package. Now I can use the ‘pivot_wider()’ function in the ‘tidyr’ package: the herbicide goes in the first column, the blocks (B1, B2, B3, B4) should go in the next four columns, and each unique level of yield should go in each cell, at the crossing of the correct herbicide row and block column. The ‘Height’ variable is not needed and it should be removed. Again, a two steps process, that is made easier by using the pipe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(reshape)
# castData &amp;lt;- cast(Herbicide ~ Block, data = rimsulfuron,
#      value = &amp;quot;Yield&amp;quot;)
# head(castData)

castData &amp;lt;- rimsulfuron %&amp;gt;%
  select(-Plot, - Code, -Box, - WeedCover) %&amp;gt;%
  pivot_wider(names_from = Block, values_from = Yield)
castData
## # A tibble: 16 x 5
##    Herbicide                                    `1`   `2`   `3`   `4`
##    &amp;lt;chr&amp;gt;                                      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Rimsulfuron (40)                            85.9  91.1 111.   93.2
##  2 Rimsulfuron (45)                            93.0 105    89.2  79.0
##  3 Rimsulfuron (50)                            86.9 106.  110.   89.1
##  4 Rimsulfuron (60)                            53.0 103.  101.   97.0
##  5 Rimsulfuron (50+30 split)                   71.4  77.6 116.   92.2
##  6 Rimsulfuron + thyfensulfuron                75.3  82.6  95.0  85.8
##  7 Rimsulfuron + hoeing                        73.2  86.1 118.   98.3
##  8 Pendimethalin (pre) + rimsulfuron (post)    65.5  88.7  95.5  82.4
##  9 Pendimethalin (post) + rimsuulfuron (post)  94.8  87.7 102.  102. 
## 10 Rimsulfuron + Atred                         94.1  89.9 104.   99.6
## 11 Thifensulfuron                              78.5  42.3  62.5  24.3
## 12 Metolachlor + terbuthylazine (pre)          51.8  52.1  49.5  34.7
## 13 Alachlor + terbuthylazine                   12.1  49.6  41.3  16.4
## 14 Hand-Weeded                                 77.6  92.1  86.6  99.6
## 15 Unweeded 1                                  10.9  31.8  23.9  20.8
## 16 Unweeded 2                                  27.6  51.6  25.1  38.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I am not clear with which it is more advantageous than which. Simply, I do not see much difference: none of the two methods is as clear as I would expect it to be!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;melting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Melting the data&lt;/h1&gt;
&lt;p&gt;In this case we do the reverse: we transform the dataset from WIDE to LONG format. In my previous post I used the ‘melt()’ function in the ‘reshape2’ package; now, I will use the ‘pivot_longer()’ function in ‘tidyr’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(reshape2)
# castData &amp;lt;- as.data.frame(castData)
# mdati &amp;lt;- melt(castData,
#               variable.name = &amp;quot;Block&amp;quot;,
#               value.name = &amp;quot;Yield&amp;quot;,
#               id=c(&amp;quot;Herbicide&amp;quot;))
# 
# head(mdati)
# 
pivot_longer(castData, names_to = &amp;quot;Block&amp;quot;, values_to = &amp;quot;Yield&amp;quot;,
             cols = c(2:5))
## # A tibble: 64 x 3
##    Herbicide        Block Yield
##    &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Rimsulfuron (40) 1      85.9
##  2 Rimsulfuron (40) 2      91.1
##  3 Rimsulfuron (40) 3     111. 
##  4 Rimsulfuron (40) 4      93.2
##  5 Rimsulfuron (45) 1      93.0
##  6 Rimsulfuron (45) 2     105  
##  7 Rimsulfuron (45) 3      89.2
##  8 Rimsulfuron (45) 4      79.0
##  9 Rimsulfuron (50) 1      86.9
## 10 Rimsulfuron (50) 2     106. 
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before with casting, neither ‘melt()’, nor ‘pivot_longer()’ let me completely satisfied.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidyverse-or-not-tidyverse&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tidyverse or not tidyverse?&lt;/h1&gt;
&lt;p&gt;This post is the result of using some functions coming from the ‘tidyverse’ and related packages, to replace other functions from more traditional packages, which I was more accustomed to, as a long-time R user. What’s my feeling about this change? Let me try to figure it out.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First of all, it didn’t take much time to adjust. I need to thank the authors of ‘tidyverse’ for being very respectful of tradition.&lt;/li&gt;
&lt;li&gt;In one case (ordering), adjusting to the new paradigm brought to a easier coding. In all other cases, the ease of coding was not affected.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Will I stick to the new paradigm or will I go back to my familiar approaches? Should I only consider the simple tasks above, my answer would be: “I’ll go back!”. However, this would be an unfair answer. Indeed, my data tasks are not as simple as those above. More frequently, my data tasks are made of several different steps. For example:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Take the ‘students’ dataset&lt;/li&gt;
&lt;li&gt;Filter the marks included between 26 and 28&lt;/li&gt;
&lt;li&gt;Remove the ‘Id’, ‘date’ and ‘high-school’ columns&lt;/li&gt;
&lt;li&gt;Calculate the mean mark for each subject in each year&lt;/li&gt;
&lt;li&gt;Spread those means along Years&lt;/li&gt;
&lt;li&gt;Get the overall mean for each subject across years&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s try to accomplish this task by using both a ‘base’ approach and a ‘tidyverse’ approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Traditional approach
library(reshape)
students2 &amp;lt;- subset(students, Mark &amp;gt;= 26 | Mark &amp;lt;= 28, 
                    select = c(-Id, -Date, -HighSchool))
mstudents2 &amp;lt;- cast(Subject ~ Year, data = students2,
      value = &amp;quot;Mark&amp;quot;, fun.aggregate = mean)
mstudents2$Mean &amp;lt;- apply(mstudents2[,2:3], 1, mean)
mstudents2
##       Subject     2001     2002     Mean
## 1    AGRONOMY 26.69565 26.25000 26.47283
## 2     BIOLOGY 26.48000 26.41379 26.44690
## 3   CHEMISTRY 24.21429 22.19048 23.20238
## 4   ECONOMICS 27.73077 27.11111 27.42094
## 5 FRUIT TREES      NaN 26.92857      NaN
## 6       MATHS 26.59259 25.00000 25.79630
# Tidyverse approach
students %&amp;gt;%
  filter(Mark &amp;gt;= 26 | Mark &amp;lt;= 28) %&amp;gt;%
  select(c(-Id,-Date,-HighSchool)) %&amp;gt;%
  group_by(Subject, Year) %&amp;gt;%
  summarise(Mark = mean(Mark)) %&amp;gt;%
  pivot_wider(names_from = Year, values_from = Mark) %&amp;gt;%
  mutate(Mean = (`2001` + `2002`)/2)
## # A tibble: 6 x 4
## # Groups:   Subject [6]
##   Subject     `2001` `2002`  Mean
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 AGRONOMY      26.7   26.2  26.5
## 2 BIOLOGY       26.5   26.4  26.4
## 3 CHEMISTRY     24.2   22.2  23.2
## 4 ECONOMICS     27.7   27.1  27.4
## 5 FRUIT TREES   NA     26.9  NA  
## 6 MATHS         26.6   25    25.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I must admit the second piece of code flows much more smooothly and it is much closer to my natural way of thinking. A collegue of mine said that, when it comes to operating on big tables and making really complex operations, the tidyverse is currently considered ‘the most powerful tool in the world’. I will have to dedicate another post to such situations. So far, I have started to reconsider my attitute towards the tidyverse.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonlinear combinations of model parameters in regression</title>
      <link>https://www.statforbiology.com/2020/stat_nls_gnlht/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2020/stat_nls_gnlht/</guid>
      <description>


&lt;p&gt;Nonlinear regression plays an important role in my research and teaching activities. While I often use the ‘drm()’ function in the ‘drc’ package for my research work, I tend to prefer the ‘nls()’ function for teaching purposes, mainly because, in my opinion, the transition from linear models to nonlinear models is smoother, for beginners. One problem with ‘nls()’ is that, in contrast to ‘drm()’, it is not specifically tailored to the needs of biologists or students in biology. Therefore, now and then, I have to build some helper functions, to perform some specific tasks; I usually share these functions within the ‘aomisc’ package, that is available on github (&lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;see this link&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, I would like to describe one of these helper functions, i.e. ‘gnlht()’, which is aimed at helping students (and practitioners; why not?) with one of their tasks, i.e. making some simple manipulations of model parameters, to obtain relevant biological information. Let’s see a typical example.&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;This is a real-life example, taken from a research published by Vischetti et al. in 1996. That research considered three herbicides for weed control in sugar beet, i.e. metamitron (M), phenmedipham (P) and cloridazon (C). Four soil samples were contaminated, respectively with: (i) M alone, (ii) M + P (iii) M + C and (iv) M + P + C. The aim was to assess whether the degradation speed of metamitron in soil depended on the presence of co-applied herbicides. To reach this aim, the soil samples were incubated at 20°C and sub-samples were taken in different times after the beginning of the experiment. The concentration of metamitron in those sub-samples was measured by HPLC analyses, performed in triplicate. The resulting dataset is available within the ‘aomisc’ package; we can load it and use the ‘lattice’ package to visualise the observed means over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
library(aomisc)
library(lattice)
data(metamitron)
xyplot(Conc ~ Time|Herbicide, data = metamitron,
       xlab = &amp;quot;Time (d)&amp;quot;, ylab = &amp;quot;Concentration&amp;quot;,
       scales = list(alternating = F),
       panel = function(x, y, ...) { 
         panel.grid(h = -1, v = -1)
         fmy &amp;lt;- tapply(y, list(factor(x)), mean)
         fmx &amp;lt;- tapply(x, list(factor(x)), mean)
         panel.xyplot(fmx, fmy, col=&amp;quot;red&amp;quot;, type=&amp;quot;b&amp;quot;, cex = 1)
       })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nls_gnlht_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Considering this exemplary dataset, let’s see how we can answer the following research questions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the degradation rate for metamitron, in the four combinations?&lt;/li&gt;
&lt;li&gt;Is there a significant difference between the degradation rate of metamitron alone and with co-applied herbicides?&lt;/li&gt;
&lt;li&gt;What is the half-life for metamitron, in the four combinations?&lt;/li&gt;
&lt;li&gt;What are the times to reach 70 and 30% of the initial concentration, for metamitron in the four combinations?&lt;/li&gt;
&lt;li&gt;Is there a significant difference between the half-life of metamitron alone and with co-applied herbicides?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-degradation-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a degradation model&lt;/h1&gt;
&lt;p&gt;The figure above shows a visible difference in the degradation pattern of metamitron, which could be attributed to the presence of co-applied herbicides. The degradation kinetics can be described by the following (first-order) model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ C(t, h) = A_h \, \exp \left(-k_h  \, t \right) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(C(t, h)\)&lt;/span&gt; is the concentration of metamitron at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; in each of the four combinations &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k_h\)&lt;/span&gt; are, respectively, the initial concentration and degradation rate for metamitron in each combination.&lt;/p&gt;
&lt;p&gt;The model is nonlinear and, therefore, we can use the ‘nls()’ function for nonlinear least squares regression. The code is given below: please, note that the two parameters are followed by the name of the factor variable in square brackets (i.e.: A[Herbicide] and k[Herbicide]). This is necessary to fit a different parameter value for each level of the ‘Herbicide’ factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Fit nls grouped model
modNlin &amp;lt;- nls(Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time), 
               start=list(A=rep(100, 4), k=rep(0.06, 4)), 
               data=metamitron)
summary(modNlin)
## 
## Formula: Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## A1 9.483e+01  4.796e+00   19.77   &amp;lt;2e-16 ***
## A2 1.021e+02  4.316e+00   23.65   &amp;lt;2e-16 ***
## A3 9.959e+01  4.463e+00   22.31   &amp;lt;2e-16 ***
## A4 1.116e+02  4.184e+00   26.68   &amp;lt;2e-16 ***
## k1 4.260e-02  4.128e-03   10.32   &amp;lt;2e-16 ***
## k2 2.574e-02  2.285e-03   11.26   &amp;lt;2e-16 ***
## k3 3.034e-02  2.733e-03   11.10   &amp;lt;2e-16 ***
## k4 2.186e-02  1.822e-03   12.00   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 9.701 on 88 degrees of freedom
## 
## Number of iterations to convergence: 5 
## Achieved convergence tolerance: 7.136e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simplicity, I will neglige an accurate model check, although I need to point out that this is highly wrong. I’ll come back to this issue in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-model-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working with model parameters&lt;/h1&gt;
&lt;p&gt;Considering the research questions, it is clear that the above output answers the first one, as it gives the four degradation rates, &lt;span class=&#34;math inline&#34;&gt;\(k1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k4\)&lt;/span&gt;. All the other questions can be translated into sets of linear/nonlinear functions (combinations) of model parameters. If we use the naming of parameter estimates in the nonlinear regression object, for the second question we can write the following functions: &lt;span class=&#34;math inline&#34;&gt;\(k1 - k2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k1 - k3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k1 - k4\)&lt;/span&gt;. The third question requires some slightly more complex math: if we invert the equation above for one herbicide, we get to the following inverse:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ t = \frac{- log \left[\frac{C(t)}{A} \right] }{k} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I do not think this is complex enough to scare the biologists, is it? The half-life is the time required for C(t) to drop to half of the initial value, so that &lt;span class=&#34;math inline&#34;&gt;\(C(t)/A\)&lt;/span&gt; is equal to &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;. Thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ t_{1/2} = \frac{- \log \left[0.5 \right] }{k} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Analogously, we can answer the question 4, by replacing &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; respectively with &lt;span class=&#34;math inline&#34;&gt;\(0.7\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0.3\)&lt;/span&gt;. The difference between the half-lives of metamitron alone and combined with the second herbicide can be calculated by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{- \log \left[0.5 \right] }{k_1} - \frac{- \log \left[0.5 \right] }{k_2} = \frac{k_2 - k_1}{k_1 \, k_2} \, \log(0.5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The other differences are obtained analogously.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inferences-and-hypotheses-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inferences and hypotheses testing&lt;/h1&gt;
&lt;p&gt;All parameter estimates are characterised by some uncertainty, which is summarised by way of the standard errors (see the code output above). Clearly, such an uncertainty propagates to their combinations. As for the first question, the combinations are linear, as only subtraction is involved. Therefore, the standard error for the difference can be easily calculated by the usual law of propagation of errors, which I have dealt with in &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_errorpropagation/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In R, linear combinations of model parameters can be built and tested by using the ‘glht()’ function in the ‘multcomp’ package. However, I wanted to find a general solution, that could handle both linear and nonlinear combinations of model parameters. Such a solution should be based on the ‘delta method’, which I have dealt with in &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_thedeltamethod/&#34;&gt;this post&lt;/a&gt;. Unfortunately, the function ‘deltaMethod()’ in the ‘car’ package is not flexible enough to the aims of my students and mine.&lt;/p&gt;
&lt;p&gt;Therefore, I wrote a wrapper for the ‘deltaMethod()’ function, which I named ‘gnlht()’, as it might play for nonlinear combinations the same role as ‘glht()’ for linear combinations. To use this function, apart from loading the ‘aomisc’ package, we need to prepare a list of formulas. Care needs to be taken to make sure that the element in the formulas correspond to the names of the estimated parameters in the model object, as returned by the ‘coef()’ method. In the box below, I show how we can calculate the differences between the degradation rates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~k1 - k2, ~k1 - k3, ~k1 - k4)
gnlht(modNlin, funList)
##      form   Estimate          SE  t-value      p-value
## 1 k1 - k2 0.01686533 0.004718465 3.574325 5.727311e-04
## 2 k1 - k3 0.01226241 0.004951372 2.476568 1.517801e-02
## 3 k1 - k4 0.02074109 0.004512710 4.596150 1.430392e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The very same code can be used for nonlinear combinations of model parameters. In order to calculate the half-lives, we can use the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ -log(0.5)/k1, ~ -log(0.5)/k2,
                ~ -log(0.5)/k3, ~ -log(0.5)/k4)
gnlht(modNlin, funList)
##           form Estimate       SE  t-value      p-value
## 1 -log(0.5)/k1 16.27089 1.576827 10.31876 7.987827e-17
## 2 -log(0.5)/k2 26.93390 2.391121 11.26413 9.552915e-19
## 3 -log(0.5)/k3 22.84747 2.058588 11.09861 2.064093e-18
## 4 -log(0.5)/k4 31.70942 2.643329 11.99601 3.257067e-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of writing ‘0.5’, we can introduce a new model term, e.g. ‘prop’, as a ‘constant’, in the sense that it is not an estimated parameter. We can pass a value for this constant in a data frame, by using the ‘const’ argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ -log(prop)/k1, ~ -log(prop)/k2,
                ~ -log(prop)/k3, ~ -log(prop)/k4)
gnlht(modNlin, funList, const = data.frame(prop = 0.5))
##            form prop Estimate       SE  t-value      p-value
## 1 -log(prop)/k1  0.5 16.27089 1.576827 10.31876 7.987827e-17
## 2 -log(prop)/k2  0.5 26.93390 2.391121 11.26413 9.552915e-19
## 3 -log(prop)/k3  0.5 22.84747 2.058588 11.09861 2.064093e-18
## 4 -log(prop)/k4  0.5 31.70942 2.643329 11.99601 3.257067e-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is very flexible, because it lets us to calculate, altogether, the half-life and the times required for the concentration to drop to 70 and 30% of the initial value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ -log(prop)/k1, ~ -log(prop)/k2,
                ~ -log(prop)/k3, ~ -log(prop)/k4)
gnlht(modNlin, funList, const = data.frame(prop = c(0.7, 0.5, 0.3)))
##             form prop  Estimate        SE  t-value      p-value
## 1  -log(prop)/k1  0.7  8.372564 0.8113927 10.31876 7.987827e-17
## 2  -log(prop)/k1  0.5 16.270892 1.5768267 10.31876 7.987827e-17
## 3  -log(prop)/k1  0.3 28.261979 2.7388937 10.31876 7.987827e-17
## 4  -log(prop)/k2  0.7 13.859465 1.2304069 11.26413 9.552915e-19
## 5  -log(prop)/k2  0.5 26.933905 2.3911214 11.26413 9.552915e-19
## 6  -log(prop)/k2  0.3 46.783265 4.1532956 11.26413 9.552915e-19
## 7  -log(prop)/k3  0.7 11.756694 1.0592942 11.09861 2.064093e-18
## 8  -log(prop)/k3  0.5 22.847468 2.0585881 11.09861 2.064093e-18
## 9  -log(prop)/k3  0.3 39.685266 3.5756966 11.09861 2.064093e-18
## 10 -log(prop)/k4  0.7 16.316814 1.3601864 11.99601 3.257067e-20
## 11 -log(prop)/k4  0.5 31.709415 2.6433295 11.99601 3.257067e-20
## 12 -log(prop)/k4  0.3 55.078163 4.5913724 11.99601 3.257067e-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The differences between the half-lives (and other degradation times) can be calculated as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ (k2 - k1)/(k1 * k2) * log(prop),
                ~ (k3 - k1)/(k1 * k3) * log(prop), 
                ~ (k4 - k1)/(k1 * k4) * log(prop))
gnlht(modNlin, funList, const = data.frame(prop = c(0.7, 0.5, 0.3)))
##                              form prop  Estimate       SE  t-value      p-value
## 1 (k2 - k1)/(k1 * k2) * log(prop)  0.7  5.486900 1.473859 3.722813 3.468973e-04
## 2 (k2 - k1)/(k1 * k2) * log(prop)  0.5 10.663013 2.864235 3.722813 3.468973e-04
## 3 (k2 - k1)/(k1 * k2) * log(prop)  0.3 18.521287 4.975078 3.722813 3.468973e-04
## 4 (k3 - k1)/(k1 * k3) * log(prop)  0.7  3.384130 1.334340 2.536183 1.297111e-02
## 5 (k3 - k1)/(k1 * k3) * log(prop)  0.5  6.576577 2.593100 2.536183 1.297111e-02
## 6 (k3 - k1)/(k1 * k3) * log(prop)  0.3 11.423287 4.504125 2.536183 1.297111e-02
## 7 (k4 - k1)/(k1 * k4) * log(prop)  0.7  7.944250 1.583814 5.015900 2.718445e-06
## 8 (k4 - k1)/(k1 * k4) * log(prop)  0.5 15.438524 3.077917 5.015900 2.718445e-06
## 9 (k4 - k1)/(k1 * k4) * log(prop)  0.3 26.816185 5.346236 5.015900 2.718445e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The possibility of passing constants in a data.frame adds flexibility with respect to the ‘deltaMethod()’ function in the ‘car’ package. For example, we can use this method to make predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ A1 * exp (- k1 * Time), ~ A2 * exp (- k2 * Time), 
                ~ A3 * exp (- k3 * Time), ~ A4 * exp (- k4 * Time))
pred &amp;lt;- gnlht(modNlin, funList, const = data.frame(Time = seq(0, 67, 1)))
head(pred)
##                   form Time Estimate       SE  t-value      p-value
## 1 A1 * exp(-k1 * Time)    0 94.83198 4.795948 19.77336 3.931107e-34
## 2 A1 * exp(-k1 * Time)    1 90.87694 4.381511 20.74101 1.223613e-35
## 3 A1 * exp(-k1 * Time)    2 87.08684 4.015039 21.69016 4.511113e-37
## 4 A1 * exp(-k1 * Time)    3 83.45482 3.695325 22.58389 2.205772e-38
## 5 A1 * exp(-k1 * Time)    4 79.97427 3.421034 23.37722 1.623774e-39
## 6 A1 * exp(-k1 * Time)    5 76.63888 3.190531 24.02072 2.050113e-40
tail(pred)
##                     form Time Estimate       SE  t-value      p-value
## 267 A4 * exp(-k4 * Time)   62 28.78518 2.657182 10.83297 7.138133e-18
## 268 A4 * exp(-k4 * Time)   63 28.16278 2.648687 10.63273 1.824651e-17
## 269 A4 * exp(-k4 * Time)   64 27.55384 2.639403 10.43942 4.525865e-17
## 270 A4 * exp(-k4 * Time)   65 26.95807 2.629361 10.25270 1.090502e-16
## 271 A4 * exp(-k4 * Time)   66 26.37517 2.618594 10.07227 2.555132e-16
## 272 A4 * exp(-k4 * Time)   67 25.80489 2.607131  9.89781 5.827812e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although this is not very fast, in contrast to the ‘predict()’ method for ‘nls’ objects, it has the advantage of reporting standard errors.&lt;/p&gt;
&lt;p&gt;Hope this is useful. Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;John Fox and Sanford Weisberg (2019). An {R} Companion to Applied Regression, Third Edition. Thousand Oaks CA:Sage. URL: &lt;a href=&#34;https://socialsciences.mcmaster.ca/jfox/Books/Companion/&#34; class=&#34;uri&#34;&gt;https://socialsciences.mcmaster.ca/jfox/Books/Companion/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Torsten Hothorn, Frank Bretz and Peter Westfall (2008). Simultaneous Inference in General Parametric Models. Biometrical Journal 50(3), 346–363.&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J. C., Gerhard, D. (2015) Dose-Response Analysis Using R PLOS ONE, 10(12), e0146021&lt;/li&gt;
&lt;li&gt;Vischetti, C., Marini, M., Businelli, M., Onofri, A., 1996. The effect of temperature and co-applied herbicides on the degradation rate of phenmedipham, chloridazon and metamitron in a clay loam soil in the laboratory, in: Re, A.D., Capri, E., Evans, S.P., Trevisan, M. (Eds.), “The Environmental Phate of Xenobiotics”, Proceedings X Symposium on Pesticide Chemistry, Piacenza. La Goliardica Pavese, Piacenza, pp. 287–294.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #2</title>
      <link>https://www.statforbiology.com/2019/stat_lmm_2-wayssplitrepeatedhet/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_lmm_2-wayssplitrepeatedhet/</guid>
      <description>


&lt;div id=&#34;a-repeated-split-plot-experiment-with-heteroscedastic-errors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A repeated split-plot experiment with heteroscedastic errors&lt;/h1&gt;
&lt;p&gt;Let’s imagine a field experiment, where different genotypes of khorasan wheat are to be compared under different nitrogen (N) fertilisation systems. Genotypes require bigger plots, with respect to fertilisation treatments and, therefore, the most convenient choice would be to lay-out the experiment as a split-plot, in a randomised complete block design. Genotypes would be randomly allocated to main plots, while fertilisation systems would be randomly allocated to sub-plots. As usual in agricultural research, the experiment should be repeated in different years, in order to explore the environmental variability of results.&lt;/p&gt;
&lt;p&gt;What could we expect from such an experiment?&lt;/p&gt;
&lt;p&gt;Please, look at the dataset ‘kamut.csv’, which is available on github. It provides the results for a split-plot experiment with 15 genotypes and 2 N fertilisation treatments, laid-out in three blocks and repeated in four years (360 observations, in all).&lt;/p&gt;
&lt;p&gt;The dataset has five columns, the ‘Year’, the ‘Genotype’, the fertilisation level (‘N’), the ‘Block’ and the response variable, i.e. ‘Yield’. The fifteen genotypes are coded by using the letters from A to O, while the levels of the other independent variables are coded by using numbers. The following snippets loads the file and recodes the numerical independent variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/kamutHet.csv&amp;quot;)
dataset$Block &amp;lt;- factor(dataset$Block)
dataset$Year &amp;lt;- factor(dataset$Year)
dataset$N &amp;lt;- factor(dataset$N)
dataset$Genotype &amp;lt;- factor(dataset$Genotype)
head(dataset)
##   Year Genotype N Block Yield
## 1 2004        A 1     1 2.235
## 2 2004        A 1     2 2.605
## 3 2004        A 1     3 2.323
## 4 2004        A 2     1 3.766
## 5 2004        A 2     2 4.094
## 6 2004        A 2     3 3.902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, it may be useful to code some ‘helper’ factors, to represent the blocks (within years) and the main-plots. The first factors (‘YearBlock’) has 12 levels (4 years and 3 blocks per year) and the second factor (‘MainPlot’) has 180 levels (4 years, 3 blocks per year and 15 genotypes per block).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$YearBlock &amp;lt;- with(dataset, factor(Year:Block))
dataset$MainPlot &amp;lt;- with(dataset, factor(Year:Block:Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the analyses, we will make use of the ‘plyr’ (Wickham, 2011), ‘car’ (Fox and Weisberg, 2011) and ‘nlme’ (Pinheiro et al., 2018) packages, which we load now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(car)
library(nlme)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is always useful to start by separately considering the results for each year. This gives us a feel for what happened in all experiments. What model do we have to fit to single-year split-plot data? In order to avoid mathematical notation, I will follow the notation proposed by Piepho (2003), by using the names of variables, as reported in the dataset. The treatment model for this split-plot design is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Genotype * N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All treatment effects are fixed. The block model, referencing all grouping structures, is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Block + Block:MainPlot + Block:MainPlot:Subplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first element references the blocks, while the second element references the main-plots, to which the genotypes are randomly allocated (randomisation unit). The third element references the sub-plots, to which N treatments are randomly allocated (another randomisation unit); this latter element corresponds to the residual error and, therefore, it is fitted by default and needs not be explicitly included in the model. Main-plot and sub-plot effects need to be random, as they reference randomisation units (Piepho, 2003). The nature of the block effect is still under debate (Dixon, 2016), but I’ll take it as random (do not worry: I will also show how we can take it as fixed).&lt;/p&gt;
&lt;p&gt;Coding a split-plot model in ‘lme’ is rather simple:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lme(Yield ~ Genotype * N, random = ~1|Block/MainPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the notation ‘Block/MainPlot’ is totally equivalent to ‘Block + Block:MainPlot’. Instead of manually fitting this model four times (one per year), we can ask R to do so by using the ‘ddply()’ function in the ‘plyr’ package. In the code below, I used this technique to retrieve the residual variance for each experiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmmFits &amp;lt;- ddply(dataset, c(&amp;quot;Year&amp;quot;),
      function(df) summary( lme(Yield ~ Genotype * N,
                 random = ~1|Block/MainPlot,
                 data = df))$sigma^2 )
lmmFits
##   Year          V1
## 1 2004 0.052761644
## 2 2005 0.001423833
## 3 2006 0.776028791
## 4 2007 0.817594477&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see great differences! The residual variance in 2005 is more that 500 times smaller than that observed in 2007. Clearly, if we pool the data and make an ANOVA, when we pool the data, we violate the homoscedasticity assumption. In general, this problem has an obvious solution: we can model the variance-covariance matrix of observations, allowing a different variance per year. In R, this is only possible by using the ‘lme()’ function (unless we want to use the ‘asreml-R’ package, which is not freeware, unfortunately). The question is: how do we code such a model?&lt;/p&gt;
&lt;p&gt;First of all, let’s derive a correct mixed model. The treatment model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Genotype * N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have mentioned that the genotype and N effects are likely to be taken as fixed. The block model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year + Year:Block + Year:Block:MainPlot + Year:Block:MainPlot:Subplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second element in the block model references the blocks within years, the second element references the main-plots, while the third element references the sub-plots and, as before, it is not needed. The year effect is likely to interact with both the treatment effects, so we need to add the following effects:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year + Year:Genotype + Year:N + Year:Genotype:N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent to writing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year*Genotype*N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The year effect can be taken as either as random or as fixed. In this post, we will show both approaches&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;year-effect-is-fixed&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Year effect is fixed&lt;/h1&gt;
&lt;p&gt;If we take the year effect as fixed and the block effect as random, we see that the random effects are nested (blocks within years and main-plots within blocks and within years). The function ‘lme()’ is specifically tailored to deal with nested random effects and, therefore, fitting the above model is rather easy. In the first snippet we fit a homoscedastic model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix1 &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = ~1|YearBlock/MainPlot,
                 data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also fit this model with the ‘lme4’ package and the ‘lmer()’; however, we are not happy with this, because we have seen clear signs of heteroscedastic within-year errors. Thus, let’s account for such an heteroscedasticity, by using the ‘weights()’ argument and the ‘varIdent()’ variance structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2 &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = ~1|YearBlock/MainPlot,
                 data = dataset,
               weights = varIdent(form = ~1|Year))
AIC(modMix1, modMix2)
##          df      AIC
## modMix1 123 856.6704
## modMix2 126 575.1967&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the Akaike Information Criterion, we see that the second model is better than the first one, which supports the idea of heteroscedastic residuals. From this moment on, the analyses proceeds as usual, e.g. by testing for fixed effects and comparing means, as necessary. Just a few words about testing for fixed effects: Wald F tests can be obtained by using the ‘anova()’ function, although I usually avoid this with ‘lme’ objects, as there is no reliable approximation to degrees of freedom. With ‘lme’ objects, I suggest using the ‘Anova()’ function in the ‘car’ package, which shows the results of Wald chi square tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Anova(modMix2)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##                    Chisq Df Pr(&amp;gt;Chisq)    
## Year              51.072  3  4.722e-11 ***
## Genotype         543.499 14  &amp;lt; 2.2e-16 ***
## N               2289.523  1  &amp;lt; 2.2e-16 ***
## Year:Genotype    123.847 42  5.281e-10 ***
## Year:N            21.695  3  7.549e-05 ***
## Genotype:N      1356.179 14  &amp;lt; 2.2e-16 ***
## Year:Genotype:N  224.477 42  &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One further aspect: do you prefer fixed blocks? Then you can fit the following model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix4 &amp;lt;- lme(Yield ~ Year * Genotype * N + Year:Block,
                 random = ~1|MainPlot,
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;year-effect-is-random&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Year effect is random&lt;/h1&gt;
&lt;p&gt;If we’d rather take the year effect as random, all the interactions therein are random as well (Year:Genotype, Year:N and Year:Genotype:N). Similarly, the block (within years) effect needs to be random. Therefore, we have several crossed random effects, which are not straightforward to code with ‘lme()’. First, I will show the code, second, I will comment it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix5 &amp;lt;- lme(Yield ~ Genotype * N,
                  random = list(Year = pdIdent(~1),
                                Year = pdIdent(~Block - 1),
                                Year = pdIdent(~MainPlot - 1),
                                Year = pdIdent(~Genotype - 1),
                                Year = pdIdent(~N - 1),
                                Genotype = pdIdent(~N - 1)),
                  data=dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that random effects are coded using a named list; each component of this list is a &lt;em&gt;pdMat&lt;/em&gt; object with name equal to a grouping factor. For example, the component ‘Year = pdIdent(~ 1)’ represents a random year effect, while ‘Year = pdIdent(~ Block - 1)’ represents a random year effect for each level of Block, i.e. a random ‘year x block’ interaction. This latter variance component is the same for all blocks (‘varIdent’), i.e. there is homoscedastic at this level.&lt;/p&gt;
&lt;p&gt;It is important to remember that the grouping factors in the list are treated as nested; however, the grouping factor is only one (‘Year’), so that the nesting is irrelevant. The only exception is the genotype, which is regarded as nested within the year. As the consequence, the component ‘Genotype = pdIdent(~N - 1)’, specifies a random year:genotype effect for each level of N treatment, i.e. a random year:genotype:N interaction.&lt;/p&gt;
&lt;p&gt;I agree, this is not straightforward to understand! If necessary, take a look at the good book of Gałecki and Burzykowski (2013). When fitting the above model, be patient; convergence may take a few seconds. I’d only like to reinforce the idea that, in case you need to test for fixed effects, you should not rely on the ‘anova()’ function, but you should prefer Wald chi square tests in the ‘Anova()’ function in the ‘car’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Anova(modMix5, type = 2)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##              Chisq Df Pr(&amp;gt;Chisq)    
## Genotype   68.6430 14  3.395e-09 ***
## N           2.4682  1     0.1162    
## Genotype:N 14.1153 14     0.4412    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another note: coding random effects as a named list is always possible. For example ‘modMix2’ can also be coded as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2b &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = list(YearBlock = ~ 1, MainPlot = ~ 1),
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, also as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2c &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = list(YearBlock = pdIdent(~ 1), MainPlot = pdIdent(~ 1)),
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this is useful! Have fun with it.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;a href=&#34;https://twitter.com/onofriandreapg?ref_src=twsrc%5Etfw&#34; class=&#34;twitter-follow-button&#34; data-show-count=&#34;false&#34;&gt;Follow &lt;span class=&#34;citation&#34;&gt;@onofriandreapg&lt;/span&gt;&lt;/a&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fox J. and Weisberg S. (2011). An {R} Companion to Applied Regression, Second Edition. Thousand Oaks CA: Sage. URL: &lt;a href=&#34;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion&#34; class=&#34;uri&#34;&gt;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2018). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-137, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Hadley Wickham (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1-29. URL: &lt;a href=&#34;http://www.jstatsoft.org/v40/i01/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v40/i01/&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #4</title>
      <link>https://www.statforbiology.com/2019/stat_nlmm_interaction/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_nlmm_interaction/</guid>
      <description>
&lt;script src=&#34;https://www.statforbiology.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;testing-for-interactions-in-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing for interactions in nonlinear regression&lt;/h1&gt;
&lt;p&gt;Factorial experiments are very common in agriculture and they are usually laid down to test for the significance of interactions between experimental factors. For example, genotype assessments may be performed at two different nitrogen fertilisation levels (e.g. high and low) to understand whether the ranking of genotypes depends on nutrient availability. For those of you who are not very much into agriculture, I will only say that such an assessment is relevant, because we need to know whether we can recommend the same genotypes, e.g., both in conventional agriculture (high nitrogen availability) and in organic agriculture (relatively lower nitrogen availability).&lt;/p&gt;
&lt;p&gt;Let’s consider an experiment where we have tested three genotypes (let’s call them A, B and C, for brevity), at two nitrogen rates (‘high’ an ‘low’) in a complete block factorial design, with four replicates. Biomass subsamples were taken from each of the 24 plots in eight different times (Days After Sowing: DAS), to evaluate the growth of biomass over time.&lt;/p&gt;
&lt;p&gt;The dataset is available on gitHub and the following code loads it and transforms the ‘Block’ variable into a factor. For this post, we will use several packages, including ‘aomisc’, the accompanying package for this blog. Please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page for downloading and installing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(lattice)
library(nlme)
library(aomisc)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/growthNGEN.csv&amp;quot;,
  header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
dataset$N &amp;lt;- factor(dataset$N)
dataset$GEN &amp;lt;- factor(dataset$GEN)
head(dataset)
##   Id DAS Block Plot GEN   N  Yield
## 1  1   0     1    1   A Low  2.786
## 2  2  15     1    1   A Low  5.871
## 3  3  30     1    1   A Low 13.265
## 4  4  45     1    1   A Low 16.926
## 5  5  60     1    1   A Low 22.812
## 6  6  75     1    1   A Low 25.346&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is composed by the following variables:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;‘Id’: a numerical code for observations&lt;/li&gt;
&lt;li&gt;‘DAS’: i.e., Days After Sowing. It is the moment when the sample was collected&lt;/li&gt;
&lt;li&gt;‘Block’, ‘Plot’, ‘GEN’ and ‘N’ represent respectively the block, plot, genotype and nitrogen level for each observation&lt;/li&gt;
&lt;li&gt;‘Yield’ represents the harvested biomass.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It may be useful to take a look at the observed growth data, as displayed on the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the growth is sygmoidal (presumably logistic) and that the variance of observations increases over time, i.e. the variance is proportional to the expected response.&lt;/p&gt;
&lt;p&gt;The question is: how do we analyse this data? Let’s build a model in a sequential fashion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The model&lt;/h1&gt;
&lt;p&gt;We could empirically assume that the relationship between biomass and time is logistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijkl} = \frac{d_{ijkl}}{1 + exp\left[b \left( X_{ijkl} - e_{ijkl}\right)\right]} + \varepsilon_{ijkl}\quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the observed biomass yield at time &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, for the i-th genotype, j-th nitrogen level, k-th block and l-th plot, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum asymptotic biomass level when time goes to infinity, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at inflection point, while &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the time when the biomass yield is equal to &lt;span class=&#34;math inline&#34;&gt;\(d/2\)&lt;/span&gt;. We are mostly interested in the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;: the first one describes the yield potential of a genotype, while the second one gives a measure of the speed of growth.&lt;/p&gt;
&lt;p&gt;There are repeated measures in each plot and, therefore, model parameters may show some variability, depending on the genotype, nitrogen level, block and plot. In particular, it may be acceptable to assume that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is pretty constant and independent on the above factors, while &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; may change according to the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left\{ {\begin{array}{*{20}{c}}
d_{ijkl} = \mu_d + g_{di} + N_{dj} + gN_{dij} + \theta_{dk} + \zeta_{dl}\\
e_{ijkl} = \mu_e + q_{ei} + N_{ej} + gN_{eij} + \theta_{ek} + \zeta_{el}
\end{array}} \right. \quad \quad (2) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for each parameter, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the fixed effect of the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed effect of j-th nitrogen level, &lt;span class=&#34;math inline&#34;&gt;\(gN\)&lt;/span&gt; is the fixed interaction effect, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the random effect of blocks, while &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; is the random effect of plots within blocks. These two equations are totally equivalent to those commonly used for linear mixed models, in the case of a two-factor factorial block design, wherein &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; would be the residual error term. Indeed, in principle, we could also think about a two-steps fitting procedure, where we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;fit the logistic model to the data for each plot and obtain estimates for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;use these estimates to fit a linear mixed model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will not pursue this two-steps technique here and we will concentrate on one-step fitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-wrong-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A wrong method&lt;/h1&gt;
&lt;p&gt;If the observations were independent (i.e. no blocks and no repeated measures), this model could be fit by using conventional nonlinear regression. My preference goes to the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2015).&lt;/p&gt;
&lt;p&gt;The coding is reported below: ‘Yield’ is a function of (&lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt;) DAS, by way of a three-parameters logistic function (‘fct = L.3()’). Different curves have to be fitted for different combinations of genotype and nitrogen levels (‘curveid = N:GEN’), although these curves should be partly based on common parameter values (‘pmodels = …). The ’pmodels’ argument requires a few additional comments. It must be a vector with as many element as there are parameters in the model (three, in this case: &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;). Each element represents a linear function of variables and refers to the parameters in alphabetical order, i.e. the first element refers to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the second refers to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the third to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not dependent on any variable (‘~ 1’) and thus a constant value is fitted across curves; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; depend on a fully factorial combination of genotype and nitrogen level (~ N*GEN = ~N + GEN + N:GEN). Finally, we used the argument ‘bcVal = 0.5’ to specify that we intend to use a Transform-Both-Sides technique, where a logarithmic transformation is performed for both sides of the equations. This is necessary to account for heteroscedasticity, but it does not affect the scale of parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive1 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = GEN:N,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N*GEN), bcVal = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model may be useful for other circumstances (no blocks and no repeated measures), but it is wrong in our example. Indeed, observations are clustered within blocks and plots; by neglecting this, we violate the assumption of independence of model residuals. A swift plot of residuals against fitted values shows that there are no problems with heteroscedasticity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Considering the above, we have to use a different model, here, although I will show that this naive fit may turn out useful.&lt;/p&gt;
&lt;div id=&#34;nonlinear-mixed-model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nonlinear mixed model fitting&lt;/h2&gt;
&lt;p&gt;In order to account for the clustering of observations, we switch to a Nonlinear Mixed-Effect model (NLME). A good choice is the ‘nlme()’ function in the ‘nlme’ package (Pinheiro and Bates, 2000), although the syntax may be cumbersome, at times. I will try to help, listing and commenting the most important arguments for this function. We need to specify the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A deterministic function. In this case, we use the ‘nlsL.3()’ function in the ‘aomisc’ package, which provides a logistic growth model with the same parameterisation as the ‘L.3()’ function in the ‘drm’ package.&lt;/li&gt;
&lt;li&gt;Linear functions for model parameters. The ‘fixed’ argument in the ‘nlme’ function is very similar to the ‘pmodels’ argument in the ‘drm’ function above, in the sense that it requires a list, wherein each element is a linear function of variables. The only difference is that the parameter name needs to be specified on the left side of the function.&lt;/li&gt;
&lt;li&gt;Random effects for model parameters. These are specified by using the ‘random’ argument. In this case, the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; are expected to show random variability from block to block and from plot to plot, within a block. For the sake of simplicity, as the parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not affected by the genotype and nitrogen level, we also expect that it does not show any random variability across blocks and plots.&lt;/li&gt;
&lt;li&gt;Starting values for model parameters. Self starting routines are not used by ‘nlme()’ and thus we need to specify a named vector, holding the initial values of model parameters. In this case, I decided to use the output from the ‘naive’ nonlinear regression above, which, therefore, turns out useful.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The transformation of both sides of the equation is made explicitely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
modnlme1 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N*GEN),
                    start = coef(modNaive1), 
                 control = list(msMaxIter = 400))
summary(modnlme1)$tTable
##                     Value   Std.Error  DF    t-value      p-value
## b              0.05652837 0.002157629 228 26.1993017 1.044744e-70
## d.(Intercept) 33.91575345 1.222612873 228 27.7403863 5.073988e-75
## d.NLow        -3.18382833 1.592502937 228 -1.9992606 4.676721e-02
## d.GENB        18.90014652 1.864712716 228 10.1356881 3.602004e-20
## d.GENC        -1.15934036 1.686405289 228 -0.6874625 4.924900e-01
## d.NLow:GENB   -5.99216674 2.455759122 228 -2.4400466 1.544863e-02
## d.NLow:GENC   -5.82864839 2.217534817 228 -2.6284360 9.160827e-03
## e.(Intercept) 55.20071251 2.320784619 228 23.7853664 1.087154e-63
## e.NLow        -9.06217439 3.127165895 228 -2.8978873 4.123120e-03
## e.GENB        -4.47038044 2.761533560 228 -1.6188036 1.068720e-01
## e.GENC         4.00746113 3.084383636 228  1.2992745 1.951620e-01
## e.NLow:GENB   -4.71367433 4.055953643 228 -1.1621618 2.463848e-01
## e.NLow:GENC    2.23951083 4.609547667 228  0.4858418 6.275459e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plots above, we see that the overall fit is good. Fixed effects and variance components for random effects are obtained as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(modnlme1)$tTable
##                     Value   Std.Error  DF    t-value      p-value
## b              0.05652837 0.002157629 228 26.1993017 1.044744e-70
## d.(Intercept) 33.91575345 1.222612873 228 27.7403863 5.073988e-75
## d.NLow        -3.18382833 1.592502937 228 -1.9992606 4.676721e-02
## d.GENB        18.90014652 1.864712716 228 10.1356881 3.602004e-20
## d.GENC        -1.15934036 1.686405289 228 -0.6874625 4.924900e-01
## d.NLow:GENB   -5.99216674 2.455759122 228 -2.4400466 1.544863e-02
## d.NLow:GENC   -5.82864839 2.217534817 228 -2.6284360 9.160827e-03
## e.(Intercept) 55.20071251 2.320784619 228 23.7853664 1.087154e-63
## e.NLow        -9.06217439 3.127165895 228 -2.8978873 4.123120e-03
## e.GENB        -4.47038044 2.761533560 228 -1.6188036 1.068720e-01
## e.GENC         4.00746113 3.084383636 228  1.2992745 1.951620e-01
## e.NLow:GENB   -4.71367433 4.055953643 228 -1.1621618 2.463848e-01
## e.NLow:GENC    2.23951083 4.609547667 228  0.4858418 6.275459e-01
VarCorr(modnlme1)
##               Variance                     StdDev       Corr  
## Block =       pdLogChol(list(d ~ 1,e ~ 1))                    
## d.(Intercept) 4.134390e-08                 2.033320e-04 d.(In)
## e.(Intercept) 1.851943e-08                 1.360861e-04 -0.001
## Plot =        pdLogChol(list(d ~ 1,e ~ 1))                    
## d.(Intercept) 3.396536e-09                 5.827981e-05 d.(In)
## e.(Intercept) 1.023108e-09                 3.198605e-05 0     
## Residual      1.750623e-01                 4.184044e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s go back to our initial aim: testing the significance of the ‘genotype x nitrogen’ interaction. Indeed, we have two available tests: on for the parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and one for the parameter &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. First of all, we code two ‘reduced’ models, where the genotype and nitrogen effects are purely addictive. To do so, we change the specification of the fixed effects from ’~ N*GEN’ to ‘~ N + GEN’. Also in this case, we use a ‘naive’ nonlinear regression fit to get starting values for model parameters, to be used in the following NLME model fitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive2 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N + GEN,  ~ N * GEN), bcVal = 0.5)

modnlme2 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N + GEN, e ~ N*GEN),
                    start = coef(modNaive2), control = list(msMaxIter = 200))
modNaive3 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N + GEN), bcVal = 0.5)

modnlme3 &amp;lt;- nlme(sqrt(Yield) ~ sqrt(NLS.L3(DAS, b, d, e)), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N + GEN),
                    start = coef(modNaive3), control = list(msMaxIter = 200))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s consider the first reduced model ‘modnlme2’. In this model, the ‘genotype x nitrogen’ interaction has been removed for the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter. We can compare this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme2)
##          Model df      AIC      BIC    logLik   Test L.Ratio p-value
## modnlme1     1 20 329.1496 400.6686 -144.5748                       
## modnlme2     2 18 334.2187 398.5857 -149.1093 1 vs 2 9.06907  0.0107&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This test is significant, but the AIC value is very close for the two models. Considering that a LRT in mixed models is usually rather liberal, it should be possible to conclude that the ‘genotype x nitrogen’ interaction is not significant and, therefore, the ranking of genotypes in terms of yield potential, as measured by the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter should be independent on nitrogen level.&lt;/p&gt;
&lt;p&gt;Let’s now consider the second reduced model ‘modnlme3’. In this second model, the ‘genotype x nitrogen’ interaction has been removed for the ‘e’ parameter. We can compare also this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme3)
##          Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## modnlme1     1 20 329.1496 400.6686 -144.5748                        
## modnlme3     2 18 328.2446 392.6117 -146.1223 1 vs 2 3.095066  0.2128&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this second test, the lack of significance for the ‘genotype x nitrogen’ interaction seems to be less questionable than in the first one.&lt;/p&gt;
&lt;p&gt;I would like to conclude by drawing your attention to the ‘medrm’ function in the ‘medrc’ package, which can also be used to fit this type of nonlinear mixed-effects models.&lt;/p&gt;
&lt;p&gt;Happy coding with R!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J.C., Gerhard, D., 2015. Dose-Response Analysis Using R. PLOS ONE 10, e0146021. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0146021&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0146021&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;. Example #1</title>
      <link>https://www.statforbiology.com/2019/stat_lmm_environmentalvariance/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_lmm_environmentalvariance/</guid>
      <description>


&lt;div id=&#34;the-environmental-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The environmental variance model&lt;/h1&gt;
&lt;p&gt;Fitting mixed models has become very common in biology and recent developments involve the manipulation of the variance-covariance matrix for random effects and residuals. To the best of my knowledge, within the frame of frequentist methods, the only freeware solution in R should be based on the ‘nlme’ package, as the ‘lmer’ package does not easily permit such manipulations. The ‘nlme’ package is fully described in Pinheiro and Bates (2000). Of course, the ‘asreml’ package can be used, but, unfortunately, this is not freeware.&lt;/p&gt;
&lt;p&gt;Coding mixed models in ‘nlme’ is not always easy, especially when we have crossed random effects, which is very common with agricultural experiments. I have been struggling with this issue very often in the last years and I thought it might be useful to publish a few examples in this blog, to save collegues from a few headaches. Please, note that I have already published other posts dealing with the use of the ‘lme()’ function in the ‘nlme’ package, for example &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence2/&#34;&gt;this post here&lt;/a&gt; about the correlation in designed experiments and &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/&#34;&gt;this other post here&lt;/a&gt;, about heteroscedastic multienvironment experiments.&lt;/p&gt;
&lt;p&gt;The first example in this series relates to a randomised complete block design with three replicates, comparing winter wheat genotypes. The experiment was repeated in seven years in the same location. The dataset (‘WinterWheat’) is available in the ‘aomisc’ package, which is the companion package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;. Please, note that this dataset shows the data for eight genotypes, but the model that we want to fit requires that the number of environments is higher than the number of genotypes. Therefore, we have to make a subset, at the beginning, removing a couple of genotypes.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset, subsets it and turns the ‘Genotype’, ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
data(WinterWheat)
WinterWheat &amp;lt;- WinterWheat[WinterWheat$Genotype != &amp;quot;SIMETO&amp;quot; &amp;amp; WinterWheat$Genotype != &amp;quot;SOLEX&amp;quot;,]
WinterWheat$Genotype &amp;lt;- factor(WinterWheat$Genotype)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996
## 11   91     2   DUILIO  5.57 1996
## 12  138     2   GRAZIA  6.09 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the so-called ‘environmental variance model’. This model is often used in stability analyses for multi-environment experiments and I will closely follow the coding proposed in Piepho (1999):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + g_i + r_{jk}  +  h_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g_i\)&lt;/span&gt; is the effect for the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; is the effect for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(h_{ij}\)&lt;/span&gt; is a random deviation from the expected yield for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual variability of yield between plots, within each environment and block.&lt;/p&gt;
&lt;p&gt;We usually assume that &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; are independent and normally distributed, with variances equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;, respectively. Such an assumption may be questioned, but we will not do it now, for the sake of simplicity.&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(h_{ij}\)&lt;/span&gt;, which we will assume as normally distributed with variance-covariance matrix equal to &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;. In particular, it is reasonable to expect that the genotypes will have different variances across environments (heteroscedasticity), which can be interpreted as static stability measures (‘environmental variances’; hence the name ‘environmental variance model’). Furthermore, it is reasonable that, if an environment is good for one genotype, it may also be good for other genotypes, so that yields in each environment are correlated, although the correlations can be different for each couple of genotypes. To reflect our expectations, the &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; matrix needs to be totally unstructured, with the only constraint that it is positive definite.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown how the above model can be coded by using SAS and I translated his code into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EnvVarMod &amp;lt;- lme(Yield ~ Genotype, 
  random = list(Year = pdSymm(~Genotype - 1), 
              Year = pdIdent(~Block - 1)),
  control = list(opt = &amp;quot;optim&amp;quot;, maxIter = 100),
  data=WinterWheat)
VarCorr(EnvVarMod)
##                  Variance             StdDev    Corr                
## Year =           pdSymm(Genotype - 1)                               
## GenotypeCOLOSSEO 0.48876512           0.6991174 GCOLOS GCRESO GDUILI
## GenotypeCRESO    0.70949309           0.8423141 0.969               
## GenotypeDUILIO   2.37438440           1.5409038 0.840  0.840        
## GenotypeGRAZIA   1.18078525           1.0866394 0.844  0.763  0.942 
## GenotypeIRIDE    1.23555204           1.1115539 0.857  0.885  0.970 
## GenotypeSANCARLO 0.93335518           0.9661031 0.928  0.941  0.962 
## Year =           pdIdent(Block - 1)                                 
## Block1           0.02748257           0.1657787                     
## Block2           0.02748257           0.1657787                     
## Block3           0.02748257           0.1657787                     
## Residual         0.12990355           0.3604214                     
##                               
## Year =                        
## GenotypeCOLOSSEO GGRAZI GIRIDE
## GenotypeCRESO                 
## GenotypeDUILIO                
## GenotypeGRAZIA                
## GenotypeIRIDE    0.896        
## GenotypeSANCARLO 0.884  0.942 
## Year =                        
## Block1                        
## Block2                        
## Block3                        
## Residual&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I coded the random effects as a list, by using the ‘Year’ as the nesting factor (Galecki and Burzykowski, 2013). In order to specify a totally unstructured variance-covariance matrix for the genotypes within years, I used the ‘pdMat’ construct ‘pdSymm()’. This model is rather complex and may take long to converge.&lt;/p&gt;
&lt;p&gt;The environmental variances are retrieved by the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;envVar &amp;lt;- as.numeric ( VarCorr(EnvVarMod)[2:7,1] )
envVar
## [1] 0.4887651 0.7094931 2.3743844 1.1807853 1.2355520 0.9333552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the correlations are given by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(EnvVarMod)[2:7,3:7]
##                  Corr                                        
## GenotypeCOLOSSEO &amp;quot;GCOLOS&amp;quot; &amp;quot;GCRESO&amp;quot; &amp;quot;GDUILI&amp;quot; &amp;quot;GGRAZI&amp;quot; &amp;quot;GIRIDE&amp;quot;
## GenotypeCRESO    &amp;quot;0.969&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeDUILIO   &amp;quot;0.840&amp;quot;  &amp;quot;0.840&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeGRAZIA   &amp;quot;0.844&amp;quot;  &amp;quot;0.763&amp;quot;  &amp;quot;0.942&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeIRIDE    &amp;quot;0.857&amp;quot;  &amp;quot;0.885&amp;quot;  &amp;quot;0.970&amp;quot;  &amp;quot;0.896&amp;quot;  &amp;quot;&amp;quot;      
## GenotypeSANCARLO &amp;quot;0.928&amp;quot;  &amp;quot;0.941&amp;quot;  &amp;quot;0.962&amp;quot;  &amp;quot;0.884&amp;quot;  &amp;quot;0.942&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unweighted-two-stage-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unweighted two-stage fitting&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit an environmental variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retrieved the means for genotypes in all years, we can fit the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu + g_i + a_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is the mean yield for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment and &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; is the residual term, which includes the genotype x environment random interaction, the block x environment random interaction and the residual error term.&lt;/p&gt;
&lt;p&gt;In this model we have only one random effect (&lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt;) and, therefore, this is a fixed linear model. However, we need to model the variance-covariance matrix of residuals (&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;), by adopting a totally unstructured form. Please, note that, when working with raw data, we have modelled &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;, i.e. the variance-covariance matrix for the random effects. I have used the ‘gls()’ function, together with the ‘weights’ and ‘correlation’ arguments. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
envVarModM &amp;lt;- gls(Yield ~ Genotype, 
  data = WinterWheatM,
  weights = varIdent(form=~1|Genotype),
  correlation = corSymm(form=~1|Year))
summary(envVarModM)
## Generalized least squares fit by REML
##   Model: Yield ~ Genotype 
##   Data: WinterWheatM 
##       AIC      BIC   logLik
##   80.6022 123.3572 -13.3011
## 
## Correlation Structure: General
##  Formula: ~1 | Year 
##  Parameter estimate(s):
##  Correlation: 
##   1     2     3     4     5    
## 2 0.947                        
## 3 0.809 0.815                  
## 4 0.816 0.736 0.921            
## 5 0.817 0.866 0.952 0.869      
## 6 0.888 0.925 0.949 0.856 0.907
## Variance function:
##  Structure: Different standard deviations per stratum
##  Formula: ~1 | Genotype 
##  Parameter estimates:
## COLOSSEO    CRESO   DUILIO   GRAZIA    IRIDE SANCARLO 
## 1.000000 1.189653 2.143713 1.528848 1.560620 1.356423 
## 
## Coefficients:
##                      Value Std.Error   t-value p-value
## (Intercept)       6.413333 0.2742314 23.386574  0.0000
## GenotypeCRESO    -0.439524 0.1107463 -3.968746  0.0003
## GenotypeDUILIO    0.178571 0.3999797  0.446451  0.6579
## GenotypeGRAZIA   -0.330952 0.2518270 -1.314205  0.1971
## GenotypeIRIDE     0.281905 0.2580726  1.092347  0.2819
## GenotypeSANCARLO -0.192857 0.1802547 -1.069915  0.2918
## 
##  Correlation: 
##                  (Intr) GCRESO GDUILI GGRAZI GIRIDE
## GenotypeCRESO     0.312                            
## GenotypeDUILIO    0.503  0.371                     
## GenotypeGRAZIA    0.269 -0.095  0.774              
## GenotypeIRIDE     0.292  0.545  0.857  0.638       
## GenotypeSANCARLO  0.310  0.612  0.856  0.537  0.713
## 
## Standardized residuals:
##        Min         Q1        Med         Q3        Max 
## -2.0949678 -0.5680656  0.1735444  0.7599596  1.3395000 
## 
## Residual standard error: 0.7255481 
## Degrees of freedom: 42 total; 36 residual&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance-covariance matrix for residuals can be obtained using the ‘getVarCov()’ function in the ‘nlme’ package, although I had to discover that there is a small buglet there, which causes problems in some instances (such as here). Please, &lt;a href=&#34;https://www.jepusto.com/bug-in-nlme-getvarcov/&#34;&gt;see this link&lt;/a&gt;; I have included the correct code in the ‘getVarCov.gls()’ function in the ‘aomisc’ package, that is the companion package for this blog.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- getVarCov.gls(envVarModM)
R
## Marginal variance covariance matrix
##         [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
## [1,] 0.52642 0.59280 0.91285 0.65647 0.67116 0.63376
## [2,] 0.59280 0.74503 1.09440 0.70422 0.84652 0.78560
## [3,] 0.91285 1.09440 2.41920 1.58850 1.67700 1.45230
## [4,] 0.65647 0.70422 1.58850 1.23040 1.09160 0.93442
## [5,] 0.67116 0.84652 1.67700 1.09160 1.28210 1.01070
## [6,] 0.63376 0.78560 1.45230 0.93442 1.01070 0.96855
##   Standard Deviations: 0.72555 0.86315 1.5554 1.1093 1.1323 0.98415&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the design is perfectly balanced, the diagonal elements of the above matrix correspond to the variances of genotypes across environments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(WinterWheatM$Yield, WinterWheatM$Genotype, var)
##  COLOSSEO     CRESO    DUILIO    GRAZIA     IRIDE  SANCARLO 
## 0.5264185 0.7450275 2.4191624 1.2304397 1.2821143 0.9685497&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can also be retreived by the ‘stability’ package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stability)
## Registered S3 methods overwritten by &amp;#39;lme4&amp;#39;:
##   method                          from
##   cooks.distance.influence.merMod car 
##   influence.merMod                car 
##   dfbeta.influence.merMod         car 
##   dfbetas.influence.merMod        car
envVarStab &amp;lt;-
  stab_measures(
    .data = WinterWheatM,
    .y = Yield,
    .gen = Genotype,
    .env = Year
  )

envVarStab$StabMeasures
## # A tibble: 6 x 7
##   Genotype  Mean GenSS   Var    CV  Ecov ShuklaVar
##   &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 COLOSSEO  6.41  3.16 0.526  11.3 1.25     0.258 
## 2 CRESO     5.97  4.47 0.745  14.4 1.01     0.198 
## 3 DUILIO    6.59 14.5  2.42   23.6 2.31     0.522 
## 4 GRAZIA    6.08  7.38 1.23   18.2 1.05     0.208 
## 5 IRIDE     6.70  7.69 1.28   16.9 0.614    0.0989
## 6 SANCARLO  6.22  5.81 0.969  15.8 0.320    0.0254&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Strictly speaking, those variances are not the environmental variances, as they also contain the within-experiment and within block random variability, which needs to be separately estimated during the first step.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Muhammad Yaseen, Kent M. Eskridge and Ghulam Murtaza (2018). stability: Stability Analysis of Genotype by Environment Interaction (GEI). R package version 0.5.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stability&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Germination data and time-to-event methods: comparing germination curves</title>
      <link>https://www.statforbiology.com/2019/stat_seedgermination_comparinglots/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_seedgermination_comparinglots/</guid>
      <description>


&lt;p&gt;Very often, seed scientists need to compare the germination behaviour of different seed populations, e.g., different plant species, or one single plant species submitted to different temperatures, light conditions, priming treatments and so on. How should such a comparison be performed?&lt;/p&gt;
&lt;p&gt;Let’s take a practical approach and start from an appropriate example: a few years ago, some collegues studied the germination behaviour for seeds of a plant species (&lt;em&gt;Verbascum arcturus&lt;/em&gt;, BTW…), in different conditions. In detail, they considered the factorial combination of two storage periods (LONG and SHORT storage) and two temperature regimes (FIX: constant daily temperature of 20°C; ALT: alternating daily temperature regime, with 25°C during daytime and 15°C during night time, with a 12:12h photoperiod). If you are a seed scientist and are interested in this experiment, you’ll find detail in Catara &lt;em&gt;et al.&lt;/em&gt; (2016).&lt;/p&gt;
&lt;p&gt;If you are not a seed scientist you may wonder why my colleagues made such an assay; well, there is evidence that, for some plant species, the germination ability improves over time after seed maturation. Therefore, if we take seeds and store them for a different period of time, there might be an effect on their germination traits. Likewise, there is also evidence that some seeds may not germinate if they are not submitted to daily temperature fluctuations. These mechanisms are very interesting, as they permit to the seed to recognise that the environmental conditions are favourable for seedling survival.My colleagues wanted to discover whether this was the case for Verbascum.&lt;/p&gt;
&lt;p&gt;Let’s go back to our assay: the experimental design consisted of four combinations (LONG-FIX, LONG-ALT, SHORT-FIX and SHORT-ALT) and four replicates for each combination. One replicate consisted of a Petri dish, that is a small plastic box containing humid blotting paper, with 25 seeds of Verbascum. In all, there were 16 Petri dishes, put in climatic chambers with the appropriate conditions. During the assay, my collegues made daily inspections: germinated seeds were counted and removed from the dishes. Inspections were made for 15 days, until no more germinations could be observed.&lt;/p&gt;
&lt;p&gt;The dataset is available from a gitHub repository: let’s load it and have a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/TempStorage.csv&amp;quot;, header = T, check.names = F)
head(dataset)
##   Dish Storage Temp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
## 1    1     Low  Fix 0 0 0 0 0 0 0 0 3  4  6  0  1  0  3
## 2    2     Low  Fix 0 0 0 0 1 0 0 0 2  7  2  3  0  5  1
## 3    3     Low  Fix 0 0 0 0 1 0 0 1 3  5  2  4  0  1  3
## 4    4     Low  Fix 0 0 0 0 1 0 3 0 0  3  1  1  0  4  4
## 5    5    High  Fix 0 0 0 0 0 0 0 0 1  2  5  4  2  3  0
## 6    6    High  Fix 0 0 0 0 0 0 0 0 2  2  7  8  1  2  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have one row per Petri dish; the first three columns show, respectively, dish number, storage and temperature conditions. The next 15 columns represent the inspection times (from 1 to 15) and contain the counts of germinated seeds. The research question is:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is germination behaviour affected by storage and temperature conditions?&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;response-feature-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Response feature analyses&lt;/h1&gt;
&lt;p&gt;One possible line of attack is to take a summary measure for each dish, e.g. the total number of germinated seeds. Taking a single value for each dish brings us back to more common methods of data analysis: for example, we can fit some sort of GLM to test the significance of effects (storage, temperature and their interaction), within a fully factorial design.&lt;/p&gt;
&lt;p&gt;Although the above method is not wrong, undoubtedly, it may be sub-optimal. Indeed, dishes may contain the same total number of germinated seeds, but, nonetheless, they may differ for some other germination traits, such as velocity or uniformity. Indeed, we do not want to express a judgment about one specific characteristic of the seed lot, we would like to express a judgment about the whole seed lot. In other words, we are not specifically asking: “do the seed lots differ for their germination capability?”. We are, more generally, asking “are the seed lots different?”.&lt;/p&gt;
&lt;p&gt;In order to get a general assessment, a different method of analysis should be sought, which considers the entire time series (from 1 to 15 days) and not only one single summary measure. This method exists and it is available within the time-to-event platform, which has shown very useful and appropriate for seed germination studies (Onofri et al., 2011; Ritz et al., 2013; Onofri et al., 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-germination-time-course&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The germination time-course&lt;/h1&gt;
&lt;p&gt;It is necessary to re-organise the dataset in a more useful way. A good format can be obtained by using the ‘makeDrm()’ function in the ‘drcSeedGerm’ package, which can be installed from gitHub (see the code at: &lt;a href=&#34;https://www.statforbiology.com/seedgermination/index.html&#34;&gt;this link&lt;/a&gt;). The function needs to receive a dataframe storing the counts (dataset[,4:18]), a dataframe storing the factor variables (dataset[,2:3]), a vector with the number of seeds in each Petri dish (rep(25, 16)) and a vector of monitoring times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drcSeedGerm)
library(dplyr)
datasetR &amp;lt;- makeDrm(dataset[,4:18], dataset[,2:3], rep(25, 16), 1:15)
head(datasetR, 16)
##      Storage Temp Dish timeBef timeAf count nCum propCum
## 1        Low  Fix    1       0      1     0    0    0.00
## 1.1      Low  Fix    1       1      2     0    0    0.00
## 1.2      Low  Fix    1       2      3     0    0    0.00
## 1.3      Low  Fix    1       3      4     0    0    0.00
## 1.4      Low  Fix    1       4      5     0    0    0.00
## 1.5      Low  Fix    1       5      6     0    0    0.00
## 1.6      Low  Fix    1       6      7     0    0    0.00
## 1.7      Low  Fix    1       7      8     0    0    0.00
## 1.8      Low  Fix    1       8      9     3    3    0.12
## 1.9      Low  Fix    1       9     10     4    7    0.28
## 1.10     Low  Fix    1      10     11     6   13    0.52
## 1.11     Low  Fix    1      11     12     0   13    0.52
## 1.12     Low  Fix    1      12     13     1   14    0.56
## 1.13     Low  Fix    1      13     14     0   14    0.56
## 1.14     Low  Fix    1      14     15     3   17    0.68
## 1.15     Low  Fix    1      15    Inf     8   NA      NA
datasetR &amp;lt;- datasetR %&amp;gt;% 
  mutate(across(c(Storage, Temp, Dish), .fns = factor))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The snippet above shows the first dish. Roughly speaking, we have gone from a WIDE format to a LONG format. The column ‘timeAf’ contains the time when the inspection was made and the column ‘count’ contains the number of germinated seeds (e.g. 9 seeds were counted at day 9). These seeds did not germinate exactly at day 9; they germinated within the interval between two inspections, that is between day 8 and day 9. The beginning of the interval is given in the variable ‘timeBef’. Apart from these columns, we have additional columns, which we are not going to use for our analyses. The cumulative counts of germinated seeds are in the column ‘nCum’; these cumulative counts have been converted into cumulative proportions by dividing by 25 (i.e., the total number of seeds in a dish; see the column ‘propCum’).&lt;/p&gt;
&lt;p&gt;We can use a time-to-event model to parameterise the germination time-course for this dish. This is easily done by using the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2013):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modPre &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(), 
              data = datasetR, 
              type = &amp;quot;event&amp;quot;, subset = c(Dish == 1))
plot(modPre, log = &amp;quot;&amp;quot;, xlab = &amp;quot;Time&amp;quot;, 
     ylab = &amp;quot;Proportion of germinated seeds&amp;quot;,
     xlim = c(0, 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_Survival_ComparingLots_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Please, note the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;we are using the counts (‘count’) as the dependent variable&lt;/li&gt;
&lt;li&gt;as the independent variable: we are using the extremes of the inspection interval, within which germinations were observed (count ~ timeBef + time Af)&lt;/li&gt;
&lt;li&gt;we have assumed a log-logistic distribution of germination times (fct = LL.3()). A three parameter model is necessary, because there is a final fraction of ungerminated seeds (truncated distribution)&lt;/li&gt;
&lt;li&gt;we have set the argument ‘type = “event”’. Indeed, we are fitting a time-to-event model, not a nonlinear regression model, which would be incorrect, in this setting (see &lt;a href=&#34;https://www.statforbiology.com/seedgermination/timetoevent&#34;&gt;this link here&lt;/a&gt; ).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we have determined the germination time-course for dish 1, we can do the same for all dishes. However, we have to instruct ‘drm()’ to define a different curve for each combination of storage and temperature. It is necessary to make an appropriate use of the ‘curveid’ argument. Please, see below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(),
            data = datasetR, type = &amp;quot;event&amp;quot;, 
            curveid = Temp:Storage)
plot(mod1, log = &amp;quot;&amp;quot;, legendPos = c(2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_Survival_ComparingLots_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that there are visible differences between the curves (the legend considers the curves in alphabetical order, i.e. 1: Fix-Low, 2: Fix-High, 3: Alt-Low and 4: Alt-High). We can test that the curves are similar by coding a reduced model, where we have only one pooled curve for all treatment levels. It is enough to remove the ‘curveid’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNull &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(),
               data = datasetR, 
               type = &amp;quot;event&amp;quot;)
anova(mod1, modNull, test = &amp;quot;Chisq&amp;quot;)
## 
## 1st model
##  fct:      LL.3()
##  pmodels: Temp:Storage (for all parameters)
## 2nd model
##  fct:      LL.3()
##  pmodels: 1 (for all parameters)
## ANOVA-like table
## 
##           ModelDf  Loglik Df LR value p value
## 1st model     244 -753.54                    
## 2nd model     253 -854.93  9   202.77       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can compare the full model (four curves) with the reduced model (one common curve) by using a Likelihood Ratio Test, which is approximately distributed as a Chi-square. The test is highly significant. Of course, we can also test some other hypotheses. For example, we can code a model with different curves for storage times, assuming that the effect of temperature is irrelevant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(), 
            data = datasetR, type = &amp;quot;event&amp;quot;, 
            curveid = Storage)
anova(mod1, mod2, test = &amp;quot;Chisq&amp;quot;)
## 
## 1st model
##  fct:      LL.3()
##  pmodels: Temp:Storage (for all parameters)
## 2nd model
##  fct:      LL.3()
##  pmodels: Storage (for all parameters)
## ANOVA-like table
## 
##           ModelDf  Loglik Df LR value p value
## 1st model     244 -753.54                    
## 2nd model     250 -797.26  6   87.436       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that such an assumption (temperature effect is irrelevant) is not supported by the data: the temperature effect cannot be removed without causing a significant decrease in the likelihood of the model. Similarly, we can test the effect of storage:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod3 &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(), 
            data = datasetR, type = &amp;quot;event&amp;quot;, 
            curveid = Temp)
anova(mod1, mod3, test = &amp;quot;Chisq&amp;quot;)
## 
## 1st model
##  fct:      LL.3()
##  pmodels: Temp:Storage (for all parameters)
## 2nd model
##  fct:      LL.3()
##  pmodels: Temp (for all parameters)
## ANOVA-like table
## 
##           ModelDf  Loglik Df LR value p value
## 1st model     244 -753.54                    
## 2nd model     250 -849.48  6   191.87       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we get significant results. So, we need to conclude that temperature and storage time caused a significant influence on the germination behavior for the species under study.&lt;/p&gt;
&lt;p&gt;Before concluding, it is necessary to mention that, in general, the above LR tests should be taken with care: the results are only approximate and the observed data are not totally independent, as multiple observations are taken in each experimental unit (Petri dish). In order to restore independence, we would need to add to the model a random effect for the Petri dish, which is not an easy task in a time-to-event framework (Onofri et al., 2019). However, we got very low p-levels, which leave us rather confident about the significance of effects. It may be a good suggestion, in general, to avoid formal hypothesis testing and compare the models by using the Akaike Information Criterion (AIC: the lowest is the best), which confirms that the complete model with four curves is, indeed, the best one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod1, mod2, mod3, modNull)
##          df      AIC
## mod1    244 1995.088
## mod2    250 2094.524
## mod3    250 2198.961
## modNull 253 2215.862&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For those who are familiar with linear model parameterisation, it is possible to reach even a higher degree of flexibility by using the ‘pmodels’ argument, within the ‘drm()’ function. However, this will require another post. Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Catara, S., Cristaudo, A., Gualtieri, A., Galesi, R., Impelluso, C., Onofri, A., 2016. Threshold temperatures for seed germination in nine species of Verbascum (Scrophulariaceae). Seed Science Research 26, 30–46.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Tei, F., Cousens, R.D., 2011. The cure model: an improved way to describe seed germination? Weed Research 51, 516–524.&lt;/li&gt;
&lt;li&gt;Onofri, A., Piepho, H.-P., Kozak, M., 2019. Analysing censored data in agricultural research: A review with examples and software tips. Annals of Applied Biology 174, 3–13. &lt;a href=&#34;https://doi.org/10.1111/aab.12477&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/aab.12477&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ritz, C., Pipper, C.B., Streibig, J.C., 2013. Analysis of germination data from agricultural experiments. European Journal of Agronomy 45, 1–6.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Survival analysis and germination data: an overlooked connection</title>
      <link>https://www.statforbiology.com/2019/stat_seedgermination_germination/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_seedgermination_germination/</guid>
      <description>


&lt;div id=&#34;the-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The background&lt;/h1&gt;
&lt;p&gt;Seed germination data describe the time until an event of interest occurs. In this sense, they are very similar to survival data, apart from the fact that we deal with a different (and less sad) event: germination instead of death. But, seed germination data are also similar to failure-time data, phenological data, time-to-remission data… the first point is: &lt;strong&gt;germination data are time-to-event data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You may wonder: what’s the matter with time-to-event data? Do they have anything special? With few exceptions, all time-to-event data are affected by a certain form of uncertainty, which takes the name of ‘censoring’. It relates to the fact that the exact time of event may not be precisely know. I think it is good to give an example.&lt;/p&gt;
&lt;p&gt;Let’s take a germination assay, where we put, say, 100 seeds in a Petri dish and make daily inspections. At each inspection, we count the number of germinated seeds. In the end, what have we learnt about the germination time of each seed? It is easy to note that we do not have a precise value, we only have an uncertainty interval. Let’s make three examples.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we found a germinated seed at the first inspection time, we only know that germination took place before the inspection (left-censoring).&lt;/li&gt;
&lt;li&gt;If we find a germinated seed at the second inspection time, we only know that germination took place somewhere between the first and the second inspection (interval-censoring).&lt;/li&gt;
&lt;li&gt;If we find an ungerminated seed at the end of the experiment, we only know that its germination time, if any, is higher than the duration of the experiment (right-censoring).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Censoring implies a lot of uncertainty, which is additional to other more common sources of uncertainty, such as the individual seed-to-seed variability or random errors in the manipulation process. Is censoring a problem? Yes, it is, although it is usually overlooked in seed science research. I made this point in a recent review (Onofri et al., 2019) and I would like to come back to this issue here. The second point is that &lt;strong&gt;the analyses of data from germination assays should always account for censoring&lt;/strong&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analyses-for-germination-assays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data analyses for germination assays&lt;/h1&gt;
&lt;p&gt;A swift search of literature shows that seed scientists are often interested in describing the time-course of germinations, for different plant species, in different environmental conditions. In simple terms, if we take a population of seeds and give it enough water, the individual seeds will start germinating. Their germination times will be different, due to natural seed-to-seed variability and, therefore, the proportion of germinated seeds will progressively and monotonically increase over time. However, this proportion will almost never reach 1, because, there will often be a fraction of seeds that will not germinate in the given conditions, because it is either dormant or nonviable.&lt;/p&gt;
&lt;p&gt;In order to describe this progress to germination, a log-logistic function is often used:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
G(t) = \frac{d}{ 1 + exp \left\{ - b \right[ \log(t) - \log(e) \left] \right\}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the fraction of germinated seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the germinable fraction, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time for the germinable fraction and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The above model is sygmoidally shaped and it is symmetric on a log-time scale. The three parameters are biologically relevant, as they describe the three main features of seed germination, i.e. capability (&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;), speed (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and uniformity (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;My third point in this post is that &lt;strong&gt;The process of data analysis for germination data is often based on fitting a log-logistic (or similar) model to the observed counts&lt;/strong&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-example-a-simulated-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example: a simulated dataset&lt;/h1&gt;
&lt;p&gt;Considering the above, we can simulate the results of a germination assay. Let’s take a 100-seed-sample from a population where we have 85% of germinable seeds (&lt;span class=&#34;math inline&#34;&gt;\(d = 0.85\)&lt;/span&gt;), with a median germination time &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt; days and &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt;. Obviously, this sample will not necessarily reflect the characteristics of the population. We can do this sampling in R, by using a three-steps approach.&lt;/p&gt;
&lt;div id=&#34;step-1-the-ungerminated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: the ungerminated fraction&lt;/h2&gt;
&lt;p&gt;First, let’s simulate the number of germinated seeds, assuming a binomial distribution with a proportion of successes equal to 0.85. We use the random number generator ‘rbinom()’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 1
d &amp;lt;- 0.85
set.seed(1234)
nGerm &amp;lt;- rbinom(1, 100, d)
nGerm
## [1] 89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, in this instance, 89 seeds germinated out of 100, which is not the expected 85%. This is a typical random fluctuation: indeed, we were lucky in selecting a good sample of seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-germination-times-for-the-germinated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: germination times for the germinated fraction&lt;/h2&gt;
&lt;p&gt;Second, let’s simulate the germination times for these 89 germinable seeds, by drawing from a log-logistic distribution with &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt;. To this aim, we use the ‘rllogis()’ function in the ‘actuar’ package (Dutang et al., 2008):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 2
library(actuar)
b &amp;lt;- 1.6; e &amp;lt;- 4.5 
Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))
Gtimes
##   [1]  3.2936708  3.4089762  3.2842199  1.4401630  3.1381457 82.1611955
##   [7]  9.4906364  2.9226745  4.3424551  2.7006042  4.0202158  8.0519663
##  [13]  0.9492013  7.8199588  1.6163588  7.9661485  8.4641154 11.2879041
##  [19]  9.5014360  7.2786264  7.5809838 12.7421713 32.7999661  9.9691944
##  [25]  1.8137333  4.2197542  1.0218849  1.6604417 30.0352308  5.0235265
##  [31]  8.5085067  7.5367739  4.4185382 11.5555259  2.1919263 10.6509339
##  [37]  8.6857151  0.2185902  1.8377033  3.9362727  3.0864702  7.3804164
##  [43]  3.2978782  7.0100360  4.4775843  2.8328842  4.6721090  9.1258796
##  [49]  2.1485568 21.8749808  7.4265984  2.5148724  4.4491466 13.1132301
##  [55]  4.4559642  4.5684584  2.2556488 11.8783556  1.5338755  1.4106592
##  [61] 31.8419420  7.2666641 65.0154287  9.2798476  2.5988399  7.4612907
##  [67]  4.4048509 27.7439121  3.8257187 15.4967751  1.1960785 62.5152642
##  [73]  2.0169970 19.1134899  4.2891084  6.0420938 22.6521417  7.1946293
##  [79]  2.9028993  0.9241876  4.8277336 13.8068124  4.0273655 10.8651761
##  [85]  1.1509735  5.9593534  7.4009589 12.6839405  1.1698335        Inf
##  [91]        Inf        Inf        Inf        Inf        Inf        Inf
##  [97]        Inf        Inf        Inf        Inf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we have a vector hosting 100 germination times (‘Gtimes’). Please, note that I added 11 infinite germination times, to represent non-germinable seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-counts-of-germinated-seeds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: counts of germinated seeds&lt;/h2&gt;
&lt;p&gt;Unfortunately, due to the monitoring schedule, we cannot observe the exact germination time for each single seed in the sample; we can only count the seeds which have germinated between two assessment times. Therefore, as the third step, we simulate the observed counts, by assuming daily monitoring for 40 days.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obsT &amp;lt;- seq(1, 40, by=1) #Observation schedule
count &amp;lt;- table( cut(Gtimes, breaks = c(0, obsT)) )
count
## 
##   (0,1]   (1,2]   (2,3]   (3,4]   (4,5]   (5,6]   (6,7]   (7,8]   (8,9] 
##       3      11      10       8      13       2       1      12       4 
##  (9,10] (10,11] (11,12] (12,13] (13,14] (14,15] (15,16] (16,17] (17,18] 
##       5       2       3       2       2       0       1       0       0 
## (18,19] (19,20] (20,21] (21,22] (22,23] (23,24] (24,25] (25,26] (26,27] 
##       0       1       0       1       1       0       0       0       0 
## (27,28] (28,29] (29,30] (30,31] (31,32] (32,33] (33,34] (34,35] (35,36] 
##       1       0       0       1       1       1       0       0       0 
## (36,37] (37,38] (38,39] (39,40] 
##       0       0       0       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, e.g., 11 germinated seeds were counted at day 2; therefore they germinated between day 1 and day 2 and their real germination time is unknown, but included in the range between 1 and 2 (left-open and right-closed). This is a typical example of interval-censoring (see above).&lt;/p&gt;
&lt;p&gt;We can also see that, in total, we counted 86 germinated seeds and, therefore, 14 seeds were still ungerminated at the end of the assay. For this simulation exercise, we know that 11 seeds were non-germinable and three seeds were germinable, but were not allowed enough time to germinate (look at the table above: there are 3 seeds with germination times higher than 40). In real life, this is another source of uncertainty: we might be able to ascertain whether these 14 seeds are viable or not (e.g. by using a tetrazolium test), but, if they are viable, we would never be able to tell whether they are dormant or their germination time is simply longer than the duration of the assay. In real life, we can only reach an uncertain conclusion: the germination time of the 14 ungerminated seeds is comprised between 40 days to infinity; this is an example of right-censoring.&lt;/p&gt;
&lt;p&gt;The above uncertainty affects our capability of describing the germination time-course from the observed data. We can try to picture the situation in the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_Survival_Germination_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the real germination time-course? The red one? The blue one? Something in between? We cannot really say this from our dataset, we are uncertain. The grey areas represent the uncertainty due to censoring. Do you think that we can reasonably neglect it?&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;The question is: how do we fit a log-logistic function to this dataset? We can either neglect censoring or account for it. Let’s see the differences.&lt;/p&gt;
&lt;div id=&#34;ignoring-censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ignoring censoring&lt;/h2&gt;
&lt;p&gt;We have seen that the time-corse of germination can be described by using a log-logistic cumulative distribution function. Therefore, seed scientists are used to re-organising their data, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
propCum &amp;lt;- cumsum(counts)/100
df &amp;lt;- data.frame(time = obsT, counts = counts, propCum = propCum) 
df
##    time counts propCum
## 1     1      3    0.03
## 2     2     11    0.14
## 3     3     10    0.24
## 4     4      8    0.32
## 5     5     13    0.45
## 6     6      2    0.47
## 7     7      1    0.48
## 8     8     12    0.60
## 9     9      4    0.64
## 10   10      5    0.69
## 11   11      2    0.71
## 12   12      3    0.74
## 13   13      2    0.76
## 14   14      2    0.78
## 15   15      0    0.78
## 16   16      1    0.79
## 17   17      0    0.79
## 18   18      0    0.79
## 19   19      0    0.79
## 20   20      1    0.80
## 21   21      0    0.80
## 22   22      1    0.81
## 23   23      1    0.82
## 24   24      0    0.82
## 25   25      0    0.82
## 26   26      0    0.82
## 27   27      0    0.82
## 28   28      1    0.83
## 29   29      0    0.83
## 30   30      0    0.83
## 31   31      1    0.84
## 32   32      1    0.85
## 33   33      1    0.86
## 34   34      0    0.86
## 35   35      0    0.86
## 36   36      0    0.86
## 37   37      0    0.86
## 38   38      0    0.86
## 39   39      0    0.86
## 40   40      0    0.86&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In practice, seed scientists often use the observed counts to determine the cumulative proportion (or percentage) of germinated seeds. The cumulative proportion (‘propCum’) is used as the response variable, while the observation time (‘time’) is used as the independent variable and a log-logistic function is fitted by non-linear least squares regression. &lt;strong&gt;Hope that you can clearly see that, by doing so, we totally neglect the grey areas in the figure above, we only look at the observed points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can fit a nonlinear regression model by using the ‘drm’ function, in the ‘drc’ package (Ritz et al., 2015). The argument ‘fct’ is used to set the fitted function to log-logistic with three parameters (the equation above).&lt;/p&gt;
&lt;p&gt;The ‘plot()’ and ‘summary()’ methods can be used to plot a graph and to retrieve the estimated parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
mod &amp;lt;- drm(propCum ~ time, data = df,
           fct = LL.3() )
plot(mod, log = &amp;quot;&amp;quot;,
      xlab = &amp;quot;Time (days)&amp;quot;,
      ylab = &amp;quot;Proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_Survival_Germination_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.8497771  0.0702626 -26.327 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.8768793  0.0070126 125.044 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.2691575  0.1020457  51.635 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.01762168 (37 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that our estimates are very close to the real values (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; = 1.85 vs. 1.6; &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; = 5.27 vs. 4.5; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; = 0.88 vs. 0.86) and we also see that standard errors are rather small (the coefficient of variability goes from 1 to 4%). There is a difference in sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which relates to the fact that the ‘LL.3()’ function in ‘drc’ removes the minus sign in the equation above. Please, disregard this aspect, which stems from the fact that the ‘drc’ package is rooted in pesticide bioassays.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting-for-censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accounting for censoring&lt;/h2&gt;
&lt;p&gt;So far, we have totally neglected censoring. But, how can we account for it? The answer is simple: as with survival studies, we should use time-to-event methods, which are specifically devised to incorporate the uncertainty due to censoring. In medicine, the body of time-to-event methods goes under the name of survival analysis, which explains the title of my post. My colleagues and I have extensively talked about these methods in two of our recent papers and related appendices (Onofri et al., 2019; Onofri et al., 2018). Therefore, I will not go into detail, now.&lt;/p&gt;
&lt;p&gt;I will just say that time-to-event methods directly consider the observed counts as the response variable. As the independent variable, they consider the extremes of each time interval (‘timeBef’ and ‘timeAf’; see below). In contrast to nonlinear regression, we do not need to transform the observed counts into cumulative proportions, as we did before. Furthermore, by using an interval as the independent variable, we inject into the model the uncertainty due to censoring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(timeBef = c(0, obsT), timeAf = c(obsT, Inf), counts = c(as.numeric(counts), 100 - sum(counts)) )
df
##    timeBef timeAf counts
## 1        0      1      3
## 2        1      2     11
## 3        2      3     10
## 4        3      4      8
## 5        4      5     13
## 6        5      6      2
## 7        6      7      1
## 8        7      8     12
## 9        8      9      4
## 10       9     10      5
## 11      10     11      2
## 12      11     12      3
## 13      12     13      2
## 14      13     14      2
## 15      14     15      0
## 16      15     16      1
## 17      16     17      0
## 18      17     18      0
## 19      18     19      0
## 20      19     20      1
## 21      20     21      0
## 22      21     22      1
## 23      22     23      1
## 24      23     24      0
## 25      24     25      0
## 26      25     26      0
## 27      26     27      0
## 28      27     28      1
## 29      28     29      0
## 30      29     30      0
## 31      30     31      1
## 32      31     32      1
## 33      32     33      1
## 34      33     34      0
## 35      34     35      0
## 36      35     36      0
## 37      36     37      0
## 38      37     38      0
## 39      38     39      0
## 40      39     40      0
## 41      40    Inf     14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time-to-event models can be easily fitted by using the same function as we used above (the ‘drm()’ function in the ‘drc’ package). However, there are some important differences in the model call. The first one relate to model definition: a nonlinear regression model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CumulativeProportion ~ timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other hand, a time-to-event model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Count ~ timeBef + timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A second difference is that we need to explicitly set the argument ‘type’, to ‘event’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Time-to-event model
modTE &amp;lt;- drm(counts ~ timeBef + timeAf, data = df, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
summary(modTE)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.826006   0.194579 -9.3844 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.881476   0.036928 23.8701 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.302109   0.565273  9.3797 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the nonlinear regression fit, the estimates from a time-to-event fit are very similar, but the standard errors are much higher (the coefficient of variability now goes from 4 to 11%).&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neglecting-or-accounting-for-censoring&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neglecting or accounting for censoring?&lt;/h1&gt;
&lt;p&gt;You may wonder which of the two analysis is right and which is wrong. We cannot say this from just one dataset. However, we can repeat the Monte Carlo simulation above to extract 1000 samples, fit the model by using the two methods and retrieve parameter estimates and standard errors for each sample and method. We do this by using the code below (please, be patient… it may take some time).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GermSampling &amp;lt;- function(nSeeds, timeLast, stepOss, e, b, d){
    
    #Draw a sample as above
    nGerm &amp;lt;- rbinom(1, nSeeds, d)
    Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
    Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))

    
    #Generate the observed data
    obsT &amp;lt;- seq(1, timeLast, by=stepOss) 
    counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
    propCum &amp;lt;- cumsum(counts)/nSeeds
    timeBef &amp;lt;- c(0, obsT)
    timeAf &amp;lt;- c(obsT, Inf)
    counts &amp;lt;- c(counts, nSeeds - sum(counts))
    
    #Calculate the T50 with two methods
    mod &amp;lt;- drm(propCum ~ obsT, fct = LL.3() )
    modTE &amp;lt;- drm(counts ~ timeBef + timeAf, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
    c(b1 = summary(mod)[[3]][1,1],
      ESb1 = summary(mod)[[3]][1,2],
      b2 = summary(modTE)[[3]][1,1],
      ESb2 = summary(modTE)[[3]][1,2],
      d1 = summary(mod)[[3]][2,1],
      ESd1 = summary(mod)[[3]][2,2],
      d2 = summary(modTE)[[3]][2,1],
      ESd2 = summary(modTE)[[3]][2,2],
      e1 = summary(mod)[[3]][3,1],
      ESe1 = summary(mod)[[3]][3,2],
      e2 = summary(modTE)[[3]][3,1],
      ESe2 = summary(modTE)[[3]][3,2] )
}

set.seed(1234)
result &amp;lt;- data.frame()
for (i in 1:1000) {
  res &amp;lt;- GermSampling(100, 40, 1, 4.5, 1.6, 0.85)
  result &amp;lt;- rbind(result, res)
} 
names(result) &amp;lt;- c(&amp;quot;b1&amp;quot;, &amp;quot;ESb1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;ESb2&amp;quot;,
                   &amp;quot;d1&amp;quot;, &amp;quot;ESd1&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;ESd2&amp;quot;,
                   &amp;quot;e1&amp;quot;, &amp;quot;ESe1&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;ESe2&amp;quot;)
result &amp;lt;- result[result$d2 &amp;gt; 0,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have stored our results in the data frame ‘result’. The means of estimates obtained for both methods should be equal to the real values that we used for the simulation, which will ensure that estimators are unbiased. The means of standard errors (in brackets, below) should represent the real sample-to-sample variability, which may be obtained from the Monte Carlo standard deviation, i.e. from the standard deviation of the 1000 estimates for each parameter and method.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Nonlinear regression&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.63 (0.051)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.006)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.086)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Time-to-event method&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.62 (0.187)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.579)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Real values&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.60 (0.188)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.593)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We clearly see that both nonlinear regression and the time-to-event method lead to unbiased estimates of model parameters. However, standard errors from nonlinear regression are severely biased and underestimated. On the contrary, standard errors from time-to-event method are unbiased.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Censoring is peculiar to germination assays and other time-to-event studies. It may have a strong impact on the reliability of our standard errors and, consequently, on hypotheses testing. Therefore, censoring should never be neglected and time-to-event methods should necessarily be used for data analyses with germination assays. The body of time-to-event methods often goes under the name of ‘survival analysis’, which creates a direct link between survival data and germination data.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;C. Dutang, V. Goulet and M. Pigeon (2008). actuar: An R Package for Actuarial Science. Journal of Statistical Software, vol. 25, no. 7, 1-37.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Paolo Benincasa, M B Mesgaran, and Christian Ritz. 2018. Hydrothermal-Time-to-Event Models for Seed Germination. European Journal of Agronomy 101: 129–39.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Hans Peter Piepho, and Marcin Kozak. 2019. Analysing Censored Data in Agricultural Research: A Review with Examples and Software Tips. Annals of Applied Biology, 174, 3-13.&lt;/li&gt;
&lt;li&gt;Ritz, C., F. Baty, J. C. Streibig, and D. Gerhard. 2015. Dose-Response Analysis Using R. PLOS ONE, 10 (e0146021, 12).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stabilising transformations: how do I present my results?</title>
      <link>https://www.statforbiology.com/2019/stat_general_reportingresults/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_reportingresults/</guid>
      <description>


&lt;p&gt;ANOVA is routinely used in applied biology for data analyses, although, in some instances, the basic assumptions of normality and homoscedasticity of residuals do not hold. In those instances, most biologists would be inclined to adopt some sort of stabilising transformations (logarithm, square root, arcsin square root…), prior to ANOVA. Yes, there might be more advanced and elegant solutions, but stabilising transformations are suggested in most traditional biometry books, they are very straightforward to apply and they do not require any specific statistical software. I do not think that this traditional technique should be underrated.&lt;/p&gt;
&lt;p&gt;However, the use of stabilising transformations has one remarkable drawback, it may hinder the clarity of results. I’d like to give a simple, but relevant example.&lt;/p&gt;
&lt;div id=&#34;an-example-with-counts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with counts&lt;/h1&gt;
&lt;p&gt;Consider the following dataset, that represents the counts of insects on 15 independent leaves, treated with the insecticides A, B and C (5 replicates):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- structure(data.frame(
  Insecticide = structure(c(1L, 1L, 1L, 1L, 1L, 
    2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L), 
    .Label = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;), class = &amp;quot;factor&amp;quot;), 
  Count = c(448, 906, 484, 477, 634, 211, 276, 
    415, 587, 298, 50, 90, 73, 44, 26)), 
  .Names = c(&amp;quot;Insecticide&amp;quot;, &amp;quot;Count&amp;quot;))
dataset
##    Insecticide Count
## 1            A   448
## 2            A   906
## 3            A   484
## 4            A   477
## 5            A   634
## 6            B   211
## 7            B   276
## 8            B   415
## 9            B   587
## 10           B   298
## 11           C    50
## 12           C    90
## 13           C    73
## 14           C    44
## 15           C    26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We should not expect that a count variable is normally distributed with equal variances. Indeed, a graph of residuals against expected values shows clear signs of heteroscedasticity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lm(Count ~ Insecticide, data=dataset)
plot(mod, which = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_General_reportingResults_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this situation, a logarithmic transformation is often suggested to produce a new normal and homoscedastic dataset. Therefore we take the log-transformed variable and submit it to ANOVA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(log(Count) ~ Insecticide, data=dataset)
print(anova(model), digits=6)
## Analysis of Variance Table
## 
## Response: log(Count)
##             Df   Sum Sq Mean Sq F value     Pr(&amp;gt;F)    
## Insecticide  2 15.82001 7.91000 50.1224 1.4931e-06 ***
## Residuals   12  1.89376 0.15781                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
summary(model)
## 
## Call:
## lm(formula = log(Count) ~ Insecticide, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.6908 -0.1849 -0.1174  0.2777  0.5605 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    6.3431     0.1777  35.704 1.49e-13 ***
## InsecticideB  -0.5286     0.2512  -2.104   0.0572 .  
## InsecticideC  -2.3942     0.2512  -9.529 6.02e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3973 on 12 degrees of freedom
## Multiple R-squared:  0.8931, Adjusted R-squared:  0.8753 
## F-statistic: 50.12 on 2 and 12 DF,  p-value: 1.493e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the standard error for each mean (SEM) corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{0.158/5}\)&lt;/span&gt;. In the end, we might show the following table of means for transformed data:&lt;/p&gt;
&lt;!-- table --&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Insecticide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Means (log n.)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6.343&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.815&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3.985&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;SEM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.178&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- table --&gt;
&lt;p&gt;Unfortunately, we loose clarity: how many insects did we have on each leaf? If we present in our manuscript a table like this one we might be asked by our readers or by the reviewer to report the means on the original measurement unit. What should we do, then? Here are some suggestions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can present the means of the original data with standard deviations. This is clearly less than optimal, if we want to suggest more than the bare variability of the observed sample. Furthermore, &lt;strong&gt;please remember that the means of original data may not be a good measure of central tendency, if the original population is strongly ‘asymmetric’ (skewed)!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;We can show back-transformed means. Accordingly, if we have done, e.g., a logarithmic transformation, we can exponentiate the means of transformed data and report them back to the original measurement unit. Back-transformed means ‘estimate’ the medians of the original populations, which may be regarded as better measures of central tendency for skewed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We suggest that the use of the second method. However, this leaves us with the problem of adding a measure of uncertainty to back-transformed means. No worries, we can use the delta method to back-transform standard errors. It is straightforward:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the first derivative of the back-transform function [in this case the first derivative of exp(X)=exp(X)] and&lt;/li&gt;
&lt;li&gt;multiply it by the standard error of the transformed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This may be simply done by hand, with e.g &lt;span class=&#34;math inline&#34;&gt;\(exp(6.343) \times 0.178 = 101.19\)&lt;/span&gt; (for insecticide A). This ‘manual’ solution is always available, regardless of the statistical software at hand. With R, we can use the ‘emmeans’ package (Lenth, 2016):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emmeans)
countM &amp;lt;- emmeans(model, ~Insecticide, transform = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is enough to set the argument ‘transform’ to ’response, although the transformation must be embedded in the model. It means: it is ok if we coded the model as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log(Count) ~ Insecticide&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the contrary, it fails if we coded the model as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logCount ~ Insecticide&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the transformation was performed prior to fitting.&lt;/p&gt;
&lt;p&gt;Obviously, the back-transformed standard error is different for each mean (there is no homogeneity of variances on the original scale, but we knew this already). Back-transformed data might be presented as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Insecticide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;568.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;101.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;335.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51.88&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9.57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It would be appropriate to state it clearly (e.g. in a footnote), that means and SEs were obtained by back-transformation via the delta method. Far clearer, isn’t it? As I said, there are other solutions, such as fitting a GLM, but stabilising transformations are simple and they are easily acceptable in biological Journals.&lt;/p&gt;
&lt;p&gt;If you want to know something more about the delta-method you might start from &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_thedeltamethod/&#34;&gt;my post here&lt;/a&gt;. A few years ago, some collegues and I have also discussed these issues in a journal paper (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Lenth, R.V., 2016. Least-Squares Means: The R Package lsmeans. Journal of Statistical Software 69. &lt;a href=&#34;https://doi.org/10.18637/jss.v069.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v069.i01&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genotype experiments: fitting a stability variance model with R</title>
      <link>https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/</guid>
      <description>


&lt;p&gt;Yield stability is a fundamental aspect for the selection of crop genotypes. The definition of stability is rather complex (see, for example, Annichiarico, 2002); in simple terms, the yield is stable when it does not change much from one environment to the other. It is an important trait, that helps farmers to maintain a good income in most years.&lt;/p&gt;
&lt;p&gt;Agronomists and plant breeders are continuosly concerned with the assessment of genotype stability; this is accomplished by planning genotype experiments, where a number of genotypes is compared on randomised complete block designs, with three to five replicates. These experiments are repeated in several years and/or several locations, in order to measure how the environment influences yield level and the ranking of genotypes.&lt;/p&gt;
&lt;p&gt;I would like to show an exemplary dataset, referring to a multienvironment experiment with winter wheat. Eight genotypes were compared in seven years in central Italy, on randomised complete block designs with three replicates. The ‘WinterWheat’ dataset is available in the ‘aomisc’ package, which is the accompanying package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset and turns the ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
## Loading required package: drc
## Loading required package: MASS
## Loading required package: drcData
## 
## &amp;#39;drc&amp;#39; has been loaded.
## Please cite R and &amp;#39;drc&amp;#39; if used for a publication,
## for references type &amp;#39;citation()&amp;#39; and &amp;#39;citation(&amp;#39;drc&amp;#39;)&amp;#39;.
## 
## Attaching package: &amp;#39;drc&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     gaussian, getInitial
data(WinterWheat)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 7    35     1   SIMETO  4.99 1996
## 8    33     1    SOLEX  6.08 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that this is a multienvironment experiment as it is repeated in several years: each year is an ‘environment’ in itself. Furthermore, please note that the year effect (i.e. the environment effect) is of random nature: we select the years, but we cannot control the weather conditions.&lt;/p&gt;
&lt;div id=&#34;defining-a-linear-mixed-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defining a linear mixed model&lt;/h1&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the following linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + \gamma_{kj} + g_i + e_j +  ge_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt; is the interaction effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, while &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the residual random term, which is assumed as normally distributed with variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As I said before, the block effect, the environment effect and the ‘genotype x environment’ interaction are usually regarded as random. Therefore, they are assumed as normally distributed, with means equal to 0 and variances respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{\gamma}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{e}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. Indeed, the above model is a Linear Mixed Model (LMM).&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. It is clear that this value is a measure of instability: if it is high, genotypes may respond differently to different environments. In this way, each genotype can be favored in some specific environments and disfavored in some others. Shukla (1974) has suggested that we should allow &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt; assume a different value for each genotype and use these components as a measure of stability (stability variances). According to Shukla, a genotype is considered stable when its stability variance is lower than &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown that stability variances can be obtained within the mixed model framework, by appropriately coding the variance-covariance matrix for random effects. He gave SAS code to accomplish this task and, to me, it was not straightforward to transport his code into R. I finally succeeded and I though I should better share my code, just in case it helps someone save a few headaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-stability-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a stability variance model&lt;/h1&gt;
&lt;p&gt;As we have to model the variance-covariance of random effects, we need to use the ‘lme’ function in the ‘nlme’ package (Pinheiro and Bates, 2000). The problem is that random effects are crossed and they are not easily coded with this package. After an extensive literature search, I could find the solution in the aforementioned book (at pag. 162-163) and in Galecki and Burzykowski (2013). The trick is made by appropriately using the ‘pdMat’ construct (‘pdBlocked’ and ‘pdIdent’). In the code below, I have built a block-diagonal variance-covariance matrix for random effects, where blocks and genotypes are nested within years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdIdent(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix)
## Year = pdIdent(1), pdIdent(Block - 1), pdIdent(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      1.07314201 1.0359257
## Block1           0.01641744 0.1281306
## Block2           0.01641744 0.1281306
## Block3           0.01641744 0.1281306
## GenotypeCOLOSSEO 0.17034091 0.4127238
## GenotypeCRESO    0.17034091 0.4127238
## GenotypeDUILIO   0.17034091 0.4127238
## GenotypeGRAZIA   0.17034091 0.4127238
## GenotypeIRIDE    0.17034091 0.4127238
## GenotypeSANCARLO 0.17034091 0.4127238
## GenotypeSIMETO   0.17034091 0.4127238
## GenotypeSOLEX    0.17034091 0.4127238
## Residual         0.14880400 0.3857512&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wee see that the variance component for the ‘genotype x environment’ interaction is the same for all genotypes and equal to 0.170.&lt;/p&gt;
&lt;p&gt;Allowing for a different variance component per genotype is relatively easy, by replacing ‘pdIdent()’ with ‘pdDiag()’, as shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix2 &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdDiag(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix2)
## Year = pdIdent(1), pdIdent(Block - 1), pdDiag(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      0.86592829 0.9305527
## Block1           0.01641744 0.1281305
## Block2           0.01641744 0.1281305
## Block3           0.01641744 0.1281305
## GenotypeCOLOSSEO 0.10427267 0.3229128
## GenotypeCRESO    0.09588553 0.3096539
## GenotypeDUILIO   0.47612340 0.6900170
## GenotypeGRAZIA   0.15286445 0.3909788
## GenotypeIRIDE    0.11860160 0.3443858
## GenotypeSANCARLO 0.02575029 0.1604690
## GenotypeSIMETO   0.42998504 0.6557324
## GenotypeSOLEX    0.06713590 0.2591060
## Residual         0.14880439 0.3857517&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we have now different variance components and we can classify some genotypes as stable (e.g. Sancarlo, Solex and Creso) and some others as unstable (e.g. Duilio and Simeto).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-the-means&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working with the means&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit a stability variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retreived the means for genotypes in all years, we can fit a stability variance model, although we have to use a different approach, with respect to the one we used with the whole dataset. In this case, we need to model the variance of residuals, introducing within-group (within-year) heteroscedasticity. The ‘weights’ argument can be used, together with the ‘pdIdent()’ variance function, to allow for a different variance for each genotype. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
model.mixM &amp;lt;- lme(Yield ~ Genotype, random = ~ 1|Year, data = WinterWheatM,
                 weights = varIdent(form=~1|Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code to retrieve the within-year variances is not obvious, unfortunately. However, you can use the folllowing snippet as a guidance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vF &amp;lt;- model.mixM$modelStruct$varStruct
sdRatios &amp;lt;- c(1, coef(vF, unconstrained = F))
names(sdRatios)[1] &amp;lt;- &amp;quot;COLOSSEO&amp;quot;
scalePar &amp;lt;- model.mixM$sigma
sigma2i &amp;lt;- (scalePar * sdRatios)^2
sigma2i
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.15387857 0.14548985 0.52571780 0.20246664 0.16820264 0.07535112 
##     SIMETO      SOLEX 
## 0.47958756 0.11673900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code outputs ‘sigma2i’, which does not contain the stability variances. Indeed, we should remove the within-experiment error (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;), which can only be estimated from the whole dataset. Indeed, if we take the estimate of 0.1488 (see code above), divide by three (the number of blocks) and subtract from ‘sigma2i’, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2i - model.mix2$sigma^2/3
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.10427711 0.09588839 0.47611634 0.15286517 0.11860118 0.02574966 
##     SIMETO      SOLEX 
## 0.42998610 0.06713754&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which are the stability variances, as obtained with the analyses of the whole dataset.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Annichiarico, P., 2002. Genotype x Environment Interactions - Challenges and Opportunities for Plant Breeding and Cultivar Recommendations. Plant Production and protection paper, Food &amp;amp; Agriculture Organization of the United Nations (FAO), Roma.&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Shukla, G.K., 1972. Some statistical aspects of partitioning genotype-environmental components of variability. Heredity 29, 237–245.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How do we combine errors, in biology? The delta method</title>
      <link>https://www.statforbiology.com/2019/stat_general_thedeltamethod/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_thedeltamethod/</guid>
      <description>


&lt;p&gt;In a recent post I have shown that we can build linear combinations of model parameters (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_errorpropagation/&#34;&gt;see here&lt;/a&gt; ). For example, if we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;, we can build a linear combination as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The standard error for this combination can be obtained as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma_Z = \sqrt{ A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In biology, nonlinear transformations are much more frequent than linear transformations. Nonlinear transformations are, e.g., &lt;span class=&#34;math inline&#34;&gt;\(Z = exp(Q + W)\)&lt;/span&gt;, or, &lt;span class=&#34;math inline&#34;&gt;\(Z = 1/(Q + W)\)&lt;/span&gt;. What is the standard error for these nonlinear transformations? This is not a complex problem, but the solution may be beyond biologists with an average level of statistical proficiency. It is named the ‘delta method’ and it provides the so called ‘delta standard errors’. I thought it might be useful to talk about it, by using a very simple language and a few examples.&lt;/p&gt;
&lt;div id=&#34;example-1-getting-the-half-life-of-a-herbicide&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1: getting the half-life of a herbicide&lt;/h1&gt;
&lt;p&gt;A herbicide has proven to follow a first order degradation kinetic in soil, with constant degradation rate &lt;span class=&#34;math inline&#34;&gt;\(k = -0.035\)&lt;/span&gt; and standard error equal to &lt;span class=&#34;math inline&#34;&gt;\(0.00195\)&lt;/span&gt;. What is the half-life (&lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;) of this herbicide and its standard error?&lt;/p&gt;
&lt;p&gt;Every pesticide chemist knows that the half-life (&lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;) is derived by the degradation rate, according to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_{1/2} = \frac{\log(0.5)}{k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, the half-life for our herbicide is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Thalf &amp;lt;- log(0.5)/-0.035
Thalf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19.80421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But … what is the standard error of this half-life? There is some uncertainty around the estimate of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and it is clear that such an uncertainty should propagate to the estimate of &lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;; unfortunately, the transformation is nonlinear and we cannot use the expression given above for linear transformations.&lt;/p&gt;
&lt;div id=&#34;the-basic-idea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The basic idea&lt;/h2&gt;
&lt;p&gt;The basic idea behind the delta method is that most of the simple nonlinear functions, which we use in biology, can be locally approximated by the tangent line through a point of interest. For example, our nonlinear half-life function is &lt;span class=&#34;math inline&#34;&gt;\(Y = \log(0.5)/X\)&lt;/span&gt; and, obviously, we are interested in the point where &lt;span class=&#34;math inline&#34;&gt;\(X = k = -0.035\)&lt;/span&gt;. In the graph below, we have represented our nonlinear function (in black) and its tangent line (in red) through the above point: we can see that the approximation is fairly good in the close vicinity of &lt;span class=&#34;math inline&#34;&gt;\(X = -0.035\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.statforbiology.com/post/Stat_General_TheDeltaMethod_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the equation of the tangent line? In general, if the nonlinear function is &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, you may remember from high school that the slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; of the tangent line is equal to the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, that is &lt;span class=&#34;math inline&#34;&gt;\(G&amp;#39;(X)\)&lt;/span&gt;. You may also remember that the equation of a line with slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; through the point &lt;span class=&#34;math inline&#34;&gt;\(P(X_1, Y_1)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y - Y_1 = m(X - X_1)\)&lt;/span&gt;. As &lt;span class=&#34;math inline&#34;&gt;\(Y_1 = G(X_1)\)&lt;/span&gt;, the tangent line has equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = G(X_1) + G&amp;#39;(X_1)(X - X_1)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-the-derivative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need the derivative!&lt;/h2&gt;
&lt;p&gt;In order to write the equation of the red line in the Figure above, we need to consider that &lt;span class=&#34;math inline&#34;&gt;\(X_1 = -0.035\)&lt;/span&gt; and we need to be able to calculate the first derivative of our nonlinear half-life fuction. I am not able to derive the expression of the first derivative for all nonlinear functions and it was a relief for me to discover that R can handle this task in simple ways, e.g. by using the function ‘D()’. For our case, it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(log(0.5)/X), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -(log(0.5)/X^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, we can use this R function to calculate the slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; of the tangent line in the figure above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- -0.035
m &amp;lt;- eval( D(expression(log(0.5)/X), &amp;quot;X&amp;quot;) )
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 565.8344&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already know that &lt;span class=&#34;math inline&#34;&gt;\(G(-0.035) = 19.80421\)&lt;/span&gt;. Therefore, we can write the equation of the tangent line (red line in the graph above):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = 19.80421 + 565.8344 \, (X + 0.035)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = 19.80421 + 565.8344 \, X + 565.8344 \cdot 0.035 = 39.60841 + 565.8344 \, X\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;replacing-a-curve-with-a-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Replacing a curve with a line&lt;/h2&gt;
&lt;p&gt;Now, we have two functions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the original nonlinear half-life function &lt;span class=&#34;math inline&#34;&gt;\(Y = \log(0.5)/X\)&lt;/span&gt;$&lt;/li&gt;
&lt;li&gt;a new linear function (&lt;span class=&#34;math inline&#34;&gt;\(Y = 39.60841 + 565.8344 \, X\)&lt;/span&gt;), that is a very close approximation to the previous one, at least near to the point &lt;span class=&#34;math inline&#34;&gt;\(X = -0.035\)&lt;/span&gt;, which we are interested in.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, we can approximate the former with the latter! If we use the linear function, we see that the half-life is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;39.60841 + 565.8344 * -0.035&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19.80421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is what we expected. The advantage is that we can now use the low of propagation of errors to estimate the standard error (see the first and second equation in this post):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma_{ \left[ 39.60841 + 565.8344 \, X \right]} = \sqrt{ 562.8344^2 \, \sigma^2_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt( m^2 * (0.00195 ^ 2) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.103377&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in-general&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In general…&lt;/h2&gt;
&lt;p&gt;If we have a nonlinear transformation &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, the standard error for this transformation is approximated by knowing the first derivative &lt;span class=&#34;math inline&#34;&gt;\(G&amp;#39;(X)\)&lt;/span&gt; and the standard error of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_{G(X)}  \simeq \sqrt{ [G&amp;#39;(X)]^2 \, \sigma^2_X }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-a-back-transformed-count&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2: a back-transformed count&lt;/h1&gt;
&lt;p&gt;A paper reports that the mean number of microorganisms in a substrate, on a logarithmic scale, was &lt;span class=&#34;math inline&#34;&gt;\(X_1 = 5\)&lt;/span&gt; with standard error &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.84\)&lt;/span&gt;. It is easy to derive that the actual number of micro-organisms was &lt;span class=&#34;math inline&#34;&gt;\(\exp{5} = 148.4132\)&lt;/span&gt;; what is the standard error of the back-transformed mean?&lt;/p&gt;
&lt;p&gt;The first derivative of our nonlinear function is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(exp(X)), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## exp(X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and thus the slope of the tangent line is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 5
m &amp;lt;- eval( D(expression(exp(X)), &amp;quot;X&amp;quot;) )
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 148.4132&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the function above, the standard error for the back-transformed mean is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma &amp;lt;- 0.84
sqrt( m^2 * sigma^2 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 124.6671&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3-selenium-concentration-in-olive-drupes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3: Selenium concentration in olive drupes&lt;/h1&gt;
&lt;p&gt;The concentration of selenium in olive drupes was found to be &lt;span class=&#34;math inline&#34;&gt;\(3.1 \, \mu g \,\, g^{-1}\)&lt;/span&gt; with standard error equal to 0.8. What is the intake of selenium when eating one drupe? Please, consider that one drupe weights, on average, 3.4 g (SE = 0.31) and that selenium concentration and drupe weight show a covariance of 0.55.&lt;/p&gt;
&lt;p&gt;The amount of selenium is easily calculated as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 3.1; W = 3.4
X * W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delta standard errors can be obtained by considering the partial derivatives for each of the two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mX &amp;lt;- eval( D(expression(X*W), &amp;quot;X&amp;quot;) )
mW &amp;lt;- eval( D(expression(X*W), &amp;quot;W&amp;quot;) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and combining them as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- 0.8; sigmaW &amp;lt;- 0.31; sigmaXW &amp;lt;- 0.55
sqrt( (mX^2)*sigmaX^2 + (mW^2)*sigmaW^2 + 2*X*W*sigmaXW )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.462726&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For those of you who would like to get involved with matrix notation: we can reach the same result via matrix multiplication (see below). This might be easier when we have more than two variables to combine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;der &amp;lt;- matrix(c(mX, mW), 1, 2)
sigma &amp;lt;- matrix(c(sigmaX^2, sigmaXW, sigmaXW, sigmaW^2), 2, 2, byrow = T)
sqrt( der %*% sigma %*% t(der) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]
## [1,] 4.462726&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-delta-method-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The delta method with R&lt;/h1&gt;
&lt;p&gt;In R there is a shortcut function to calculate delta standard errors, that is available in the ‘car’ package. In order to use it, we need to have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a named vector for the variables that we have to combine&lt;/li&gt;
&lt;li&gt;an expression for the transformation&lt;/li&gt;
&lt;li&gt;a variance-covariance matrix&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the first example, we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;k&amp;quot; = -0.035)
sigma &amp;lt;- matrix(c(0.00195^2), 1, 1)

library(car)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;deltaMethod(object = obj, g=&amp;quot;log(0.5)/k&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Estimate       SE    2.5 %   97.5 %
## log(0.5)/k 19.80421 1.103377 17.64163 21.96678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the second example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;X1&amp;quot; = 5)
sigma &amp;lt;- matrix(c(0.84^2), 1, 1)
deltaMethod(object = obj, g=&amp;quot;exp(X1)&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Estimate       SE     2.5 %   97.5 %
## exp(X1) 148.4132 124.6671 -95.92978 392.7561&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the third example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;X&amp;quot; = 3.1, &amp;quot;W&amp;quot; = 3.4)
sigma &amp;lt;- matrix(c(0.8^2, 0.55, 0.55, 0.31^2), 2, 2, byrow = T)
deltaMethod(object = obj, g=&amp;quot;X * W&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate       SE    2.5 %   97.5 %
## X * W    10.54 4.462726 1.793218 19.28678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function ‘deltaMethod()’ is very handy to be used in connection with model objects, as we do not need to provide anything, but the transformation function. But this is something that requires another post!&lt;/p&gt;
&lt;p&gt;However, two final notes relating to the delta method need to be pointed out here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the delta standard error is always approximate;&lt;/li&gt;
&lt;li&gt;if the original variables are gaussian, the transformed variable, usually, is not gaussian.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>https://www.statforbiology.com/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_correlationindependence2/</guid>
      <description>


&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;
&lt;div id=&#34;a-routine-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A ‘routine’ experiment&lt;/h1&gt;
&lt;p&gt;I will use the same example as presented &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in my previous post&lt;/a&gt;, which should allow us to compare the results with those obtained by using more traditional methods of data analyses. It is a genotype experiment, laid out in randomised complete blocks, with 27 wheat genotypes and three replicates. For each plot, the collegue who made the experiment recorded several traits, including yield (Yield) and weight of thousand kernels (TKW). The dataset ‘WheatQuality.csv’ is available on ‘gitHub’; it consists of 81 records (plots) and just as many couples of measures in all. The code below loads the necessary packages, the data and transforms the numeric variable ‘Block’ into a factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(plyr)
library(nlme)
library(asreml)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;In my previous post&lt;/a&gt;, I used the above dataset to calculate the Pearson’s correlation coefficient between Yield and TKW for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;plot measurements,&lt;/li&gt;
&lt;li&gt;residuals,&lt;/li&gt;
&lt;li&gt;treatment/block means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Piepho (2018) showed that, for an experiment like this one, the above correlations can be estimated by coding a multiresponse mixed model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijk} = \mu_i + \beta_{ik} + \tau_{ij} + \epsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ijk}\)&lt;/span&gt; is the response for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the rootstock &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the mean for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{ik}\)&lt;/span&gt; is the effect of the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ij}\)&lt;/span&gt; is the effect of genotype &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; is the residual for each of 81 observations for two traits.&lt;/p&gt;
&lt;p&gt;In the above model, the residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; need to be normally distributed and heteroscedastic, with trait-specific variances. Furthermore, residuals belonging to the same plot (the two observed traits) need to be correlated (correlation of errors).&lt;/p&gt;
&lt;p&gt;Hans-Peter Piepho, in his paper, put forward the idea that the ‘genotype’ and ‘block’ effects for the two traits can be taken as random, even though they might be of fixed nature, especially the genotype effect. This idea makes sense, because, for this application, we are mainly interested in variances and covariances. Both random effects need to be heteroscedastic (trait-specific variance components) and there must be a correlation between the two traits.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge, there is no way to fit such a complex model with the ‘nlme’ package and related ‘lme()’ function (I’ll gave a hint later on, for a simpler model). Therefore, I decided to use the package ‘asreml’ (Butler et al., 2018), although this is not freeware. With the function ‘asreml()’, we need to specify the following components.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The response variables. When we set a bivariate model with ‘asreml’, we can ‘cbind()’ Yield and TKW and use the name ‘trait’ to refer to them.&lt;/li&gt;
&lt;li&gt;The fixed model, that only contains the trait effect. The specification is, therefore, ‘cbind(Yield, TKW) ~ trait - 1’. Following Piepho (2018), I removed the intercept, to separately estimate the means for the two traits.&lt;/li&gt;
&lt;li&gt;The random model, that is composed by the interactions ‘genotype x trait’ and ‘block x trait’. For both, I specified a general unstructured variance covariance matrix, so that the traits are heteroscedastic and correlated. Therefore, the random model is ~ Genotype:us(trait) + Block:us(trait).&lt;/li&gt;
&lt;li&gt;The residual structure, where the observations in the same plot (the term ‘units’ is used in ‘asreml’ to represent the observational units, i.e. the plots) are heteroscedastic and correlated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait, init = c(3.7, -0.25, 1.7)) + 
          Block:us(trait, init = c(77, 38, 53)),
        residual = ~ units:us(trait, init = c(6, 0.16, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:22 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -641.556           1.0    160 18:20:22    0.0 (3 restrained)
##  2      -548.767           1.0    160 18:20:22    0.0
##  3      -448.970           1.0    160 18:20:22    0.0
##  4      -376.952           1.0    160 18:20:22    0.0
##  5      -334.100           1.0    160 18:20:22    0.0
##  6      -317.511           1.0    160 18:20:22    0.0
##  7      -312.242           1.0    160 18:20:22    0.0
##  8      -311.145           1.0    160 18:20:22    0.0
##  9      -311.057           1.0    160 18:20:22    0.0
## 10      -311.056           1.0    160 18:20:22    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   component  std.error    z.ratio
## Block:trait!trait_Yield:Yield     3.7104778  3.9364268  0.9426005
## Block:trait!trait_TKW:Yield      -0.2428390  1.9074544 -0.1273105
## Block:trait!trait_TKW:TKW         1.6684568  1.8343662  0.9095549
## Genotype:trait!trait_Yield:Yield 77.6346623 22.0956257  3.5135761
## Genotype:trait!trait_TKW:Yield   38.8322972 15.0909109  2.5732242
## Genotype:trait!trait_TKW:TKW     53.8616088 15.3520661  3.5084274
## units:trait!R                     1.0000000         NA         NA
## units:trait!trait_Yield:Yield     6.0939037  1.1951128  5.0990195
## units:trait!trait_TKW:Yield       0.1635551  0.7242690  0.2258209
## units:trait!trait_TKW:TKW         4.4717901  0.8769902  5.0990195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The box above shows the results about the variance-covariance parameters. In order to get the correlations, I specified the necessary combinations of variance-covariance parameters. It is necessary to remember that estimates, in ‘asreml’, are named as V1, V2, … Vn, according to their ordering in model output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parms &amp;lt;- mod.asreml$vparameters
vpredict(mod.asreml, rb ~ V2 / (sqrt(V1)*sqrt(V3) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate        SE
## rb -0.09759916 0.7571335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, rt ~ V5 / (sqrt(V4)*sqrt(V6) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate       SE
## rt 0.6005174 0.130663&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, re ~ V9 / (sqrt(V8)*sqrt(V10) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate        SE
## re 0.03133109 0.1385389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the estimates are very close to those obtained by using the Pearson’s correlation coefficients (see my previous post). The advantage of this mixed model solution is that we can also test hypotheses in a relatively reliable way. For example, I tested the hypothesis that residuals are not correlated by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;coding a reduced model where residuals are heteroscedastic and independent, and&lt;/li&gt;
&lt;li&gt;comparing this reduced model with the complete model by way of a REML-based Likelihood Ratio Test (LRT).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Removing the correlation of residuals is easily done, by changing the correlation structure from ‘us’ (unstructured variance-covariance matrix) to ‘idh’ (diagonal variance-covariance matrix).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:us(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:23 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.023           1.0    160 18:20:23    0.0 (2 restrained)
##  2      -383.859           1.0    160 18:20:23    0.0
##  3      -344.687           1.0    160 18:20:23    0.0
##  4      -321.489           1.0    160 18:20:23    0.0
##  5      -312.488           1.0    160 18:20:23    0.0
##  6      -311.167           1.0    160 18:20:23    0.0
##  7      -311.083           1.0    160 18:20:23    0.0
##  8      -311.082           1.0    160 18:20:23    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml2  1      0.05107    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, I tried to further reduce the model to test the significance of the correlation between block means and genotype means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:24 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.027           1.0    160 18:20:24    0.0 (2 restrained)
##  2      -383.866           1.0    160 18:20:24    0.0
##  3      -344.694           1.0    160 18:20:24    0.0
##  4      -321.497           1.0    160 18:20:24    0.0
##  5      -312.496           1.0    160 18:20:24    0.0
##  6      -311.175           1.0    160 18:20:24    0.0
##  7      -311.090           1.0    160 18:20:24    0.0
##  8      -311.090           1.0    160 18:20:24    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml3  2     0.066663    0.6399&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml4 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:idh(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:25 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -406.458           1.0    160 18:20:25    0.0 (2 restrained)
##  2      -394.578           1.0    160 18:20:25    0.0
##  3      -352.769           1.0    160 18:20:25    0.0
##  4      -327.804           1.0    160 18:20:25    0.0
##  5      -318.007           1.0    160 18:20:25    0.0
##  6      -316.616           1.0    160 18:20:25    0.0
##  7      -316.549           1.0    160 18:20:25    0.0
##  8      -316.549           1.0    160 18:20:25    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)   
## mod.asreml/mod.asreml4  3       10.986  0.003364 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that only the genotype means are significantly correlated.&lt;/p&gt;
&lt;p&gt;An alternative (and more useful) way to code the same model is by using the ‘corgh’ structure, instead of ‘us’. These two structures are totally similar, apart from the fact that the first one uses the correlations, instead of the covariances. Another difference, which we should consider when giving starting values, is that correlations come before variances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:26 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -632.445           1.0    160 18:20:26    0.0 (3 restrained)
##  2      -539.383           1.0    160 18:20:26    0.0 (2 restrained)
##  3      -468.810           1.0    160 18:20:26    0.0 (1 restrained)
##  4      -422.408           1.0    160 18:20:26    0.0
##  5      -371.304           1.0    160 18:20:26    0.0
##  6      -336.191           1.0    160 18:20:26    0.0
##  7      -317.547           1.0    160 18:20:26    0.0
##  8      -312.105           1.0    160 18:20:26    0.0
##  9      -311.118           1.0    160 18:20:26    0.0
## 10      -311.057           1.0    160 18:20:26    0.0
## 11      -311.056           1.0    160 18:20:26    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09759916  0.7571335
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60051740  0.1306748
## Genotype:trait!trait_Yield                77.63466334 22.0981962
## Genotype:trait!trait_TKW                  53.86160957 15.3536792
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.03133109  0.1385389
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of this parameterisation is that we can test our hypotheses by easily setting up contraints on correlations. One way to do this is to run the model with the argument ‘start.values = T’. In this way I could derive a data frame (‘mod.init$parameters’), with the starting values for REML maximisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the starting values
mod.init &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset, start.values = T)
init &amp;lt;- mod.init$vparameters
init&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                    Component Value Constraint
## 1  Genotype:trait!trait!TKW:!trait!Yield.cor -0.10          U
## 2                 Genotype:trait!trait_Yield  3.80          P
## 3                   Genotype:trait!trait_TKW  1.80          P
## 4     Block:trait!trait!TKW:!trait!Yield.cor  0.60          U
## 5                    Block:trait!trait_Yield 77.00          P
## 6                      Block:trait!trait_TKW 53.00          P
## 7                              units:trait!R  1.00          F
## 8     units:trait!trait!TKW:!trait!Yield.cor  0.03          U
## 9                    units:trait!trait_Yield  6.00          P
## 10                     units:trait!trait_TKW  4.50          P&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘init’ data frame has three columns: (i) names of parameters, (ii) initial values and (iii) type of constraint (U: unconstrained, P = positive, F = fixed). Now, if we take the seventh row (correlation of residuals), set the initial value to 0 and set the third column to ‘F’ (meaning: keep the initial value fixed), we are ready to fit a model without correlation of residuals (same as the ‘model.asreml2’ above). What I had to do was just to pass this data frame as the starting value matrix for a new model fit (see the argument ‘R.param’, below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init2 &amp;lt;- init
init2[8, 2] &amp;lt;- 0
init2[8, 3] &amp;lt;- &amp;quot;F&amp;quot;

mod.asreml.r2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:28 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1136.066           1.0    160 18:20:28    0.0 (1 restrained)
##  2      -939.365           1.0    160 18:20:28    0.0 (1 restrained)
##  3      -719.371           1.0    160 18:20:28    0.0 (1 restrained)
##  4      -550.513           1.0    160 18:20:28    0.0
##  5      -427.355           1.0    160 18:20:28    0.0
##  6      -353.105           1.0    160 18:20:28    0.0
##  7      -323.421           1.0    160 18:20:28    0.0
##  8      -313.616           1.0    160 18:20:28    0.0
##  9      -311.338           1.0    160 18:20:28    0.0
## 10      -311.087           1.0    160 18:20:28    0.0
## 11      -311.082           1.0    160 18:20:28    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r2)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09516456  0.7572040
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60136047  0.1305180
## Genotype:trait!trait_Yield                77.63463977 22.0809306
## Genotype:trait!trait_TKW                  53.86159936 15.3451466
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.00000000         NA
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r2, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)
## mod.asreml.r/mod.asreml.r2  1     0.051075    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is even more interesting is that ‘asreml’ permits to force the parameters to be linear combinations of one another. For instance, we can code a model where the residual correlation is contrained to be equal to the treatment correlation. To do so, we have to set up a two-column matrix (M), with row names matching the component names in the ‘asreml’ parameter vector. The matrix M should contain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the first column, the equality relationships (same number, same value)&lt;/li&gt;
&lt;li&gt;in the second column, the coefficients for the multiplicative relationships&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we would set the matrix M as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;firstCol &amp;lt;- c(1, 2, 3, 4, 5, 6, 7, 1, 8, 9)
secCol &amp;lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
M &amp;lt;- cbind(firstCol, secCol)
dimnames(M)[[1]] &amp;lt;- mod.init$vparameters$Component
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           firstCol secCol
## Genotype:trait!trait!TKW:!trait!Yield.cor        1      1
## Genotype:trait!trait_Yield                       2      1
## Genotype:trait!trait_TKW                         3      1
## Block:trait!trait!TKW:!trait!Yield.cor           4      1
## Block:trait!trait_Yield                          5      1
## Block:trait!trait_TKW                            6      1
## units:trait!R                                    7      1
## units:trait!trait!TKW:!trait!Yield.cor           1      1
## units:trait!trait_Yield                          8      1
## units:trait!trait_TKW                            9      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that in ‘firstCol’, the 1st and 8th element are both equal to 1, which contraints them to assume the same value. We can now pass the matrix M as the value of the ‘vcc’ argument in the model call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init, vcc = M)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:29 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1122.762           1.0    160 18:20:29    0.0 (1 restrained)
##  2      -900.308           1.0    160 18:20:29    0.0
##  3      -665.864           1.0    160 18:20:29    0.0
##  4      -492.020           1.0    160 18:20:29    0.0
##  5      -383.085           1.0    160 18:20:29    0.0
##  6      -336.519           1.0    160 18:20:29    0.0 (1 restrained)
##  7      -319.561           1.0    160 18:20:29    0.0
##  8      -315.115           1.0    160 18:20:29    0.0
##  9      -314.540           1.0    160 18:20:29    0.0
## 10      -314.523           1.0    160 18:20:29    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r3)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            component  std.error    z.ratio
## Block:trait!trait!TKW:!trait!Yield.cor    -0.1146908  0.7592678 -0.1510545
## Block:trait!trait_Yield                    3.6991785  3.9364622  0.9397216
## Block:trait!trait_TKW                      1.6601799  1.8344090  0.9050217
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.2336876  0.1117699  2.0907919
## Genotype:trait!trait_Yield                70.5970531 19.9293722  3.5423621
## Genotype:trait!trait_TKW                  48.9763800 13.8464106  3.5371174
## units:trait!R                              1.0000000         NA         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.2336876  0.1117699  2.0907919
## units:trait!trait_Yield                    6.3989855  1.2811965  4.9945387
## units:trait!trait_TKW                      4.6952670  0.9400807  4.9945358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r3, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)   
## mod.asreml.r/mod.asreml.r3  1       6.9336   0.00423 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the residual and treatment correlations are equal in this latter model. We also see that this reduced model fits significantly worse than the complete model ‘mod.asreml.r’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-freeware&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going freeware&lt;/h1&gt;
&lt;p&gt;Considering that the block means are not correlated, if we were willing to take the ‘block’ effect as fixed, we could fit the resulting model also with the ‘nlme’ package and the function ‘lme()’ (Pinheiro and Bates, 2000). However, we should cast the model as a univariate model.&lt;/p&gt;
&lt;p&gt;To this aim, the two variables (Yield and TKW) need to be piled up and a new factor (‘Trait’) needs to be introduced to identify the observations for the two traits. Another factor is also necessary to identify the different plots, i.e. the observational units. To perform such a restructuring, I used the ‘melt()’ function in the ‘reshape2’ package Wickham, 2007) and assigned the name ‘Y’ to the response variable, that is now composed by the two original variables Yield and TKW, one on top of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$Plot &amp;lt;- 1:81
mdataset &amp;lt;- melt(dataset[,c(-3,-6)], variable.name= &amp;quot;Trait&amp;quot;, value.name=&amp;quot;Y&amp;quot;, id=c(&amp;quot;Genotype&amp;quot;, &amp;quot;Block&amp;quot;, &amp;quot;Plot&amp;quot;))
mdataset$Block &amp;lt;- factor(mdataset$Block)
head(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait    Y
## 1 arcobaleno     1    1   TKW 44.5
## 2 arcobaleno     2    2   TKW 42.8
## 3 arcobaleno     3    3   TKW 42.7
## 4       baio     1    4   TKW 40.6
## 5       baio     2    5   TKW 42.7
## 6       baio     3    6   TKW 41.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait     Y
## 157  vesuvio     1   76 Yield 54.37
## 158  vesuvio     2   77 Yield 55.02
## 159  vesuvio     3   78 Yield 53.41
## 160 vitromax     1   79 Yield 54.39
## 161 vitromax     2   80 Yield 50.97
## 162 vitromax     3   81 Yield 48.83&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fixed model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y ~ Trait*Block&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to introduce a totally unstructured variance-covariance matrix for the random effect, I used the ‘pdMat’ construct:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;random = list(Genotype = pdSymm(~Trait - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relating to the residuals, heteroscedasticity can be included by using the ‘weight()’ argument and the ‘varIdent’ variance function, which allows a different standard deviation per trait:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weight = varIdent(form = ~1|Trait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also allowed the residuals to be correlated within each plot, by using the ‘correlation’ argument and specifying a general ‘corSymm()’ correlation form. Plots are nested within genotypes, thus I used a nesting operator, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;correlation = corSymm(form = ~1|Genotype/Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Y ~ Trait*Block, 
                 random = list(Genotype = pdSymm(~Trait - 1)),
                 weight = varIdent(form=~1|Trait), 
                 correlation = corCompSymm(form=~1|Genotype/Plot),
                 data = mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retreiving the variance-covariance matrices needs some effort, as the function ‘getVarCov()’ does not appear to work properly with this multistratum model. First of all, we can retreive the variance-covariance matrix for the genotype random effect (G) and the corresponding correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Variance structure for random effects
obj &amp;lt;- mod
G &amp;lt;- matrix( as.numeric(getVarCov(obj)), 2, 2 )
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]
## [1,] 53.86081 38.83246
## [2,] 38.83246 77.63485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.6005237
## [2,] 0.6005237 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we can retreive the ‘conditional’ variance-covariance matrix (R), that describes the correlation of errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conditional variance-covariance matrix (residual error)
V &amp;lt;- corMatrix(obj$modelStruct$corStruct)[[1]] #Correlation for residuals
sds &amp;lt;- 1/varWeights(obj$modelStruct$varStruct)[1:2]
sds &amp;lt;- obj$sigma * sds #Standard deviations for residuals (one per trait)
R &amp;lt;- t(V * sds) * sds #Going from correlation to covariance
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 4.4718234 0.1635375
## [2,] 0.1635375 6.0939251&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]
## [1,] 1.00000000 0.03132756
## [2,] 0.03132756 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total correlation matrix is simply obtained as the sum of G and R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tr &amp;lt;- G + R
cov2cor(Tr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.5579906
## [2,] 0.5579906 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are the same as those obtained by using ‘asreml’. Hope these snippets are useful!&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Butler, D., Cullis, B.R., Gilmour, A., Gogel, B., Thomson, R., 2018. ASReml-r reference manual - version 4. UK.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-effects models in s and s-plus. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Wickham, H., 2007. Reshaping data with the reshape package. Journal of Statistical Software 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part I</title>
      <link>https://www.statforbiology.com/2019/stat_general_correlationindependence1/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_correlationindependence1/</guid>
      <description>


&lt;div id=&#34;observations-are-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observations are grouped&lt;/h1&gt;
&lt;p&gt;When we have recorded two traits in different subjects, we can be interested in describing their joint variability, by using the Pearson’s correlation coefficient. That’s ok, altough we have to respect some basic assumptions (e.g. linearity) that have been detailed elsewhere (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlation_alookat/&#34;&gt;see here&lt;/a&gt;). Problems may arise when we need to test the hypothesis that the correlation coefficient is equal to 0. In this case, we need to make sure that all the couples of observations are taken on independent subjects.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is most often false whenever measures are taken from designed field experiments. In this case, observations may be grouped by one or more treatment/blocking factors. This has been clearly described by Bland and Altman (1994); we would like to give an example that is more closely related to plant/crop science. Think about a genotype experiment, where we compare the behaviour of several genotypes in a randomised blocks design. Usually, we do not only measure yield. We also measure other traits, such as crop height. At the end of the experiment, we might be interested in reporting the correlation between yield and height. How should we proceed? It would seem an easy task, but it is not.&lt;/p&gt;
&lt;p&gt;Let’s assume that we have a randomised blocks design, with 27 genotypes and 3 replicates. For each plot, we recorded two traits, i.e. yield and the weight of thousand kernels (TKW). In the end, we have 81 plots and just as many couples of measures in all. We will use the dataset ‘WheatQuality.csv’, that is available on ‘gitHub’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Hmisc)
library(knitr)
library(plyr)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How many correlations?&lt;/h1&gt;
&lt;p&gt;It may be tempting to consider the whole lot of measures and calculate the correlation coefficient between yield and TKW. This is the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra &amp;lt;- with(dataset, rcorr(Yield, TKW) )
ra$r[1,2] #Correlation coefficient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.540957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra$P[1,2] #P-level&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.850931e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We observe a positive correlation, and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; seems to be significantly different from 0. Therefore, we would be encouraged to conclude that plots with a high value on yield tend to have a high value on TKW, as well. Unfortunately, such a conclusion is not supported by the data.&lt;/p&gt;
&lt;p&gt;Indeed, the test of significance is clearly invalid, as the 81 plots are not independent; they are grouped by block and genotype and we are totally neglecting these two effects. Are we sure that the same correlation holds for all genotypes/blocks? Let’s check this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor &amp;lt;- function(x) cor(x$Yield, x$TKW)
wgCors &amp;lt;- ddply(dataset, ~Genotype, wCor)
wgCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Genotype         V1
## 1   arcobaleno  0.9847228
## 2         baio  0.1611952
## 3      claudio -0.9993872
## 4     colorado  0.9837293
## 5     colosseo  0.4564855
## 6        creso -0.5910193
## 7       duilio -0.9882330
## 8       gianni -0.7603802
## 9       giotto  0.9520211
## 10      grazia  0.4980828
## 11       iride  0.7563338
## 12    meridano  0.1174342
## 13      neodur  0.4805871
## 14      orobel  0.8907754
## 15 pietrafitta -0.9633891
## 16  portobello  0.9210135
## 17   portofino -0.9900764
## 18   portorico  0.1394211
## 19       preco  0.9007067
## 20    quadrato -0.5840238
## 21    sancarlo -0.6460670
## 22      simeto -0.4051779
## 23       solex -0.6066363
## 24 terrabianca -0.4076416
## 25       verdi  0.5801404
## 26     vesuvio -0.7797493
## 27    vitromax -0.8056514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wbCors &amp;lt;- ddply(dataset, ~Block, wCor)
wbCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Block        V1
## 1     1 0.5998137
## 2     2 0.5399990
## 3     3 0.5370398&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for the genotypes, we have 27 correlation coefficients, ranging from -0.999 to 0.985. We only have three couples of measurements per genotype and it is clear that this is not much, to reliably estimate a correlation coefficient. However, it is enough to be suspicious about the extent of correlation between yield and TKW, as it may depend on the genotype.&lt;/p&gt;
&lt;p&gt;On the other hand, the correlation within blocks is more constant, independent on the block and similar to the correlation between plots.&lt;/p&gt;
&lt;p&gt;It may be interesting to get an estimate of the average within-group correlation. To this aim, we can perform two separate ANOVAs (one per trait), including all relevant effects (blocks and genotypes) and calculate the correlation between the residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Yield ~ factor(Block) + Genotype, data = dataset)
mod2 &amp;lt;- lm(TKW ~ factor(Block) + Genotype, data = dataset)
wCor &amp;lt;- rcorr(residuals(mod1), residuals(mod2))
wCor$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x          y
## x 1.00000000 0.03133109
## y 0.03133109 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x         y
## x        NA 0.7812693
## y 0.7812693        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average within-group correlation is very small and unsignificant. Let’s think about this correlation: residuals represent yield and TKW values for all plots, once the effects of blocks and genotypes have been removed. A high correlation of residuals would mean that, letting aside the effects of the block and genotype to which it belongs, a plot with a high value on yield also shows a high value on TKW. The existence of such correlation is clearly unsopported by our dataset.&lt;/p&gt;
&lt;p&gt;As the next step, we could consider the means for genotypes/blocks and see whether they are correlated. Blocks and genotypes are independent and, in principle, significance testing is permitted. However, this is not recommended with block means, as three data are too few to make tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Genotype, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rgPear &amp;lt;- rcorr( as.matrix(means[,2:3]) )
rgPear$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Mu1       Mu2
## Mu1 1.0000000 0.5855966
## Mu2 0.5855966 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgPear$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Mu1        Mu2
## Mu1         NA 0.00133149
## Mu2 0.00133149         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Block, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rbPear &amp;lt;- cor( as.matrix(means[,2:3]) )
rbPear&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Mu1         Mu2
## Mu1  1.00000000 -0.08812544
## Mu2 -0.08812544  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We note that the correlation between genotype means is high and significant. On the contrary, the correlation between block means is near to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-so-what&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And so what?&lt;/h1&gt;
&lt;p&gt;At this stage, you may be confused… Let’s try to clear the fog.&lt;/p&gt;
&lt;p&gt;Obtaining a reliable measure of correlation from designed experiments is not obvious. Indeed, in every designed field experiment we have groups of subjects and there are several possible types of correlation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;correlation between plot measurements&lt;/li&gt;
&lt;li&gt;correlation between the residuals&lt;/li&gt;
&lt;li&gt;correlation between treatment/block means&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All these correlations should be investigated and used for interpretation.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The ‘naive’ correlation between the plot measurements is very easily calculated, but it is grossy misleading. Indeed, it disregards the treatment/block effects and it does not permit hypotheses testing, as the subjects are not independent. In this example, looking at the ‘naive’ correlation coefficient, we would wrongly conclude that plots with high yield also have high TKW, while further analyses show that this is not true, in general. We would reasonably suggest that the ‘naive’ correlation coefficient is never used for interpretation.&lt;/li&gt;
&lt;li&gt;The correlation between the residuals is a reliable measure of joint variation, because the experimental design is adequately referenced, by removing the effects of tratments/blocks. In this example, residuals are not correlated. Further analyses show that the correlation between yield and TKW, if any, may depend on the genotype, while it does not depend on the block.&lt;/li&gt;
&lt;li&gt;The correlation between treatment/block means permits to assess whether the treatment/block effects on the two traits are correlated. In this case, while we are not allowed to conclude that yield and TKW are, in general, correlated, we can conclude that the genotypes with a high level of yield also show a high level of TKW.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Whenever we have data from designed field experiments, our correlation analyses should never be limited to the calculation of the ‘naive’ correlation coefficient between the observed values. This may not be meaningful! On the contrary, our interpretation should be mainly focused on the correlation between residuals and on the correlation between the effects of treatments/blocks.&lt;/p&gt;
&lt;p&gt;An elegant and advanced method to perform sound correlation analyses on data from designed field experiments has been put forward by Piepho (2018), within the frame of mixed models. Such an approach will be described in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bland, J.M., Altman, D.G., 1994. Statistics Notes: Correlation, regression, and repeated data. BMJ 308, 896–896.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How do we combine errors? The linear case</title>
      <link>https://www.statforbiology.com/2019/stat_general_errorpropagation/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_errorpropagation/</guid>
      <description>


&lt;p&gt;In our research work, we usually fit models to experimental data. Our aim is to estimate some biologically relevant parameters, together with their standard errors. Very often, these parameters are interesting in themselves, as they represent means, differences, rates or other important descriptors. In other cases, we use those estimates to derive further indices, by way of some appropriate calculations. For example, think that we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;: it might be relevant to calculate the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The above operation is named ‘linear combination’; it is a sort of a weighted sum of model parameters. The question is: what is the standard error of Z? I would like to show this by way of a simple biological example.&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;We have measured the germination rate for seeds of &lt;em&gt;Brassica rapa&lt;/em&gt; at six levels of water potential in the substrate (-1, -0.9, -0.8, -0.7, -0.6 and -0.5 MPa). We would like to predict the germination rate for a water potential level of -0.75 MPa.&lt;/p&gt;
&lt;p&gt;Literature references suggest that the relationship between germination rate and water potential in the substrate is linear. Therefore, as the first step, we fit a linear regression model to our observed data. If we are into R, the code to accomplish this task is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GR &amp;lt;- c(0.11, 0.20, 0.34, 0.46, 0.56, 0.68)
Psi &amp;lt;- c(-1, -0.9, -0.8, -0.7, -0.6, -0.5)
lMod &amp;lt;- lm(GR ~ Psi)
summary(lMod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GR ~ Psi)
## 
## Residuals:
##          1          2          3          4          5          6 
##  0.0076190 -0.0180952  0.0061905  0.0104762 -0.0052381 -0.0009524 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.25952    0.02179   57.79 5.37e-07 ***
## Psi          1.15714    0.02833   40.84 2.15e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.01185 on 4 degrees of freedom
## Multiple R-squared:  0.9976, Adjusted R-squared:  0.997 
## F-statistic:  1668 on 1 and 4 DF,  p-value: 2.148e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is clear that we can use the fitted model to calculate the GR-value at -0.75 MPa, as &lt;span class=&#34;math inline&#34;&gt;\(GR = 1.26 - 1.16 \times 0.75 = 0.39\)&lt;/span&gt;. This is indeed a linear combination of model parameters, as we have shown above. Q and W are the estimated model parameters, while &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(B = -0.75\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Of course, the derived response is also an estimate and we need to give a measure of uncertainty. For both model parameters we have standard errors; the question is: how does the uncertainty in parameter estimates propagates to their linear combination? In simpler words: it is easy to build a weightes sum of model parameters, but, how do we make a weighted sum of their standard errors?&lt;/p&gt;
&lt;p&gt;Sokal and Rohlf (1981) at pag. 818 of their book, show that, in general, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{var}(A \, Q + B \, W) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{QW}\)&lt;/span&gt; is the covariance of Q and W. I attach the proof below; it is pretty simple to understand and it is worth the effort. However, several students in biology are rather reluctant when they have to delve into maths. Therefore, I would also like to give an empirical ‘proof’, by using some simple R code.&lt;/p&gt;
&lt;p&gt;Let’s take two samples (Q and W) and combine them by using two coefficients A and B. Let’s calculate the variance for the combination:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q &amp;lt;- c(12, 14, 11, 9)
W &amp;lt;- c(2, 4, 7, 8)
A &amp;lt;- 2
B &amp;lt;- 3
C &amp;lt;- 4
var(A * Q + B * W + C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.58333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A^2 * var(Q) + B^2 * var(W) + 2 * A * B * cov(Q, W)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.58333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results match exactly! In our example, the variance and covariance of estimates are retrieved by using the ‘vcov()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vcov(lMod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)          Psi
## (Intercept) 0.0004749433 0.0006020408
## Psi         0.0006020408 0.0008027211&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2Q &amp;lt;- vcov(lMod)[1,1]
sigma2W &amp;lt;- vcov(lMod)[2,2]
sigmaQW &amp;lt;- vcov(lMod)[1,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error for the prediction is simply obtained as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt( sigma2Q + 0.75^2 * sigma2W - 2 * 0.75 * sigmaQW )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004838667&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-functions-predict-and-glht&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The functions ‘predict()’ and ‘glht()’&lt;/h1&gt;
&lt;p&gt;Now that we have understood the concept, we can use R to make the calculations. For this example, the ‘predict()’ method represents the most logical approach. We only need to pass the model object and the X value which we have to make a prediction for. This latter value needs to be organised as a data frame, with column name(s) matching the name(s) of the X-variable(s) in the original dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(lMod, newdata = data.frame(Psi = -0.75), 
        se.fit = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $fit
##         1 
## 0.3916667 
## 
## $se.fit
## [1] 0.004838667
## 
## $df
## [1] 4
## 
## $residual.scale
## [1] 0.01185227&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from the predict method, there is another function of more general interest, which can be used to build linear combinations of model parameters. It is the ‘glht()’ function in the ‘multcomp’ package. To use this function, we need a model object and we need to organise the coefficients for the transformation in a matrix, with as many rows as there are combinations to calculate. When writing the coefficients, we disregard C: we have seen that every constant value does not affect the variance of the transformation.&lt;/p&gt;
&lt;p&gt;For example, just imagine that we want to predict the GR for two levels of water potential, i.e. -0.75 (as above) and -0.55 MPa. The coefficients (A, B) for the combinations are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Z1 &amp;lt;- c(1, -0.75)
Z2 &amp;lt;- c(1, -0.55)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We pile up the two vectors in one matrix with two rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M &amp;lt;- matrix(c(Z1, Z2), 2, 2, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we pass that matrix to the ‘glht()’ function as an argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
lcombs &amp;lt;- glht(lMod, linfct = M, adjust = &amp;quot;none&amp;quot;)
summary(lcombs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = GR ~ Psi)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## 1 == 0 0.391667   0.004839   80.94 2.30e-07 ***
## 2 == 0 0.623095   0.007451   83.62 2.02e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- single-step method)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above method can be easily extended to other types of linear models and linear combinations of model parameters. The ‘adjust’ argument is useful whenever we want to obtain familywise confidence intervals, which can account for the multiplicity problem. But this is another story…&lt;/p&gt;
&lt;p&gt;Happy work with these functions!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;We know that the variance for a random variable is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ var(Z) = \frac{1}{n-1}\sum \left(Z - \hat{Z}\right)^2 = \\ = \frac{1}{n-1}\sum \left(Z - \frac{1}{n} \sum{Z}\right)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(Z = AQ + BW + C\)&lt;/span&gt;, where A, B and C are the coefficients and Q and W are two random variables, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ var(Z) = \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ \left(AQ + BW + C\right)}\right]^2 = \\ 
= \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ AQ} - \frac{1}{n} \sum{ BW} - \frac{1}{n} \sum{ C}\right]^2 = \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= \frac{1}{n-1}\sum \left[AQ + BW + C - A \hat{Q} - B \hat{W} - C \right]^2 = \\
= \frac{1}{n-1}\sum \left[\left( AQ - A \hat{Q}\right) + \left( BW - B \hat{W}\right) \right]^2 = \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ =\frac{1}{n-1}\sum \left[A \left( Q - \hat{Q}\right) + B \left( W - \hat{W}\right) \right]^2 = \\
= \frac{1}{n-1}\sum \left[A^2 \left( Q - \hat{Q}\right)^2 + B^2 \left( W - \hat{W}\right)^2 + 2AB \left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right] =\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ = A^2 \frac{1}{n-1} \sum{\left( Q - \hat{Q}\right)^2} + B^2 \frac{1}{n-1}\sum \left( W - \hat{W}\right)^2 + 2AB \frac{1}{n-1}\sum{\left[\left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right]}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{var}(Z) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{Q,W}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some everyday data tasks: a few hints with R</title>
      <link>https://www.statforbiology.com/2019/r_shapingdata/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/r_shapingdata/</guid>
      <description>


&lt;p&gt;We all work with data frames and it is important that we know how we can reshape them, as necessary to meet our needs. I think that there are, at least, four routine tasks that we need to be able to accomplish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;subsetting&lt;/li&gt;
&lt;li&gt;sorting&lt;/li&gt;
&lt;li&gt;casting&lt;/li&gt;
&lt;li&gt;melting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Obviously, there is a wide array of possibilities; I’ll just mention a few, which I regularly use.&lt;/p&gt;
&lt;div id=&#34;subsetting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subsetting the data&lt;/h1&gt;
&lt;p&gt;Subsetting means selecting the records (rows) or the variables (columns) which satisfy certain criteria. Let’s take the ‘students.csv’ dataset, which is available on one of my repositories. It is a database of student’s marks in a series of exams for different subjects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;students &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/students.csv&amp;quot;, header=T)
head(students)
##   Id  Subject       Date Mark Year  HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 2  2 AGRONOMY 08/07/2002   24 2001 AGRICULTURE
## 3  3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 4  4 AGRONOMY 24/06/2002   26 2001  HUMANITIES
## 5  5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6  6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say that we want a new dataset, which contains only the students with marks higher than 28.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;gt;= 28)
head(subData)
##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 3   3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 5   5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6   6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE
## 11 11 AGRONOMY 09/09/2002   28 2001  SCIENTIFIC
## 17 17 AGRONOMY 10/06/2002   30 2001  HUMANITIES&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s make it more difficult and extract the records were mark is ranging from 26 to 28 (margins included. Look at the AND clause):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26)
head(subData)
##    Id  Subject       Date Mark Year  HighSchool
## 4   4 AGRONOMY 24/06/2002   26 2001  HUMANITIES
## 6   6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE
## 7   7 AGRONOMY 24/02/2003   26 2001  HUMANITIES
## 8   8 AGRONOMY 09/09/2002   26 2001  SCIENTIFIC
## 10 10 AGRONOMY 08/07/2002   27 2001  HUMANITIES
## 11 11 AGRONOMY 09/09/2002   28 2001  SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are interested in those students who got a mark ranging from 26 to 28 in MATHS (please note the equality relationship written as ‘==’):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot;)
head(subData)
##      Id Subject       Date Mark Year  HighSchool
## 115 115   MATHS 15/07/2002   26 2001 AGRICULTURE
## 124 124   MATHS 16/09/2002   26 2001  SCIENTIFIC
## 138 138   MATHS 04/02/2002   27 2001  HUMANITIES
## 144 144   MATHS 10/02/2003   27 2001  HUMANITIES
## 145 145   MATHS 04/07/2003   27 2002  HUMANITIES
## 146 146   MATHS 28/02/2002   28 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets’ look for good students either in MATHS or in CHEMISTRY (OR clause; note the ‘|’ operator):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;)
head(subData)
##    Id   Subject       Date Mark Year   HighSchool
## 64 64 CHEMISTRY 18/06/2003   20 2002  AGRICULTURE
## 65 65 CHEMISTRY 06/06/2002   21 2001   HUMANITIES
## 66 66 CHEMISTRY 20/02/2003   21 2002   HUMANITIES
## 67 67 CHEMISTRY 20/02/2003   18 2002  AGRICULTURE
## 68 68 CHEMISTRY 04/06/2002   28 2001 OTHER SCHOOL
## 69 69 CHEMISTRY 26/06/2002   23 2001   ACCOUNTING&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also select columns; for example we may want to display only the ‘Subject’, ‘Mark’ and ‘HighSchool’ columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;,
                  select = c(Subject, Mark, HighSchool))
head(subData)
##      Subject Mark   HighSchool
## 64 CHEMISTRY   20  AGRICULTURE
## 65 CHEMISTRY   21   HUMANITIES
## 66 CHEMISTRY   21   HUMANITIES
## 67 CHEMISTRY   18  AGRICULTURE
## 68 CHEMISTRY   28 OTHER SCHOOL
## 69 CHEMISTRY   23   ACCOUNTING&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can as well drop the unwanted columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;,
                  select = c(-Id, 
                             -Date,
                             -Year))
head(subData)
##      Subject Mark   HighSchool
## 64 CHEMISTRY   20  AGRICULTURE
## 65 CHEMISTRY   21   HUMANITIES
## 66 CHEMISTRY   21   HUMANITIES
## 67 CHEMISTRY   18  AGRICULTURE
## 68 CHEMISTRY   28 OTHER SCHOOL
## 69 CHEMISTRY   23   ACCOUNTING&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the function ‘subset()’ is very easy. However, we might have higher flexibility if we subset by using indices. We already know that the notation ‘dataframe[i,j]’ returns the element in the i-th row and j-th column in a data frame. We can of course replace i and j with some subsetting rules. For example, taking the exams where the mark is comprised between 25 and 29 is done as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students[(students$Mark &amp;lt;= 29 &amp;amp; students$Mark &amp;gt;=25),]
head(subData)
##    Id  Subject       Date Mark Year  HighSchool
## 4   4 AGRONOMY 24/06/2002   26 2001  HUMANITIES
## 6   6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE
## 7   7 AGRONOMY 24/02/2003   26 2001  HUMANITIES
## 8   8 AGRONOMY 09/09/2002   26 2001  SCIENTIFIC
## 10 10 AGRONOMY 08/07/2002   27 2001  HUMANITIES
## 11 11 AGRONOMY 09/09/2002   28 2001  SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful to quickly edit the data. For example, if we want to replace all marks from 25 to 29 with NAs (Not Available), we can simply do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students
subData[(subData$Mark &amp;lt;= 29 &amp;amp; subData$Mark &amp;gt;=25), &amp;quot;Mark&amp;quot;] &amp;lt;- NA
head(subData)
##   Id  Subject       Date Mark Year  HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 2  2 AGRONOMY 08/07/2002   24 2001 AGRICULTURE
## 3  3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 4  4 AGRONOMY 24/06/2002   NA 2001  HUMANITIES
## 5  5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6  6 AGRONOMY 09/09/2002   NA 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that I created a new dataset to make the replacement, in order not to modify the original dataset. Of course, I can use the ‘is.na()’ function to find the missing data and edit them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData[is.na(subData$Mark) == T, &amp;quot;Mark&amp;quot;] &amp;lt;- 0 
head(subData)
##   Id  Subject       Date Mark Year  HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 2  2 AGRONOMY 08/07/2002   24 2001 AGRICULTURE
## 3  3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 4  4 AGRONOMY 24/06/2002    0 2001  HUMANITIES
## 5  5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6  6 AGRONOMY 09/09/2002    0 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sorting the data&lt;/h1&gt;
&lt;p&gt;Sorting is very much like subsetting by indexing. I just need to use the ‘order’ function. For example, let’s sort the students dataset by mark:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(students$Mark), ]
head(sortedData)
##    Id   Subject       Date Mark Year   HighSchool
## 51 51   BIOLOGY 01/03/2002   18 2001   HUMANITIES
## 67 67 CHEMISTRY 20/02/2003   18 2002  AGRICULTURE
## 76 76 CHEMISTRY 24/02/2003   18 2002 OTHER SCHOOL
## 79 79 CHEMISTRY 18/06/2003   18 2002  AGRICULTURE
## 82 82 CHEMISTRY 18/07/2002   18 2001  AGRICULTURE
## 83 83 CHEMISTRY 23/01/2003   18 2001   SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also sort by decreasing order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark), ]
head(sortedData)
##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 3   3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 5   5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 17 17 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 18 18 AGRONOMY 10/06/2002   30 2001 AGRICULTURE
## 19 19 AGRONOMY 09/09/2002   30 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obviously use multiple keys. For example, let’s sort by subject within marks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark, students$Subject), ]
head(sortedData)
##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 3   3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 5   5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 17 17 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 18 18 AGRONOMY 10/06/2002   30 2001 AGRICULTURE
## 19 19 AGRONOMY 09/09/2002   30 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I want to sort in decreasing order on a character variable (such as Subject), I need to use the helper function ‘xtfrm()’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark, -xtfrm(students$Subject)), ]
head(sortedData)
##      Id Subject       Date Mark Year   HighSchool
## 116 116   MATHS 01/07/2002   30 2001 OTHER SCHOOL
## 117 117   MATHS 18/06/2002   30 2001   ACCOUNTING
## 118 118   MATHS 09/07/2002   30 2001  AGRICULTURE
## 121 121   MATHS 18/06/2002   30 2001   ACCOUNTING
## 123 123   MATHS 09/07/2002   30 2001   HUMANITIES
## 130 130   MATHS 07/02/2002   30 2001   SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;casting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Casting the data&lt;/h1&gt;
&lt;p&gt;When we have a dataset in the LONG format, we might be interested in reshaping it in the WIDE format. This is the same as what the ‘pivot table’ function in Excel does. For example, take the ‘rimsulfuron.csv’ dataset in my repository. This contains the results of a randomised block experiment, where we have 16 herbicides in four blocks. The dataset is in the LONG format, with one row per plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rimsulfuron &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/rimsulfuron.csv&amp;quot;, header=T)
head(rimsulfuron)
##                      Herbicide Plot Code Block Box WeedCover Yield
## 1             Rimsulfuron (40)    1    1     1   1      27.8 85.91
## 2             Rimsulfuron (45)    2    2     1   1      27.8 93.03
## 3             Rimsulfuron (50)    3    3     1   1      23.0 86.93
## 4             Rimsulfuron (60)    4    4     1   1      42.8 52.99
## 5    Rimsulfuron (50+30 split)    5    5     1   2      15.1 71.36
## 6 Rimsulfuron + thyfensulfuron    6    6     1   2      22.9 75.28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets’put this data frame in the WIDE format, with one row per herbicide and one column per block. To do so, I usually use the ‘cast()’ function in the ‘reshape’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape)
castData &amp;lt;- cast(Herbicide ~ Block, data = rimsulfuron,
     value = &amp;quot;Yield&amp;quot;)
head(castData)
##                                    Herbicide     1     2      3      4
## 1                  Alachlor + terbuthylazine 12.06 49.58  41.34  16.37
## 2                                Hand-Weeded 77.58 92.08  86.59  99.63
## 3         Metolachlor + terbuthylazine (pre) 51.77 52.10  49.46  34.67
## 4 Pendimethalin (post) + rimsuulfuron (post) 94.82 87.72 102.05 101.94
## 5   Pendimethalin (pre) + rimsulfuron (post) 65.51 88.72  95.52  82.39
## 6                           Rimsulfuron (40) 85.91 91.09 111.42  93.15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other similar functions are available within the ‘reshape2’ and ‘tidyr’ packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;melting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Melting the data&lt;/h1&gt;
&lt;p&gt;In this case we do the reverse: we transform the dataset from WIDE to LONG format. For this task, I like the ‘melt()’ function in the ‘reshape2’ package, which requires a data frame as input. I would like to use the ‘castData’ object which we have just created by using the ‘cast()’ function above. Unfortunately, this object has a ‘cast_df’ class. Therefore, in order to avoid weird results, I need to change ‘castData’ into a data frame, by using the ‘as.data.frame()’ function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
castData &amp;lt;- as.data.frame(castData)
mdati &amp;lt;- melt(castData,
              variable.name = &amp;quot;Block&amp;quot;,
              value.name = &amp;quot;Yield&amp;quot;,
              id=c(&amp;quot;Herbicide&amp;quot;))

head(mdati)
##                                    Herbicide Block Yield
## 1                  Alachlor + terbuthylazine     1 12.06
## 2                                Hand-Weeded     1 77.58
## 3         Metolachlor + terbuthylazine (pre)     1 51.77
## 4 Pendimethalin (post) + rimsuulfuron (post)     1 94.82
## 5   Pendimethalin (pre) + rimsulfuron (post)     1 65.51
## 6                           Rimsulfuron (40)     1 85.91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have a nice work with these functions!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Drowning in a glass of water: variance-covariance and correlation matrices</title>
      <link>https://www.statforbiology.com/2019/stat_general_correlationcovariance/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_correlationcovariance/</guid>
      <description>


&lt;p&gt;One of the easiest tasks in R is to get correlations between each pair of variables in a dataset. As an example, let’s take the first four columns in the ‘mtcars’ dataset, that is available within R. Getting the variances-covariances and the correlations is straightforward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
matr &amp;lt;- mtcars[,1:4]

#Covariances
cov(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlations
cor(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s really a piece of cake! Unfortunately, a few days ago I had a covariance matrix without the original dataset and I wanted the corresponding correlation matrix. Although this is an easy task as well, at first I was stuck, because I could not find an immediate solution… So I started wondering how I could make it.&lt;/p&gt;
&lt;p&gt;Indeed, having the two variables X and Y, their covariance is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cov(X, Y) = \sum\limits_{i=1}^{n} {(X_i - \hat{X})(Y_i - \hat{Y})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}\)&lt;/span&gt; are the means for each variable. The correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cor(X, Y) = \frac{cov(X, Y)}{\sigma_x \sigma_y} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y\)&lt;/span&gt; are the standard deviations for X and Y.&lt;/p&gt;
&lt;p&gt;The opposite relationship is clear:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ cov(X, Y) = cor(X, Y) \sigma_x \sigma_y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, converting from covariance to correlation is pretty easy. For example, take the covariance between ‘cyl’ and ‘mpg’ above (-9.172379), the correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-633.097208 / (sqrt(36.324103) * sqrt(15360.7998))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8475514&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the reverse, if we have the correlation (-0.8521620), the covariance is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.8475514 * sqrt(36.324103) * sqrt(15360.7998)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -633.0972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My covariance matrix was pretty large, so I started wondering how I could perform this task altogether. What I had to do was to take each element in the covariance matrix and divide it by the square root of the diagonal elements in the same column and in the same row (see below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://lu4yxa.db.files.1drv.com/y4mZ-7ZQc0LCMyDG3kqC_0_bzMZYyEpb37ug_I616tXoPNL_DbILLSOa8HujEZCekvRNeeYsfwtrYP-0T_PfzlOUqUNliHdKU3sDLHwBnr5C4jF-U-u1QkOlWg3ZbQXKJw4TM2VrQIQqjh-Pb-5cOEY49q-3pfnt4ZYJUAYZIBhW4GgJ0svrEEAnKQZfNTs2LW5iZhGyYFYVKFT2Y1O7SjKjA?width=637&amp;amp;height=156&amp;amp;cropmode=none&#34; style=&#34;width:95.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This is easily done by matrix multiplication. I need a square matrix where the standard deviations for each variable are repeated along the rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V &amp;lt;- cov(matr)
SM1 &amp;lt;- matrix(rep(sqrt(diag(V)), 4), 4, 4)
SM1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]
## [1,]   6.026948   6.026948   6.026948   6.026948
## [2,]   1.785922   1.785922   1.785922   1.785922
## [3,] 123.938694 123.938694 123.938694 123.938694
## [4,]  68.562868  68.562868  68.562868  68.562868&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and another one where they are repeated along the columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SM2 &amp;lt;- matrix(rep(sqrt(diag(V)), each = 4), 4, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can take my covariance matrix (V) and simply multiply these three matrices as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V * 1/SM1 * 1/SM2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, there is not even the need to use ‘rep’ when we create SM1, as R will recycle the elements as needed.&lt;/p&gt;
&lt;p&gt;Going from correlation to covariance can be done similarly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- cor(matr)
R / (1/SM1 * 1/SM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an easy task, but it got me stuck for a few minutes…&lt;/p&gt;
&lt;p&gt;Lately, I finally discovered that there is (at least) one function in R taking care of the above task; it is the ‘cov2cor()’ function in the ‘nlme’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
cov2cor(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is really easy to get drown in a glass of water!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>https://www.statforbiology.com/2019/stat_general_correlation_alookat/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2019/stat_general_correlation_alookat/</guid>
      <description>


&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-for-the-pearson-correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions for the Pearson Correlation Coefficient&lt;/h1&gt;
&lt;p&gt;The Pearson correlation coefficients makes a few assumptions, which should be carefully checked.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interval-level measurement. Both variables should be measured on a quantitative scale.&lt;/li&gt;
&lt;li&gt;Random sampling. Each subject in the sample should contribute one value on X, and one value on Y. The values for both variables should represent a random sample drawn from the population of interest.&lt;/li&gt;
&lt;li&gt;Linearity. The relationship between X and Y should be linear.&lt;/li&gt;
&lt;li&gt;Bivarlate normal distribution. This means that (i) values of X should form a normal distribution at each value of Y and (ii) values of Y should form a normal distribution at each value of X.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;It is possible to test whether &lt;span class=&#34;math inline&#34;&gt;\(r = 0\)&lt;/span&gt; against the alternative $ r 0$. The test is based on the idea that the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}\]&lt;/span&gt;
is distributed as a Student’s t variable.&lt;/p&gt;
&lt;p&gt;Let’s take the two variables ‘cyl’ and ‘mpg’ from the ‘mtcars’ data frame. The correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- cor(mtcars$cyl, mtcars$gear)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.4926866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The T statistic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- r * sqrt(32 - 2) / sqrt(1 - r^2)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.101051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value for the null is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * pt(T, 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004173297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is clearly highly significant. The null can be rejected.&lt;/p&gt;
&lt;p&gt;As for hypothesis testing, it should be considered that the individuals where couple of measurements were taken should be independent. If they are not, the t test is invalid. I am dealing with this aspect somewhere else in my blog.&lt;/p&gt;
&lt;p&gt;#Correlation in R&lt;/p&gt;
&lt;p&gt;We have already seen that we can use the usual function ‘cor(matrix, method=)’. In order to obtain the significance, we can use the ‘rcorr()’ function in the Hmisc package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlations with significance levels
library(Hmisc)
corr2 &amp;lt;- rcorr(as.matrix(mtcars), type=&amp;quot;pearson&amp;quot;)
print(corr2$r, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg   cyl  disp    hp   drat    wt   qsec    vs     am  gear   carb
## mpg   1.00 -0.85 -0.85 -0.78  0.681 -0.87  0.419  0.66  0.600  0.48 -0.551
## cyl  -0.85  1.00  0.90  0.83 -0.700  0.78 -0.591 -0.81 -0.523 -0.49  0.527
## disp -0.85  0.90  1.00  0.79 -0.710  0.89 -0.434 -0.71 -0.591 -0.56  0.395
## hp   -0.78  0.83  0.79  1.00 -0.449  0.66 -0.708 -0.72 -0.243 -0.13  0.750
## drat  0.68 -0.70 -0.71 -0.45  1.000 -0.71  0.091  0.44  0.713  0.70 -0.091
## wt   -0.87  0.78  0.89  0.66 -0.712  1.00 -0.175 -0.55 -0.692 -0.58  0.428
## qsec  0.42 -0.59 -0.43 -0.71  0.091 -0.17  1.000  0.74 -0.230 -0.21 -0.656
## vs    0.66 -0.81 -0.71 -0.72  0.440 -0.55  0.745  1.00  0.168  0.21 -0.570
## am    0.60 -0.52 -0.59 -0.24  0.713 -0.69 -0.230  0.17  1.000  0.79  0.058
## gear  0.48 -0.49 -0.56 -0.13  0.700 -0.58 -0.213  0.21  0.794  1.00  0.274
## carb -0.55  0.53  0.39  0.75 -0.091  0.43 -0.656 -0.57  0.058  0.27  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(corr2$P, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mpg     cyl    disp      hp    drat      wt    qsec      vs
## mpg       NA 6.1e-10 9.4e-10 1.8e-07 1.8e-05 1.3e-10 1.7e-02 3.4e-05
## cyl  6.1e-10      NA 1.8e-12 3.5e-09 8.2e-06 1.2e-07 3.7e-04 1.8e-08
## disp 9.4e-10 1.8e-12      NA 7.1e-08 5.3e-06 1.2e-11 1.3e-02 5.2e-06
## hp   1.8e-07 3.5e-09 7.1e-08      NA 1.0e-02 4.1e-05 5.8e-06 2.9e-06
## drat 1.8e-05 8.2e-06 5.3e-06 1.0e-02      NA 4.8e-06 6.2e-01 1.2e-02
## wt   1.3e-10 1.2e-07 1.2e-11 4.1e-05 4.8e-06      NA 3.4e-01 9.8e-04
## qsec 1.7e-02 3.7e-04 1.3e-02 5.8e-06 6.2e-01 3.4e-01      NA 1.0e-06
## vs   3.4e-05 1.8e-08 5.2e-06 2.9e-06 1.2e-02 9.8e-04 1.0e-06      NA
## am   2.9e-04 2.2e-03 3.7e-04 1.8e-01 4.7e-06 1.1e-05 2.1e-01 3.6e-01
## gear 5.4e-03 4.2e-03 9.6e-04 4.9e-01 8.4e-06 4.6e-04 2.4e-01 2.6e-01
## carb 1.1e-03 1.9e-03 2.5e-02 7.8e-07 6.2e-01 1.5e-02 4.5e-05 6.7e-04
##           am    gear    carb
## mpg  2.9e-04 5.4e-03 1.1e-03
## cyl  2.2e-03 4.2e-03 1.9e-03
## disp 3.7e-04 9.6e-04 2.5e-02
## hp   1.8e-01 4.9e-01 7.8e-07
## drat 4.7e-06 8.4e-06 6.2e-01
## wt   1.1e-05 4.6e-04 1.5e-02
## qsec 2.1e-01 2.4e-01 4.5e-05
## vs   3.6e-01 2.6e-01 6.7e-04
## am        NA 5.8e-08 7.5e-01
## gear 5.8e-08      NA 1.3e-01
## carb 7.5e-01 1.3e-01      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also use these functions with two matrices, to obtain the correlations of each column in one matrix with each column in the other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation matrix from mtcars
x &amp;lt;- mtcars[1:3]
y &amp;lt;- mtcars[4:6]
cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              hp       drat         wt
## mpg  -0.7761684  0.6811719 -0.8676594
## cyl   0.8324475 -0.6999381  0.7824958
## disp  0.7909486 -0.7102139  0.8879799&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Relationship to slope in linear regression&lt;/p&gt;
&lt;p&gt;The correlation coefficient and slope in linear regression bear some similarities, as both describe how Y changes when X is changed. However, in correlation, we have two random variables, while in regression we have Y random, X fixed and Y is regarded as a function of X (not the other way round).&lt;/p&gt;
&lt;p&gt;Without neglecting their different meaning, it may be useful to show the algebraic relationship between the correlation coefficient and the slope in regression. Let’s simulate a dataset with two variables, coming from a multivariate normal distribution, with means respectively equal to 10 and 2, and variance-covariance matrix of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
cov &amp;lt;- matrix(c(2.20, 0.48, 0.48, 0.20), 2, 2)
cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] 2.20 0.48
## [2,] 0.48 0.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the ‘mvrnomr()’ function to generate the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
dataset &amp;lt;- data.frame( mvrnorm(n=10, mu = c(10, 2), Sigma = cov) )
names(dataset) &amp;lt;- c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;)
dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X        Y
## 1  11.756647 2.547203
## 2   9.522180 2.199740
## 3   8.341254 1.862362
## 4  13.480005 2.772031
## 5   9.428296 1.573435
## 6   9.242788 1.861756
## 7  10.817449 2.343918
## 8  10.749047 2.451999
## 9  10.780400 2.436263
## 10 11.480301 1.590436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficient and slope are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- with(dataset, cor(X, Y))
b1 &amp;lt;- coef( lm(Y ~ X, data=dataset) )[2]
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equation for the slope is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 = \frac{ \sum_{i = 1}^n \left[ ( X-\mu_X )( Y-\mu_Y )\right] }{ \sigma^2_X } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From there, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r = b_1 \frac{\sigma_X}{ \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ b_1 = r \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- with(dataset, sd(X) )
sigmaY &amp;lt;- with(dataset, sd(Y) )
b1 * sigmaX / sigmaY &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r * sigmaY / sigmaX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also easy to see that the correlation coefficient is the slope of regression of standardised Y against standardised X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yst &amp;lt;- with(dataset, scale(Y, scale=T) )
summary( lm(Yst ~ I(scale(X, scale = T) ), data = dataset) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Yst ~ I(scale(X, scale = T)), data = dataset)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082006 -0.067143 -0.036850  0.009214  0.237923 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)            -5.633e-18  3.478e-02   0.000   1.0000  
## I(scale(X, scale = T))  1.785e-01  7.633e-02   2.339   0.0475 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.11 on 8 degrees of freedom
## Multiple R-squared:  0.4061, Adjusted R-squared:  0.3319 
## F-statistic: 5.471 on 1 and 8 DF,  p-value: 0.04748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Intra-class correlation (ICC)&lt;/p&gt;
&lt;p&gt;It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intra-class correlation coefficient is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IC = {\displaystyle {\frac {\sigma _{\alpha }^{2}}{\sigma _{\alpha }^{2}+\sigma _{\varepsilon }^{2}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\alpha }^{2}\)&lt;/span&gt; is the variance between groups and &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\varepsilon }^{2}\)&lt;/span&gt; is the variance within a group (better, the variance of one observation within a group). The sum of those two variances is the total variance of observations. In words, the intra-class correlation coefficient measures the joint variability of subjects in the same group (that relates on how groups are different from one another), with respect to the total variability of observations. If subjects in one group are very similar to one another (small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\varepsilon}\)&lt;/span&gt;) but groups are very different (high &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha}\)&lt;/span&gt;), the ICC is very high.&lt;/p&gt;
&lt;p&gt;The existence of grouping of residuals is very important in ANOVA, as it means that independence is violated, which calls for the use of mixed models.&lt;/p&gt;
&lt;p&gt;But … this is a totally different story …&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My first experience with blogdown</title>
      <link>https://www.statforbiology.com/2018/2018-11_15-first-day-with-blogdown/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2018/2018-11_15-first-day-with-blogdown/</guid>
      <description>&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;
&lt;p&gt;I thought that it might be useful to write down a few notes, to summarise my steps ahead, during the learning process. I do not work with blogdown everyday and I tend to forget things quite easily. Therefore, these notes may help me recap how far I have come. And they might also help other beginners, to speed up their initial steps with such a powerful blogging platform.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll find my notes &lt;a href=&#34;https://www.statforbiology.com/articles/blogdownSteps/&#34;&gt;here&lt;/a&gt;; I&amp;rsquo;ll try to keep them updated.&lt;/p&gt;
&lt;p&gt;Happy reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is R dangerous? Side effects of free software for biologists</title>
      <link>https://www.statforbiology.com/2014/2014-06-08-the-danger-of-r/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://www.statforbiology.com/2014/2014-06-08-the-danger-of-r/</guid>
      <description>


&lt;p&gt;When I started my career in the biological field (it’s already 25 years ago), only the luckiest of us had access to very advanced statistical software. Licenses were very expensive and it was not easy to convince the boss that they were really necessary: “Why do you need to spend so much money to perform an ANOVA?”. Indeed, simple one-way or two-ways ANOVAs were quite easy to perform and one of the people in my group had already built the appropriate routines for several designs, by using the GW-BASIC language. But I wanted more!&lt;/p&gt;
&lt;p&gt;My agriculture experiments often showed complex designs and split-plot, strip-plot, subsampling and repeated measures/experiments were much more than an exception. I decided to start writing several Quick-BASIC routines to implement those types of ANOVAs on my PC. At the beginning of the ’90s, nonlinear response models became in fashion and I had to programme my first Gauss-Newton optimiser, also with Quick-BASIC. GLMs were not yet widespread among biologists and I mainly relied on stabilising transformations for those cases where the basic assumptions for linear/nonlinear models were not met.&lt;/p&gt;
&lt;p&gt;This ‘poor and humble’ statistical life gave me an undeniable advantage: it forced me into thoroughly studying and understanding the principles of each new technique and algorithm, before I could be able to programme it. I had to become acquainted with all those strange mathematical objects, such as matrices, eigenvalues and determinants, which are not usually a part of the mathematical background of biologists (at least in Italy). And my BASIC routines, once completed, could only do what they were programmed to do; only one specific solution to one specific task, no further options and no error management. In one sentence: no general solutions.&lt;/p&gt;
&lt;p&gt;Nowadays I have R: it is free and everything is possible and smooth. A few lines of code and I can fit whatever model comes to my mind in a few minutes. I can try several options: which is the best one? Which is the one that makes my data tell the story I would like to tell? Biometry books have changed as well; they have taken a more ‘algorithmic’ approach and math is confined within boxes that may be easily skipped. I have to admit that I frequently skip them: code snippets are more than enough to do the trick and I can also find thousands of them in the Internet. In other words, why should I bother studying such an abstract thing called statistics when I have R?&lt;/p&gt;
&lt;p&gt;Obviously this is just an exaggeration. However, I have the feeling that there might be some drawbacks relating to the availability of such a powerful free software. Biologists (especially students) may mistake studying R for studying stats. I am very much surprised to see how many complex models are fit on these days, with hardly any biological and statistical justifications and with very little care about the basic assumptions that these models make. A few days ago a PhD student at my Department showed me the results of fitting a reduced rank regression to a biological dataset. He was very proud of how he mastered the R coding process: by using the correct option (found after a thorough search over the Internet). He had even managed to avoid a ‘pretty strange’ warning message. Unfortunately, that warning message had been misinterpreted and therefore the analysis was wrong from the very beginning.&lt;/p&gt;
&lt;p&gt;A warning message for all biologists (including myself): R is really wonderful, but it will not necessarily bring to sound data analyses. Let’s use R, but let’s study stats first!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>