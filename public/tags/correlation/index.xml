<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Correlation on The broken bridge between biologists and statisticians</title>
    <link>/tags/correlation/</link>
    <description>Recent content in Correlation on The broken bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2018, @AndreaOnofri</copyright>
    <lastBuildDate>Fri, 10 May 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/correlation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationindependence2/</guid>
      <description>


&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;
&lt;div id=&#34;a-routine-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A ‘routine’ experiment&lt;/h1&gt;
&lt;p&gt;I will use the same example as presented &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in my previous post&lt;/a&gt;, which should allow us to compare the results with those obtained by using more traditional methods of data analyses. It is a genotype experiment, laid out in randomised complete blocks, with 27 wheat genotypes and three replicates. For each plot, the collegue who made the experiment recorded several traits, including yield (Yield) and weight of thousand kernels (TKW). The dataset ‘WheatQuality.csv’ is available on ‘gitHub’; it consists of 81 records (plots) and just as many couples of measures in all. The code below loads the necessary packages, the data and transforms the numeric variable ‘Block’ into a factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(plyr)
library(nlme)
library(plyr)
library(asreml)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;In my previous post&lt;/a&gt;, I used the above dataset to calculate the Pearson’s correlation coefficient between Yield and TKW for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;plot measurements,&lt;/li&gt;
&lt;li&gt;residuals,&lt;/li&gt;
&lt;li&gt;treatment/block means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Piepho (2018) showed that, for an experiment like this one, the above correlations can be estimated by coding a multiresponse mixed model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijk} = \mu_i + \beta_{ik} + \tau_{ij} + \epsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ijk}\)&lt;/span&gt; is the response for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the rootstock &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the mean for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{ik}\)&lt;/span&gt; is the effect of the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ij}\)&lt;/span&gt; is the effect of genotype &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; is the residual for each of 81 observations for two traits.&lt;/p&gt;
&lt;p&gt;In the above model, the residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; need to be normally distributed and heteroscedastic, with trait-specific variances. Furthermore, residuals belonging to the same plot (the two observed traits) need to be correlated (correlation of errors).&lt;/p&gt;
&lt;p&gt;Hans-Peter Piepho, in his paper, put forward the idea that the ‘genotype’ and ‘block’ effects for the two traits can be taken as random, even though they might be of fixed nature, especially the genotype effect. This idea makes sense, because, for this application, we are mainly interested in variances and covariances. Both random effects need to be heteroscedastic (trait-specific variance components) and there must be a correlation between the two traits.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge, there is no way to fit such a complex model with the ‘nlme’ package and related ‘lme()’ function (I’ll gave a hint later on, for a simpler model). Therefore, I decided to use the package ‘asreml’ (Butler et al., 2018), although this is not freeware. With the function ‘asreml()’, we need to specify the following components.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The response variables. When we set a bivariate model with ‘asreml’, we can ‘cbind()’ Yield and TKW and use the name ‘trait’ to refer to them.&lt;/li&gt;
&lt;li&gt;The fixed model, that only contains the trait effect. The specification is, therefore, ‘cbind(Yield, TKW) ~ trait - 1’. Following Piepho (2018), I removed the intercept, to separately estimate the means for the two traits.&lt;/li&gt;
&lt;li&gt;The random model, that is composed by the interactions ‘genotype x trait’ and ‘block x trait’. For both, I specified a general unstructured variance covariance matrix, so that the traits are heteroscedastic and correlated. Therefore, the random model is ~ Genotype:us(trait) + Block:us(trait).&lt;/li&gt;
&lt;li&gt;The residual structure, where the observations in the same plot (the term ‘units’ is used in ‘asreml’ to represent the observational units, i.e. the plots) are heteroscedastic and correlated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait, init = c(3.7, -0.25, 1.7)) + 
          Block:us(trait, init = c(77, 38, 53)),
        residual = ~ units:us(trait, init = c(6, 0.16, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:27 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -641.556           1.0    160 17:33:27    0.0 (3 restrained)
##  2      -548.767           1.0    160 17:33:27    0.0
##  3      -448.970           1.0    160 17:33:27    0.0
##  4      -376.952           1.0    160 17:33:27    0.0
##  5      -334.100           1.0    160 17:33:27    0.0
##  6      -317.511           1.0    160 17:33:27    0.0
##  7      -312.242           1.0    160 17:33:27    0.0
##  8      -311.145           1.0    160 17:33:27    0.0
##  9      -311.057           1.0    160 17:33:27    0.0
## 10      -311.056           1.0    160 17:33:27    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   component  std.error    z.ratio
## Block:trait!trait_Yield:Yield     3.7104778  3.9364268  0.9426005
## Block:trait!trait_TKW:Yield      -0.2428390  1.9074544 -0.1273105
## Block:trait!trait_TKW:TKW         1.6684568  1.8343662  0.9095549
## Genotype:trait!trait_Yield:Yield 77.6346623 22.0956257  3.5135761
## Genotype:trait!trait_TKW:Yield   38.8322972 15.0909109  2.5732242
## Genotype:trait!trait_TKW:TKW     53.8616088 15.3520661  3.5084274
## units:trait!R                     1.0000000         NA         NA
## units:trait!trait_Yield:Yield     6.0939037  1.1951128  5.0990195
## units:trait!trait_TKW:Yield       0.1635551  0.7242690  0.2258209
## units:trait!trait_TKW:TKW         4.4717901  0.8769902  5.0990195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The box above shows the results about the variance-covariance parameters. In order to get the correlations, I specified the necessary combinations of variance-covariance parameters. It is necessary to remember that estimates, in ‘asreml’, are named as V1, V2, … Vn, according to their ordering in model output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parms &amp;lt;- mod.asreml$vparameters
vpredict(mod.asreml, rb ~ V2 / (sqrt(V1)*sqrt(V3) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate        SE
## rb -0.09759916 0.7571335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, rt ~ V5 / (sqrt(V4)*sqrt(V6) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate       SE
## rt 0.6005174 0.130663&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, re ~ V9 / (sqrt(V8)*sqrt(V10) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate        SE
## re 0.03133109 0.1385389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the estimates are very close to those obtained by using the Pearson’s correlation coefficients (see my previous post). The advantage of this mixed model solution is that we can also test hypotheses in a relatively reliable way. For example, I tested the hypothesis that residuals are not correlated by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;coding a reduced model where residuals are heteroscedastic and independent, and&lt;/li&gt;
&lt;li&gt;comparing this reduced model with the complete model by way of a REML-based Likelihood Ratio Test (LRT).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Removing the correlation of residuals is easily done, by changing the correlation structure from ‘us’ (unstructured variance-covariance matrix) to ‘idh’ (diagonal variance-covariance matrix).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:us(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:28 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.023           1.0    160 17:33:28    0.0 (2 restrained)
##  2      -383.859           1.0    160 17:33:28    0.0
##  3      -344.687           1.0    160 17:33:28    0.0
##  4      -321.489           1.0    160 17:33:28    0.0
##  5      -312.488           1.0    160 17:33:28    0.0
##  6      -311.167           1.0    160 17:33:28    0.0
##  7      -311.083           1.0    160 17:33:28    0.0
##  8      -311.082           1.0    160 17:33:28    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml2  1      0.05107    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, I tried to further reduce the model to test the significance of the correlation between block means and genotype means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:29 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.027           1.0    160 17:33:29    0.0 (2 restrained)
##  2      -383.866           1.0    160 17:33:29    0.0
##  3      -344.694           1.0    160 17:33:29    0.0
##  4      -321.497           1.0    160 17:33:29    0.0
##  5      -312.496           1.0    160 17:33:29    0.0
##  6      -311.175           1.0    160 17:33:29    0.0
##  7      -311.090           1.0    160 17:33:29    0.0
##  8      -311.090           1.0    160 17:33:29    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml3  2     0.066663    0.6399&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml4 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:idh(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:30 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -406.458           1.0    160 17:33:30    0.0 (2 restrained)
##  2      -394.578           1.0    160 17:33:30    0.0
##  3      -352.769           1.0    160 17:33:30    0.0
##  4      -327.804           1.0    160 17:33:30    0.0
##  5      -318.007           1.0    160 17:33:30    0.0
##  6      -316.616           1.0    160 17:33:30    0.0
##  7      -316.549           1.0    160 17:33:30    0.0
##  8      -316.549           1.0    160 17:33:30    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)   
## mod.asreml/mod.asreml4  3       10.986  0.003364 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that only the genotype means are significantly correlated.&lt;/p&gt;
&lt;p&gt;An alternative (and more useful) way to code the same model is by using the ‘corgh’ structure, instead of ‘us’. These two structures are totally similar, apart from the fact that the first one uses the correlations, instead of the covariances. Another difference, which we should consider when giving starting values, is that correlations come before variances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:31 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -632.445           1.0    160 17:33:31    0.0 (3 restrained)
##  2      -539.383           1.0    160 17:33:31    0.0 (2 restrained)
##  3      -468.810           1.0    160 17:33:31    0.0 (1 restrained)
##  4      -422.408           1.0    160 17:33:31    0.0
##  5      -371.304           1.0    160 17:33:31    0.0
##  6      -336.191           1.0    160 17:33:31    0.0
##  7      -317.547           1.0    160 17:33:31    0.0
##  8      -312.105           1.0    160 17:33:31    0.0
##  9      -311.118           1.0    160 17:33:31    0.0
## 10      -311.057           1.0    160 17:33:31    0.0
## 11      -311.056           1.0    160 17:33:31    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09759916  0.7571335
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60051740  0.1306748
## Genotype:trait!trait_Yield                77.63466334 22.0981962
## Genotype:trait!trait_TKW                  53.86160957 15.3536792
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.03133109  0.1385389
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of this parameterisation is that we can test our hypotheses by easily setting up contraints on correlations. One way to do this is to run the model with the argument ‘start.values = T’. In this way I could derive a data frame (‘mod.init$parameters’), with the starting values for REML maximisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the starting values
mod.init &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset, start.values = T)
init &amp;lt;- mod.init$vparameters
init&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                    Component Value Constraint
## 1  Genotype:trait!trait!TKW:!trait!Yield.cor -0.10          U
## 2                 Genotype:trait!trait_Yield  3.80          P
## 3                   Genotype:trait!trait_TKW  1.80          P
## 4     Block:trait!trait!TKW:!trait!Yield.cor  0.60          U
## 5                    Block:trait!trait_Yield 77.00          P
## 6                      Block:trait!trait_TKW 53.00          P
## 7                              units:trait!R  1.00          F
## 8     units:trait!trait!TKW:!trait!Yield.cor  0.03          U
## 9                    units:trait!trait_Yield  6.00          P
## 10                     units:trait!trait_TKW  4.50          P&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘init’ data frame has three columns: (i) names of parameters, (ii) initial values and (iii) type of constraint (U: unconstrained, P = positive, F = fixed). Now, if we take the seventh row (correlation of residuals), set the initial value to 0 and set the third column to ‘F’ (meaning: keep the initial value fixed), we are ready to fit a model without correlation of residuals (same as the ‘model.asreml2’ above). What I had to do was just to pass this data frame as the starting value matrix for a new model fit (see the argument ‘R.param’, below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init2 &amp;lt;- init
init2[8, 2] &amp;lt;- 0
init2[8, 3] &amp;lt;- &amp;quot;F&amp;quot;

mod.asreml.r2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:33 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1136.066           1.0    160 17:33:33    0.0 (1 restrained)
##  2      -939.365           1.0    160 17:33:33    0.0 (1 restrained)
##  3      -719.371           1.0    160 17:33:33    0.0 (1 restrained)
##  4      -550.513           1.0    160 17:33:33    0.0
##  5      -427.355           1.0    160 17:33:33    0.0
##  6      -353.105           1.0    160 17:33:33    0.0
##  7      -323.421           1.0    160 17:33:33    0.0
##  8      -313.616           1.0    160 17:33:33    0.0
##  9      -311.338           1.0    160 17:33:33    0.0
## 10      -311.087           1.0    160 17:33:33    0.0
## 11      -311.082           1.0    160 17:33:33    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r2)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09516456  0.7572040
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60136047  0.1305180
## Genotype:trait!trait_Yield                77.63463977 22.0809306
## Genotype:trait!trait_TKW                  53.86159936 15.3451466
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.00000000         NA
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r2, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)
## mod.asreml.r/mod.asreml.r2  1     0.051075    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is even more interesting is that ‘asreml’ permits to force the parameters to be linear combinations of one another. For instance, we can code a model where the residual correlation is contrained to be equal to the treatment correlation. To do so, we have to set up a two-column matrix (M), with row names matching the component names in the ‘asreml’ parameter vector. The matrix M should contain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the first column, the equality relationships (same number, same value)&lt;/li&gt;
&lt;li&gt;in the second column, the coefficients for the multiplicative relationships&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we would set the matrix M as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;firstCol &amp;lt;- c(1, 2, 3, 4, 5, 6, 7, 1, 8, 9)
secCol &amp;lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
M &amp;lt;- cbind(firstCol, secCol)
dimnames(M)[[1]] &amp;lt;- mod.init$vparameters$Component
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           firstCol secCol
## Genotype:trait!trait!TKW:!trait!Yield.cor        1      1
## Genotype:trait!trait_Yield                       2      1
## Genotype:trait!trait_TKW                         3      1
## Block:trait!trait!TKW:!trait!Yield.cor           4      1
## Block:trait!trait_Yield                          5      1
## Block:trait!trait_TKW                            6      1
## units:trait!R                                    7      1
## units:trait!trait!TKW:!trait!Yield.cor           1      1
## units:trait!trait_Yield                          8      1
## units:trait!trait_TKW                            9      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that in ‘firstCol’, the 1st and 8th element are both equal to 1, which contraints them to assume the same value. We can now pass the matrix M as the value of the ‘vcc’ argument in the model call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init, vcc = M)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:34 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1122.762           1.0    160 17:33:34    0.0 (1 restrained)
##  2      -900.308           1.0    160 17:33:34    0.0
##  3      -665.864           1.0    160 17:33:34    0.0
##  4      -492.020           1.0    160 17:33:34    0.0
##  5      -383.085           1.0    160 17:33:34    0.0
##  6      -336.519           1.0    160 17:33:34    0.0 (1 restrained)
##  7      -319.561           1.0    160 17:33:34    0.0
##  8      -315.115           1.0    160 17:33:34    0.0
##  9      -314.540           1.0    160 17:33:34    0.0
## 10      -314.523           1.0    160 17:33:34    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r3)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            component  std.error    z.ratio
## Block:trait!trait!TKW:!trait!Yield.cor    -0.1146908  0.7592678 -0.1510545
## Block:trait!trait_Yield                    3.6991785  3.9364622  0.9397216
## Block:trait!trait_TKW                      1.6601799  1.8344090  0.9050217
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.2336876  0.1117699  2.0907919
## Genotype:trait!trait_Yield                70.5970531 19.9293722  3.5423621
## Genotype:trait!trait_TKW                  48.9763800 13.8464106  3.5371174
## units:trait!R                              1.0000000         NA         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.2336876  0.1117699  2.0907919
## units:trait!trait_Yield                    6.3989855  1.2811965  4.9945387
## units:trait!trait_TKW                      4.6952670  0.9400807  4.9945358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r3, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)   
## mod.asreml.r/mod.asreml.r3  1       6.9336   0.00423 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the residual and treatment correlations are equal in this latter model. We also see that this reduced model fits significantly worse than the complete model ‘mod.asreml.r’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-freeware&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going freeware&lt;/h1&gt;
&lt;p&gt;Considering that the block means are not correlated, if we were willing to take the ‘block’ effect as fixed, we could fit the resulting model also with the ‘nlme’ package and the function ‘lme()’ (Pinheiro and Bates, 2000). However, we should cast the model as a univariate model.&lt;/p&gt;
&lt;p&gt;To this aim, the two variables (Yield and TKW) need to be piled up and a new factor (‘Trait’) needs to be introduced to identify the observations for the two traits. Another factor is also necessary to identify the different plots, i.e. the observational units. To perform such a restructuring, I used the ‘melt()’ function in the ‘reshape2’ package Wickham, 2007) and assigned the name ‘Y’ to the response variable, that is now composed by the two original variables Yield and TKW, one on top of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$Plot &amp;lt;- 1:81
mdataset &amp;lt;- melt(dataset[,c(-3,-6)], variable.name= &amp;quot;Trait&amp;quot;, value.name=&amp;quot;Y&amp;quot;, id=c(&amp;quot;Genotype&amp;quot;, &amp;quot;Block&amp;quot;, &amp;quot;Plot&amp;quot;))
mdataset$Block &amp;lt;- factor(mdataset$Block)
head(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait    Y
## 1 arcobaleno     1    1   TKW 44.5
## 2 arcobaleno     2    2   TKW 42.8
## 3 arcobaleno     3    3   TKW 42.7
## 4       baio     1    4   TKW 40.6
## 5       baio     2    5   TKW 42.7
## 6       baio     3    6   TKW 41.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait     Y
## 157  vesuvio     1   76 Yield 54.37
## 158  vesuvio     2   77 Yield 55.02
## 159  vesuvio     3   78 Yield 53.41
## 160 vitromax     1   79 Yield 54.39
## 161 vitromax     2   80 Yield 50.97
## 162 vitromax     3   81 Yield 48.83&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fixed model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y ~ Trait*Block&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to introduce a totally unstructured variance-covariance matrix for the random effect, I used the ‘pdMat’ construct:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;random = list(Genotype = pdSymm(~Trait - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relating to the residuals, heteroscedasticity can be included by using the ‘weight()’ argument and the ‘varIdent’ variance function, which allows a different standard deviation per trait:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weight = varIdent(form = ~1|Trait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also allowed the residuals to be correlated within each plot, by using the ‘correlation’ argument and specifying a general ‘corSymm()’ correlation form. Plots are nested within genotypes, thus I used a nesting operator, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;correlation = corSymm(form = ~1|Genotype/Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Y ~ Trait*Block, 
                 random = list(Genotype = pdSymm(~Trait - 1)),
                 weight = varIdent(form=~1|Trait), 
                 correlation = corCompSymm(form=~1|Genotype/Plot),
                 data = mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retreiving the variance-covariance matrices needs some effort, as the function ‘getVarCov()’ does not appear to work properly with this multistratum model. First of all, we can retreive the variance-covariance matrix for the genotype random effect (G) and the corresponding correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Variance structure for random effects
obj &amp;lt;- mod
G &amp;lt;- matrix( as.numeric(getVarCov(obj)), 2, 2 )
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]
## [1,] 53.86081 38.83246
## [2,] 38.83246 77.63485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.6005237
## [2,] 0.6005237 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we can retreive the ‘conditional’ variance-covariance matrix (R), that describes the correlation of errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conditional variance-covariance matrix (residual error)
V &amp;lt;- corMatrix(obj$modelStruct$corStruct)[[1]] #Correlation for residuals
sds &amp;lt;- 1/varWeights(obj$modelStruct$varStruct)[1:2]
sds &amp;lt;- obj$sigma * sds #Standard deviations for residuals (one per trait)
R &amp;lt;- t(V * sds) * sds #Going from correlation to covariance
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 4.4718234 0.1635375
## [2,] 0.1635375 6.0939251&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]
## [1,] 1.00000000 0.03132756
## [2,] 0.03132756 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total correlation matrix is simply obtained as the sum of G and R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tr &amp;lt;- G + R
cov2cor(Tr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.5579906
## [2,] 0.5579906 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are the same as those obtained by using ‘asreml’. Hope these snippets are useful!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Butler, D., Cullis, B.R., Gilmour, A., Gogel, B., Thomson, R., 2018. ASReml-r reference manual - version 4. UK.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-effects models in s and s-plus. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Wickham, H., 2007. Reshaping data with the reshape package. Journal of Statistical Software 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part I</title>
      <link>/2019/stat_general_correlationindependence1/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationindependence1/</guid>
      <description>


&lt;div id=&#34;observations-are-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observations are grouped&lt;/h1&gt;
&lt;p&gt;When we have recorded two traits in different subjects, we can be interested in describing their joint variability, by using the Pearson’s correlation coefficient. That’s ok, altough we have to respect some basic assumptions (e.g. linearity) that have been detailed elsewhere (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlation_alookat/&#34;&gt;see here&lt;/a&gt;). Problems may arise when we need to test the hypothesis that the correlation coefficient is equal to 0. In this case, we need to make sure that all the couples of observations are taken on independent subjects.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is most often false whenever measures are taken from designed field experiments. In this case, observations may be grouped by one or more treatment/blocking factors. This has been clearly described by Bland and Altman (1994); we would like to give an example that is more closely related to plant/crop science. Think about a genotype experiment, where we compare the behaviour of several genotypes in a randomised blocks design. Usually, we do not only measure yield. We also measure other traits, such as crop height. At the end of the experiment, we might be interested in reporting the correlation between yield and height. How should we proceed? It would seem an easy task, but it is not.&lt;/p&gt;
&lt;p&gt;Let’s assume that we have a randomised blocks design, with 27 genotypes and 3 replicates. For each plot, we recorded two traits, i.e. yield and the weight of thousand kernels (TKW). In the end, we have 81 plots and just as many couples of measures in all. We will use the dataset ‘WheatQuality.csv’, that is available on ‘gitHub’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Hmisc)
library(knitr)
library(plyr)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How many correlations?&lt;/h1&gt;
&lt;p&gt;It may be tempting to consider the whole lot of measures and calculate the correlation coefficient between yield and TKW. This is the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra &amp;lt;- with(dataset, rcorr(Yield, TKW) )
ra$r[1,2] #Correlation coefficient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.540957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra$P[1,2] #P-level&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.850931e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We observe a positive correlation, and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; seems to be significantly different from 0. Therefore, we would be encouraged to conclude that plots with a high value on yield tend to have a high value on TKW, as well. Unfortunately, such a conclusion is not supported by the data.&lt;/p&gt;
&lt;p&gt;Indeed, the test of significance is clearly invalid, as the 81 plots are not independent; they are grouped by block and genotype and we are totally neglecting these two effects. Are we sure that the same correlation holds for all genotypes/blocks? Let’s check this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor &amp;lt;- function(x) cor(x$Yield, x$TKW)
wgCors &amp;lt;- ddply(dataset, ~Genotype, wCor)
wgCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Genotype         V1
## 1   arcobaleno  0.9847228
## 2         baio  0.1611952
## 3      claudio -0.9993872
## 4     colorado  0.9837293
## 5     colosseo  0.4564855
## 6        creso -0.5910193
## 7       duilio -0.9882330
## 8       gianni -0.7603802
## 9       giotto  0.9520211
## 10      grazia  0.4980828
## 11       iride  0.7563338
## 12    meridano  0.1174342
## 13      neodur  0.4805871
## 14      orobel  0.8907754
## 15 pietrafitta -0.9633891
## 16  portobello  0.9210135
## 17   portofino -0.9900764
## 18   portorico  0.1394211
## 19       preco  0.9007067
## 20    quadrato -0.5840238
## 21    sancarlo -0.6460670
## 22      simeto -0.4051779
## 23       solex -0.6066363
## 24 terrabianca -0.4076416
## 25       verdi  0.5801404
## 26     vesuvio -0.7797493
## 27    vitromax -0.8056514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wbCors &amp;lt;- ddply(dataset, ~Block, wCor)
wbCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Block        V1
## 1     1 0.5998137
## 2     2 0.5399990
## 3     3 0.5370398&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for the genotypes, we have 27 correlation coefficients, ranging from -0.999 to 0.985. We only have three couples of measurements per genotype and it is clear that this is not much, to reliably estimate a correlation coefficient. However, it is enough to be suspicious about the extent of correlation between yield and TKW, as it may depend on the genotype.&lt;/p&gt;
&lt;p&gt;On the other hand, the correlation within blocks is more constant, independent on the block and similar to the correlation between plots.&lt;/p&gt;
&lt;p&gt;It may be interesting to get an estimate of the average within-group correlation. To this aim, we can perform two separate ANOVAs (one per trait), including all relevant effects (blocks and genotypes) and calculate the correlation between the residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Yield ~ factor(Block) + Genotype, data = dataset)
mod2 &amp;lt;- lm(TKW ~ factor(Block) + Genotype, data = dataset)
wCor &amp;lt;- rcorr(residuals(mod1), residuals(mod2))
wCor$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x          y
## x 1.00000000 0.03133109
## y 0.03133109 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x         y
## x        NA 0.7812693
## y 0.7812693        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average within-group correlation is very small and unsignificant. Let’s think about this correlation: residuals represent yield and TKW values for all plots, once the effects of blocks and genotypes have been removed. A high correlation of residuals would mean that, letting aside the effects of the block and genotype to which it belongs, a plot with a high value on yield also shows a high value on TKW. The existence of such correlation is clearly unsopported by our dataset.&lt;/p&gt;
&lt;p&gt;As the next step, we could consider the means for genotypes/blocks and see whether they are correlated. Blocks and genotypes are independent and, in principle, significance testing is permitted. However, this is not recommended with block means, as three data are too few to make tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Genotype, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rgPear &amp;lt;- rcorr( as.matrix(means[,2:3]) )
rgPear$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Mu1       Mu2
## Mu1 1.0000000 0.5855966
## Mu2 0.5855966 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgPear$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Mu1        Mu2
## Mu1         NA 0.00133149
## Mu2 0.00133149         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Block, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rbPear &amp;lt;- cor( as.matrix(means[,2:3]) )
rbPear&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Mu1         Mu2
## Mu1  1.00000000 -0.08812544
## Mu2 -0.08812544  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We note that the correlation between genotype means is high and significant. On the contrary, the correlation between block means is near to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-so-what&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And so what?&lt;/h1&gt;
&lt;p&gt;At this stage, you may be confused… Let’s try to clear the fog.&lt;/p&gt;
&lt;p&gt;Obtaining a reliable measure of correlation from designed experiments is not obvious. Indeed, in every designed field experiment we have groups of subjects and there are several possible types of correlation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;correlation between plot measurements&lt;/li&gt;
&lt;li&gt;correlation between the residuals&lt;/li&gt;
&lt;li&gt;correlation between treatment/block means&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All these correlations should be investigated and used for interpretation.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The ‘naive’ correlation between the plot measurements is very easily calculated, but it is grossy misleading. Indeed, it disregards the treatment/block effects and it does not permit hypotheses testing, as the subjects are not independent. In this example, looking at the ‘naive’ correlation coefficient, we would wrongly conclude that plots with high yield also have high TKW, while further analyses show that this is not true, in general. We would reasonably suggest that the ‘naive’ correlation coefficient is never used for interpretation.&lt;/li&gt;
&lt;li&gt;The correlation between the residuals is a reliable measure of joint variation, because the experimental design is adequately referenced, by removing the effects of tratments/blocks. In this example, residuals are not correlated. Further analyses show that the correlation between yield and TKW, if any, may depend on the genotype, while it does not depend on the block.&lt;/li&gt;
&lt;li&gt;The correlation between treatment/block means permits to assess whether the treatment/block effects on the two traits are correlated. In this case, while we are not allowed to conclude that yield and TKW are, in general, correlated, we can conclude that the genotypes with a high level of yield also show a high level of TKW.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Whenever we have data from designed field experiments, our correlation analyses should never be limited to the calculation of the ‘naive’ correlation coefficient between the observed values. This may not be meaningful! On the contrary, our interpretation should be mainly focused on the correlation between residuals and on the correlation between the effects of treatments/blocks.&lt;/p&gt;
&lt;p&gt;An elegant and advanced method to perform sound correlation analyses on data from designed field experiments has been put forward by Piepho (2018), within the frame of mixed models. Such an approach will be described in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bland, J.M., Altman, D.G., 1994. Statistics Notes: Correlation, regression, and repeated data. BMJ 308, 896–896.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Drowning in a glass of water: variance-covariance and correlation matrices</title>
      <link>/2019/stat_general_correlationcovariance/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationcovariance/</guid>
      <description>


&lt;p&gt;One of the easiest tasks in R is to get correlations between each pair of variables in a dataset. As an example, let’s take the first four columns in the ‘mtcars’ dataset, that is available within R. Getting the variances-covariances and the correlations is straightforward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
matr &amp;lt;- mtcars[,1:4]

#Covariances
cov(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlations
cor(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s really a piece of cake! Unfortunately, a few days ago I had a covariance matrix without the original dataset and I wanted the corresponding correlation matrix. Although this is an easy task as well, at first I was stuck, because I could not find an immediate solution… So I started wondering how I could make it.&lt;/p&gt;
&lt;p&gt;Indeed, having the two variables X and Y, their covariance is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cov(X, Y) = \sum\limits_{i=1}^{n} {(X_i - \hat{X})(Y_i - \hat{Y})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}\)&lt;/span&gt; are the means for each variable. The correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cor(X, Y) = \frac{cov(X, Y)}{\sigma_x \sigma_y} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y\)&lt;/span&gt; are the standard deviations for X and Y.&lt;/p&gt;
&lt;p&gt;The opposite relationship is clear:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ cov(X, Y) = cor(X, Y) \sigma_x \sigma_y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, converting from covariance to correlation is pretty easy. For example, take the covariance between ‘cyl’ and ‘mpg’ above (-9.172379), the correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-633.097208 / (sqrt(36.324103) * sqrt(15360.7998))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8475514&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the reverse, if we have the correlation (-0.8521620), the covariance is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.8475514 * sqrt(36.324103) * sqrt(15360.7998)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -633.0972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My covariance matrix was pretty large, so I started wondering how I could perform this task altogether. What I had to do was to take each element in the covariance matrix and divide it by the square root of the diagonal elements in the same column and in the same row (see below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://lu4yxa.db.files.1drv.com/y4mZ-7ZQc0LCMyDG3kqC_0_bzMZYyEpb37ug_I616tXoPNL_DbILLSOa8HujEZCekvRNeeYsfwtrYP-0T_PfzlOUqUNliHdKU3sDLHwBnr5C4jF-U-u1QkOlWg3ZbQXKJw4TM2VrQIQqjh-Pb-5cOEY49q-3pfnt4ZYJUAYZIBhW4GgJ0svrEEAnKQZfNTs2LW5iZhGyYFYVKFT2Y1O7SjKjA?width=637&amp;amp;height=156&amp;amp;cropmode=none&#34; style=&#34;width:95.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This is easily done by matrix multiplication. I need a square matrix where the standard deviations for each variable are repeated along the rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V &amp;lt;- cov(matr)
SM1 &amp;lt;- matrix(rep(sqrt(diag(V)), 4), 4, 4)
SM1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]
## [1,]   6.026948   6.026948   6.026948   6.026948
## [2,]   1.785922   1.785922   1.785922   1.785922
## [3,] 123.938694 123.938694 123.938694 123.938694
## [4,]  68.562868  68.562868  68.562868  68.562868&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and another one where they are repeated along the columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SM2 &amp;lt;- matrix(rep(sqrt(diag(V)), each = 4), 4, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can take my covariance matrix (V) and simply multiply these three matrices as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V * 1/SM1 * 1/SM2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, there is not even the need to use ‘rep’ when we create SM1, as R will recycle the elements as needed.&lt;/p&gt;
&lt;p&gt;Going from correlation to covariance can be done similarly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- cor(matr)
R / (1/SM1 * 1/SM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an easy task, but it got me stuck for a few minutes…&lt;/p&gt;
&lt;p&gt;Lately, I finally discovered that there is (at least) one function in R taking care of the above task; it is the ‘cov2cor()’ function in the ‘nlme’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
cov2cor(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is really easy to get drown in a glass of water!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>/2019/stat_general_correlation_alookat/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlation_alookat/</guid>
      <description>


&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-for-the-pearson-correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions for the Pearson Correlation Coefficient&lt;/h1&gt;
&lt;p&gt;The Pearson correlation coefficients makes a few assumptions, which should be carefully checked.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interval-level measurement. Both variables should be measured on a quantitative scale.&lt;/li&gt;
&lt;li&gt;Random sampling. Each subject in the sample should contribute one value on X, and one value on Y. The values for both variables should represent a random sample drawn from the population of interest.&lt;/li&gt;
&lt;li&gt;Linearity. The relationship between X and Y should be linear.&lt;/li&gt;
&lt;li&gt;Bivarlate normal distribution. This means that (i) values of X should form a normal distribution at each value of Y and (ii) values of Y should form a normal distribution at each value of X.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;It is possible to test whether &lt;span class=&#34;math inline&#34;&gt;\(r = 0\)&lt;/span&gt; against the alternative $ r 0$. The test is based on the idea that the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}\]&lt;/span&gt; is distributed as a Student’s t variable.&lt;/p&gt;
&lt;p&gt;Let’s take the two variables ‘cyl’ and ‘mpg’ from the ‘mtcars’ data frame. The correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- cor(mtcars$cyl, mtcars$gear)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.4926866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The T statistic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- r * sqrt(32 - 2) / sqrt(1 - r^2)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.101051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value for the null is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * pt(T, 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004173297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is clearly highly significant. The null can be rejected.&lt;/p&gt;
&lt;p&gt;As for hypothesis testing, it should be considered that the individuals where couple of measurements were taken should be independent. If they are not, the t test is invalid. I am dealing with this aspect somewhere else in my blog.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation in R&lt;/h1&gt;
&lt;p&gt;We have already seen that we can use the usual function ‘cor(matrix, method=)’. In order to obtain the significance, we can use the ‘rcorr()’ function in the Hmisc package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlations with significance levels
library(Hmisc)
corr2 &amp;lt;- rcorr(as.matrix(mtcars), type=&amp;quot;pearson&amp;quot;)
print(corr2$r, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg   cyl  disp    hp   drat    wt   qsec    vs     am  gear   carb
## mpg   1.00 -0.85 -0.85 -0.78  0.681 -0.87  0.419  0.66  0.600  0.48 -0.551
## cyl  -0.85  1.00  0.90  0.83 -0.700  0.78 -0.591 -0.81 -0.523 -0.49  0.527
## disp -0.85  0.90  1.00  0.79 -0.710  0.89 -0.434 -0.71 -0.591 -0.56  0.395
## hp   -0.78  0.83  0.79  1.00 -0.449  0.66 -0.708 -0.72 -0.243 -0.13  0.750
## drat  0.68 -0.70 -0.71 -0.45  1.000 -0.71  0.091  0.44  0.713  0.70 -0.091
## wt   -0.87  0.78  0.89  0.66 -0.712  1.00 -0.175 -0.55 -0.692 -0.58  0.428
## qsec  0.42 -0.59 -0.43 -0.71  0.091 -0.17  1.000  0.74 -0.230 -0.21 -0.656
## vs    0.66 -0.81 -0.71 -0.72  0.440 -0.55  0.745  1.00  0.168  0.21 -0.570
## am    0.60 -0.52 -0.59 -0.24  0.713 -0.69 -0.230  0.17  1.000  0.79  0.058
## gear  0.48 -0.49 -0.56 -0.13  0.700 -0.58 -0.213  0.21  0.794  1.00  0.274
## carb -0.55  0.53  0.39  0.75 -0.091  0.43 -0.656 -0.57  0.058  0.27  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(corr2$P, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mpg     cyl    disp      hp    drat      wt    qsec      vs
## mpg       NA 6.1e-10 9.4e-10 1.8e-07 1.8e-05 1.3e-10 1.7e-02 3.4e-05
## cyl  6.1e-10      NA 1.8e-12 3.5e-09 8.2e-06 1.2e-07 3.7e-04 1.8e-08
## disp 9.4e-10 1.8e-12      NA 7.1e-08 5.3e-06 1.2e-11 1.3e-02 5.2e-06
## hp   1.8e-07 3.5e-09 7.1e-08      NA 1.0e-02 4.1e-05 5.8e-06 2.9e-06
## drat 1.8e-05 8.2e-06 5.3e-06 1.0e-02      NA 4.8e-06 6.2e-01 1.2e-02
## wt   1.3e-10 1.2e-07 1.2e-11 4.1e-05 4.8e-06      NA 3.4e-01 9.8e-04
## qsec 1.7e-02 3.7e-04 1.3e-02 5.8e-06 6.2e-01 3.4e-01      NA 1.0e-06
## vs   3.4e-05 1.8e-08 5.2e-06 2.9e-06 1.2e-02 9.8e-04 1.0e-06      NA
## am   2.9e-04 2.2e-03 3.7e-04 1.8e-01 4.7e-06 1.1e-05 2.1e-01 3.6e-01
## gear 5.4e-03 4.2e-03 9.6e-04 4.9e-01 8.4e-06 4.6e-04 2.4e-01 2.6e-01
## carb 1.1e-03 1.9e-03 2.5e-02 7.8e-07 6.2e-01 1.5e-02 4.5e-05 6.7e-04
##           am    gear    carb
## mpg  2.9e-04 5.4e-03 1.1e-03
## cyl  2.2e-03 4.2e-03 1.9e-03
## disp 3.7e-04 9.6e-04 2.5e-02
## hp   1.8e-01 4.9e-01 7.8e-07
## drat 4.7e-06 8.4e-06 6.2e-01
## wt   1.1e-05 4.6e-04 1.5e-02
## qsec 2.1e-01 2.4e-01 4.5e-05
## vs   3.6e-01 2.6e-01 6.7e-04
## am        NA 5.8e-08 7.5e-01
## gear 5.8e-08      NA 1.3e-01
## carb 7.5e-01 1.3e-01      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also use these functions with two matrices, to obtain the correlations of each column in one matrix with each column in the other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation matrix from mtcars
x &amp;lt;- mtcars[1:3]
y &amp;lt;- mtcars[4:6]
cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              hp       drat         wt
## mpg  -0.7761684  0.6811719 -0.8676594
## cyl   0.8324475 -0.6999381  0.7824958
## disp  0.7909486 -0.7102139  0.8879799&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-to-slope-in-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Relationship to slope in linear regression&lt;/h1&gt;
&lt;p&gt;The correlation coefficient and slope in linear regression bear some similarities, as both describe how Y changes when X is changed. However, in correlation, we have two random variables, while in regression we have Y random, X fixed and Y is regarded as a function of X (not the other way round).&lt;/p&gt;
&lt;p&gt;Without neglecting their different meaning, it may be useful to show the algebraic relationship between the correlation coefficient and the slope in regression. Let’s simulate a dataset with two variables, coming from a multivariate normal distribution, with means respectively equal to 10 and 2, and variance-covariance matrix of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
cov &amp;lt;- matrix(c(2.20, 0.48, 0.48, 0.20), 2, 2)
cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] 2.20 0.48
## [2,] 0.48 0.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the ‘mvrnomr()’ function to generate the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
dataset &amp;lt;- data.frame( mvrnorm(n=10, mu = c(10, 2), Sigma = cov) )
names(dataset) &amp;lt;- c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;)
dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X        Y
## 1  11.756647 2.547203
## 2   9.522180 2.199740
## 3   8.341254 1.862362
## 4  13.480005 2.772031
## 5   9.428296 1.573435
## 6   9.242788 1.861756
## 7  10.817449 2.343918
## 8  10.749047 2.451999
## 9  10.780400 2.436263
## 10 11.480301 1.590436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficient and slope are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- with(dataset, cor(X, Y))
b1 &amp;lt;- coef( lm(Y ~ X, data=dataset) )[2]
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equation for the slope is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 = \frac{ \sum_{i = 1}^n \left[ ( X-\mu_X )( Y-\mu_Y )\right] }{ \sigma^2_X } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From there, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r = b_1 \frac{\sigma_X}{ \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ b_1 = r \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- with(dataset, sd(X) )
sigmaY &amp;lt;- with(dataset, sd(Y) )
b1 * sigmaX / sigmaY &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r * sigmaY / sigmaX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also easy to see that the correlation coefficient is the slope of regression of standardised Y against standardised X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yst &amp;lt;- with(dataset, scale(Y, scale=T) )
summary( lm(Yst ~ I(scale(X, scale = T) ), data = dataset) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Yst ~ I(scale(X, scale = T)), data = dataset)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082006 -0.067143 -0.036850  0.009214  0.237923 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)            -5.633e-18  3.478e-02   0.000   1.0000  
## I(scale(X, scale = T))  1.785e-01  7.633e-02   2.339   0.0475 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.11 on 8 degrees of freedom
## Multiple R-squared:  0.4061, Adjusted R-squared:  0.3319 
## F-statistic: 5.471 on 1 and 8 DF,  p-value: 0.04748&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intra-class-correlation-icc&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intra-class correlation (ICC)&lt;/h1&gt;
&lt;p&gt;It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intra-class correlation coefficient is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IC = {\displaystyle {\frac {\sigma _{\alpha }^{2}}{\sigma _{\alpha }^{2}+\sigma _{\varepsilon }^{2}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\alpha }^{2}\)&lt;/span&gt; is the variance between groups and &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\varepsilon }^{2}\)&lt;/span&gt; is the variance within a group (better, the variance of one observation within a group). The sum of those two variances is the total variance of observations. In words, the intra-class correlation coefficient measures the joint variability of subjects in the same group (that relates on how groups are different from one another), with respect to the total variability of observations. If subjects in one group are very similar to one another (small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\varepsilon}\)&lt;/span&gt;) but groups are very different (high &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha}\)&lt;/span&gt;), the ICC is very high.&lt;/p&gt;
&lt;p&gt;The existence of grouping of residuals is very important in ANOVA, as it means that independence is violated, which calls for the use of mixed model.&lt;/p&gt;
&lt;p&gt;But … this is a totally different story …&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>