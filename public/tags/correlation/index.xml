<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Correlation on Fixing the bridge between biologists and statisticians</title>
    <link>http://localhost:4321/tags/correlation/</link>
    <description>Recent content in Correlation on Fixing the bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2023, @AndreaOnofri</copyright>
    <lastBuildDate>Sat, 26 Nov 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://localhost:4321/tags/correlation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The coefficient of determination: is it the R-squared or r-squared?</title>
      <link>http://localhost:4321/2022/stat_general_r2/</link>
      <pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2022/stat_general_r2/</guid>
      <description>


&lt;p&gt;We often use the &lt;strong&gt;coefficient of determination&lt;/strong&gt; as a swift ‘measure’ of goodness of fit for our regression models. Unfortunately, there is no unique symbol for such a coefficient and both &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; are used in literature, almost interchangeably. Such an interchangeability is also endorsed by the Wikipedia (see at: &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34;&gt;https://en.wikipedia.org/wiki/Coefficient_of_determination&lt;/a&gt; ), where both symbols are reported as the abbreviations for this statistical index.&lt;/p&gt;
&lt;p&gt;As an editor of several International Journals, I should not agree with such an approach; indeed, the two symbols &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; mean two different things, and they are not necessarily interchangeable, because, depending on the setting, either of the two may be wrong or ambiguous. Let’s pay a little attention to such an issue.&lt;/p&gt;
&lt;div id=&#34;what-are-the-r-and-r-indices&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are the ‘r’ and ‘R’ indices?&lt;/h1&gt;
&lt;p&gt;When studying the relationship between quantitative variables, we have two main statistical indices:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;strong&gt;Pearson’s (simple linear) correlation coefficient&lt;/strong&gt;, that is almost always indicated as the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; coefficient. Correlation is different from regression, as it does not assume any sort of dependency between two quantitative variables and it is only meant to express their joint variability;&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;coefficient of multiple correlation&lt;/strong&gt;, that is usually indicated with &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and represents (definition from Wikipedia) a &lt;em&gt;measure of how well a given variable can be predicted using a linear function of a set of other variables&lt;/em&gt;. Although &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is based on correlation (it is the correlation between the observed values for the dependent variable and the predictions made by the model), it is used in the context of multiple regression, where we are studying a dependency relationship.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And, what about the &lt;strong&gt;coefficient of determination&lt;/strong&gt;? It is yet another concept and another index, measuring (again from Wikipedia) the &lt;em&gt;proportion of the variation in the dependent variable that is predictable from the independent variable(s)&lt;/em&gt;. As you see, we are still in the context of regression and our aim is to describe the goodness of fit.&lt;/p&gt;
&lt;p&gt;To start with, let’s abbreviate the coefficient of determination as &lt;span class=&#34;math inline&#34;&gt;\(CD\)&lt;/span&gt;, in order to avoid any confusion with &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;; this index can be be obtained as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ CD_1 = \frac{SS_{reg}}{SS_{tot}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ CD_2 = 1 - \frac{SS_{res}}{SS_{tot}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where: &lt;span class=&#34;math inline&#34;&gt;\(SS_{reg}\)&lt;/span&gt; is the regression sum of squares, &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt; is the total sum of squares and &lt;span class=&#34;math inline&#34;&gt;\(SS_{res}\)&lt;/span&gt; is the residual sum of squares, after a linear regression fit. The second formula is preferable: sum of squares are always positive and, thus, we clearly see that &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; may not be higher than 1 (this is less obvious, for &lt;span class=&#34;math inline&#34;&gt;\(CD_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So far, so good; we have three different indices and three different symbols: &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CD\)&lt;/span&gt;. But, in practice, things did not go that smoothly! The early statisticians, instead of proposing a brand new symbol for the coefficient of determination, made the choice of highlighting the connections with &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;. For example, Sokal and Rohlf, in their very famous biometry book, at page 570 (2nd. Edition) demonstrated that the coefficient of determination could be obtained as the square of the coefficient of correlation and, thus, they used the symbol &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt;. Later, in the same book (pag. 660), these same authors demonstrated that the coefficient of multiple correlation &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; was equal to the positive square root of the &lt;strong&gt;coefficient of multiple determination&lt;/strong&gt;, for which they used the symbol &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Obviously, Sokal and Rohlf (and other authors of other textbooks) meant to say that the coefficient of determination, depending on the context, can be obtained either as the square of the correlation coefficient, or as the square of the multiple correlation coefficient. An uncareful interpretation led to the idea that the coefficient of determination can be indicated either as the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; or as the &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; and that the two symbols are always interchangeable. But, is this really true? No, it depends on the context.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple linear regression&lt;/h1&gt;
&lt;p&gt;Let’s have a look at the following example: we fit a simple linear regression model to a dataset and retrieve the coefficient of determination by using the &lt;code&gt;summary()&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 1:20
Y &amp;lt;- c(17.79, 18.81, 19.02, 14.14, 16.72, 16.05, 13.99, 13.26,
       12.48, 11.33, 11.07, 9.68, 9.19, 9.44, 9.75, 7.71, 6.47, 
       5.22, 4.55, 7.7)
mod &amp;lt;- lm(Y ~ X)
summary(mod)$r.squared # Coeff. determination with R
## [1] 0.9270622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very easy to see that &lt;span class=&#34;math inline&#34;&gt;\(R = |r|\)&lt;/span&gt; and it is also easy to demonstrate that &lt;span class=&#34;math inline&#34;&gt;\(r^2 = CD_1\)&lt;/span&gt; (look, e.g., at Sokal and Rohlf for a mathematical proof). Furthermore, due to the equality &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot} = SS_{reg} + SS_{res}\)&lt;/span&gt;, it is also easy to see that &lt;span class=&#34;math inline&#34;&gt;\(CD_1 = CD_2\)&lt;/span&gt;. We are ready to draw our first conclusion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SSreg &amp;lt;- sum((predict(mod) - mean(Y))^2)
SStot &amp;lt;- sum((Y - mean(Y))^2)
SSres &amp;lt;- sum(residuals(mod)^2)
SSreg/SStot
## [1] 0.9270622
 r
1 - SSres/SStot
## [1] 0.9270622
 r
r.coef &amp;lt;- cor(X, Y)
R.coef &amp;lt;- cor(Y, fitted(mod))
r.coef^2
## [1] 0.9270622
 r
R.coef^2
## [1] 0.9270622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 1. For simple linear regression, the coefficient of determination is always equal to &lt;span class=&#34;math inline&#34;&gt;\(R^2 = r^2\)&lt;/span&gt; and both symbols are acceptable (and correct).&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-regression-and-multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomial regression and multiple regression&lt;/h1&gt;
&lt;p&gt;Apart from simple linear regression, for all other types of linear models, provided that an intercept is fitted, it is not, in general, true that &lt;span class=&#34;math inline&#34;&gt;\(R = |r|\)&lt;/span&gt;, while it is, in general, true that that the coefficient of determination is equal to the squared coefficient of multiple correlation &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;. I’ll show a swift example with a polynomial regression in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lm(Y ~ X + I(X^2))
cor.coef &amp;lt;- cor(X, Y)
R.coef &amp;lt;- cor(Y, fitted(mod2))

# R and r are not equal
cor.coef; R.coef
## [1] -0.9628407
## [1] 0.9652451
 r
# The coefficient of determination is R2
R.coef^2; summary(mod2)$r.squared
## [1] 0.931698
## [1] 0.931698&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, when we have several predictors (e.g., multiple regression), the correlation coefficient is not unique and we could calculate as many &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; values as there are predictors in the model.&lt;/p&gt;
&lt;p&gt;In the box below I show another example where I analysed the ‘mtcars’ dataset by using multiple regression; we see that &lt;span class=&#34;math inline&#34;&gt;\(R^2 = CD_1 = CD_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
mod &amp;lt;- lm(mpg ~ wt+disp+hp - 1, data = mtcars)  
summary(mod)$r.squared # Coeff. determination with R
## [1] 0.8328665
 r
SSreg &amp;lt;- sum((predict(mod) - mean(mtcars$mpg))^2)
SStot &amp;lt;- sum((mtcars$mpg - mean(mtcars$mpg))^2)
SSres &amp;lt;- sum(residuals(mod)^2)
SSreg/SStot
## [1] 0.9852479
 r
1 - SSres/SStot
## [1] -1.084229
 r
R.coef &amp;lt;- cor(mtcars$mpg, fitted(mod))
R.coef^2
## [1] 0.002746157&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to draw our second conclusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 2: with all linear models, apart from simple linear regression, the &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; symbol should not be used for the coefficient of determination, because this latter index IS NOT, in general, equal to the square of the coefficient of correlation. The &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; symbol is a much better choice.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models-with-no-intercept&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Linear models with no intercept&lt;/h1&gt;
&lt;p&gt;The situation becomes much more complex for linear models with no intercept. For these models, the squared multiple correlation coefficient IS NOT ALWAYS equal to the proportion of variance accounted for. Let’s look at the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lm(Y ~ - 1 + X)
summary(mod2)$r.squared # Proportion of variance accounted for
## [1] 0.4390065
 r
R.coef &amp;lt;- cor(Y, fitted(mod2))
R.coef^2
## [1] 0.9270622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, the coefficient of determination IS NOT ALWAYS the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;; however, the coefficient of determination can be calculated by using &lt;span class=&#34;math inline&#34;&gt;\(CD_1 = CD_2\)&lt;/span&gt;, provided that &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(SS_{reg}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SS_{res}\)&lt;/span&gt; are obtained in a way that accounts for the missing intercept. Schabenberger and Pierce recommend the following equations and the symbols they use clearly reflect that those equations do not return the squared multiple correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R^2_{noint} = \frac{\sum_{i = 1}^{n}{\hat{y_i}^2}}{\sum_{i = 1}^{n}{y_i^2}} \quad \textrm{or} \quad R^{2*}_{noint} =1 - \frac{SS_{res}}{\sum_{i = 1}^{n}{y_i^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SSreg &amp;lt;- sum(fitted(mod2)^2)
SStot &amp;lt;- sum(Y^2)
SSres &amp;lt;- sum(residuals(mod2)^2)
SSreg/SStot # R^2 ok
## [1] 0.4390065
 r
1 - SSres/SStot # R^2 ok
## [1] 0.4390065&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are ready for our third conclusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 3: in the case of models with no intercept, neither the &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; nor the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; symbols should be used for the coefficient of determination. The proportion of variability accounted for by the model can be calculated by using a modified formula and should be reported by using a different symbol (e.g. &lt;span class=&#34;math inline&#34;&gt;\(R^2_{noint}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(R^2_0\)&lt;/span&gt; or similar).&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nonlinear regression&lt;/h1&gt;
&lt;p&gt;With this class of models, we have two main problems:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;they do not have an intercept term, at least, not in the usual sense. Consequently, the square of the multiple coefficient of correlation does not represent the proportion of variance accounted for by the model;&lt;/li&gt;
&lt;li&gt;the equality &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot} = SS_{reg} + SS_{res}\)&lt;/span&gt; may not hold and thus the equations for &lt;span class=&#34;math inline&#34;&gt;\(CD_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; may produce different results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In contrast to linear models with no intercept, for nonlinear models we do not have any general modified formula that consistently returns the proportion of variance accounted for by the model (i.e., the coefficient of determination). However, Schabenberger and Pierce (2002) suggested that we can still use &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; as a swift measure of goodness of fit, but they also proposed that we use the term ‘Pseudo-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;’ instead oft &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;. Why ‘Pseudo’?. For two good reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the ’Pseudo-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;’cannot exceed 1, but it may lower than 0;&lt;/li&gt;
&lt;li&gt;the ‘Pseudo-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;’ &lt;strong&gt;cannot be interpreted as the proportion of variance explained by the model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In R, the ‘Pseudo-R&lt;sup&gt;2&lt;/sup&gt;’ can be calculated by using the &lt;code&gt;R2.nls()&lt;/code&gt; function in the ‘aomisc’ package, for nonlinear models fitted with both the &lt;code&gt;nls()&lt;/code&gt; and &lt;code&gt;drm()&lt;/code&gt; functions (this latter function is in the ‘drc’ package).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
X &amp;lt;- c(0.1, 5, 7, 22, 28, 39, 46, 200)
Y &amp;lt;- c(1, 13.66, 14.11, 14.43, 14.78, 14.86, 14.78, 14.91)

# nls fit
library(aomisc)
model &amp;lt;- nls(Y ~ SSmicmen(X, Vm, K))
R2nls(model)$PseudoR2
## [1] 0.9930399
 r
# It is not the R2, in strict sense
R.coef &amp;lt;- cor(Y, fitted(model))
R.coef^2
## [1] 0.9957255
 r
# It cannot be calculated by the usual expression!
SSreg &amp;lt;- sum(fitted(model) - mean(Y))
SStot &amp;lt;- sum( (Y - mean(Y))^2 )
SSreg/SStot
## [1] 0.003622005
 r
# It can be calculated by using the alternative form
# that is no longer equivalent
SSres &amp;lt;- sum(residuals(model)^2)
1 - SSres/SStot
## [1] 0.9930399&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may now come to our final conclusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CONCLUSION 4. With nonlinear models, we should never use either &lt;span class=&#34;math inline&#34;&gt;\(r^2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; because they are both wrong. If we need a swift measure of goodness of fit, we can use the &lt;span class=&#34;math inline&#34;&gt;\(CD_2\)&lt;/span&gt; index above, but we should not name it as the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;, because, in general, it does not correspond to the coefficient of determination. We should better use the term Pseudo-R&lt;sup&gt;2&lt;/sup&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hope this was useful; for those who are interested in the use of the Pseud-&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; in nonlinear regression, I hav already published one post at this link: &lt;a href=&#34;https://www.statforbiology.com/2021/stat_nls_r2/&#34;&gt;https://www.statforbiology.com/2021/stat_nls_r2/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Schabenberger, O., Pierce, F.J., 2002. Contemporary statistical models for the plant and soil sciences. Taylor &amp;amp; Francis, CRC Press, Books.&lt;/li&gt;
&lt;li&gt;Sokal, R.R., Rohlf F.J., 1981. Biometry. Second Edition, W.H. Freeman and Company, USA.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>http://localhost:4321/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlationindependence2/</guid>
      <description>


&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;
&lt;div id=&#34;a-routine-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A ‘routine’ experiment&lt;/h1&gt;
&lt;p&gt;I will use the same example as presented &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in my previous post&lt;/a&gt;, which should allow us to compare the results with those obtained by using more traditional methods of data analyses. It is a genotype experiment, laid out in randomised complete blocks, with 27 wheat genotypes and three replicates. For each plot, the collegue who made the experiment recorded several traits, including yield (Yield) and weight of thousand kernels (TKW). The dataset ‘WheatQuality.csv’ is available on ‘gitHub’; it consists of 81 records (plots) and just as many couples of measures in all. The code below loads the necessary packages, the data and transforms the numeric variable ‘Block’ into a factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(plyr)
library(nlme)
# library(asreml)
dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/WheatQuality.csv&amp;quot;, header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Whectol Yield
## 1 arcobaleno     1     90 44.5    83.2 64.40
## 2 arcobaleno     2     90 42.8    82.2 60.58
## 3 arcobaleno     3     88 42.7    83.1 59.42
## 4       baio     1     80 40.6    81.8 51.93
## 5       baio     2     75 42.7    81.3 51.34
## 6       baio     3     76 41.1    81.1 47.78&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;In my previous post&lt;/a&gt;, I used the above dataset to calculate the Pearson’s correlation coefficient between Yield and TKW for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;plot measurements,&lt;/li&gt;
&lt;li&gt;residuals,&lt;/li&gt;
&lt;li&gt;treatment/block means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Piepho (2018) showed that, for an experiment like this one, the above correlations can be estimated by coding a multiresponse mixed model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijk} = \mu_i + \beta_{ik} + \tau_{ij} + \epsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ijk}\)&lt;/span&gt; is the response for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the rootstock &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the mean for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{ik}\)&lt;/span&gt; is the effect of the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ij}\)&lt;/span&gt; is the effect of genotype &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; is the residual for each of 81 observations for two traits.&lt;/p&gt;
&lt;p&gt;In the above model, the residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; need to be normally distributed and heteroscedastic, with trait-specific variances. Furthermore, residuals belonging to the same plot (the two observed traits) need to be correlated (correlation of errors).&lt;/p&gt;
&lt;p&gt;Hans-Peter Piepho, in his paper, put forward the idea that the ‘genotype’ and ‘block’ effects for the two traits can be taken as random, even though they might be of fixed nature, especially the genotype effect. This idea makes sense, because, for this application, we are mainly interested in variances and covariances. Both random effects need to be heteroscedastic (trait-specific variance components) and there must be a correlation between the two traits.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge, there is no way to fit such a complex model with the ‘nlme’ package and related ‘lme()’ function (I’ll gave a hint later on, for a simpler model). Therefore, I decided to use the package ‘asreml’ (Butler et al., 2018), although this is not freeware. With the function ‘asreml()’, we need to specify the following components.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The response variables. When we set a bivariate model with ‘asreml’, we can ‘cbind()’ Yield and TKW and use the name ‘trait’ to refer to them.&lt;/li&gt;
&lt;li&gt;The fixed model, that only contains the trait effect. The specification is, therefore, ‘cbind(Yield, TKW) ~ trait - 1’. Following Piepho (2018), I removed the intercept, to separately estimate the means for the two traits.&lt;/li&gt;
&lt;li&gt;The random model, that is composed by the interactions ‘genotype x trait’ and ‘block x trait’. For both, I specified a general unstructured variance covariance matrix, so that the traits are heteroscedastic and correlated. Therefore, the random model is ~ Genotype:us(trait) + Block:us(trait).&lt;/li&gt;
&lt;li&gt;The residual structure, where the observations in the same plot (the term ‘units’ is used in ‘asreml’ to represent the observational units, i.e. the plots) are heteroscedastic and correlated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model call is (sorry, I have not renewed my license):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mod.asreml &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#        random = ~ Genotype:us(trait, init = c(3.7, -0.25, 1.7)) + 
#          Block:us(trait, init = c(77, 38, 53)),
#        residual = ~ units:us(trait, init = c(6, 0.16, 4.5)), 
#        data=dataset)
# summary(mod.asreml)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The box above shows the results about the variance-covariance parameters. In order to get the correlations, I specified the necessary combinations of variance-covariance parameters. It is necessary to remember that estimates, in ‘asreml’, are named as V1, V2, … Vn, according to their ordering in model output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# parms &amp;lt;- mod.asreml$vparameters
# vpredict(mod.asreml, rb ~ V2 / (sqrt(V1)*sqrt(V3) ) )
# vpredict(mod.asreml, rt ~ V5 / (sqrt(V4)*sqrt(V6) ) )
# vpredict(mod.asreml, re ~ V9 / (sqrt(V8)*sqrt(V10) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the estimates are very close to those obtained by using the Pearson’s correlation coefficients (see my previous post). The advantage of this mixed model solution is that we can also test hypotheses in a relatively reliable way. For example, I tested the hypothesis that residuals are not correlated by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;coding a reduced model where residuals are heteroscedastic and independent, and&lt;/li&gt;
&lt;li&gt;comparing this reduced model with the complete model by way of a REML-based Likelihood Ratio Test (LRT).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Removing the correlation of residuals is easily done, by changing the correlation structure from ‘us’ (unstructured variance-covariance matrix) to ‘idh’ (diagonal variance-covariance matrix).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mod.asreml2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:us(trait) + Block:us(trait),
#         residual = ~ units:idh(trait), 
#         data=dataset)
# lrt.asreml(mod.asreml, mod.asreml2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, I tried to further reduce the model to test the significance of the correlation between block means and genotype means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mod.asreml3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:us(trait) + Block:idh(trait),
#         residual = ~ units:idh(trait), 
#         data=dataset)
# lrt.asreml(mod.asreml, mod.asreml3)

# mod.asreml4 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:idh(trait) + Block:idh(trait),
#         residual = ~ units:idh(trait), 
#         data=dataset)
# lrt.asreml(mod.asreml, mod.asreml4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that only the genotype means are significantly correlated.&lt;/p&gt;
&lt;p&gt;An alternative (and more useful) way to code the same model is by using the ‘corgh’ structure, instead of ‘us’. These two structures are totally similar, apart from the fact that the first one uses the correlations, instead of the covariances. Another difference, which we should consider when giving starting values, is that correlations come before variances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mod.asreml.r &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
#         + Block:corgh(trait, init = c(0.6, 77, 53)),
#         residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
#         data=dataset)
# summary(mod.asreml.r)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of this parameterisation is that we can test our hypotheses by easily setting up contraints on correlations. One way to do this is to run the model with the argument ‘start.values = T’. In this way I could derive a data frame (‘mod.init$parameters’), with the starting values for REML maximisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the starting values
# mod.init &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
#         + Block:corgh(trait, init = c(0.6, 77, 53)),
#         residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
#         data=dataset, start.values = T)
# init &amp;lt;- mod.init$vparameters
# init&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘init’ data frame has three columns: (i) names of parameters, (ii) initial values and (iii) type of constraint (U: unconstrained, P = positive, F = fixed). Now, if we take the seventh row (correlation of residuals), set the initial value to 0 and set the third column to ‘F’ (meaning: keep the initial value fixed), we are ready to fit a model without correlation of residuals (same as the ‘model.asreml2’ above). What I had to do was just to pass this data frame as the starting value matrix for a new model fit (see the argument ‘R.param’, below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# init2 &amp;lt;- init
# init2[8, 2] &amp;lt;- 0
# init2[8, 3] &amp;lt;- &amp;quot;F&amp;quot;

# mod.asreml.r2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:corgh(trait)
#         + Block:corgh(trait),
#         residual = ~ units:corgh(trait), 
#         data=dataset, R.param = init2)
# summary(mod.asreml.r2)$varcomp[,1:2]
# lrt.asreml(mod.asreml.r2, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is even more interesting is that ‘asreml’ permits to force the parameters to be linear combinations of one another. For instance, we can code a model where the residual correlation is contrained to be equal to the treatment correlation. To do so, we have to set up a two-column matrix (M), with row names matching the component names in the ‘asreml’ parameter vector. The matrix M should contain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the first column, the equality relationships (same number, same value)&lt;/li&gt;
&lt;li&gt;in the second column, the coefficients for the multiplicative relationships&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we would set the matrix M as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# firstCol &amp;lt;- c(1, 2, 3, 4, 5, 6, 7, 1, 8, 9)
# secCol &amp;lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
# M &amp;lt;- cbind(firstCol, secCol)
# dimnames(M)[[1]] &amp;lt;- mod.init$vparameters$Component
# M&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that in ‘firstCol’, the 1st and 8th element are both equal to 1, which contraints them to assume the same value. We can now pass the matrix M as the value of the ‘vcc’ argument in the model call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mod.asreml.r3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
#         random = ~ Genotype:corgh(trait)
#         + Block:corgh(trait),
#         residual = ~ units:corgh(trait), 
#         data=dataset, R.param = init, vcc = M)
# summary(mod.asreml.r3)$varcomp[,1:3]
# lrt.asreml(mod.asreml.r3, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the residual and treatment correlations are equal in this latter model. We also see that this reduced model fits significantly worse than the complete model ‘mod.asreml.r’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-freeware&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going freeware&lt;/h1&gt;
&lt;p&gt;Considering that the block means are not correlated, if we were willing to take the ‘block’ effect as fixed, we could fit the resulting model also with the ‘nlme’ package and the function ‘lme()’ (Pinheiro and Bates, 2000). However, we should cast the model as a univariate model.&lt;/p&gt;
&lt;p&gt;To this aim, the two variables (Yield and TKW) need to be piled up and a new factor (‘Trait’) needs to be introduced to identify the observations for the two traits. Another factor is also necessary to identify the different plots, i.e. the observational units. To perform such a restructuring, I used the ‘melt()’ function in the ‘reshape2’ package Wickham, 2007) and assigned the name ‘Y’ to the response variable, that is now composed by the two original variables Yield and TKW, one on top of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$Plot &amp;lt;- 1:81
mdataset &amp;lt;- melt(dataset[,c(-3,-6)], variable.name= &amp;quot;Trait&amp;quot;, value.name=&amp;quot;Y&amp;quot;, id=c(&amp;quot;Genotype&amp;quot;, &amp;quot;Block&amp;quot;, &amp;quot;Plot&amp;quot;))
mdataset$Block &amp;lt;- factor(mdataset$Block)
head(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait    Y
## 1 arcobaleno     1    1   TKW 44.5
## 2 arcobaleno     2    2   TKW 42.8
## 3 arcobaleno     3    3   TKW 42.7
## 4       baio     1    4   TKW 40.6
## 5       baio     2    5   TKW 42.7
## 6       baio     3    6   TKW 41.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot   Trait    Y
## 157  vesuvio     1   76 Whectol 80.4
## 158  vesuvio     2   77 Whectol 80.9
## 159  vesuvio     3   78 Whectol 82.1
## 160 vitromax     1   79 Whectol 81.2
## 161 vitromax     2   80 Whectol 81.2
## 162 vitromax     3   81 Whectol 81.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fixed model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y ~ Trait*Block&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to introduce a totally unstructured variance-covariance matrix for the random effect, I used the ‘pdMat’ construct:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;random = list(Genotype = pdSymm(~Trait - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relating to the residuals, heteroscedasticity can be included by using the ‘weight()’ argument and the ‘varIdent’ variance function, which allows a different standard deviation per trait:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weight = varIdent(form = ~1|Trait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also allowed the residuals to be correlated within each plot, by using the ‘correlation’ argument and specifying a general ‘corSymm()’ correlation form. Plots are nested within genotypes, thus I used a nesting operator, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;correlation = corSymm(form = ~1|Genotype/Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Y ~ Trait*Block, 
                 random = list(Genotype = pdSymm(~Trait - 1)),
                 weight = varIdent(form=~1|Trait), 
                 correlation = corCompSymm(form=~1|Genotype/Plot),
                 data = mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retreiving the variance-covariance matrices needs some effort, as the function ‘getVarCov()’ does not appear to work properly with this multistratum model. First of all, we can retreive the variance-covariance matrix for the genotype random effect (G) and the corresponding correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Variance structure for random effects
obj &amp;lt;- mod
G &amp;lt;- matrix( as.numeric(getVarCov(obj)), 2, 2 )
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]      [,2]
## [1,] 53.86158 18.043241
## [2,] 18.04324  9.479339&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.7985203
## [2,] 0.7985203 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we can retreive the ‘conditional’ variance-covariance matrix (R), that describes the correlation of errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conditional variance-covariance matrix (residual error)
V &amp;lt;- corMatrix(obj$modelStruct$corStruct)[[1]] #Correlation for residuals
sds &amp;lt;- 1/varWeights(obj$modelStruct$varStruct)[1:2]
sds &amp;lt;- obj$sigma * sds #Standard deviations for residuals (one per trait)
R &amp;lt;- t(V * sds) * sds #Going from correlation to covariance
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 4.4717929 0.6413052
## [2,] 0.6413052 1.1079777&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.2881101
## [2,] 0.2881101 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total correlation matrix is simply obtained as the sum of G and R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tr &amp;lt;- G + R
cov2cor(Tr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.7518498
## [2,] 0.7518498 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are the same as those obtained by using ‘asreml’. Hope these snippets are useful!&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Butler, D., Cullis, B.R., Gilmour, A., Gogel, B., Thomson, R., 2018. ASReml-r reference manual - version 4. UK.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-effects models in s and s-plus. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Wickham, H., 2007. Reshaping data with the reshape package. Journal of Statistical Software 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part I</title>
      <link>http://localhost:4321/2019/stat_general_correlationindependence1/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlationindependence1/</guid>
      <description>


&lt;div id=&#34;observations-are-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observations are grouped&lt;/h1&gt;
&lt;p&gt;When we have recorded two traits in different subjects, we can be interested in describing their joint variability, by using the Pearson’s correlation coefficient. That’s ok, altough we have to respect some basic assumptions (e.g. linearity) that have been detailed elsewhere (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlation_alookat/&#34;&gt;see here&lt;/a&gt;). Problems may arise when we need to test the hypothesis that the correlation coefficient is equal to 0. In this case, we need to make sure that all the couples of observations are taken on independent subjects.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is most often false whenever measures are taken from designed field experiments. In this case, observations may be grouped by one or more treatment/blocking factors. This has been clearly described by Bland and Altman (1994); we would like to give an example that is more closely related to plant/crop science. Think about a genotype experiment, where we compare the behaviour of several genotypes in a randomised blocks design. Usually, we do not only measure yield. We also measure other traits, such as crop height. At the end of the experiment, we might be interested in reporting the correlation between yield and height. How should we proceed? It would seem an easy task, but it is not.&lt;/p&gt;
&lt;p&gt;Let’s assume that we have a randomised blocks design, with 27 genotypes and 3 replicates. For each plot, we recorded two traits, i.e. yield and the weight of thousand kernels (TKW). In the end, we have 81 plots and just as many couples of measures in all. We will use the dataset ‘WheatQuality.csv’, that is available on ‘gitHub’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Hmisc)
library(knitr)
library(plyr)
dataset &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/WheatQuality.csv&amp;quot;, header=T)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Whectol Yield
## 1 arcobaleno     1     90 44.5    83.2 64.40
## 2 arcobaleno     2     90 42.8    82.2 60.58
## 3 arcobaleno     3     88 42.7    83.1 59.42
## 4       baio     1     80 40.6    81.8 51.93
## 5       baio     2     75 42.7    81.3 51.34
## 6       baio     3     76 41.1    81.1 47.78&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How many correlations?&lt;/h1&gt;
&lt;p&gt;It may be tempting to consider the whole lot of measures and calculate the correlation coefficient between yield and TKW. This is the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra &amp;lt;- with(dataset, rcorr(Yield, TKW) )
ra$r[1,2] #Correlation coefficient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.540957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra$P[1,2] #P-level&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.850931e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We observe a positive correlation, and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; seems to be significantly different from 0. Therefore, we would be encouraged to conclude that plots with a high value on yield tend to have a high value on TKW, as well. Unfortunately, such a conclusion is not supported by the data.&lt;/p&gt;
&lt;p&gt;Indeed, the test of significance is clearly invalid, as the 81 plots are not independent; they are grouped by block and genotype and we are totally neglecting these two effects. Are we sure that the same correlation holds for all genotypes/blocks? Let’s check this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor &amp;lt;- function(x) cor(x$Yield, x$TKW)
wgCors &amp;lt;- ddply(dataset, ~Genotype, wCor)
wgCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Genotype         V1
## 1   arcobaleno  0.9847228
## 2         baio  0.1611952
## 3      claudio -0.9993872
## 4     colorado  0.9837293
## 5     colosseo  0.4564855
## 6        creso -0.5910193
## 7       duilio -0.9882330
## 8       gianni -0.7603802
## 9       giotto  0.9520211
## 10      grazia  0.4980828
## 11       iride  0.7563338
## 12    meridano  0.1174342
## 13      neodur  0.4805871
## 14      orobel  0.8907754
## 15 pietrafitta -0.9633891
## 16  portobello  0.9210135
## 17   portofino -0.9900764
## 18   portorico  0.1394211
## 19       preco  0.9007067
## 20    quadrato -0.5840238
## 21    sancarlo -0.6460670
## 22      simeto -0.4051779
## 23       solex -0.6066363
## 24 terrabianca -0.4076416
## 25       verdi  0.5801404
## 26     vesuvio -0.7797493
## 27    vitromax -0.8056514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wbCors &amp;lt;- ddply(dataset, ~Block, wCor)
wbCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Block        V1
## 1     1 0.5998137
## 2     2 0.5399990
## 3     3 0.5370398&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for the genotypes, we have 27 correlation coefficients, ranging from -0.999 to 0.985. We only have three couples of measurements per genotype and it is clear that this is not much, to reliably estimate a correlation coefficient. However, it is enough to be suspicious about the extent of correlation between yield and TKW, as it may depend on the genotype.&lt;/p&gt;
&lt;p&gt;On the other hand, the correlation within blocks is more constant, independent on the block and similar to the correlation between plots.&lt;/p&gt;
&lt;p&gt;It may be interesting to get an estimate of the average within-group correlation. To this aim, we can perform two separate ANOVAs (one per trait), including all relevant effects (blocks and genotypes) and calculate the correlation between the residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Yield ~ factor(Block) + Genotype, data = dataset)
mod2 &amp;lt;- lm(TKW ~ factor(Block) + Genotype, data = dataset)
wCor &amp;lt;- rcorr(residuals(mod1), residuals(mod2))
wCor$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x          y
## x 1.00000000 0.03133109
## y 0.03133109 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x         y
## x        NA 0.7812693
## y 0.7812693        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average within-group correlation is very small and unsignificant. Let’s think about this correlation: residuals represent yield and TKW values for all plots, once the effects of blocks and genotypes have been removed. A high correlation of residuals would mean that, letting aside the effects of the block and genotype to which it belongs, a plot with a high value on yield also shows a high value on TKW. The existence of such correlation is clearly unsopported by our dataset.&lt;/p&gt;
&lt;p&gt;As the next step, we could consider the means for genotypes/blocks and see whether they are correlated. Blocks and genotypes are independent and, in principle, significance testing is permitted. However, this is not recommended with block means, as three data are too few to make tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Genotype, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rgPear &amp;lt;- rcorr( as.matrix(means[,2:3]) )
rgPear$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Mu1       Mu2
## Mu1 1.0000000 0.5855966
## Mu2 0.5855966 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgPear$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Mu1        Mu2
## Mu1         NA 0.00133149
## Mu2 0.00133149         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Block, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rbPear &amp;lt;- cor( as.matrix(means[,2:3]) )
rbPear&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Mu1         Mu2
## Mu1  1.00000000 -0.08812544
## Mu2 -0.08812544  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We note that the correlation between genotype means is high and significant. On the contrary, the correlation between block means is near to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-so-what&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And so what?&lt;/h1&gt;
&lt;p&gt;At this stage, you may be confused… Let’s try to clear the fog.&lt;/p&gt;
&lt;p&gt;Obtaining a reliable measure of correlation from designed experiments is not obvious. Indeed, in every designed field experiment we have groups of subjects and there are several possible types of correlation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;correlation between plot measurements&lt;/li&gt;
&lt;li&gt;correlation between the residuals&lt;/li&gt;
&lt;li&gt;correlation between treatment/block means&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All these correlations should be investigated and used for interpretation.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The ‘naive’ correlation between the plot measurements is very easily calculated, but it is grossy misleading. Indeed, it disregards the treatment/block effects and it does not permit hypotheses testing, as the subjects are not independent. In this example, looking at the ‘naive’ correlation coefficient, we would wrongly conclude that plots with high yield also have high TKW, while further analyses show that this is not true, in general. We would reasonably suggest that the ‘naive’ correlation coefficient is never used for interpretation.&lt;/li&gt;
&lt;li&gt;The correlation between the residuals is a reliable measure of joint variation, because the experimental design is adequately referenced, by removing the effects of tratments/blocks. In this example, residuals are not correlated. Further analyses show that the correlation between yield and TKW, if any, may depend on the genotype, while it does not depend on the block.&lt;/li&gt;
&lt;li&gt;The correlation between treatment/block means permits to assess whether the treatment/block effects on the two traits are correlated. In this case, while we are not allowed to conclude that yield and TKW are, in general, correlated, we can conclude that the genotypes with a high level of yield also show a high level of TKW.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Whenever we have data from designed field experiments, our correlation analyses should never be limited to the calculation of the ‘naive’ correlation coefficient between the observed values. This may not be meaningful! On the contrary, our interpretation should be mainly focused on the correlation between residuals and on the correlation between the effects of treatments/blocks.&lt;/p&gt;
&lt;p&gt;An elegant and advanced method to perform sound correlation analyses on data from designed field experiments has been put forward by Piepho (2018), within the frame of mixed models. Such an approach will be described in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bland, J.M., Altman, D.G., 1994. Statistics Notes: Correlation, regression, and repeated data. BMJ 308, 896–896.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Drowning in a glass of water: variance-covariance and correlation matrices</title>
      <link>http://localhost:4321/2019/stat_general_correlationcovariance/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlationcovariance/</guid>
      <description>


&lt;p&gt;One of the easiest tasks in R is to get correlations between each pair of variables in a dataset. As an example, let’s take the first four columns in the ‘mtcars’ dataset, that is available within R. Getting the variances-covariances and the correlations is straightforward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
matr &amp;lt;- mtcars[,1:4]

#Covariances
cov(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlations
cor(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s really a piece of cake! Unfortunately, a few days ago I had a covariance matrix without the original dataset and I wanted the corresponding correlation matrix. Although this is an easy task as well, at first I was stuck, because I could not find an immediate solution… So I started wondering how I could make it.&lt;/p&gt;
&lt;p&gt;Indeed, having the two variables X and Y, their covariance is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cov(X, Y) = \sum\limits_{i=1}^{n} {(X_i - \hat{X})(Y_i - \hat{Y})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}\)&lt;/span&gt; are the means for each variable. The correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cor(X, Y) = \frac{cov(X, Y)}{\sigma_x \sigma_y} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y\)&lt;/span&gt; are the standard deviations for X and Y.&lt;/p&gt;
&lt;p&gt;The opposite relationship is clear:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ cov(X, Y) = cor(X, Y) \sigma_x \sigma_y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, converting from covariance to correlation is pretty easy. For example, take the covariance between ‘cyl’ and ‘mpg’ above (-9.172379), the correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-633.097208 / (sqrt(36.324103) * sqrt(15360.7998))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8475514&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the reverse, if we have the correlation (-0.8521620), the covariance is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.8475514 * sqrt(36.324103) * sqrt(15360.7998)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -633.0972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My covariance matrix was pretty large, so I started wondering how I could perform this task altogether. What I had to do was to take each element in the covariance matrix and divide it by the square root of the diagonal elements in the same column and in the same row (see below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lu4yxa.db.files.1drv.com/y4mZ-7ZQc0LCMyDG3kqC_0_bzMZYyEpb37ug_I616tXoPNL_DbILLSOa8HujEZCekvRNeeYsfwtrYP-0T_PfzlOUqUNliHdKU3sDLHwBnr5C4jF-U-u1QkOlWg3ZbQXKJw4TM2VrQIQqjh-Pb-5cOEY49q-3pfnt4ZYJUAYZIBhW4GgJ0svrEEAnKQZfNTs2LW5iZhGyYFYVKFT2Y1O7SjKjA?width=637&amp;amp;height=156&amp;amp;cropmode=none&#34; style=&#34;width:95.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is easily done by matrix multiplication. I need a square matrix where the standard deviations for each variable are repeated along the rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V &amp;lt;- cov(matr)
SM1 &amp;lt;- matrix(rep(sqrt(diag(V)), 4), 4, 4)
SM1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]
## [1,]   6.026948   6.026948   6.026948   6.026948
## [2,]   1.785922   1.785922   1.785922   1.785922
## [3,] 123.938694 123.938694 123.938694 123.938694
## [4,]  68.562868  68.562868  68.562868  68.562868&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and another one where they are repeated along the columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SM2 &amp;lt;- matrix(rep(sqrt(diag(V)), each = 4), 4, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can take my covariance matrix (V) and simply multiply these three matrices as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V * 1/SM1 * 1/SM2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, there is not even the need to use ‘rep’ when we create SM1, as R will recycle the elements as needed.&lt;/p&gt;
&lt;p&gt;Going from correlation to covariance can be done similarly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- cor(matr)
R / (1/SM1 * 1/SM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an easy task, but it got me stuck for a few minutes…&lt;/p&gt;
&lt;p&gt;Lately, I finally discovered that there is (at least) one function in R taking care of the above task; it is the ‘cov2cor()’ function in the ‘nlme’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
cov2cor(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is really easy to get drown in a glass of water!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>http://localhost:4321/2019/stat_general_correlation_alookat/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:4321/2019/stat_general_correlation_alookat/</guid>
      <description>


&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-for-the-pearson-correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions for the Pearson Correlation Coefficient&lt;/h1&gt;
&lt;p&gt;The Pearson correlation coefficients makes a few assumptions, which should be carefully checked.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interval-level measurement. Both variables should be measured on a quantitative scale.&lt;/li&gt;
&lt;li&gt;Random sampling. Each subject in the sample should contribute one value on X, and one value on Y. The values for both variables should represent a random sample drawn from the population of interest.&lt;/li&gt;
&lt;li&gt;Linearity. The relationship between X and Y should be linear.&lt;/li&gt;
&lt;li&gt;Bivarlate normal distribution. This means that (i) values of X should form a normal distribution at each value of Y and (ii) values of Y should form a normal distribution at each value of X.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;It is possible to test whether &lt;span class=&#34;math inline&#34;&gt;\(r = 0\)&lt;/span&gt; against the alternative $ r $. The test is based on the idea that the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}\]&lt;/span&gt;
is distributed as a Student’s t variable.&lt;/p&gt;
&lt;p&gt;Let’s take the two variables ‘cyl’ and ‘mpg’ from the ‘mtcars’ data frame. The correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- cor(mtcars$cyl, mtcars$gear)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.4926866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The T statistic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- r * sqrt(32 - 2) / sqrt(1 - r^2)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.101051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value for the null is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * pt(T, 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004173297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is clearly highly significant. The null can be rejected.&lt;/p&gt;
&lt;p&gt;As for hypothesis testing, it should be considered that the individuals where couple of measurements were taken should be independent. If they are not, the t test is invalid. I am dealing with this aspect somewhere else in my blog.&lt;/p&gt;
&lt;p&gt;#Correlation in R&lt;/p&gt;
&lt;p&gt;We have already seen that we can use the usual function ‘cor(matrix, method=)’. In order to obtain the significance, we can use the ‘rcorr()’ function in the Hmisc package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlations with significance levels
library(Hmisc)
corr2 &amp;lt;- rcorr(as.matrix(mtcars), type=&amp;quot;pearson&amp;quot;)
print(corr2$r, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg   cyl  disp    hp   drat    wt   qsec    vs     am  gear   carb
## mpg   1.00 -0.85 -0.85 -0.78  0.681 -0.87  0.419  0.66  0.600  0.48 -0.551
## cyl  -0.85  1.00  0.90  0.83 -0.700  0.78 -0.591 -0.81 -0.523 -0.49  0.527
## disp -0.85  0.90  1.00  0.79 -0.710  0.89 -0.434 -0.71 -0.591 -0.56  0.395
## hp   -0.78  0.83  0.79  1.00 -0.449  0.66 -0.708 -0.72 -0.243 -0.13  0.750
## drat  0.68 -0.70 -0.71 -0.45  1.000 -0.71  0.091  0.44  0.713  0.70 -0.091
## wt   -0.87  0.78  0.89  0.66 -0.712  1.00 -0.175 -0.55 -0.692 -0.58  0.428
## qsec  0.42 -0.59 -0.43 -0.71  0.091 -0.17  1.000  0.74 -0.230 -0.21 -0.656
## vs    0.66 -0.81 -0.71 -0.72  0.440 -0.55  0.745  1.00  0.168  0.21 -0.570
## am    0.60 -0.52 -0.59 -0.24  0.713 -0.69 -0.230  0.17  1.000  0.79  0.058
## gear  0.48 -0.49 -0.56 -0.13  0.700 -0.58 -0.213  0.21  0.794  1.00  0.274
## carb -0.55  0.53  0.39  0.75 -0.091  0.43 -0.656 -0.57  0.058  0.27  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(corr2$P, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mpg     cyl    disp      hp    drat      wt    qsec      vs      am
## mpg       NA 6.1e-10 9.4e-10 1.8e-07 1.8e-05 1.3e-10 1.7e-02 3.4e-05 2.9e-04
## cyl  6.1e-10      NA 1.8e-12 3.5e-09 8.2e-06 1.2e-07 3.7e-04 1.8e-08 2.2e-03
## disp 9.4e-10 1.8e-12      NA 7.1e-08 5.3e-06 1.2e-11 1.3e-02 5.2e-06 3.7e-04
## hp   1.8e-07 3.5e-09 7.1e-08      NA 1.0e-02 4.1e-05 5.8e-06 2.9e-06 1.8e-01
## drat 1.8e-05 8.2e-06 5.3e-06 1.0e-02      NA 4.8e-06 6.2e-01 1.2e-02 4.7e-06
## wt   1.3e-10 1.2e-07 1.2e-11 4.1e-05 4.8e-06      NA 3.4e-01 9.8e-04 1.1e-05
## qsec 1.7e-02 3.7e-04 1.3e-02 5.8e-06 6.2e-01 3.4e-01      NA 1.0e-06 2.1e-01
## vs   3.4e-05 1.8e-08 5.2e-06 2.9e-06 1.2e-02 9.8e-04 1.0e-06      NA 3.6e-01
## am   2.9e-04 2.2e-03 3.7e-04 1.8e-01 4.7e-06 1.1e-05 2.1e-01 3.6e-01      NA
## gear 5.4e-03 4.2e-03 9.6e-04 4.9e-01 8.4e-06 4.6e-04 2.4e-01 2.6e-01 5.8e-08
## carb 1.1e-03 1.9e-03 2.5e-02 7.8e-07 6.2e-01 1.5e-02 4.5e-05 6.7e-04 7.5e-01
##         gear    carb
## mpg  5.4e-03 1.1e-03
## cyl  4.2e-03 1.9e-03
## disp 9.6e-04 2.5e-02
## hp   4.9e-01 7.8e-07
## drat 8.4e-06 6.2e-01
## wt   4.6e-04 1.5e-02
## qsec 2.4e-01 4.5e-05
## vs   2.6e-01 6.7e-04
## am   5.8e-08 7.5e-01
## gear      NA 1.3e-01
## carb 1.3e-01      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also use these functions with two matrices, to obtain the correlations of each column in one matrix with each column in the other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation matrix from mtcars
x &amp;lt;- mtcars[1:3]
y &amp;lt;- mtcars[4:6]
cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              hp       drat         wt
## mpg  -0.7761684  0.6811719 -0.8676594
## cyl   0.8324475 -0.6999381  0.7824958
## disp  0.7909486 -0.7102139  0.8879799&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Relationship to slope in linear regression&lt;/p&gt;
&lt;p&gt;The correlation coefficient and slope in linear regression bear some similarities, as both describe how Y changes when X is changed. However, in correlation, we have two random variables, while in regression we have Y random, X fixed and Y is regarded as a function of X (not the other way round).&lt;/p&gt;
&lt;p&gt;Without neglecting their different meaning, it may be useful to show the algebraic relationship between the correlation coefficient and the slope in regression. Let’s simulate a dataset with two variables, coming from a multivariate normal distribution, with means respectively equal to 10 and 2, and variance-covariance matrix of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
cov &amp;lt;- matrix(c(2.20, 0.48, 0.48, 0.20), 2, 2)
cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] 2.20 0.48
## [2,] 0.48 0.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the ‘mvrnomr()’ function to generate the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
dataset &amp;lt;- data.frame( mvrnorm(n=10, mu = c(10, 2), Sigma = cov) )
names(dataset) &amp;lt;- c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;)
dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X        Y
## 1  11.756647 2.547203
## 2   9.522180 2.199740
## 3   8.341254 1.862362
## 4  13.480005 2.772031
## 5   9.428296 1.573435
## 6   9.242788 1.861756
## 7  10.817449 2.343918
## 8  10.749047 2.451999
## 9  10.780400 2.436263
## 10 11.480301 1.590436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficient and slope are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- with(dataset, cor(X, Y))
b1 &amp;lt;- coef( lm(Y ~ X, data=dataset) )[2]
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equation for the slope is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 = \frac{ \sum_{i = 1}^n \left[ ( X-\mu_X )( Y-\mu_Y )\right] }{ \sigma^2_X } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From there, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r = b_1 \frac{\sigma_X}{ \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ b_1 = r \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- with(dataset, sd(X) )
sigmaY &amp;lt;- with(dataset, sd(Y) )
b1 * sigmaX / sigmaY &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r * sigmaY / sigmaX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also easy to see that the correlation coefficient is the slope of regression of standardised Y against standardised X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yst &amp;lt;- with(dataset, scale(Y, scale=T) )
summary( lm(Yst ~ I(scale(X, scale = T) ), data = dataset) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Yst ~ I(scale(X, scale = T)), data = dataset)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082006 -0.067143 -0.036850  0.009214  0.237923 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)            1.009e-16  3.478e-02   0.000   1.0000  
## I(scale(X, scale = T)) 1.785e-01  7.633e-02   2.339   0.0475 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.11 on 8 degrees of freedom
## Multiple R-squared:  0.4061,	Adjusted R-squared:  0.3319 
## F-statistic: 5.471 on 1 and 8 DF,  p-value: 0.04748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Intra-class correlation (ICC)&lt;/p&gt;
&lt;p&gt;It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intra-class correlation coefficient is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IC = {\displaystyle {\frac {\sigma _{\alpha }^{2}}{\sigma _{\alpha }^{2}+\sigma _{\varepsilon }^{2}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\alpha }^{2}\)&lt;/span&gt; is the variance between groups and &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\varepsilon }^{2}\)&lt;/span&gt; is the variance within a group (better, the variance of one observation within a group). The sum of those two variances is the total variance of observations. In words, the intra-class correlation coefficient measures the joint variability of subjects in the same group (that relates on how groups are different from one another), with respect to the total variability of observations. If subjects in one group are very similar to one another (small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\varepsilon}\)&lt;/span&gt;) but groups are very different (high &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha}\)&lt;/span&gt;), the ICC is very high.&lt;/p&gt;
&lt;p&gt;The existence of grouping of residuals is very important in ANOVA, as it means that independence is violated, which calls for the use of mixed models.&lt;/p&gt;
&lt;p&gt;But … this is a totally different story …&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>