<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Variance on Fixing the bridge between biologists and statisticians</title>
    <link>http://localhost:6346/tags/variance/</link>
    <description>Recent content in Variance on Fixing the bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2023, @AndreaOnofri</copyright>
    <lastBuildDate>Fri, 09 Nov 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://localhost:6346/tags/variance/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sample variance and population variance: which of the two?</title>
      <link>http://localhost:6346/2018/stat_general_variancesamplepop/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:6346/2018/stat_general_variancesamplepop/</guid>
      <description>


&lt;p&gt;Teaching experimental methodology in agriculture related master courses poses some peculiar problems. One of these is to explain the difference between sample variance and population variance. For the students it is usually easy to grasp the idea that, being the mean the ‘center’ of the dataset, it is relevant to measure the average distance to the mean for all individuals in the dataset. Of course, we need to take the sum of squared distances, otherwise negative and positive residuals cancel each other out.&lt;/p&gt;
&lt;p&gt;It is also very intuitive that the average of squared residuals (mean square or variance) is calculated by using the following expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - \mu )^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, this is not what R (and other software) does. R divides by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; and, at this stage, the students usually ask: “&lt;em&gt;why are you telling me that I have to divide the sum of squares by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;? I have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals, not &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;!&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;You may think that the answer is pretty obvious. I don’t think so. The concept of variance is introduced at the beginning of the course, within the frame of descriptive statistics. At this stage, the students know nothing about probability distributions, sampling, inference and all the related concepts. They do not even know anything about degrees of freedom, yet!&lt;/p&gt;
&lt;p&gt;If I were a mathematician and were facing students in math/stat, I could show the class how clever I am by giving a formal proof that &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; is the correct denominator in most circumstances. In particular, this is true when we have a sample taken from a population and we want to infere the variance of the population by using the sample. The formal proof can be found in most stat books and I am attaching it at the end of this post. However, I will never show this formal proof to my students at this stage. Indeed, such a proof assumes that the students know what a sampling distribution is, what the standard error is and how it is calculated. Furthermore, students in agriculture are usually very reluctant, when it comes to dealing with maths!&lt;/p&gt;
&lt;p&gt;Therefore, I usually refrain from trying to appear more clever than I am. My question: “is it possible to teach the difference between the population variance and the sample variance, without talking about degrees of freedom or other ‘difficult’ concepts?”. Here is how I try to put it.&lt;/p&gt;
&lt;p&gt;Let’s take a big, but finite population, i.e. an hectar of maize plants (roughly 70,000 plants). Let’s imagine that we know the individual yields for all plants in the population. With R we can get those yields by taking random values in the interval from 150 to 250 grams (this is reasonable… students in agriculture know these things pretty well! And, I am not referring to any density distribution… it would be too much at this stage).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
pop &amp;lt;- runif(70000, min = 150, max = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our population (I will not show it… lack of space!). It is a finite population, so we can easily calculate the mean and the variance, i.e. the mean squared distance of all 70,000 individuals to the mean. You see that I am using the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; underneath).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0893&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2 &amp;lt;- sum( ( pop - mean(pop) )^2 )/70000
sigma2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 835.184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now forget about the population. In the field, we would never be able to measure the yield for all the plants in one hectar, due to lack of resources (time and money). Therefore, let’s measure 10 randomly selected plants (one in 7000; samples are often small in agriculture!). We can do this easily in R by sampling the original population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(pop, 10)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 174.0269 239.5843 209.6669 222.3267 160.7107 229.6837 238.1470 202.0382
##  [9] 249.2965 170.6348&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now calculate the mean and the variance for the sample, by using the equation above (i.e. using &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; as the denominator).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 209.6116&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2_s &amp;lt;- sum( ( x - mean(x) )^2 )/10
sigma2_s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 908.6202&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not know the population but we know our sample. As usual, in lack of other information, we conclude that the population should have the same characteristics of our sample. This is usually seen as a reasonable guess; at least the most reasonable one. The procedure we use to make a guess is called ‘estimator’ and the guess in itself is an ‘estimate’. Have we used a good estimator?&lt;/p&gt;
&lt;p&gt;We can’t answer. Unless we repeat the sampling process for a lot of times and see how our estimates behave in the long run. Our estimator is good if, in the long run, it converges on the real value for the population. Let’s check this: we repeatedly take samples of 10 plants from our population and calculate the mean and variance as above. We repeat this process 10,000 times, storing the 10,000 means and variances in two vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanS &amp;lt;- c(); varS &amp;lt;- c()
for(i in 1:10000){
  x &amp;lt;- sample(pop, 10)
  meanS[i] &amp;lt;- mean(x)
  varS[i] &amp;lt;- sum( ( x - mean(x) )^2 )/10
}
mean(meanS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 199.9096&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 752.4658&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the mean of the means is 199.91. We got really very close to the real mean of the population. Indeed, the mean of the sample is a good (unbiased) estimator of the mean of the population.&lt;/p&gt;
&lt;p&gt;On the other hand, the mean of the variances is 752.47. Please note that this is much smaller than the real variance of the population. It means that if we use the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; at the denominator) to calculate the variance for the sample, we do not have a good estimator of the variance for the whole population. We can now look at the ratio between the variance of the population and our guess in the long run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(pop)/mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.109945&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that this is roughly equal to &lt;span class=&#34;math inline&#34;&gt;\(10/9\)&lt;/span&gt;. In other words, if we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the sample and calculate the variance by using the equation above,&lt;/li&gt;
&lt;li&gt;multiply by &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;divide by &lt;span class=&#34;math inline&#34;&gt;\(n - 1 = 9\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we can get a good estimator. This leads us to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - m )^2}{n - 1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is exactly the variance for the sample. This one we can easily calculate with R.&lt;/p&gt;
&lt;div id=&#34;which-of-the-two-then&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which of the two, then?&lt;/h1&gt;
&lt;p&gt;Just rememeber:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the variance (mean square) is calculated by dividing the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. This makes sense, because we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals.&lt;/li&gt;
&lt;li&gt;However, if we have a random sample taken from a population, and if our aim is to estimate the variance for the whole population, we need to divide the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;. Otherwise, we get an understimation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-formal-proof-look-how-clever-i-am&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The formal proof (look how clever I am!)&lt;/h1&gt;
&lt;p&gt;We have a population with mean equal to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Let’s take a sample with its mean &lt;span class=&#34;math inline&#34;&gt;\(m \neq \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m - \mu = \varepsilon\)&lt;/span&gt;. For the population, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu )^2  = \sum\limits_{i = 1}^n (X_ i - m + \varepsilon)^2 = \sum\limits_{i = 1}^n ({X_i} - m)^2 + n \varepsilon^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We already see that the sum of squared residuals is higher if we take &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, unless &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon = 0\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m = \mu\)&lt;/span&gt;. From the above, we derive&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  \sum\limits_{i = 1}^n (X_i - \mu )^2  - n ( {m - \mu } )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the first equation above (variance for the population) we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  n \sigma^2 - n( m - \mu  )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that the rightmost term of this equation (&lt;span class=&#34;math inline&#34;&gt;\(n ( m - \mu  )^2\)&lt;/span&gt; is the sum of squared distances for the sampling distribution of the mean. That is &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times the squared standard error. Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } n{\sigma ^2} - n\left( {\frac{{{\sigma ^2}}}{n}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } (n - 1){\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go! Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{\sum\limits_{i = 1}^n (X_i - m)^2 }{n - 1} = {\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>