<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Models are wrong on The broken bridge between biologists and statisticians</title>
    <link>/</link>
    <description>Recent content in Models are wrong on The broken bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2018, @AndreaOnofri</copyright>
    <lastBuildDate>Tue, 02 Jul 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Survival analysis and germination data: an overlooked connection</title>
      <link>/2019/stat_survival_germination/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_survival_germination/</guid>
      <description>


&lt;div id=&#34;the-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The background&lt;/h1&gt;
&lt;p&gt;Seed germination data describe the time until an event of interest occurs. In this sense, they are very similar to survival data, apart from the fact that we deal with a different (and less sad) event: germination instead of death. But, seed germination data are also similar to failure-time data, phenological data, time-to-remission data… the first point is: &lt;strong&gt;germination data are time-to-event data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You may wonder: what’s the matter with time-to-event data? Do they have anything special? With few exceptions, all time-to-event data are affected by a certain form of uncertainty, which takes the name of ‘censoring’. It relates to the fact that the exact time of event may not be precisely know. I think it is good to give an example.&lt;/p&gt;
&lt;p&gt;Let’s take a germination assay, where we put, say, 100 seeds in a Petri dish and make daily inspections. At each inspection, we count the number of germinated seeds. In the end, what have we learnt about the germination time of each seed? It is easy to note that we do not have a precise value, we only have an uncertainty interval. Let’s make three examples.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we found a germinated seed at the first inspection time, we only know that germination took place before the inspection (left-censoring).&lt;/li&gt;
&lt;li&gt;If we find a germinated seed at the second inspection time, we only know that germination took place somewhere between the first and the second inspection (interval-censoring).&lt;/li&gt;
&lt;li&gt;If we find an ungerminated seed at the end of the experiment, we only know that its germination time, if any, is higher than the duration of the experiment (right-censoring).&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;censoring-implies-a-lot-of-uncertainty-which-is-additional-to-other-more-common-sources-of-uncertainty-such-as-the-individual-seed-to-seed-variability-or-random-errors-in-the-manipulation-process.-is-censoring-a-problem-yes-it-is-although-it-is-usually-overlooked-in-seed-science-research.-i-made-this-point-in-a-recent-review-onofri-et-al.-2019-and-i-would-like-to-come-back-to-this-issue-here.-the-second-point-is-that-the-analyses-of-data-from-germination-assays-should-always-account-for-censoring.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Censoring implies a lot of uncertainty, which is additional to other more common sources of uncertainty, such as the individual seed-to-seed variability or random errors in the manipulation process. Is censoring a problem? Yes, it is, although it is usually overlooked in seed science research. I made this point in a recent review (Onofri et al., 2019) and I would like to come back to this issue here. The second point is that &lt;strong&gt;the analyses of data from germination assays should always account for censoring&lt;/strong&gt;.&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analyses-for-germination-assays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data analyses for germination assays&lt;/h1&gt;
&lt;p&gt;A swift search of literature shows that seed scientists are often interested in describing the time-course of germinations, for different plant species, in different environmental conditions. In simple terms, if we take a population of seeds and give it enough water, the individual seeds will start germinating. Their germination times will be different, due to natural seed-to-seed variability and, therefore, the proportion of germinated seeds will progressively and monotonically increase over time. However, this proportion will almost never reach 1, because, there will often be a fraction of seeds that will not germinate in the given conditions, because it is either dormant or nonviable.&lt;/p&gt;
&lt;p&gt;In order to describe this progress to germination, a log-logistic function is often used:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
G(t) = \frac{d}{ 1 + exp \left\{ - b \right[ \log(t) - \log(e) \left] \right\}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the fraction of germinated seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the germinable fraction, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time for the germinable fraction and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The above model is sygmoidally shaped and it is symmetric on a log-time scale. The three parameters are biologically relevant, as they describe the three main features of seed germination, i.e. capability (&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;), speed (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and uniformity (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;my-third-point-in-this-post-is-that-the-process-of-data-analysis-for-germination-data-is-often-based-on-fitting-a-log-logistic-or-similar-model-to-the-observed-counts.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My third point in this post is that &lt;strong&gt;The process of data analysis for germination data is often based on fitting a log-logistic (or similar) model to the observed counts&lt;/strong&gt;.&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-example-a-simulated-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example: a simulated dataset&lt;/h1&gt;
&lt;p&gt;Considering the above, we can simulate the results of a germination assay. Let’s take a 100-seed-sample from a population where we have 85% of germinable seeds (&lt;span class=&#34;math inline&#34;&gt;\(d = 0.85\)&lt;/span&gt;), with a median germination time &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt; days and &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt;. Obviously, this sample will not necessarily reflect the characteristics of the population. We can do this sampling in R, by using a three-steps approach.&lt;/p&gt;
&lt;div id=&#34;step-1-the-ungerminated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: the ungerminated fraction&lt;/h2&gt;
&lt;p&gt;First, let’s simulate the number of germinated seeds, assuming a binomial distribution with a proportion of successes equal to 0.85. We use the random number generator ‘rbinom()’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 1
d &amp;lt;- 0.85
set.seed(1234)
nGerm &amp;lt;- rbinom(1, 100, d)
nGerm
## [1] 89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, in this instance, 89 seeds germinated out of 100, which is not the expected 85%. This is a typical random fluctuation: indeed, we were lucky in selecting a good sample of seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-germination-times-for-the-germinated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: germination times for the germinated fraction&lt;/h2&gt;
&lt;p&gt;Second, let’s simulate the germination times for these 89 germinable seeds, by drawing from a log-logistic distribution with &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt;. To this aim, we use the ‘rllogis()’ function in the ‘actuar’ package (Dutang et al., 2008):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 2
library(actuar)
b &amp;lt;- 1.6; e &amp;lt;- 4.5 
Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))
Gtimes
##   [1]  3.2936708  3.4089762  3.2842199  1.4401630  3.1381457 82.1611955
##   [7]  9.4906364  2.9226745  4.3424551  2.7006042  4.0202158  8.0519663
##  [13]  0.9492013  7.8199588  1.6163588  7.9661485  8.4641154 11.2879041
##  [19]  9.5014360  7.2786264  7.5809838 12.7421713 32.7999661  9.9691944
##  [25]  1.8137333  4.2197542  1.0218849  1.6604417 30.0352308  5.0235265
##  [31]  8.5085067  7.5367739  4.4185382 11.5555259  2.1919263 10.6509339
##  [37]  8.6857151  0.2185902  1.8377033  3.9362727  3.0864702  7.3804164
##  [43]  3.2978782  7.0100360  4.4775843  2.8328842  4.6721090  9.1258796
##  [49]  2.1485568 21.8749808  7.4265984  2.5148724  4.4491466 13.1132301
##  [55]  4.4559642  4.5684584  2.2556488 11.8783556  1.5338755  1.4106592
##  [61] 31.8419420  7.2666641 65.0154287  9.2798476  2.5988399  7.4612907
##  [67]  4.4048509 27.7439121  3.8257187 15.4967751  1.1960785 62.5152642
##  [73]  2.0169970 19.1134899  4.2891084  6.0420938 22.6521417  7.1946293
##  [79]  2.9028993  0.9241876  4.8277336 13.8068124  4.0273655 10.8651761
##  [85]  1.1509735  5.9593534  7.4009589 12.6839405  1.1698335        Inf
##  [91]        Inf        Inf        Inf        Inf        Inf        Inf
##  [97]        Inf        Inf        Inf        Inf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we have a vector hosting 100 germination times (‘Gtimes’). Please, note that I added 11 infinite germination times, to represent non-germinable seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-counts-of-germinated-seeds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: counts of germinated seeds&lt;/h2&gt;
&lt;p&gt;Unfortunately, due to the monitoring schedule, we cannot observe the exact germination time for each single seed in the sample; we can only count the seeds which have germinated between two assessment times. Therefore, as the third step, we simulate the observed counts, by assuming daily monitoring for 40 days.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obsT &amp;lt;- seq(1, 40, by=1) #Observation schedule
count &amp;lt;- table( cut(Gtimes, breaks = c(0, obsT)) )
count
## 
##   (0,1]   (1,2]   (2,3]   (3,4]   (4,5]   (5,6]   (6,7]   (7,8]   (8,9] 
##       3      11      10       8      13       2       1      12       4 
##  (9,10] (10,11] (11,12] (12,13] (13,14] (14,15] (15,16] (16,17] (17,18] 
##       5       2       3       2       2       0       1       0       0 
## (18,19] (19,20] (20,21] (21,22] (22,23] (23,24] (24,25] (25,26] (26,27] 
##       0       1       0       1       1       0       0       0       0 
## (27,28] (28,29] (29,30] (30,31] (31,32] (32,33] (33,34] (34,35] (35,36] 
##       1       0       0       1       1       1       0       0       0 
## (36,37] (37,38] (38,39] (39,40] 
##       0       0       0       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, e.g., 11 germinated seeds were counted at day 2; therefore they germinated between day 1 and day 2 and their real germination time is unknown, but included in the range between 1 and 2 (left-open and right-closed). This is a typical example of interval-censoring (see above).&lt;/p&gt;
&lt;p&gt;We can also see that, in total, we counted 86 germinated seeds and, therefore, 14 seeds were still ungerminated at the end of the assay. For this simulation exercise, we know that 11 seeds were non-germinable and three seeds were germinable, but were not allowed enough time to germinate (look at the table above: there are 3 seeds with germination times higher than 40). In real life, this is another source of uncertainty: we might be able to ascertain whether these 14 seeds are viable or not (e.g. by using a tetrazolium test), but, if they are viable, we would never be able to tell whether they are dormant or their germination time is simply longer than the duration of the assay. In real life, we can only reach an uncertain conclusion: the germination time of the 14 ungerminated seeds is comprised between 40 days to infinity; this is an example of right-censoring.&lt;/p&gt;
&lt;p&gt;The above uncertainty affects our capability of describing the germination time-course from the observed data. We can try to picture the situation in the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_Germination_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-real-germination-time-course-the-red-one-the-blue-one-something-in-between-we-cannot-really-say-this-from-our-dataset-we-are-uncertain.-the-grey-areas-represent-the-uncertainty-due-to-censoring.-do-you-think-that-we-can-reasonably-neglect-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the real germination time-course? The red one? The blue one? Something in between? We cannot really say this from our dataset, we are uncertain. The grey areas represent the uncertainty due to censoring. Do you think that we can reasonably neglect it?&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;The question is: how do we fit a log-logistic function to this dataset? We can either neglect censoring or account for it. Let’s see the differences.&lt;/p&gt;
&lt;div id=&#34;ignoring-censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ignoring censoring&lt;/h2&gt;
&lt;p&gt;We have seen that the time-corse of germination can be described by using a log-logistic cumulative distribution function. Therefore, seed scientists are used to re-organising their data, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
propCum &amp;lt;- cumsum(counts)/100
df &amp;lt;- data.frame(time = obsT, counts = counts, propCum = propCum) 
df
##    time counts propCum
## 1     1      3    0.03
## 2     2     11    0.14
## 3     3     10    0.24
## 4     4      8    0.32
## 5     5     13    0.45
## 6     6      2    0.47
## 7     7      1    0.48
## 8     8     12    0.60
## 9     9      4    0.64
## 10   10      5    0.69
## 11   11      2    0.71
## 12   12      3    0.74
## 13   13      2    0.76
## 14   14      2    0.78
## 15   15      0    0.78
## 16   16      1    0.79
## 17   17      0    0.79
## 18   18      0    0.79
## 19   19      0    0.79
## 20   20      1    0.80
## 21   21      0    0.80
## 22   22      1    0.81
## 23   23      1    0.82
## 24   24      0    0.82
## 25   25      0    0.82
## 26   26      0    0.82
## 27   27      0    0.82
## 28   28      1    0.83
## 29   29      0    0.83
## 30   30      0    0.83
## 31   31      1    0.84
## 32   32      1    0.85
## 33   33      1    0.86
## 34   34      0    0.86
## 35   35      0    0.86
## 36   36      0    0.86
## 37   37      0    0.86
## 38   38      0    0.86
## 39   39      0    0.86
## 40   40      0    0.86&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In practice, seed scientists often use the observed counts to determine the cumulative proportion (or percentage) of germinated seeds. The cumulative proportion (‘propCum’) is used as the response variable, while the observation time (‘time’) is used as the independent variable and a log-logistic function is fitted by non-linear least squares regression. &lt;strong&gt;Hope that you can clearly see that, by doing so, we totally neglect the grey areas in the figure above, we only look at the observed points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can fit a nonlinear regression model by using the ‘drm’ function, in the ‘drc’ package (Ritz et al., 2015). The argument ‘fct’ is used to set the fitted function to log-logistic with three parameters (the equation above).&lt;/p&gt;
&lt;p&gt;The ‘plot()’ and ‘summary()’ methods can be used to plot a graph and to retrieve the estimated parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
mod &amp;lt;- drm(propCum ~ time, data = df,
           fct = LL.3() )
plot(mod, log = &amp;quot;&amp;quot;,
      xlab = &amp;quot;Time (days)&amp;quot;,
      ylab = &amp;quot;Proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_Germination_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.8497771  0.0702626 -26.327 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.8768793  0.0070126 125.044 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.2691575  0.1020457  51.635 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.01762168 (37 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that our estimates are very close to the real values (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; = 1.85 vs. 1.6; &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; = 5.27 vs. 4.5; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; = 0.88 vs. 0.86) and we also see that standard errors are rather small (the coefficient of variability goes from 1 to 4%). There is a difference in sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which relates to the fact that the ‘LL.3()’ function in ‘drc’ removes the minus sign in the equation above. Please, disregard this aspect, which stems from the fact that the ‘drc’ package is rooted in pesticide bioassays.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting-for-censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accounting for censoring&lt;/h2&gt;
&lt;p&gt;So far, we have totally neglected censoring. But, how can we account for it? The answer is simple: as with survival studies, we should use time-to-event methods, which are specifically devised to incorporate the uncertainty due to censoring. In medicine, the body of time-to-event methods goes under the name of survival analysis, which explains the title of my post. My colleagues and I have extensively talked about these methods in two of our recent papers and related appendices (Onofri et al., 2019; Onofri et al., 2018). Therefore, I will not go into detail, now.&lt;/p&gt;
&lt;p&gt;I will just say that time-to-event methods directly consider the observed counts as the response variable. As the independent variable, they consider the extremes of each time interval (‘timeBef’ and ‘timeAf’; see below). In contrast to nonlinear regression, we do not need to transform the observed counts into cumulative proportions, as we did before. Furthermore, by using an interval as the independent variable, we inject into the model the uncertainty due to censoring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(timeBef = c(0, obsT), timeAf = c(obsT, Inf), counts = c(as.numeric(counts), 100 - sum(counts)) )
df
##    timeBef timeAf counts
## 1        0      1      3
## 2        1      2     11
## 3        2      3     10
## 4        3      4      8
## 5        4      5     13
## 6        5      6      2
## 7        6      7      1
## 8        7      8     12
## 9        8      9      4
## 10       9     10      5
## 11      10     11      2
## 12      11     12      3
## 13      12     13      2
## 14      13     14      2
## 15      14     15      0
## 16      15     16      1
## 17      16     17      0
## 18      17     18      0
## 19      18     19      0
## 20      19     20      1
## 21      20     21      0
## 22      21     22      1
## 23      22     23      1
## 24      23     24      0
## 25      24     25      0
## 26      25     26      0
## 27      26     27      0
## 28      27     28      1
## 29      28     29      0
## 30      29     30      0
## 31      30     31      1
## 32      31     32      1
## 33      32     33      1
## 34      33     34      0
## 35      34     35      0
## 36      35     36      0
## 37      36     37      0
## 38      37     38      0
## 39      38     39      0
## 40      39     40      0
## 41      40    Inf     14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time-to-event models can be easily fitted by using the same function as we used above (the ‘drm()’ function in the ‘drc’ package). However, there are some important differences in the model call. The first one relate to model definition: a nonlinear regression model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CumulativeProportion ~ timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other hand, a time-to-event model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Count ~ timeBef + timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A second difference is that we need to explicitly set the argument ‘type’, to ‘event’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Time-to-event model
modTE &amp;lt;- drm(counts ~ timeBef + timeAf, data = df, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
summary(modTE)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.826006   0.194579 -9.3844 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.881476   0.036928 23.8701 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.302109   0.565273  9.3797 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;with-respect-to-the-nonlinear-regression-fit-the-estimates-from-a-time-to-event-fit-are-very-similar-but-the-standard-errors-are-much-higher-the-coefficient-of-variability-now-goes-from-4-to-11.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;With respect to the nonlinear regression fit, the estimates from a time-to-event fit are very similar, but the standard errors are much higher (the coefficient of variability now goes from 4 to 11%).&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neglecting-or-accounting-for-censoring&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neglecting or accounting for censoring?&lt;/h1&gt;
&lt;p&gt;You may wonder which of the two analysis is right and which is wrong. We cannot say this from just one dataset. However, we can repeat the Monte Carlo simulation above to extract 1000 samples, fit the model by using the two methods and retrieve parameter estimates and standard errors for each sample and method. We do this by using the code below (please, be patient… it may take some time).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GermSampling &amp;lt;- function(nSeeds, timeLast, stepOss, e, b, d){
    
    #Draw a sample as above
    nGerm &amp;lt;- rbinom(1, nSeeds, d)
    Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
    Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))

    
    #Generate the observed data
    obsT &amp;lt;- seq(1, timeLast, by=stepOss) 
    counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
    propCum &amp;lt;- cumsum(counts)/nSeeds
    timeBef &amp;lt;- c(0, obsT)
    timeAf &amp;lt;- c(obsT, Inf)
    counts &amp;lt;- c(counts, nSeeds - sum(counts))
    
    #Calculate the T50 with two methods
    mod &amp;lt;- drm(propCum ~ obsT, fct = LL.3() )
    modTE &amp;lt;- drm(counts ~ timeBef + timeAf, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
    c(b1 = summary(mod)[[3]][1,1],
      ESb1 = summary(mod)[[3]][1,2],
      b2 = summary(modTE)[[3]][1,1],
      ESb2 = summary(modTE)[[3]][1,2],
      d1 = summary(mod)[[3]][2,1],
      ESd1 = summary(mod)[[3]][2,2],
      d2 = summary(modTE)[[3]][2,1],
      ESd2 = summary(modTE)[[3]][2,2],
      e1 = summary(mod)[[3]][3,1],
      ESe1 = summary(mod)[[3]][3,2],
      e2 = summary(modTE)[[3]][3,1],
      ESe2 = summary(modTE)[[3]][3,2] )
}

set.seed(1234)
result &amp;lt;- data.frame()
for (i in 1:1000) {
  res &amp;lt;- GermSampling(100, 40, 1, 4.5, 1.6, 0.85)
  result &amp;lt;- rbind(result, res)
} 
names(result) &amp;lt;- c(&amp;quot;b1&amp;quot;, &amp;quot;ESb1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;ESb2&amp;quot;,
                   &amp;quot;d1&amp;quot;, &amp;quot;ESd1&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;ESd2&amp;quot;,
                   &amp;quot;e1&amp;quot;, &amp;quot;ESe1&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;ESe2&amp;quot;)
result &amp;lt;- result[result$d2 &amp;gt; 0,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have stored our results in the data frame ‘result’. The means of estimates obtained for both methods should be equal to the real values that we used for the simulation, which will ensure that estimators are unbiased. The means of standard errors (in brackets, below) should represent the real sample-to-sample variability, which may be obtained from the Monte Carlo standard deviation, i.e. from the standard deviation of the 1000 estimates for each parameter and method.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Nonlinear regression&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.63 (0.051)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.006)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.086)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Time-to-event method&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.62 (0.187)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.579)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Real values&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.60 (0.188)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.593)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;we-clearly-see-that-both-nonlinear-regression-and-the-time-to-event-method-lead-to-unbiased-estimates-of-model-parameters.-however-standard-errors-from-nonlinear-regression-are-severely-biased-and-underestimated.-on-the-contrary-standard-errors-from-time-to-event-method-are-unbiased.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We clearly see that both nonlinear regression and the time-to-event method lead to unbiased estimates of model parameters. However, standard errors from nonlinear regression are severely biased and underestimated. On the contrary, standard errors from time-to-event method are unbiased.&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Censoring is peculiar to germination assays and other time-to-event studies. It may have a strong impact on the reliability of our standard errors and, consequently, on hypotheses testing. Therefore, censoring should never be neglected and time-to-event methods should necessarily be used for data analyses with germination assays. The body of time-to-event methods often goes under the name of ‘survival analysis’, which creates a direct link between survival data and germination data.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;C. Dutang, V. Goulet and M. Pigeon (2008). actuar: An R Package for Actuarial Science. Journal of Statistical Software, vol. 25, no. 7, 1-37.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Paolo Benincasa, M B Mesgaran, and Christian Ritz. 2018. Hydrothermal-Time-to-Event Models for Seed Germination. European Journal of Agronomy 101: 129–39.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Hans Peter Piepho, and Marcin Kozak. 2019. Analysing Censored Data in Agricultural Research: A Review with Examples and Software Tips. Annals of Applied Biology, 174, 3-13.&lt;/li&gt;
&lt;li&gt;Ritz, C., F. Baty, J. C. Streibig, and D. Gerhard. 2015. Dose-Response Analysis Using R. PLOS ONE, 10 (e0146021, 12).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stabilising transformations: how do I present my results?</title>
      <link>/2019/stat_general_reportingresults/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_reportingresults/</guid>
      <description>


&lt;p&gt;ANOVA is routinely used in applied biology for data analyses, although, in some instances, the basic assumptions of normality and homoscedasticity of residuals do not hold. In those instances, most biologists would be inclined to adopt some sort of stabilising transformations (logarithm, square root, arcsin square root…), prior to ANOVA. Yes, there might be more advanced and elegant solutions, but stabilising transformations are suggested in most traditional biometry books, they are very straightforward to apply and they do not require any specific statistical software. I do not think that this traditional technique should be underrated.&lt;/p&gt;
&lt;p&gt;However, the use of stabilising transformations has one remarkable drawback, it may hinder the clarity of results. I’d like to give a simple, but relevant example.&lt;/p&gt;
&lt;div id=&#34;an-example-with-counts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with counts&lt;/h1&gt;
&lt;p&gt;Consider the following dataset, that represents the counts of insects on 15 independent leaves, treated with the insecticides A, B and C (5 replicates):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- structure(data.frame(
  Insecticide = structure(c(1L, 1L, 1L, 1L, 1L, 
    2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L), 
    .Label = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;), class = &amp;quot;factor&amp;quot;), 
  Count = c(448, 906, 484, 477, 634, 211, 276, 
    415, 587, 298, 50, 90, 73, 44, 26)), 
  .Names = c(&amp;quot;Insecticide&amp;quot;, &amp;quot;Count&amp;quot;))
dataset
##    Insecticide Count
## 1            A   448
## 2            A   906
## 3            A   484
## 4            A   477
## 5            A   634
## 6            B   211
## 7            B   276
## 8            B   415
## 9            B   587
## 10           B   298
## 11           C    50
## 12           C    90
## 13           C    73
## 14           C    44
## 15           C    26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We should not expect that a count variable is normally distributed with equal variances. Indeed, a graph of residuals against expected values shows clear signs of heteroscedasticity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lm(Count ~ Insecticide, data=dataset)
plot(mod, which = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_General_reportingResults_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this situation, a logarithmic transformation is often suggested to produce a new normal and homoscedastic dataset. Therefore we take the log-transformed variable and submit it to ANOVA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(log(Count) ~ Insecticide, data=dataset)
print(anova(model), digits=6)
## Analysis of Variance Table
## 
## Response: log(Count)
##             Df   Sum Sq Mean Sq F value     Pr(&amp;gt;F)    
## Insecticide  2 15.82001 7.91000 50.1224 1.4931e-06 ***
## Residuals   12  1.89376 0.15781                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
summary(model)
## 
## Call:
## lm(formula = log(Count) ~ Insecticide, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.6908 -0.1849 -0.1174  0.2777  0.5605 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    6.3431     0.1777  35.704 1.49e-13 ***
## InsecticideB  -0.5286     0.2512  -2.104   0.0572 .  
## InsecticideC  -2.3942     0.2512  -9.529 6.02e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3973 on 12 degrees of freedom
## Multiple R-squared:  0.8931, Adjusted R-squared:  0.8753 
## F-statistic: 50.12 on 2 and 12 DF,  p-value: 1.493e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the standard error for each mean (SEM) corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{0.158/5}\)&lt;/span&gt;. In the end, we might show the following table of means for transformed data:&lt;/p&gt;
&lt;!-- table --&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Insecticide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Means (log n.)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6.343&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.815&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3.985&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;SEM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.178&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- table --&gt;
&lt;p&gt;Unfortunately, we loose clarity: how many insects did we have on each leaf? If we present in our manuscript a table like this one we might be asked by our readers or by the reviewer to report the means on the original measurement unit. What should we do, then? Here are some suggestions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can present the means of the original data with standard deviations. This is clearly less than optimal, if we want to suggest more than the bare variability of the observed sample. Furthermore, &lt;strong&gt;please remember that the means of original data may not be a good measure of central tendency, if the original population is strongly ‘asymmetric’ (skewed)!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;We can show back-transformed means. Accordingly, if we have done, e.g., a logarithmic transformation, we can exponentiate the means of transformed data and report them back to the original measurement unit. Back-transformed means ‘estimate’ the medians of the original populations, which may be regarded as better measures of central tendency for skewed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We suggest that the use of the second method. However, this leaves us with the problem of adding a measure of uncertainty to back-transformed means. No worries, we can use the delta method to back-transform standard errors. It is straightforward:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the first derivative of the back-transform function [in this case the first derivative of exp(X)=exp(X)] and&lt;/li&gt;
&lt;li&gt;multiply it by the standard error of the transformed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This may be simply done by hand, with e.g &lt;span class=&#34;math inline&#34;&gt;\(exp(6.343) \times 0.178 = 101.19\)&lt;/span&gt; (for insecticide A). This ‘manual’ solution is always available, regardless of the statistical software at hand. With R, we can use the ‘emmeans’ package (Lenth, 2016):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emmeans)
countM &amp;lt;- emmeans(model, ~Insecticide, transform = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is enough to set the argument ‘transform’ to ’response, although the transformation must be embedded in the model. It means: it is ok if we coded the model as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log(Count) ~ Insecticide&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the contrary, it fails if we coded the model as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logCount ~ Insecticide&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the transformation was performed prior to fitting.&lt;/p&gt;
&lt;p&gt;Obviously, the back-transformed standard error is different for each mean (there is no homogeneity of variances on the original scale, but we knew this already). Back-transformed data might be presented as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Insecticide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;568.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;101.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;335.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51.88&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9.57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It would be appropriate to state it clearly (e.g. in a footnote), that means and SEs were obtained by back-transformation via the delta method. Far clearer, isn’t it? As I said, there are other solutions, such as fitting a GLM, but stabilising transformations are simple and they are easily acceptable in biological Journals.&lt;/p&gt;
&lt;p&gt;If you want to know something more about the delta-method you might start from &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_thedeltamethod/&#34;&gt;my post here&lt;/a&gt;. A few years ago, some collegues and I have also discussed these issues in a journal paper (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Lenth, R.V., 2016. Least-Squares Means: The R Package lsmeans. Journal of Statistical Software 69. &lt;a href=&#34;https://doi.org/10.18637/jss.v069.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v069.i01&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genotype experiments: fitting a stability variance model with R</title>
      <link>/2019/stat_lmm_stabilityvariance/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_lmm_stabilityvariance/</guid>
      <description>


&lt;p&gt;Yield stability is a fundamental aspect for the selection of crop genotypes. The definition of stability is rather complex (see, for example, Annichiarico, 2002); in simple terms, the yield is stable when it does not change much from one environment to the other. It is an important trait, that helps farmers to maintain a good income in most years.&lt;/p&gt;
&lt;p&gt;Agronomists and plant breeders are continuosly concerned with the assessment of genotype stability; this is accomplished by planning genotype experiments, where a number of genotypes is compared on randomised complete block designs, with three to five replicates. These experiments are repeated in several years and/or several locations, in order to measure how the environment influences yield level and the ranking of genotypes.&lt;/p&gt;
&lt;p&gt;I would like to show an exemplary dataset, referring to a multienvironment experiment with winter wheat. Eight genotypes were compared in seven years in central Italy, on randomised complete block designs with three replicates. The ‘WinterWheat’ dataset is available in the ‘aomisc’ package, which is the accompanying package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset and turns the ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
data(WinterWheat)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 7    35     1   SIMETO  4.99 1996
## 8    33     1    SOLEX  6.08 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that this is a multienvironment experiment as it is repeated in several years: each year is an ‘environment’ in itself. Furthermore, please note that the year effect (i.e. the environment effect) is of random nature: we select the years, but we cannot control the weather conditions.&lt;/p&gt;
&lt;div id=&#34;defining-a-linear-mixed-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defining a linear mixed model&lt;/h1&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the following linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + \gamma_{kj} + g_i + e_j +  ge_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt; is the interaction effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, while &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the residual random term, which is assumed as normally distributed with variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As I said before, the block effect, the environment effect and the ‘genotype x environment’ interaction are usually regarded as random. Therefore, they are assumed as normally distributed, with means equal to 0 and variances respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{\gamma}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{e}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. Indeed, the above model is a Linear Mixed Model (LMM).&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. It is clear that this value is a measure of instability: if it is high, genotypes may respond differently to different environments. In this way, each genotype can be favored in some specific environments and disfavored in some others. Shukla (1974) has suggested that we should allow &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt; assume a different value for each genotype and use these components as a measure of stability (stability variances). According to Shukla, a genotype is considered stable when its stability variance is lower than &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown that stability variances can be obtained within the mixed model framework, by appropriately coding the variance-covariance matrix for random effects. He gave SAS code to accomplish this task and, to me, it was not straightforward to transport his code into R. I finally succeeded and I though I should better share my code, just in case it helps someone save a few headaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-stability-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a stability variance model&lt;/h1&gt;
&lt;p&gt;As we have to model the variance-covariance of random effects, we need to use the ‘lme’ function in the ‘nlme’ package (Pinheiro and Bates, 2000). The problem is that random effects are crossed and they are not easily coded with this package. After an extensive literature search, I could find the solution in the aforementioned book (at pag. 162-163) and in Galecki and Burzykowski (2013). The trick is made by appropriately using the ‘pdMat’ construct (‘pdBlocked’ and ‘pdIdent’). In the code below, I have built a block-diagonal variance-covariance matrix for random effects, where blocks and genotypes are nested within years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdIdent(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix)
## Year = pdIdent(1), pdIdent(Block - 1), pdIdent(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      1.07314201 1.0359257
## Block1           0.01641744 0.1281306
## Block2           0.01641744 0.1281306
## Block3           0.01641744 0.1281306
## GenotypeCOLOSSEO 0.17034091 0.4127238
## GenotypeCRESO    0.17034091 0.4127238
## GenotypeDUILIO   0.17034091 0.4127238
## GenotypeGRAZIA   0.17034091 0.4127238
## GenotypeIRIDE    0.17034091 0.4127238
## GenotypeSANCARLO 0.17034091 0.4127238
## GenotypeSIMETO   0.17034091 0.4127238
## GenotypeSOLEX    0.17034091 0.4127238
## Residual         0.14880400 0.3857512&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wee see that the variance component for the ‘genotype x environment’ interaction is the same for all genotypes and equal to 0.170.&lt;/p&gt;
&lt;p&gt;Allowing for a different variance component per genotype is relatively easy, by replacing ‘pdIdent()’ with ‘pdDiag()’, as shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix2 &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdDiag(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix2)
## Year = pdIdent(1), pdIdent(Block - 1), pdDiag(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      0.86592829 0.9305527
## Block1           0.01641744 0.1281305
## Block2           0.01641744 0.1281305
## Block3           0.01641744 0.1281305
## GenotypeCOLOSSEO 0.10427267 0.3229128
## GenotypeCRESO    0.09588553 0.3096539
## GenotypeDUILIO   0.47612340 0.6900170
## GenotypeGRAZIA   0.15286445 0.3909788
## GenotypeIRIDE    0.11860160 0.3443858
## GenotypeSANCARLO 0.02575029 0.1604690
## GenotypeSIMETO   0.42998504 0.6557324
## GenotypeSOLEX    0.06713590 0.2591060
## Residual         0.14880439 0.3857517&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we have now different variance components and we can classify some genotypes as stable (e.g. Sancarlo, Solex and Creso) and some others as unstable (e.g. Duilio and Simeto).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-the-means&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working with the means&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit a stability variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retreived the means for genotypes in all years, we can fit a stability variance model, although we have to use a different approach, with respect to the one we used with the whole dataset. In this case, we need to model the variance of residuals, introducing within-group (within-year) heteroscedasticity. The ‘weights’ argument can be used, together with the ‘pdIdent()’ variance function, to allow for a different variance for each genotype. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
model.mixM &amp;lt;- lme(Yield ~ Genotype, random = ~ 1|Year, data = WinterWheatM,
                 weights = varIdent(form=~1|Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code to retrieve the within-year variances is not obvious, unfortunately. However, you can use the folllowing snippet as a guidance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vF &amp;lt;- model.mixM$modelStruct$varStruct
sdRatios &amp;lt;- c(1, coef(vF, unconstrained = F))
names(sdRatios)[1] &amp;lt;- &amp;quot;COLOSSEO&amp;quot;
scalePar &amp;lt;- model.mixM$sigma
sigma2i &amp;lt;- (scalePar * sdRatios)^2
sigma2i
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.15387857 0.14548985 0.52571780 0.20246664 0.16820264 0.07535112 
##     SIMETO      SOLEX 
## 0.47958756 0.11673900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code outputs ‘sigma2i’, which does not contain the stability variances. Indeed, we should remove the within-experiment error (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;), which can only be estimated from the whole dataset. Indeed, if we take the estimate of 0.1488 (see code above), divide by three (the number of blocks) and subtract from ‘sigma2i’, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2i - model.mix2$sigma^2/3
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.10427711 0.09588839 0.47611634 0.15286517 0.11860118 0.02574966 
##     SIMETO      SOLEX 
## 0.42998610 0.06713754&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which are the stability variances, as obtained with the analyses of the whole dataset.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Annichiarico, P., 2002. Genotype x Environment Interactions - Challenges and Opportunities for Plant Breeding and Cultivar Recommendations. Plant Production and protection paper, Food &amp;amp; Agriculture Organization of the United Nations (FAO), Roma. variability. Heredity 29, 237–245.&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Shukla, G.K., 1972. Some statistical aspects of partitioning genotype-environmental components of&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Companion R Packages</title>
      <link>/rpackages/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/rpackages/</guid>
      <description>


&lt;p&gt;This blog is supported by a few R packages containing all functions, datasets and other utilities, which are necessary to work through posts, tutorials and books. The packages are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/aomisc&#34;&gt;aomisc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/agriCensData&#34;&gt;AgriCensData&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/drcSeedGerm&#34;&gt;drcSeedGerm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All packages are hosted on gitHub and can be installed from there. To do so, you need the ‘devtools’ package, so, if necessary, install this package first. Next, load this library and use the ‘install_github()’ function to install the three packages. For any problem, please, &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;email me&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)
library(devtools)
install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
install_github(&amp;quot;OnofriAndreaPG/agriCensData&amp;quot;)
install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How do we combine errors, in biology? The delta method</title>
      <link>/2019/stat_general_thedeltamethod/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_thedeltamethod/</guid>
      <description>


&lt;p&gt;In a recent post I have shown that we can build linear combinations of model parameters (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_errorpropagation/&#34;&gt;see here&lt;/a&gt; ). For example, if we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;, we can build a linear combination as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The standard error for this combination can be obtained as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma_Z = \sqrt{ A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In biology, nonlinear transformations are much more frequent than linear transformations. Nonlinear transformations are, e.g., &lt;span class=&#34;math inline&#34;&gt;\(Z = exp(Q + W)\)&lt;/span&gt;, or, &lt;span class=&#34;math inline&#34;&gt;\(Z = 1/(Q + W)\)&lt;/span&gt;. What is the standard error for these nonlinear transformations? This is not a complex problem, but the solution may be beyond biologists with an average level of statistical proficiency. It is named the ‘delta method’ and it provides the so called ‘delta standard errors’. I thought it might be useful to talk about it, by using a very simple language and a few examples.&lt;/p&gt;
&lt;div id=&#34;example-1-getting-the-half-life-of-a-herbicide&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1: getting the half-life of a herbicide&lt;/h1&gt;
&lt;p&gt;A herbicide has proven to follow a first order degradation kinetic in soil, with constant degradation rate &lt;span class=&#34;math inline&#34;&gt;\(k = -0.035\)&lt;/span&gt; and standard error equal to &lt;span class=&#34;math inline&#34;&gt;\(0.00195\)&lt;/span&gt;. What is the half-life (&lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;) of this herbicide and its standard error?&lt;/p&gt;
&lt;p&gt;Every pesticide chemist knows that the half-life (&lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;) is derived by the degradation rate, according to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_{1/2} = \frac{\log(0.5)}{k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, the half-life for our herbicide is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Thalf &amp;lt;- log(0.5)/-0.035
Thalf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19.80421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But … what is the standard error of this half-life? There is some uncertainty around the estimate of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and it is clear that such an uncertainty should propagate to the estimate of &lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;; unfortunately, the transformation is nonlinear and we cannot use the expression given above for linear transformations.&lt;/p&gt;
&lt;div id=&#34;the-basic-idea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The basic idea&lt;/h2&gt;
&lt;p&gt;The basic idea behind the delta method is that most of the simple nonlinear functions, which we use in biology, can be locally approximated by the tangent line through a point of interest. For example, our nonlinear half-life function is &lt;span class=&#34;math inline&#34;&gt;\(Y = \log(0.5)/X\)&lt;/span&gt; and, obviously, we are interested in the point where &lt;span class=&#34;math inline&#34;&gt;\(X = k = -0.035\)&lt;/span&gt;. In the graph below, we have represented our nonlinear function (in black) and its tangent line (in red) through the above point: we can see that the approximation is fairly good in the close vicinity of &lt;span class=&#34;math inline&#34;&gt;\(X = -0.035\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_General_TheDeltaMethod_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the equation of the tangent line? In general, if the nonlinear function is &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, you may remember from high school that the slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; of the tangent line is equal to the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, that is &lt;span class=&#34;math inline&#34;&gt;\(G&amp;#39;(X)\)&lt;/span&gt;. You may also remember that the equation of a line with slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; through the point &lt;span class=&#34;math inline&#34;&gt;\(P(X_1, Y_1)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y - Y_1 = m(X - X_1)\)&lt;/span&gt;. As &lt;span class=&#34;math inline&#34;&gt;\(Y_1 = G(X_1)\)&lt;/span&gt;, the tangent line has equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = G(X_1) + G&amp;#39;(X_1)(X - X_1)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-the-derivative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need the derivative!&lt;/h2&gt;
&lt;p&gt;In order to write the equation of the red line in the Figure above, we need to consider that &lt;span class=&#34;math inline&#34;&gt;\(X_1 = -0.035\)&lt;/span&gt; and we need to be able to calculate the first derivative of our nonlinear half-life fuction. I am not able to derive the expression of the first derivative for all nonlinear functions and it was a relief for me to discover that R can handle this task in simple ways, e.g. by using the function ‘D()’. For our case, it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(log(0.5)/X), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -(log(0.5)/X^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, we can use this R function to calculate the slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; of the tangent line in the figure above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- -0.035
m &amp;lt;- eval( D(expression(log(0.5)/X), &amp;quot;X&amp;quot;) )
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 565.8344&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already know that &lt;span class=&#34;math inline&#34;&gt;\(G(-0.035) = 19.80421\)&lt;/span&gt;. Therefore, we can write the equation of the tangent line (red line in the graph above):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = 19.80421 + 565.8344 \, (X + 0.035)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = 19.80421 + 565.8344 \, X + 565.8344 \cdot 0.035 = 39.60841 + 565.8344 \, X\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;replacing-a-curve-with-a-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Replacing a curve with a line&lt;/h2&gt;
&lt;p&gt;Now, we have two functions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the original nonlinear half-life function &lt;span class=&#34;math inline&#34;&gt;\(Y = \log(0.5)/X\)&lt;/span&gt;$&lt;/li&gt;
&lt;li&gt;a new linear function (&lt;span class=&#34;math inline&#34;&gt;\(Y = 39.60841 + 565.8344 \, X\)&lt;/span&gt;), that is a very close approximation to the previous one, at least near to the point &lt;span class=&#34;math inline&#34;&gt;\(X = -0.035\)&lt;/span&gt;, which we are interested in.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, we can approximate the former with the latter! If we use the linear function, we see that the half-life is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;39.60841 + 565.8344 * -0.035&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19.80421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is what we expected. The advantage is that we can now use the low of propagation of errors to estimate the standard error (see the first and second equation in this post):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma_{ \left[ 39.60841 + 565.8344 \, X \right]} = \sqrt{ 562.8344^2 \, \sigma^2_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt( m^2 * (0.00195 ^ 2) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.103377&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in-general&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In general…&lt;/h2&gt;
&lt;p&gt;If we have a nonlinear transformation &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, the standard error for this transformation is approximated by knowing the first derivative &lt;span class=&#34;math inline&#34;&gt;\(G&amp;#39;(X)\)&lt;/span&gt; and the standard error of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_{G(X)}  \simeq \sqrt{ [G&amp;#39;(X)]^2 \, \sigma^2_X }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-a-back-transformed-count&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2: a back-transformed count&lt;/h1&gt;
&lt;p&gt;A paper reports that the mean number of microorganisms in a substrate, on a logarithmic scale, was &lt;span class=&#34;math inline&#34;&gt;\(X_1 = 5\)&lt;/span&gt; with standard error &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.84\)&lt;/span&gt;. It is easy to derive that the actual number of micro-organisms was &lt;span class=&#34;math inline&#34;&gt;\(\exp{5} = 148.4132\)&lt;/span&gt;; what is the standard error of the back-transformed mean?&lt;/p&gt;
&lt;p&gt;The first derivative of our nonlinear function is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(exp(X)), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## exp(X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and thus the slope of the tangent line is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 5
m &amp;lt;- eval( D(expression(exp(X)), &amp;quot;X&amp;quot;) )
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 148.4132&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the function above, the standard error for the back-transformed mean is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma &amp;lt;- 0.84
sqrt( m^2 * sigma^2 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 124.6671&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3-selenium-concentration-in-olive-drupes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3: Selenium concentration in olive drupes&lt;/h1&gt;
&lt;p&gt;The concentration of selenium in olive drupes was found to be &lt;span class=&#34;math inline&#34;&gt;\(3.1 \, \mu g \,\, g^{-1}\)&lt;/span&gt; with standard error equal to 0.8. What is the intake of selenium when eating one drupe? Please, consider that one drupe weights, on average, 3.4 g (SE = 0.31) and that selenium concentration and drupe weight show a covariance of 0.55.&lt;/p&gt;
&lt;p&gt;The amount of selenium is easily calculated as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 3.1; W = 3.4
X * W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delta standard errors can be obtained by considering the partial derivatives for each of the two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mX &amp;lt;- eval( D(expression(X*W), &amp;quot;X&amp;quot;) )
mW &amp;lt;- eval( D(expression(X*W), &amp;quot;W&amp;quot;) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and combining them as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- 0.8; sigmaW &amp;lt;- 0.31; sigmaXW &amp;lt;- 0.55
sqrt( (mX^2)*sigmaX^2 + (mW^2)*sigmaW^2 + 2*X*W*sigmaXW )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.462726&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For those of you who would like to get involved with matrix notation: we can reach the same result via matrix multiplication (see below). This might be easier when we have more than two variables to combine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;der &amp;lt;- matrix(c(mX, mW), 1, 2)
sigma &amp;lt;- matrix(c(sigmaX^2, sigmaXW, sigmaXW, sigmaW^2), 2, 2, byrow = T)
sqrt( der %*% sigma %*% t(der) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]
## [1,] 4.462726&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-delta-method-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The delta method with R&lt;/h1&gt;
&lt;p&gt;In R there is a shortcut function to calculate delta standard errors, that is available in the ‘car’ package. In order to use it, we need to have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a named vector for the variables that we have to combine&lt;/li&gt;
&lt;li&gt;an expression for the transformation&lt;/li&gt;
&lt;li&gt;a variance-covariance matrix&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the first example, we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;k&amp;quot; = -0.035)
sigma &amp;lt;- matrix(c(0.00195^2), 1, 1)

library(car)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;deltaMethod(object = obj, g=&amp;quot;log(0.5)/k&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Estimate       SE    2.5 %   97.5 %
## log(0.5)/k 19.80421 1.103377 17.64163 21.96678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the second example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;X1&amp;quot; = 5)
sigma &amp;lt;- matrix(c(0.84^2), 1, 1)
deltaMethod(object = obj, g=&amp;quot;exp(X1)&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Estimate       SE     2.5 %   97.5 %
## exp(X1) 148.4132 124.6671 -95.92978 392.7561&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the third example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;X&amp;quot; = 3.1, &amp;quot;W&amp;quot; = 3.4)
sigma &amp;lt;- matrix(c(0.8^2, 0.55, 0.55, 0.31^2), 2, 2, byrow = T)
deltaMethod(object = obj, g=&amp;quot;X * W&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate       SE    2.5 %   97.5 %
## X * W    10.54 4.462726 1.793218 19.28678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function ‘deltaMethod()’ is very handy to be used in connection with model objects, as we do not need to provide anything, but the transformation function. But this is something that requires another post!&lt;/p&gt;
&lt;p&gt;However, two final notes relating to the delta method need to be pointed out here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the delta standard error is always approximate;&lt;/li&gt;
&lt;li&gt;if the original variables are gaussian, the transformed variable, usually, is not gaussian.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationindependence2/</guid>
      <description>


&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;
&lt;div id=&#34;a-routine-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A ‘routine’ experiment&lt;/h1&gt;
&lt;p&gt;I will use the same example as presented &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in my previous post&lt;/a&gt;, which should allow us to compare the results with those obtained by using more traditional methods of data analyses. It is a genotype experiment, laid out in randomised complete blocks, with 27 wheat genotypes and three replicates. For each plot, the collegue who made the experiment recorded several traits, including yield (Yield) and weight of thousand kernels (TKW). The dataset ‘WheatQuality.csv’ is available on ‘gitHub’; it consists of 81 records (plots) and just as many couples of measures in all. The code below loads the necessary packages, the data and transforms the numeric variable ‘Block’ into a factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(plyr)
library(nlme)
library(plyr)
library(asreml)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;In my previous post&lt;/a&gt;, I used the above dataset to calculate the Pearson’s correlation coefficient between Yield and TKW for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;plot measurements,&lt;/li&gt;
&lt;li&gt;residuals,&lt;/li&gt;
&lt;li&gt;treatment/block means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Piepho (2018) showed that, for an experiment like this one, the above correlations can be estimated by coding a multiresponse mixed model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijk} = \mu_i + \beta_{ik} + \tau_{ij} + \epsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ijk}\)&lt;/span&gt; is the response for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the rootstock &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the mean for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{ik}\)&lt;/span&gt; is the effect of the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ij}\)&lt;/span&gt; is the effect of genotype &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; is the residual for each of 81 observations for two traits.&lt;/p&gt;
&lt;p&gt;In the above model, the residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; need to be normally distributed and heteroscedastic, with trait-specific variances. Furthermore, residuals belonging to the same plot (the two observed traits) need to be correlated (correlation of errors).&lt;/p&gt;
&lt;p&gt;Hans-Peter Piepho, in his paper, put forward the idea that the ‘genotype’ and ‘block’ effects for the two traits can be taken as random, even though they might be of fixed nature, especially the genotype effect. This idea makes sense, because, for this application, we are mainly interested in variances and covariances. Both random effects need to be heteroscedastic (trait-specific variance components) and there must be a correlation between the two traits.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge, there is no way to fit such a complex model with the ‘nlme’ package and related ‘lme()’ function (I’ll gave a hint later on, for a simpler model). Therefore, I decided to use the package ‘asreml’ (Butler et al., 2018), although this is not freeware. With the function ‘asreml()’, we need to specify the following components.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The response variables. When we set a bivariate model with ‘asreml’, we can ‘cbind()’ Yield and TKW and use the name ‘trait’ to refer to them.&lt;/li&gt;
&lt;li&gt;The fixed model, that only contains the trait effect. The specification is, therefore, ‘cbind(Yield, TKW) ~ trait - 1’. Following Piepho (2018), I removed the intercept, to separately estimate the means for the two traits.&lt;/li&gt;
&lt;li&gt;The random model, that is composed by the interactions ‘genotype x trait’ and ‘block x trait’. For both, I specified a general unstructured variance covariance matrix, so that the traits are heteroscedastic and correlated. Therefore, the random model is ~ Genotype:us(trait) + Block:us(trait).&lt;/li&gt;
&lt;li&gt;The residual structure, where the observations in the same plot (the term ‘units’ is used in ‘asreml’ to represent the observational units, i.e. the plots) are heteroscedastic and correlated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait, init = c(3.7, -0.25, 1.7)) + 
          Block:us(trait, init = c(77, 38, 53)),
        residual = ~ units:us(trait, init = c(6, 0.16, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:27 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -641.556           1.0    160 17:33:27    0.0 (3 restrained)
##  2      -548.767           1.0    160 17:33:27    0.0
##  3      -448.970           1.0    160 17:33:27    0.0
##  4      -376.952           1.0    160 17:33:27    0.0
##  5      -334.100           1.0    160 17:33:27    0.0
##  6      -317.511           1.0    160 17:33:27    0.0
##  7      -312.242           1.0    160 17:33:27    0.0
##  8      -311.145           1.0    160 17:33:27    0.0
##  9      -311.057           1.0    160 17:33:27    0.0
## 10      -311.056           1.0    160 17:33:27    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   component  std.error    z.ratio
## Block:trait!trait_Yield:Yield     3.7104778  3.9364268  0.9426005
## Block:trait!trait_TKW:Yield      -0.2428390  1.9074544 -0.1273105
## Block:trait!trait_TKW:TKW         1.6684568  1.8343662  0.9095549
## Genotype:trait!trait_Yield:Yield 77.6346623 22.0956257  3.5135761
## Genotype:trait!trait_TKW:Yield   38.8322972 15.0909109  2.5732242
## Genotype:trait!trait_TKW:TKW     53.8616088 15.3520661  3.5084274
## units:trait!R                     1.0000000         NA         NA
## units:trait!trait_Yield:Yield     6.0939037  1.1951128  5.0990195
## units:trait!trait_TKW:Yield       0.1635551  0.7242690  0.2258209
## units:trait!trait_TKW:TKW         4.4717901  0.8769902  5.0990195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The box above shows the results about the variance-covariance parameters. In order to get the correlations, I specified the necessary combinations of variance-covariance parameters. It is necessary to remember that estimates, in ‘asreml’, are named as V1, V2, … Vn, according to their ordering in model output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parms &amp;lt;- mod.asreml$vparameters
vpredict(mod.asreml, rb ~ V2 / (sqrt(V1)*sqrt(V3) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate        SE
## rb -0.09759916 0.7571335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, rt ~ V5 / (sqrt(V4)*sqrt(V6) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate       SE
## rt 0.6005174 0.130663&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, re ~ V9 / (sqrt(V8)*sqrt(V10) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate        SE
## re 0.03133109 0.1385389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the estimates are very close to those obtained by using the Pearson’s correlation coefficients (see my previous post). The advantage of this mixed model solution is that we can also test hypotheses in a relatively reliable way. For example, I tested the hypothesis that residuals are not correlated by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;coding a reduced model where residuals are heteroscedastic and independent, and&lt;/li&gt;
&lt;li&gt;comparing this reduced model with the complete model by way of a REML-based Likelihood Ratio Test (LRT).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Removing the correlation of residuals is easily done, by changing the correlation structure from ‘us’ (unstructured variance-covariance matrix) to ‘idh’ (diagonal variance-covariance matrix).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:us(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:28 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.023           1.0    160 17:33:28    0.0 (2 restrained)
##  2      -383.859           1.0    160 17:33:28    0.0
##  3      -344.687           1.0    160 17:33:28    0.0
##  4      -321.489           1.0    160 17:33:28    0.0
##  5      -312.488           1.0    160 17:33:28    0.0
##  6      -311.167           1.0    160 17:33:28    0.0
##  7      -311.083           1.0    160 17:33:28    0.0
##  8      -311.082           1.0    160 17:33:28    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml2  1      0.05107    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, I tried to further reduce the model to test the significance of the correlation between block means and genotype means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:29 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.027           1.0    160 17:33:29    0.0 (2 restrained)
##  2      -383.866           1.0    160 17:33:29    0.0
##  3      -344.694           1.0    160 17:33:29    0.0
##  4      -321.497           1.0    160 17:33:29    0.0
##  5      -312.496           1.0    160 17:33:29    0.0
##  6      -311.175           1.0    160 17:33:29    0.0
##  7      -311.090           1.0    160 17:33:29    0.0
##  8      -311.090           1.0    160 17:33:29    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml3  2     0.066663    0.6399&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml4 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:idh(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:30 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -406.458           1.0    160 17:33:30    0.0 (2 restrained)
##  2      -394.578           1.0    160 17:33:30    0.0
##  3      -352.769           1.0    160 17:33:30    0.0
##  4      -327.804           1.0    160 17:33:30    0.0
##  5      -318.007           1.0    160 17:33:30    0.0
##  6      -316.616           1.0    160 17:33:30    0.0
##  7      -316.549           1.0    160 17:33:30    0.0
##  8      -316.549           1.0    160 17:33:30    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)   
## mod.asreml/mod.asreml4  3       10.986  0.003364 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that only the genotype means are significantly correlated.&lt;/p&gt;
&lt;p&gt;An alternative (and more useful) way to code the same model is by using the ‘corgh’ structure, instead of ‘us’. These two structures are totally similar, apart from the fact that the first one uses the correlations, instead of the covariances. Another difference, which we should consider when giving starting values, is that correlations come before variances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:31 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1      -632.445           1.0    160 17:33:31    0.0 (3 restrained)
##  2      -539.383           1.0    160 17:33:31    0.0 (2 restrained)
##  3      -468.810           1.0    160 17:33:31    0.0 (1 restrained)
##  4      -422.408           1.0    160 17:33:31    0.0
##  5      -371.304           1.0    160 17:33:31    0.0
##  6      -336.191           1.0    160 17:33:31    0.0
##  7      -317.547           1.0    160 17:33:31    0.0
##  8      -312.105           1.0    160 17:33:31    0.0
##  9      -311.118           1.0    160 17:33:31    0.0
## 10      -311.057           1.0    160 17:33:31    0.0
## 11      -311.056           1.0    160 17:33:31    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09759916  0.7571335
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60051740  0.1306748
## Genotype:trait!trait_Yield                77.63466334 22.0981962
## Genotype:trait!trait_TKW                  53.86160957 15.3536792
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.03133109  0.1385389
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of this parameterisation is that we can test our hypotheses by easily setting up contraints on correlations. One way to do this is to run the model with the argument ‘start.values = T’. In this way I could derive a data frame (‘mod.init$parameters’), with the starting values for REML maximisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the starting values
mod.init &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset, start.values = T)
init &amp;lt;- mod.init$vparameters
init&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                    Component Value Constraint
## 1  Genotype:trait!trait!TKW:!trait!Yield.cor -0.10          U
## 2                 Genotype:trait!trait_Yield  3.80          P
## 3                   Genotype:trait!trait_TKW  1.80          P
## 4     Block:trait!trait!TKW:!trait!Yield.cor  0.60          U
## 5                    Block:trait!trait_Yield 77.00          P
## 6                      Block:trait!trait_TKW 53.00          P
## 7                              units:trait!R  1.00          F
## 8     units:trait!trait!TKW:!trait!Yield.cor  0.03          U
## 9                    units:trait!trait_Yield  6.00          P
## 10                     units:trait!trait_TKW  4.50          P&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘init’ data frame has three columns: (i) names of parameters, (ii) initial values and (iii) type of constraint (U: unconstrained, P = positive, F = fixed). Now, if we take the seventh row (correlation of residuals), set the initial value to 0 and set the third column to ‘F’ (meaning: keep the initial value fixed), we are ready to fit a model without correlation of residuals (same as the ‘model.asreml2’ above). What I had to do was just to pass this data frame as the starting value matrix for a new model fit (see the argument ‘R.param’, below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init2 &amp;lt;- init
init2[8, 2] &amp;lt;- 0
init2[8, 3] &amp;lt;- &amp;quot;F&amp;quot;

mod.asreml.r2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:33 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1136.066           1.0    160 17:33:33    0.0 (1 restrained)
##  2      -939.365           1.0    160 17:33:33    0.0 (1 restrained)
##  3      -719.371           1.0    160 17:33:33    0.0 (1 restrained)
##  4      -550.513           1.0    160 17:33:33    0.0
##  5      -427.355           1.0    160 17:33:33    0.0
##  6      -353.105           1.0    160 17:33:33    0.0
##  7      -323.421           1.0    160 17:33:33    0.0
##  8      -313.616           1.0    160 17:33:33    0.0
##  9      -311.338           1.0    160 17:33:33    0.0
## 10      -311.087           1.0    160 17:33:33    0.0
## 11      -311.082           1.0    160 17:33:33    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r2)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09516456  0.7572040
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60136047  0.1305180
## Genotype:trait!trait_Yield                77.63463977 22.0809306
## Genotype:trait!trait_TKW                  53.86159936 15.3451466
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.00000000         NA
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r2, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)
## mod.asreml.r/mod.asreml.r2  1     0.051075    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is even more interesting is that ‘asreml’ permits to force the parameters to be linear combinations of one another. For instance, we can code a model where the residual correlation is contrained to be equal to the treatment correlation. To do so, we have to set up a two-column matrix (M), with row names matching the component names in the ‘asreml’ parameter vector. The matrix M should contain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the first column, the equality relationships (same number, same value)&lt;/li&gt;
&lt;li&gt;in the second column, the coefficients for the multiplicative relationships&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we would set the matrix M as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;firstCol &amp;lt;- c(1, 2, 3, 4, 5, 6, 7, 1, 8, 9)
secCol &amp;lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
M &amp;lt;- cbind(firstCol, secCol)
dimnames(M)[[1]] &amp;lt;- mod.init$vparameters$Component
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           firstCol secCol
## Genotype:trait!trait!TKW:!trait!Yield.cor        1      1
## Genotype:trait!trait_Yield                       2      1
## Genotype:trait!trait_TKW                         3      1
## Block:trait!trait!TKW:!trait!Yield.cor           4      1
## Block:trait!trait_Yield                          5      1
## Block:trait!trait_TKW                            6      1
## units:trait!R                                    7      1
## units:trait!trait!TKW:!trait!Yield.cor           1      1
## units:trait!trait_Yield                          8      1
## units:trait!trait_TKW                            9      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that in ‘firstCol’, the 1st and 8th element are both equal to 1, which contraints them to assume the same value. We can now pass the matrix M as the value of the ‘vcc’ argument in the model call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init, vcc = M)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Fri May 10 17:33:34 2019
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1122.762           1.0    160 17:33:34    0.0 (1 restrained)
##  2      -900.308           1.0    160 17:33:34    0.0
##  3      -665.864           1.0    160 17:33:34    0.0
##  4      -492.020           1.0    160 17:33:34    0.0
##  5      -383.085           1.0    160 17:33:34    0.0
##  6      -336.519           1.0    160 17:33:34    0.0 (1 restrained)
##  7      -319.561           1.0    160 17:33:34    0.0
##  8      -315.115           1.0    160 17:33:34    0.0
##  9      -314.540           1.0    160 17:33:34    0.0
## 10      -314.523           1.0    160 17:33:34    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r3)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            component  std.error    z.ratio
## Block:trait!trait!TKW:!trait!Yield.cor    -0.1146908  0.7592678 -0.1510545
## Block:trait!trait_Yield                    3.6991785  3.9364622  0.9397216
## Block:trait!trait_TKW                      1.6601799  1.8344090  0.9050217
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.2336876  0.1117699  2.0907919
## Genotype:trait!trait_Yield                70.5970531 19.9293722  3.5423621
## Genotype:trait!trait_TKW                  48.9763800 13.8464106  3.5371174
## units:trait!R                              1.0000000         NA         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.2336876  0.1117699  2.0907919
## units:trait!trait_Yield                    6.3989855  1.2811965  4.9945387
## units:trait!trait_TKW                      4.6952670  0.9400807  4.9945358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r3, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)   
## mod.asreml.r/mod.asreml.r3  1       6.9336   0.00423 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the residual and treatment correlations are equal in this latter model. We also see that this reduced model fits significantly worse than the complete model ‘mod.asreml.r’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-freeware&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going freeware&lt;/h1&gt;
&lt;p&gt;Considering that the block means are not correlated, if we were willing to take the ‘block’ effect as fixed, we could fit the resulting model also with the ‘nlme’ package and the function ‘lme()’ (Pinheiro and Bates, 2000). However, we should cast the model as a univariate model.&lt;/p&gt;
&lt;p&gt;To this aim, the two variables (Yield and TKW) need to be piled up and a new factor (‘Trait’) needs to be introduced to identify the observations for the two traits. Another factor is also necessary to identify the different plots, i.e. the observational units. To perform such a restructuring, I used the ‘melt()’ function in the ‘reshape2’ package Wickham, 2007) and assigned the name ‘Y’ to the response variable, that is now composed by the two original variables Yield and TKW, one on top of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$Plot &amp;lt;- 1:81
mdataset &amp;lt;- melt(dataset[,c(-3,-6)], variable.name= &amp;quot;Trait&amp;quot;, value.name=&amp;quot;Y&amp;quot;, id=c(&amp;quot;Genotype&amp;quot;, &amp;quot;Block&amp;quot;, &amp;quot;Plot&amp;quot;))
mdataset$Block &amp;lt;- factor(mdataset$Block)
head(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait    Y
## 1 arcobaleno     1    1   TKW 44.5
## 2 arcobaleno     2    2   TKW 42.8
## 3 arcobaleno     3    3   TKW 42.7
## 4       baio     1    4   TKW 40.6
## 5       baio     2    5   TKW 42.7
## 6       baio     3    6   TKW 41.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait     Y
## 157  vesuvio     1   76 Yield 54.37
## 158  vesuvio     2   77 Yield 55.02
## 159  vesuvio     3   78 Yield 53.41
## 160 vitromax     1   79 Yield 54.39
## 161 vitromax     2   80 Yield 50.97
## 162 vitromax     3   81 Yield 48.83&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fixed model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y ~ Trait*Block&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to introduce a totally unstructured variance-covariance matrix for the random effect, I used the ‘pdMat’ construct:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;random = list(Genotype = pdSymm(~Trait - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relating to the residuals, heteroscedasticity can be included by using the ‘weight()’ argument and the ‘varIdent’ variance function, which allows a different standard deviation per trait:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weight = varIdent(form = ~1|Trait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also allowed the residuals to be correlated within each plot, by using the ‘correlation’ argument and specifying a general ‘corSymm()’ correlation form. Plots are nested within genotypes, thus I used a nesting operator, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;correlation = corSymm(form = ~1|Genotype/Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Y ~ Trait*Block, 
                 random = list(Genotype = pdSymm(~Trait - 1)),
                 weight = varIdent(form=~1|Trait), 
                 correlation = corCompSymm(form=~1|Genotype/Plot),
                 data = mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retreiving the variance-covariance matrices needs some effort, as the function ‘getVarCov()’ does not appear to work properly with this multistratum model. First of all, we can retreive the variance-covariance matrix for the genotype random effect (G) and the corresponding correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Variance structure for random effects
obj &amp;lt;- mod
G &amp;lt;- matrix( as.numeric(getVarCov(obj)), 2, 2 )
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]
## [1,] 53.86081 38.83246
## [2,] 38.83246 77.63485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.6005237
## [2,] 0.6005237 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we can retreive the ‘conditional’ variance-covariance matrix (R), that describes the correlation of errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conditional variance-covariance matrix (residual error)
V &amp;lt;- corMatrix(obj$modelStruct$corStruct)[[1]] #Correlation for residuals
sds &amp;lt;- 1/varWeights(obj$modelStruct$varStruct)[1:2]
sds &amp;lt;- obj$sigma * sds #Standard deviations for residuals (one per trait)
R &amp;lt;- t(V * sds) * sds #Going from correlation to covariance
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 4.4718234 0.1635375
## [2,] 0.1635375 6.0939251&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]
## [1,] 1.00000000 0.03132756
## [2,] 0.03132756 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total correlation matrix is simply obtained as the sum of G and R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tr &amp;lt;- G + R
cov2cor(Tr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.5579906
## [2,] 0.5579906 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are the same as those obtained by using ‘asreml’. Hope these snippets are useful!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Butler, D., Cullis, B.R., Gilmour, A., Gogel, B., Thomson, R., 2018. ASReml-r reference manual - version 4. UK.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-effects models in s and s-plus. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Wickham, H., 2007. Reshaping data with the reshape package. Journal of Statistical Software 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part I</title>
      <link>/2019/stat_general_correlationindependence1/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationindependence1/</guid>
      <description>


&lt;div id=&#34;observations-are-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observations are grouped&lt;/h1&gt;
&lt;p&gt;When we have recorded two traits in different subjects, we can be interested in describing their joint variability, by using the Pearson’s correlation coefficient. That’s ok, altough we have to respect some basic assumptions (e.g. linearity) that have been detailed elsewhere (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlation_alookat/&#34;&gt;see here&lt;/a&gt;). Problems may arise when we need to test the hypothesis that the correlation coefficient is equal to 0. In this case, we need to make sure that all the couples of observations are taken on independent subjects.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is most often false whenever measures are taken from designed field experiments. In this case, observations may be grouped by one or more treatment/blocking factors. This has been clearly described by Bland and Altman (1994); we would like to give an example that is more closely related to plant/crop science. Think about a genotype experiment, where we compare the behaviour of several genotypes in a randomised blocks design. Usually, we do not only measure yield. We also measure other traits, such as crop height. At the end of the experiment, we might be interested in reporting the correlation between yield and height. How should we proceed? It would seem an easy task, but it is not.&lt;/p&gt;
&lt;p&gt;Let’s assume that we have a randomised blocks design, with 27 genotypes and 3 replicates. For each plot, we recorded two traits, i.e. yield and the weight of thousand kernels (TKW). In the end, we have 81 plots and just as many couples of measures in all. We will use the dataset ‘WheatQuality.csv’, that is available on ‘gitHub’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Hmisc)
library(knitr)
library(plyr)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How many correlations?&lt;/h1&gt;
&lt;p&gt;It may be tempting to consider the whole lot of measures and calculate the correlation coefficient between yield and TKW. This is the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra &amp;lt;- with(dataset, rcorr(Yield, TKW) )
ra$r[1,2] #Correlation coefficient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.540957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra$P[1,2] #P-level&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.850931e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We observe a positive correlation, and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; seems to be significantly different from 0. Therefore, we would be encouraged to conclude that plots with a high value on yield tend to have a high value on TKW, as well. Unfortunately, such a conclusion is not supported by the data.&lt;/p&gt;
&lt;p&gt;Indeed, the test of significance is clearly invalid, as the 81 plots are not independent; they are grouped by block and genotype and we are totally neglecting these two effects. Are we sure that the same correlation holds for all genotypes/blocks? Let’s check this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor &amp;lt;- function(x) cor(x$Yield, x$TKW)
wgCors &amp;lt;- ddply(dataset, ~Genotype, wCor)
wgCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Genotype         V1
## 1   arcobaleno  0.9847228
## 2         baio  0.1611952
## 3      claudio -0.9993872
## 4     colorado  0.9837293
## 5     colosseo  0.4564855
## 6        creso -0.5910193
## 7       duilio -0.9882330
## 8       gianni -0.7603802
## 9       giotto  0.9520211
## 10      grazia  0.4980828
## 11       iride  0.7563338
## 12    meridano  0.1174342
## 13      neodur  0.4805871
## 14      orobel  0.8907754
## 15 pietrafitta -0.9633891
## 16  portobello  0.9210135
## 17   portofino -0.9900764
## 18   portorico  0.1394211
## 19       preco  0.9007067
## 20    quadrato -0.5840238
## 21    sancarlo -0.6460670
## 22      simeto -0.4051779
## 23       solex -0.6066363
## 24 terrabianca -0.4076416
## 25       verdi  0.5801404
## 26     vesuvio -0.7797493
## 27    vitromax -0.8056514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wbCors &amp;lt;- ddply(dataset, ~Block, wCor)
wbCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Block        V1
## 1     1 0.5998137
## 2     2 0.5399990
## 3     3 0.5370398&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for the genotypes, we have 27 correlation coefficients, ranging from -0.999 to 0.985. We only have three couples of measurements per genotype and it is clear that this is not much, to reliably estimate a correlation coefficient. However, it is enough to be suspicious about the extent of correlation between yield and TKW, as it may depend on the genotype.&lt;/p&gt;
&lt;p&gt;On the other hand, the correlation within blocks is more constant, independent on the block and similar to the correlation between plots.&lt;/p&gt;
&lt;p&gt;It may be interesting to get an estimate of the average within-group correlation. To this aim, we can perform two separate ANOVAs (one per trait), including all relevant effects (blocks and genotypes) and calculate the correlation between the residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Yield ~ factor(Block) + Genotype, data = dataset)
mod2 &amp;lt;- lm(TKW ~ factor(Block) + Genotype, data = dataset)
wCor &amp;lt;- rcorr(residuals(mod1), residuals(mod2))
wCor$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x          y
## x 1.00000000 0.03133109
## y 0.03133109 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x         y
## x        NA 0.7812693
## y 0.7812693        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average within-group correlation is very small and unsignificant. Let’s think about this correlation: residuals represent yield and TKW values for all plots, once the effects of blocks and genotypes have been removed. A high correlation of residuals would mean that, letting aside the effects of the block and genotype to which it belongs, a plot with a high value on yield also shows a high value on TKW. The existence of such correlation is clearly unsopported by our dataset.&lt;/p&gt;
&lt;p&gt;As the next step, we could consider the means for genotypes/blocks and see whether they are correlated. Blocks and genotypes are independent and, in principle, significance testing is permitted. However, this is not recommended with block means, as three data are too few to make tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Genotype, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rgPear &amp;lt;- rcorr( as.matrix(means[,2:3]) )
rgPear$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Mu1       Mu2
## Mu1 1.0000000 0.5855966
## Mu2 0.5855966 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgPear$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Mu1        Mu2
## Mu1         NA 0.00133149
## Mu2 0.00133149         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Block, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rbPear &amp;lt;- cor( as.matrix(means[,2:3]) )
rbPear&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Mu1         Mu2
## Mu1  1.00000000 -0.08812544
## Mu2 -0.08812544  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We note that the correlation between genotype means is high and significant. On the contrary, the correlation between block means is near to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-so-what&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And so what?&lt;/h1&gt;
&lt;p&gt;At this stage, you may be confused… Let’s try to clear the fog.&lt;/p&gt;
&lt;p&gt;Obtaining a reliable measure of correlation from designed experiments is not obvious. Indeed, in every designed field experiment we have groups of subjects and there are several possible types of correlation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;correlation between plot measurements&lt;/li&gt;
&lt;li&gt;correlation between the residuals&lt;/li&gt;
&lt;li&gt;correlation between treatment/block means&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All these correlations should be investigated and used for interpretation.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The ‘naive’ correlation between the plot measurements is very easily calculated, but it is grossy misleading. Indeed, it disregards the treatment/block effects and it does not permit hypotheses testing, as the subjects are not independent. In this example, looking at the ‘naive’ correlation coefficient, we would wrongly conclude that plots with high yield also have high TKW, while further analyses show that this is not true, in general. We would reasonably suggest that the ‘naive’ correlation coefficient is never used for interpretation.&lt;/li&gt;
&lt;li&gt;The correlation between the residuals is a reliable measure of joint variation, because the experimental design is adequately referenced, by removing the effects of tratments/blocks. In this example, residuals are not correlated. Further analyses show that the correlation between yield and TKW, if any, may depend on the genotype, while it does not depend on the block.&lt;/li&gt;
&lt;li&gt;The correlation between treatment/block means permits to assess whether the treatment/block effects on the two traits are correlated. In this case, while we are not allowed to conclude that yield and TKW are, in general, correlated, we can conclude that the genotypes with a high level of yield also show a high level of TKW.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Whenever we have data from designed field experiments, our correlation analyses should never be limited to the calculation of the ‘naive’ correlation coefficient between the observed values. This may not be meaningful! On the contrary, our interpretation should be mainly focused on the correlation between residuals and on the correlation between the effects of treatments/blocks.&lt;/p&gt;
&lt;p&gt;An elegant and advanced method to perform sound correlation analyses on data from designed field experiments has been put forward by Piepho (2018), within the frame of mixed models. Such an approach will be described in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bland, J.M., Altman, D.G., 1994. Statistics Notes: Correlation, regression, and repeated data. BMJ 308, 896–896.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Useful links</title>
      <link>/links/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/links/</guid>
      <description>&lt;p&gt;This site makes is written in markdown and Rmarkdown and makes use of a wide range of open source technologies. I&amp;rsquo;m adding a few links, for you to learn more.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;The R project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com&#34;&gt;R studio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com&#34;&gt;R markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown and bookdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gohugo.io&#34;&gt;Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would also like to add a few other links, which I keep an eye on, to have useful hints on R and statistics&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-bloggers.com&#34;&gt;R bloggers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-users.com&#34;&gt;R users: jobs for R-users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How do we combine errors? The linear case</title>
      <link>/2019/stat_general_errorpropagation/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_errorpropagation/</guid>
      <description>


&lt;p&gt;In our research work, we usually fit models to experimental data. Our aim is to estimate some biologically relevant parameters, together with their standard errors. Very often, these parameters are interesting in themselves, as they represent means, differences, rates or other important descriptors. In other cases, we use those estimates to derive further indices, by way of some appropriate calculations. For example, think that we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;: it might be relevant to calculate the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The above operation is named ‘linear combination’; it is a sort of a weighted sum of model parameters. The question is: what is the standard error of Z? I would like to show this by way of a simple biological example.&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;We have measured the germination rate for seeds of &lt;em&gt;Brassica rapa&lt;/em&gt; at six levels of water potential in the substrate (-1, -0.9, -0.8, -0.7, -0.6 and -0.5 MPa). We would like to predict the germination rate for a water potential level of -0.75 MPa.&lt;/p&gt;
&lt;p&gt;Literature references suggest that the relationship between germination rate and water potential in the substrate is linear. Therefore, as the first step, we fit a linear regression model to our observed data. If we are into R, the code to accomplish this task is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GR &amp;lt;- c(0.11, 0.20, 0.34, 0.46, 0.56, 0.68)
Psi &amp;lt;- c(-1, -0.9, -0.8, -0.7, -0.6, -0.5)
lMod &amp;lt;- lm(GR ~ Psi)
summary(lMod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GR ~ Psi)
## 
## Residuals:
##          1          2          3          4          5          6 
##  0.0076190 -0.0180952  0.0061905  0.0104762 -0.0052381 -0.0009524 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.25952    0.02179   57.79 5.37e-07 ***
## Psi          1.15714    0.02833   40.84 2.15e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.01185 on 4 degrees of freedom
## Multiple R-squared:  0.9976, Adjusted R-squared:  0.997 
## F-statistic:  1668 on 1 and 4 DF,  p-value: 2.148e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is clear that we can use the fitted model to calculate the GR-value at -0.75 MPa, as &lt;span class=&#34;math inline&#34;&gt;\(GR = 1.26 - 1.16 \times 0.75 = 0.39\)&lt;/span&gt;. This is indeed a linear combination of model parameters, as we have shown above. Q and W are the estimated model parameters, while &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(B = -0.75\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Of course, the derived response is also an estimate and we need to give a measure of uncertainty. For both model parameters we have standard errors; the question is: how does the uncertainty in parameter estimates propagates to their linear combination? In simpler words: it is easy to build a weightes sum of model parameters, but, how do we make a weighted sum of their standard errors?&lt;/p&gt;
&lt;p&gt;Sokal and Rohlf (1981) at pag. 818 of their book, show that, in general, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{var}(A \, Q + B \, W) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{QW}\)&lt;/span&gt; is the covariance of Q and W. I attach the proof below; it is pretty simple to understand and it is worth the effort. However, several students in biology are rather reluctant when they have to delve into maths. Therefore, I would also like to give an empirical ‘proof’, by using some simple R code.&lt;/p&gt;
&lt;p&gt;Let’s take two samples (Q and W) and combine them by using two coefficients A and B. Let’s calculate the variance for the combination:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q &amp;lt;- c(12, 14, 11, 9)
W &amp;lt;- c(2, 4, 7, 8)
A &amp;lt;- 2
B &amp;lt;- 3
C &amp;lt;- 4
var(A * Q + B * W + C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.58333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A^2 * var(Q) + B^2 * var(W) + 2 * A * B * cov(Q, W)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.58333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results match exactly! In our example, the variance and covariance of estimates are retrieved by using the ‘vcov()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vcov(lMod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)          Psi
## (Intercept) 0.0004749433 0.0006020408
## Psi         0.0006020408 0.0008027211&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2Q &amp;lt;- vcov(lMod)[1,1]
sigma2W &amp;lt;- vcov(lMod)[2,2]
sigmaQW &amp;lt;- vcov(lMod)[1,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error for the prediction is simply obtained as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt( sigma2Q + 0.75^2 * sigma2W - 2 * 0.75 * sigmaQW )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004838667&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-functions-predict-and-glht&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The functions ‘predict()’ and ‘glht()’&lt;/h1&gt;
&lt;p&gt;Now that we have understood the concept, we can use R to make the calculations. For this example, the ‘predict()’ method represents the most logical approach. We only need to pass the model object and the X value which we have to make a prediction for. This latter value needs to be organised as a data frame, with column name(s) matching the name(s) of the X-variable(s) in the original dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(lMod, newdata = data.frame(Psi = -0.75), 
        se.fit = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $fit
##         1 
## 0.3916667 
## 
## $se.fit
## [1] 0.004838667
## 
## $df
## [1] 4
## 
## $residual.scale
## [1] 0.01185227&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from the predict method, there is another function of more general interest, which can be used to build linear combinations of model parameters. It is the ‘glht()’ function in the ‘multcomp’ package. To use this function, we need a model object and we need to organise the coefficients for the transformation in a matrix, with as many rows as there are combinations to calculate. When writing the coefficients, we disregard C: we have seen that every constant value does not affect the variance of the transformation.&lt;/p&gt;
&lt;p&gt;For example, just imagine that we want to predict the GR for two levels of water potential, i.e. -0.75 (as above) and -0.55 MPa. The coefficients (A, B) for the combinations are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Z1 &amp;lt;- c(1, -0.75)
Z2 &amp;lt;- c(1, -0.55)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We pile up the two vectors in one matrix with two rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M &amp;lt;- matrix(c(Z1, Z2), 2, 2, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we pass that matrix to the ‘glht()’ function as an argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
lcombs &amp;lt;- glht(lMod, linfct = M, adjust = &amp;quot;none&amp;quot;)
summary(lcombs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = GR ~ Psi)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## 1 == 0 0.391667   0.004839   80.94 2.30e-07 ***
## 2 == 0 0.623095   0.007451   83.62 2.02e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- single-step method)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above method can be easily extended to other types of linear models and linear combinations of model parameters. The ‘adjust’ argument is useful whenever we want to obtain familywise confidence intervals, which can account for the multiplicity problem. But this is another story…&lt;/p&gt;
&lt;p&gt;Happy work with these functions!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;We know that the variance for a random variable is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ var(Z) = \frac{1}{n-1}\sum \left(Z - \hat{Z}\right)^2 = \\ = \frac{1}{n-1}\sum \left(Z - \frac{1}{n} \sum{Z}\right)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(Z = AQ + BW + C\)&lt;/span&gt;, where A, B and C are the coefficients and Q and W are two random variables, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ var(Z) = \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ \left(AQ + BW + C\right)}\right]^2 = \\ 
= \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ AQ} - \frac{1}{n} \sum{ BW} - \frac{1}{n} \sum{ C}\right]^2 = \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= \frac{1}{n-1}\sum \left[AQ + BW + C - A \hat{Q} - B \hat{W} - C \right]^2 = \\
= \frac{1}{n-1}\sum \left[\left( AQ - A \hat{Q}\right) + \left( BW - B \hat{W}\right) \right]^2 = \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ =\frac{1}{n-1}\sum \left[A \left( Q - \hat{Q}\right) + B \left( W - \hat{W}\right) \right]^2 = \\
= \frac{1}{n-1}\sum \left[A^2 \left( Q - \hat{Q}\right)^2 + B^2 \left( W - \hat{W}\right)^2 + 2AB \left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right] =\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ = A^2 \frac{1}{n-1} \sum{\left( Q - \hat{Q}\right)^2} + B^2 \frac{1}{n-1}\sum \left( W - \hat{W}\right)^2 + 2AB \frac{1}{n-1}\sum{\left[\left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right]}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{var}(Z) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{Q,W}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some everyday data tasks: a few hints with R</title>
      <link>/2019/r_shapingdata/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/r_shapingdata/</guid>
      <description>


&lt;p&gt;We all work with data frames and it is important that we know how we can reshape them, as necessary to meet our needs. I think that there are, at least, four routine tasks that we need to be able to accomplish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;subsetting&lt;/li&gt;
&lt;li&gt;sorting&lt;/li&gt;
&lt;li&gt;casting&lt;/li&gt;
&lt;li&gt;melting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Obviously, there is a wide array of possibilities; I’ll just mention a few, which I regularly use.&lt;/p&gt;
&lt;p&gt;#Subsetting the data&lt;/p&gt;
&lt;p&gt;Subsetting means selecting the records (rows) or the variables (columns) which satisfy certain criteria. Let’s take the ‘students.csv’ dataset, which is available on one of my repositories. It is a database of student’s marks in a series of exams for different subjects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;students &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/students.csv&amp;quot;, header=T)
head(students)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Id  Subject       Date Mark Year HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 2  2 AGRONOMY 08/07/2002   24 2001    AGRARIO
## 3  3 AGRONOMY 24/06/2002   30 2001    AGRARIO
## 4  4 AGRONOMY 24/06/2002   26 2001   CLASSICO
## 5  5 AGRONOMY 23/01/2003   30 2001   CLASSICO
## 6  6 AGRONOMY 09/09/2002   28 2001    AGRARIO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say that we want a new dataset, which contains only the students with marks higher than 28.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;gt;= 28)
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001    CLASSICO
## 3   3 AGRONOMY 24/06/2002   30 2001     AGRARIO
## 5   5 AGRONOMY 23/01/2003   30 2001    CLASSICO
## 6   6 AGRONOMY 09/09/2002   28 2001     AGRARIO
## 11 11 AGRONOMY 09/09/2002   28 2001 SCIENTIFICO
## 17 17 AGRONOMY 10/06/2002   30 2001    CLASSICO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s make it more difficult and extract the records were mark is ranging from 26 to 28 (margins included. Look at the AND clause):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26)
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id  Subject       Date Mark Year  HighSchool
## 4   4 AGRONOMY 24/06/2002   26 2001    CLASSICO
## 6   6 AGRONOMY 09/09/2002   28 2001     AGRARIO
## 7   7 AGRONOMY 24/02/2003   26 2001    CLASSICO
## 8   8 AGRONOMY 09/09/2002   26 2001 SCIENTIFICO
## 10 10 AGRONOMY 08/07/2002   27 2001    CLASSICO
## 11 11 AGRONOMY 09/09/2002   28 2001 SCIENTIFICO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are interested in those students who got a mark ranging from 26 to 28 in MATHS (please note the equality relationship written as ‘==’):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot;)
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Id Subject       Date Mark Year  HighSchool
## 115 115   MATHS 15/07/2002   26 2001     AGRARIO
## 124 124   MATHS 16/09/2002   26 2001 SCIENTIFICO
## 138 138   MATHS 04/02/2002   27 2001    CLASSICO
## 144 144   MATHS 10/02/2003   27 2001    CLASSICO
## 145 145   MATHS 04/07/2003   27 2002    CLASSICO
## 146 146   MATHS 28/02/2002   28 2001     AGRARIO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets’ look for good students either in MATHS or in CHEMISTRY (OR clause; note the ‘|’ operator):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;)
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id   Subject       Date Mark Year HighSchool
## 64 64 CHEMISTRY 18/06/2003   20 2002    AGRARIO
## 65 65 CHEMISTRY 06/06/2002   21 2001   CLASSICO
## 66 66 CHEMISTRY 20/02/2003   21 2002   CLASSICO
## 67 67 CHEMISTRY 20/02/2003   18 2002    AGRARIO
## 68 68 CHEMISTRY 04/06/2002   28 2001      ALTRO
## 69 69 CHEMISTRY 26/06/2002   23 2001 RAGIONERIA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also select columns; for example we may want to display only the ‘Subject’, ‘Mark’ and ‘HighSchool’ columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;,
                  select = c(Subject, Mark, HighSchool))
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Subject Mark HighSchool
## 64 CHEMISTRY   20    AGRARIO
## 65 CHEMISTRY   21   CLASSICO
## 66 CHEMISTRY   21   CLASSICO
## 67 CHEMISTRY   18    AGRARIO
## 68 CHEMISTRY   28      ALTRO
## 69 CHEMISTRY   23 RAGIONERIA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can as well drop the unwanted columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;,
                  select = c(-Id, 
                             -Date,
                             -Year))
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Subject Mark HighSchool
## 64 CHEMISTRY   20    AGRARIO
## 65 CHEMISTRY   21   CLASSICO
## 66 CHEMISTRY   21   CLASSICO
## 67 CHEMISTRY   18    AGRARIO
## 68 CHEMISTRY   28      ALTRO
## 69 CHEMISTRY   23 RAGIONERIA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the function ‘subset()’ is very easy. However, we might have higher flexibility if we subset by using indices. We already know that the notation ‘dataframe[i,j]’ returns the element in the i-th row and j-th column in a data frame. We can of course replace i and j with some subsetting rules. For example, taking the exams where the mark is comprised between 25 and 29 is done as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students[(students$Mark &amp;lt;= 29 &amp;amp; students$Mark &amp;gt;=25),]
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id  Subject       Date Mark Year  HighSchool
## 4   4 AGRONOMY 24/06/2002   26 2001    CLASSICO
## 6   6 AGRONOMY 09/09/2002   28 2001     AGRARIO
## 7   7 AGRONOMY 24/02/2003   26 2001    CLASSICO
## 8   8 AGRONOMY 09/09/2002   26 2001 SCIENTIFICO
## 10 10 AGRONOMY 08/07/2002   27 2001    CLASSICO
## 11 11 AGRONOMY 09/09/2002   28 2001 SCIENTIFICO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful to quickly edit the data. For example, if we want to replace all marks from 25 to 29 with NAs (Not Available), we can simply do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students
subData[(subData$Mark &amp;lt;= 29 &amp;amp; subData$Mark &amp;gt;=25), &amp;quot;Mark&amp;quot;] &amp;lt;- NA
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Id  Subject       Date Mark Year HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 2  2 AGRONOMY 08/07/2002   24 2001    AGRARIO
## 3  3 AGRONOMY 24/06/2002   30 2001    AGRARIO
## 4  4 AGRONOMY 24/06/2002   NA 2001   CLASSICO
## 5  5 AGRONOMY 23/01/2003   30 2001   CLASSICO
## 6  6 AGRONOMY 09/09/2002   NA 2001    AGRARIO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that I created a new dataset to make the replacement, in order not to modify the original dataset. Of course, I can use the ‘is.na()’ function to find the missing data and edit them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData[is.na(subData$Mark) == T, &amp;quot;Mark&amp;quot;] &amp;lt;- 0 
head(subData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Id  Subject       Date Mark Year HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 2  2 AGRONOMY 08/07/2002   24 2001    AGRARIO
## 3  3 AGRONOMY 24/06/2002   30 2001    AGRARIO
## 4  4 AGRONOMY 24/06/2002    0 2001   CLASSICO
## 5  5 AGRONOMY 23/01/2003   30 2001   CLASSICO
## 6  6 AGRONOMY 09/09/2002    0 2001    AGRARIO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Sorting the data&lt;/p&gt;
&lt;p&gt;Sorting is very much like subsetting by indexing. I just need to use the ‘order’ function. For example, let’s sort the students dataset by mark:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(students$Mark), ]
head(sortedData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id   Subject       Date Mark Year  HighSchool
## 51 51   BIOLOGY 01/03/2002   18 2001    CLASSICO
## 67 67 CHEMISTRY 20/02/2003   18 2002     AGRARIO
## 76 76 CHEMISTRY 24/02/2003   18 2002       ALTRO
## 79 79 CHEMISTRY 18/06/2003   18 2002     AGRARIO
## 82 82 CHEMISTRY 18/07/2002   18 2001     AGRARIO
## 83 83 CHEMISTRY 23/01/2003   18 2001 SCIENTIFICO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also sort by decreasing order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark), ]
head(sortedData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id  Subject       Date Mark Year HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 3   3 AGRONOMY 24/06/2002   30 2001    AGRARIO
## 5   5 AGRONOMY 23/01/2003   30 2001   CLASSICO
## 17 17 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 18 18 AGRONOMY 10/06/2002   30 2001    AGRARIO
## 19 19 AGRONOMY 09/09/2002   30 2001    AGRARIO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obviously use multiple keys. For example, let’s sort by subject within marks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark, students$Subject), ]
head(sortedData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Id  Subject       Date Mark Year HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 3   3 AGRONOMY 24/06/2002   30 2001    AGRARIO
## 5   5 AGRONOMY 23/01/2003   30 2001   CLASSICO
## 17 17 AGRONOMY 10/06/2002   30 2001   CLASSICO
## 18 18 AGRONOMY 10/06/2002   30 2001    AGRARIO
## 19 19 AGRONOMY 09/09/2002   30 2001    AGRARIO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I want to sort in decreasing order on a character variable (such as Subject), I need to use the helper function ‘xtfrm()’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark, -xtfrm(students$Subject)), ]
head(sortedData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Id Subject       Date Mark Year  HighSchool
## 116 116   MATHS 01/07/2002   30 2001       ALTRO
## 117 117   MATHS 18/06/2002   30 2001  RAGIONERIA
## 118 118   MATHS 09/07/2002   30 2001     AGRARIO
## 121 121   MATHS 18/06/2002   30 2001  RAGIONERIA
## 123 123   MATHS 09/07/2002   30 2001    CLASSICO
## 130 130   MATHS 07/02/2002   30 2001 SCIENTIFICO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Casting the data&lt;/p&gt;
&lt;p&gt;When we have a dataset in the LONG format, we might be interested in reshaping it in the WIDE format. This is the same as what the ‘pivot table’ function in Excel does. For example, take the ‘rimsulfuron.csv’ dataset in my repository. This contains the results of a randomised block experiment, where we have 16 herbicides in four blocks. The dataset is in the LONG format, with one row per plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rimsulfuron &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/rimsulfuron.csv&amp;quot;, header=T)
head(rimsulfuron)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Herbicide Block Height     Yield
## 1         A    B1  183.4  93.86601
## 2         B    B1  187.0 103.14767
## 3         C    B1  188.4  92.70994
## 4         D    B1  183.2  88.74208
## 5         E    B1  184.2  90.96575
## 6         F    B1  178.8  98.40900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets’put this data frame in the WIDE format, with one row per herbicide and one column per block. To do so, I usually use the ‘cast()’ function in the ‘reshape’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape)
castData &amp;lt;- cast(Herbicide ~ Block, data = rimsulfuron,
     value = &amp;quot;Yield&amp;quot;)
head(castData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Herbicide        B1        B2        B3        B4
## 1         A  93.86601 105.38976  94.13755 111.19865
## 2         B 103.14767  98.48589 106.13508 128.31921
## 3         C  92.70994 106.73288  97.11650 117.76346
## 4         D  88.74208 113.74683 104.51089 126.79076
## 5         E  90.96575 113.00256 104.39530 113.00646
## 6         F  98.40900  99.89433 101.68348  93.90775&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other similar functions are available within the ‘reshape2’ and ‘tidyr’ packages.&lt;/p&gt;
&lt;p&gt;#Melting the data&lt;/p&gt;
&lt;p&gt;In this case we do the reverse: we transform the dataset from WIDE to LONG format. For this task, I like the ‘melt()’ function in the ‘reshape2’ package, which requires a data frame as input. I would like to use the ‘castData’ object which we have just created by using the ‘cast()’ function above. Unfortunately, this object has a ‘cast_df’ class. Therefore, in order to avoid weird results, I need to change ‘castData’ into a data frame, by using the ‘as.data.frame()’ function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
castData &amp;lt;- as.data.frame(castData)
mdati &amp;lt;- melt(castData,
              variable.name = &amp;quot;Block&amp;quot;,
              value.name = &amp;quot;Yield&amp;quot;,
              id=c(&amp;quot;Herbicide&amp;quot;))

head(mdati)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Herbicide Block     Yield
## 1         A    B1  93.86601
## 2         B    B1 103.14767
## 3         C    B1  92.70994
## 4         D    B1  88.74208
## 5         E    B1  90.96575
## 6         F    B1  98.40900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have a nice work with these functions!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Drowning in a glass of water: variance-covariance and correlation matrices</title>
      <link>/2019/stat_general_correlationcovariance/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationcovariance/</guid>
      <description>


&lt;p&gt;One of the easiest tasks in R is to get correlations between each pair of variables in a dataset. As an example, let’s take the first four columns in the ‘mtcars’ dataset, that is available within R. Getting the variances-covariances and the correlations is straightforward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
matr &amp;lt;- mtcars[,1:4]

#Covariances
cov(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlations
cor(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s really a piece of cake! Unfortunately, a few days ago I had a covariance matrix without the original dataset and I wanted the corresponding correlation matrix. Although this is an easy task as well, at first I was stuck, because I could not find an immediate solution… So I started wondering how I could make it.&lt;/p&gt;
&lt;p&gt;Indeed, having the two variables X and Y, their covariance is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cov(X, Y) = \sum\limits_{i=1}^{n} {(X_i - \hat{X})(Y_i - \hat{Y})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}\)&lt;/span&gt; are the means for each variable. The correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cor(X, Y) = \frac{cov(X, Y)}{\sigma_x \sigma_y} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y\)&lt;/span&gt; are the standard deviations for X and Y.&lt;/p&gt;
&lt;p&gt;The opposite relationship is clear:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ cov(X, Y) = cor(X, Y) \sigma_x \sigma_y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, converting from covariance to correlation is pretty easy. For example, take the covariance between ‘cyl’ and ‘mpg’ above (-9.172379), the correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-633.097208 / (sqrt(36.324103) * sqrt(15360.7998))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8475514&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the reverse, if we have the correlation (-0.8521620), the covariance is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.8475514 * sqrt(36.324103) * sqrt(15360.7998)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -633.0972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My covariance matrix was pretty large, so I started wondering how I could perform this task altogether. What I had to do was to take each element in the covariance matrix and divide it by the square root of the diagonal elements in the same column and in the same row (see below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://lu4yxa.db.files.1drv.com/y4mZ-7ZQc0LCMyDG3kqC_0_bzMZYyEpb37ug_I616tXoPNL_DbILLSOa8HujEZCekvRNeeYsfwtrYP-0T_PfzlOUqUNliHdKU3sDLHwBnr5C4jF-U-u1QkOlWg3ZbQXKJw4TM2VrQIQqjh-Pb-5cOEY49q-3pfnt4ZYJUAYZIBhW4GgJ0svrEEAnKQZfNTs2LW5iZhGyYFYVKFT2Y1O7SjKjA?width=637&amp;amp;height=156&amp;amp;cropmode=none&#34; style=&#34;width:95.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This is easily done by matrix multiplication. I need a square matrix where the standard deviations for each variable are repeated along the rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V &amp;lt;- cov(matr)
SM1 &amp;lt;- matrix(rep(sqrt(diag(V)), 4), 4, 4)
SM1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]
## [1,]   6.026948   6.026948   6.026948   6.026948
## [2,]   1.785922   1.785922   1.785922   1.785922
## [3,] 123.938694 123.938694 123.938694 123.938694
## [4,]  68.562868  68.562868  68.562868  68.562868&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and another one where they are repeated along the columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SM2 &amp;lt;- matrix(rep(sqrt(diag(V)), each = 4), 4, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can take my covariance matrix (V) and simply multiply these three matrices as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V * 1/SM1 * 1/SM2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, there is not even the need to use ‘rep’ when we create SM1, as R will recycle the elements as needed.&lt;/p&gt;
&lt;p&gt;Going from correlation to covariance can be done similarly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- cor(matr)
R / (1/SM1 * 1/SM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an easy task, but it got me stuck for a few minutes…&lt;/p&gt;
&lt;p&gt;Lately, I finally discovered that there is (at least) one function in R taking care of the above task; it is the ‘cov2cor()’ function in the ‘nlme’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
cov2cor(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is really easy to get drown in a glass of water!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>/2019/stat_general_correlation_alookat/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlation_alookat/</guid>
      <description>


&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-for-the-pearson-correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions for the Pearson Correlation Coefficient&lt;/h1&gt;
&lt;p&gt;The Pearson correlation coefficients makes a few assumptions, which should be carefully checked.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interval-level measurement. Both variables should be measured on a quantitative scale.&lt;/li&gt;
&lt;li&gt;Random sampling. Each subject in the sample should contribute one value on X, and one value on Y. The values for both variables should represent a random sample drawn from the population of interest.&lt;/li&gt;
&lt;li&gt;Linearity. The relationship between X and Y should be linear.&lt;/li&gt;
&lt;li&gt;Bivarlate normal distribution. This means that (i) values of X should form a normal distribution at each value of Y and (ii) values of Y should form a normal distribution at each value of X.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;It is possible to test whether &lt;span class=&#34;math inline&#34;&gt;\(r = 0\)&lt;/span&gt; against the alternative $ r 0$. The test is based on the idea that the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}\]&lt;/span&gt; is distributed as a Student’s t variable.&lt;/p&gt;
&lt;p&gt;Let’s take the two variables ‘cyl’ and ‘mpg’ from the ‘mtcars’ data frame. The correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- cor(mtcars$cyl, mtcars$gear)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.4926866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The T statistic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- r * sqrt(32 - 2) / sqrt(1 - r^2)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.101051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value for the null is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * pt(T, 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004173297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is clearly highly significant. The null can be rejected.&lt;/p&gt;
&lt;p&gt;As for hypothesis testing, it should be considered that the individuals where couple of measurements were taken should be independent. If they are not, the t test is invalid. I am dealing with this aspect somewhere else in my blog.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation in R&lt;/h1&gt;
&lt;p&gt;We have already seen that we can use the usual function ‘cor(matrix, method=)’. In order to obtain the significance, we can use the ‘rcorr()’ function in the Hmisc package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlations with significance levels
library(Hmisc)
corr2 &amp;lt;- rcorr(as.matrix(mtcars), type=&amp;quot;pearson&amp;quot;)
print(corr2$r, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg   cyl  disp    hp   drat    wt   qsec    vs     am  gear   carb
## mpg   1.00 -0.85 -0.85 -0.78  0.681 -0.87  0.419  0.66  0.600  0.48 -0.551
## cyl  -0.85  1.00  0.90  0.83 -0.700  0.78 -0.591 -0.81 -0.523 -0.49  0.527
## disp -0.85  0.90  1.00  0.79 -0.710  0.89 -0.434 -0.71 -0.591 -0.56  0.395
## hp   -0.78  0.83  0.79  1.00 -0.449  0.66 -0.708 -0.72 -0.243 -0.13  0.750
## drat  0.68 -0.70 -0.71 -0.45  1.000 -0.71  0.091  0.44  0.713  0.70 -0.091
## wt   -0.87  0.78  0.89  0.66 -0.712  1.00 -0.175 -0.55 -0.692 -0.58  0.428
## qsec  0.42 -0.59 -0.43 -0.71  0.091 -0.17  1.000  0.74 -0.230 -0.21 -0.656
## vs    0.66 -0.81 -0.71 -0.72  0.440 -0.55  0.745  1.00  0.168  0.21 -0.570
## am    0.60 -0.52 -0.59 -0.24  0.713 -0.69 -0.230  0.17  1.000  0.79  0.058
## gear  0.48 -0.49 -0.56 -0.13  0.700 -0.58 -0.213  0.21  0.794  1.00  0.274
## carb -0.55  0.53  0.39  0.75 -0.091  0.43 -0.656 -0.57  0.058  0.27  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(corr2$P, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mpg     cyl    disp      hp    drat      wt    qsec      vs
## mpg       NA 6.1e-10 9.4e-10 1.8e-07 1.8e-05 1.3e-10 1.7e-02 3.4e-05
## cyl  6.1e-10      NA 1.8e-12 3.5e-09 8.2e-06 1.2e-07 3.7e-04 1.8e-08
## disp 9.4e-10 1.8e-12      NA 7.1e-08 5.3e-06 1.2e-11 1.3e-02 5.2e-06
## hp   1.8e-07 3.5e-09 7.1e-08      NA 1.0e-02 4.1e-05 5.8e-06 2.9e-06
## drat 1.8e-05 8.2e-06 5.3e-06 1.0e-02      NA 4.8e-06 6.2e-01 1.2e-02
## wt   1.3e-10 1.2e-07 1.2e-11 4.1e-05 4.8e-06      NA 3.4e-01 9.8e-04
## qsec 1.7e-02 3.7e-04 1.3e-02 5.8e-06 6.2e-01 3.4e-01      NA 1.0e-06
## vs   3.4e-05 1.8e-08 5.2e-06 2.9e-06 1.2e-02 9.8e-04 1.0e-06      NA
## am   2.9e-04 2.2e-03 3.7e-04 1.8e-01 4.7e-06 1.1e-05 2.1e-01 3.6e-01
## gear 5.4e-03 4.2e-03 9.6e-04 4.9e-01 8.4e-06 4.6e-04 2.4e-01 2.6e-01
## carb 1.1e-03 1.9e-03 2.5e-02 7.8e-07 6.2e-01 1.5e-02 4.5e-05 6.7e-04
##           am    gear    carb
## mpg  2.9e-04 5.4e-03 1.1e-03
## cyl  2.2e-03 4.2e-03 1.9e-03
## disp 3.7e-04 9.6e-04 2.5e-02
## hp   1.8e-01 4.9e-01 7.8e-07
## drat 4.7e-06 8.4e-06 6.2e-01
## wt   1.1e-05 4.6e-04 1.5e-02
## qsec 2.1e-01 2.4e-01 4.5e-05
## vs   3.6e-01 2.6e-01 6.7e-04
## am        NA 5.8e-08 7.5e-01
## gear 5.8e-08      NA 1.3e-01
## carb 7.5e-01 1.3e-01      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also use these functions with two matrices, to obtain the correlations of each column in one matrix with each column in the other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation matrix from mtcars
x &amp;lt;- mtcars[1:3]
y &amp;lt;- mtcars[4:6]
cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              hp       drat         wt
## mpg  -0.7761684  0.6811719 -0.8676594
## cyl   0.8324475 -0.6999381  0.7824958
## disp  0.7909486 -0.7102139  0.8879799&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-to-slope-in-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Relationship to slope in linear regression&lt;/h1&gt;
&lt;p&gt;The correlation coefficient and slope in linear regression bear some similarities, as both describe how Y changes when X is changed. However, in correlation, we have two random variables, while in regression we have Y random, X fixed and Y is regarded as a function of X (not the other way round).&lt;/p&gt;
&lt;p&gt;Without neglecting their different meaning, it may be useful to show the algebraic relationship between the correlation coefficient and the slope in regression. Let’s simulate a dataset with two variables, coming from a multivariate normal distribution, with means respectively equal to 10 and 2, and variance-covariance matrix of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
cov &amp;lt;- matrix(c(2.20, 0.48, 0.48, 0.20), 2, 2)
cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] 2.20 0.48
## [2,] 0.48 0.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the ‘mvrnomr()’ function to generate the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
dataset &amp;lt;- data.frame( mvrnorm(n=10, mu = c(10, 2), Sigma = cov) )
names(dataset) &amp;lt;- c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;)
dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X        Y
## 1  11.756647 2.547203
## 2   9.522180 2.199740
## 3   8.341254 1.862362
## 4  13.480005 2.772031
## 5   9.428296 1.573435
## 6   9.242788 1.861756
## 7  10.817449 2.343918
## 8  10.749047 2.451999
## 9  10.780400 2.436263
## 10 11.480301 1.590436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficient and slope are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- with(dataset, cor(X, Y))
b1 &amp;lt;- coef( lm(Y ~ X, data=dataset) )[2]
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equation for the slope is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 = \frac{ \sum_{i = 1}^n \left[ ( X-\mu_X )( Y-\mu_Y )\right] }{ \sigma^2_X } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From there, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r = b_1 \frac{\sigma_X}{ \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ b_1 = r \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- with(dataset, sd(X) )
sigmaY &amp;lt;- with(dataset, sd(Y) )
b1 * sigmaX / sigmaY &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r * sigmaY / sigmaX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also easy to see that the correlation coefficient is the slope of regression of standardised Y against standardised X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yst &amp;lt;- with(dataset, scale(Y, scale=T) )
summary( lm(Yst ~ I(scale(X, scale = T) ), data = dataset) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Yst ~ I(scale(X, scale = T)), data = dataset)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082006 -0.067143 -0.036850  0.009214  0.237923 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)            -5.633e-18  3.478e-02   0.000   1.0000  
## I(scale(X, scale = T))  1.785e-01  7.633e-02   2.339   0.0475 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.11 on 8 degrees of freedom
## Multiple R-squared:  0.4061, Adjusted R-squared:  0.3319 
## F-statistic: 5.471 on 1 and 8 DF,  p-value: 0.04748&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intra-class-correlation-icc&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intra-class correlation (ICC)&lt;/h1&gt;
&lt;p&gt;It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intra-class correlation coefficient is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IC = {\displaystyle {\frac {\sigma _{\alpha }^{2}}{\sigma _{\alpha }^{2}+\sigma _{\varepsilon }^{2}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\alpha }^{2}\)&lt;/span&gt; is the variance between groups and &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\varepsilon }^{2}\)&lt;/span&gt; is the variance within a group (better, the variance of one observation within a group). The sum of those two variances is the total variance of observations. In words, the intra-class correlation coefficient measures the joint variability of subjects in the same group (that relates on how groups are different from one another), with respect to the total variability of observations. If subjects in one group are very similar to one another (small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\varepsilon}\)&lt;/span&gt;) but groups are very different (high &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha}\)&lt;/span&gt;), the ICC is very high.&lt;/p&gt;
&lt;p&gt;The existence of grouping of residuals is very important in ANOVA, as it means that independence is violated, which calls for the use of mixed model.&lt;/p&gt;
&lt;p&gt;But … this is a totally different story …&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some useful equations for nonlinear regression in R</title>
      <link>/articles/usefulequations/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/articles/usefulequations/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Very rarely, biological processes follow linear trends. Just think about how a crop grows, or responds to increasing doses of fertilisers/xenobiotics. Or think about how an herbicide degrades in the soil, or about the germination pattern of a seed population. It is very easy to realise that curvilinear trends are far more common than linear trends. Furthermore, asymptotes and/or inflection points are very common in nature. We can be sure: linear equations in biology are just a way to approximate a response over a very narrow range for the independent variable.&lt;/p&gt;
&lt;p&gt;Therefore, as biologists, we need to be able to describe our experimental data by using a wide range of curvilinear equations. We need to be able to ‘read’ those equations and use their parameters to interpret and understand biological processes. I thought that it would be useful to list the most commonly used curvilinear functions and show examples of how they can be fit by using R.&lt;/p&gt;
&lt;p&gt;When it comes to nonlinear regression, I have a strong personal preference for the ‘drc’ package and the ‘drm()’ function therein. However, it is also worth mentioning the traditional ‘nls()’ function in the ‘stats’ package. You may know that nonlinear least squares work iteratively: we need to provide initial guesses for model parameters and the algorithm adjusts them step by step, finally converging on the approximate least squares solution. To my experience, providing initial guesses may be troublesome. Therefore, it is very convenient to use R functions together with the appropriate self-starting routines, which can greatly semplify the fitting process. These self-starters can be found in the ‘drc’, ‘nlme’ and ‘aomisc’ packages.&lt;/p&gt;
&lt;p&gt;Let’s load the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
library(nlme)
library(aomisc)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;curve-shapes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Curve shapes&lt;/h1&gt;
&lt;p&gt;Curves can be easily classified by their shape, which is very helpful to select the correct one, according to the trend of the process under study. We have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomials
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Straight line&lt;/li&gt;
&lt;li&gt;Quadratic polynomial&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Concave/Convex curves (no inflection)
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exponential curve&lt;/li&gt;
&lt;li&gt;Asymptotic regression&lt;/li&gt;
&lt;li&gt;Negative exponential function&lt;/li&gt;
&lt;li&gt;Power curve&lt;/li&gt;
&lt;li&gt;Logarithmic equation&lt;/li&gt;
&lt;li&gt;Rectangular hyperbola&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Sygmoidal curves
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Logistic curve&lt;/li&gt;
&lt;li&gt;Gompertz curve&lt;/li&gt;
&lt;li&gt;Extreme value equation&lt;/li&gt;
&lt;li&gt;Log-logistic curve (Hill equation)&lt;/li&gt;
&lt;li&gt;Weibull-type 1&lt;/li&gt;
&lt;li&gt;Weibull-type 2&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomials&lt;/h1&gt;
&lt;p&gt;Polynomials are the most flexible tool to describe biological processes. They are simple and, although curvilinear, they are linear in the parameters and can be fitted by using linear regression. One disadvantage is that they cannot describe asymptotic processes, which are very common in biology. Furthermore, they are prone to overfitting, as we may be tempted to add terms to improve the fit, with little care for biological realism.&lt;/p&gt;
&lt;p&gt;Nowadays, thanks to the wide availability of nonlinear regression algorithms, the use of polynomials has sensibly decreased; linear or quadratic polynomials are mainly used when we want to approximate the observed response within a narrow range of a quantitative predictor. On the other hand, higher order polynomials are very rarely seen, in practice.&lt;/p&gt;
&lt;div id=&#34;straight-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Straight line&lt;/h2&gt;
&lt;p&gt;Obviously, this is not a curve, although it deserves to be mentioned here. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1 \, X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is the slope, i.e. the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The Y increases as X increases when &lt;span class=&#34;math inline&#34;&gt;\(b_1 &amp;gt; 0\)&lt;/span&gt;, otherwise it decreases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic function&lt;/h2&gt;
&lt;p&gt;The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1\, X + b_2 \, X^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt;, taken separately, lack a clear biological meaning. However, it is interesting to consider that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a + b*X + c*X^2), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## b + c * (2 * X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which measures the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. We see that such an increase/decrease is not constant, but it changes according to the level of X. The stationary point is &lt;span class=&#34;math inline&#34;&gt;\(X_m = - b_1 / 2 b_2\)&lt;/span&gt;; it is a maximum when &lt;span class=&#34;math inline&#34;&gt;\(b_2 &amp;gt; 0\)&lt;/span&gt;, otherwise it is a minimum.&lt;/p&gt;
&lt;p&gt;At the maximum/minimum, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_m = \frac{4\,b_0\,b_2 - b_1^2}{4\,b_2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-fitting-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polynomial fitting in R&lt;/h2&gt;
&lt;p&gt;Polynomials in R are fit by using the linear model function ‘lm()’. Although this is not efficient, in a couple of cases I found myself in the need of fitting a polynomial by using the ‘nls()’ o ‘drm()’ functions. For these unusual cases, one can use the ‘NLS.Linear()’, NLS.poly2(), ‘DRC.Linear()’ and DRC.Poly2() self-starting function, as available in the ‘aomisc’ package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concaveconvex-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concave/Convex curves&lt;/h1&gt;
&lt;div id=&#34;exponential-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential curve&lt;/h2&gt;
&lt;p&gt;The exponential function describes an increasing/decreasing &lt;em&gt;trend&lt;/em&gt;, with constant relative rate. The most common equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  e^{k X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other possible parameterisations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  b^X  =  e^{d + k X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above parameterisations are equivalent, as proved by setting &lt;span class=&#34;math inline&#34;&gt;\(b = e^k\)&lt;/span&gt; e &lt;span class=&#34;math inline&#34;&gt;\(a = e^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  b^X  = a  (e^k)^{X} =  a  e^{kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  e^{kX} = e^d \cdot e^{kX} =  e^{d + kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The meaning of parameters is clear: &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; represents the relative increase/decrease of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit increase of X. &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases if &lt;span class=&#34;math inline&#34;&gt;\(k &amp;gt; 0\)&lt;/span&gt; (exponential growth), while it decreases when &lt;span class=&#34;math inline&#34;&gt;\(k &amp;lt; 0\)&lt;/span&gt; (exponential decay). This curve is used to describe the growth of populations in unlimiting environmental conditions, or to describe the degradation of xenobiotics in the environment (first-order degradation kinetic).&lt;/p&gt;
&lt;p&gt;The exponential function is nonlinear in &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and needs to be fitted by using ‘nls()’ or ‘drm()’. It is possible to make profit of the self-starting routines in ‘NLS.expoGrowth()’, ‘NLS.expoDecay()’, ‘DRC.expoGrowth()’ and ‘DRC.expoDecay()’. All these functions are available in the ‘aomisc’ package. The ‘drc’ package also contains the function ‘EXD.2()’, that fits an exponential decay model, with a slightly different parameterisation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \exp(-x/e) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the same as &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in the model above and &lt;span class=&#34;math inline&#34;&gt;\(e = 1/k\)&lt;/span&gt;. For all the forementioned exponential decay equations &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The function ‘EXD.3()’ in the ‘drc’ package also includes a lower asymptote &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d -c) \exp(-x/e) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(degradation)
degradation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Time    Conc
## 1     0  96.400
## 2    10  46.300
## 3    20  21.200
## 4    30  17.890
## 5    40  10.100
## 6    50   6.900
## 7    60   3.500
## 8    70   1.900
## 9     0 102.300
## 10   10  49.200
## 11   20  26.310
## 12   30  14.220
## 13   40   5.400
## 14   50   3.400
## 15   60   0.500
## 16   70   0.200
## 17    0 101.330
## 18   10  54.890
## 19   20  28.120
## 20   30  13.330
## 21   40   6.110
## 22   50   0.350
## 23   60   2.100
## 24   70   0.922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Conc ~ Time, fct = DRC.expoDecay(),
             data = degradation)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Exponential Decay Model (2 parms)
## 
## Parameter estimates:
## 
##                    Estimate Std. Error t-value   p-value    
## init:(Intercept) 99.6349312  1.4646680  68.026 &amp;lt; 2.2e-16 ***
## k:(Intercept)     0.0670391  0.0019089  35.120 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.621386 (22 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotic regression model&lt;/h2&gt;
&lt;p&gt;The asymptotic regression model describes a limited growth, where Y approaches an horizontal asymptote as X tends to infinity. The rate of growth is maximum at the beginning and approaches 0 as Y approaches the plateau. This equation is used in several different parameterisations and it is also known as Monomolecular Growth, Mitscherlich law or von Bertalanffy law.&lt;/p&gt;
&lt;p&gt;Due to its biological meaning, the most widespread parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a - (a - b) \, \exp (- c  X)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum attainable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is proportional to the relative rate of Y increase while X increases. Indeed, we can see that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a - (a - b) * exp (- c * X)), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (a - b) * (exp(-c * X) * c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;that is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y&amp;#39; = c \, (a - Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model can be fit with R by using the self starter functions ‘NLS.asymReg()’ and DRC.asymReg(), in the ‘aomisc’ package. The ‘drc’ package contains the function AR.3(), that is a similar parameterisation where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation named ‘SSasymp()’, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We simulate an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X &amp;lt;- c(1, 3, 5, 7, 9, 11, 13, 20)
a &amp;lt;- 20; b &amp;lt;- 5; c &amp;lt;- 0.3
Ye &amp;lt;- asymReg.fun(X, a, b, c)
epsilon &amp;lt;- rnorm(8, 0, 0.5)
Y &amp;lt;- Ye + epsilon
model &amp;lt;- drm(Y ~ X, fct = DRC.asymReg())
plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we take the above equation and add the constraint that &lt;span class=&#34;math inline&#34;&gt;\(b = 0\)&lt;/span&gt;, we get the following equation, that is often known as negative exponential equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a [1 -  \exp (- c  X) ]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This equation has a similar shape to the asymptotic regression, but Y is 0 when X is 0 (the curve passes through the origin). It is often used to model the absorbed Photosintetically Active Radiation (&lt;span class=&#34;math inline&#34;&gt;\(Y = PAR_a\)&lt;/span&gt;) as a function of incident PAR (&lt;span class=&#34;math inline&#34;&gt;\(a = PAR_i\)&lt;/span&gt;), Leaf Area Index (X = LAI) and the extinction coefficient (c = k).&lt;/p&gt;
&lt;p&gt;This model can be fit with R by using the self starter functions ‘NLS.negExp()’ and DRC.negExp(), in the ‘aomisc’ package. The ‘drc’ package contains the function AR.2(), where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named ‘SSasympOrig()’, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power curve&lt;/h2&gt;
&lt;p&gt;The power curve is also known as Freundlich equation or allometric function and the most common parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a \, X^b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is perfectly equivalent to an exponential curve on the
logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a\,X^b  = a\, e^{\log( X^b )}  = a\,e^{b \, \log(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve does not have an asymptote for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The slope (first derivative) of the curve is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * X^b), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a * (X^(b - 1) * b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both paraeters relate to the slope of the curve and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates its shape. If &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt;- b &amp;lt; 1\)&lt;/span&gt;, Y increases as X increases and the curve is convex up. This is used, e.g., to model the number of plant species as a function of sampling area (Muller-Dumbois method).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(speciesArea)
speciesArea&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Area numSpecies
## 1    1          4
## 2    2          5
## 3    4          7
## 4    8          8
## 5   16         10
## 6   32         14
## 7   64         19
## 8  128         22
## 9  256         26&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(numSpecies ~ Area, fct = DRC.powerCurve(),
             data = speciesArea)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Power curve (Freundlich equation) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 4.348404   0.337197  12.896 3.917e-06 ***
## b:(Intercept) 0.329770   0.016723  19.719 2.155e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.9588598 (7 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curve(powerCurve.fun(x, coef(model)[1], -coef(model)[2]),
      xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 1\)&lt;/span&gt; is negative, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curve(powerCurve.fun(x, coef(model)[1], 2),
      xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logarithmic-equation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logarithmic equation&lt;/h2&gt;
&lt;p&gt;This is indeed a linear model on log-transformed X:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = a + b \, \log(X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to the logarithmic function, X must be $ &amp;gt; 0$. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates the shape: if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 0\)&lt;/span&gt;, the curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The logarithmic equation can be fit by using ‘lm()’. If necessary, it can also be fit by using ‘nls()’ and ‘drm()’; the self-starting functions ‘NLS.logCurve()’ and ‘DRC.logCurve()’ are available within the ‘aomisc’ package. We show some simulated data as examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#b is positive
set.seed(5678)
X &amp;lt;- c(1,2,4,5,7,12)
a&amp;lt;-2; b&amp;lt;- 0.5
Ye &amp;lt;-  a + b*log(X)
res &amp;lt;- rnorm(6, 0, 0.1)
Y &amp;lt;- Ye + res
model &amp;lt;- lm(Y ~ log(X) )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ log(X))
## 
## Residuals:
##         1         2         3         4         5         6 
## -0.025947  0.013207  0.050827 -0.011989 -0.008408 -0.017690 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.99653    0.02494   80.06 1.46e-07 ***
## log(X)       0.45088    0.01580   28.54 8.97e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.03146 on 4 degrees of freedom
## Multiple R-squared:  0.9951, Adjusted R-squared:  0.9939 
## F-statistic: 814.7 on 1 and 4 DF,  p-value: 8.967e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Linear regression on log-transformed x (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 1.996534   0.024939  80.058 1.459e-07 ***
## b:(Intercept) 0.450883   0.015797  28.543 8.967e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.03145785 (4 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#b is negative
X &amp;lt;- c(1,2,4,5,7,12)
a &amp;lt;- 2; b &amp;lt;- -0.5
Ye &amp;lt;-  a + b*log(X)
res &amp;lt;- rnorm(6, 0, 0.1)
Y &amp;lt;- Ye + res
model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Linear regression on log-transformed x (2 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## a:(Intercept)  2.115759   0.103920 20.3595 3.437e-05 ***
## b:(Intercept) -0.569125   0.065826 -8.6459 0.0009843 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.1310851 (4 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;michaelis-menten-equation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Michaelis-Menten equation&lt;/h2&gt;
&lt;p&gt;This is a rectangular hyperbola, often parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, up to a plateau level. The parameter &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the higher asymptote (for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;), while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the X value giving a response equal to &lt;span class=&#34;math inline&#34;&gt;\(a/2\)&lt;/span&gt;. Indeed, it is easily shown that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a}{2} = \frac{a\,X_{50} } {b + X_{50} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which leads to &lt;span class=&#34;math inline&#34;&gt;\(b = x_{50}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The slope (first derivative) is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression( (a*X) / (b + X) ), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a/(b + X) - (a * X)/(b + X)^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we can see that the initial slope (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) is $i = a/b $.&lt;/p&gt;
&lt;p&gt;In R, this model can be fit by using ‘nls()’ and the self starting functions ‘SSmicmen()’, within the package ‘nlme’. If we prefer a ‘drm()’ fit, we can use the ‘MM.2()’ function in the package ‘drc’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X &amp;lt;- c(3, 5, 7, 22, 28, 39, 46, 200)
a &amp;lt;- 15; b &amp;lt;- 0.5
Ye &amp;lt;- as.numeric( SSmicmen(X, a, b) )
res &amp;lt;- rnorm(8, 0, 0.1)
Y &amp;lt;- Ye + res

#nls fit
model &amp;lt;- nls(Y ~ SSmicmen(X, a, b))
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: Y ~ SSmicmen(X, a, b)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## a 14.97166    0.06057  247.17 2.96e-13 ***
## b  0.50207    0.03345   15.01 5.51e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1196 on 6 degrees of freedom
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 3.043e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#drm fit
model &amp;lt;- drm(Y ~ X, fct = MM.2())
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Michaelis-Menten (2 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## d:(Intercept) 14.971733   0.060492 247.499 2.936e-13 ***
## e:(Intercept)  0.502126   0.033359  15.052 5.418e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.1195748 (6 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The ‘drc’ package also contains the self starting function ‘MM.3()’, where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is allowed to be equal to &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt;, when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-loss-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield-loss curve&lt;/h2&gt;
&lt;p&gt;Weed-crop competition studies make use of a reparameterised Michaelis-Menten model. Indeed, th initial slope of a Michaelis-Menten can be assumed as a measure of competition, that is the reduction in yield (Y) when the first weed is added to the system. Therefore, the Michaelis-Methen model has been reparameterised to include &lt;span class=&#34;math inline&#34;&gt;\(i = a/b\)&lt;/span&gt; as an explicit parameter. The reparameterised equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{i \, X}{1 + \frac{i \, X}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model can be used to describe yield losses as a function of weed density. It can be fit by using the self starting functions ‘NLS.YL()’ or ‘DRC.YL()’ in the ‘aomisc’ package. Usually, competion studies produce yield data and, therefore, yield lossed need to be calculated by using the weed-free yield and the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_L  = \frac{Y_{WF}  - Y_w }{Y_{WF} } \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_W\)&lt;/span&gt; is the observed yield and &lt;span class=&#34;math inline&#34;&gt;\(Y_{WF}\)&lt;/span&gt; is the weed-free yield. We show an example relating to sunflower grown at increasing densities of the weed &lt;em&gt;Sinapis arvensis&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(competition)
Ywf &amp;lt;- mean( competition$Yield[competition$Dens == 0] )
competition$YL &amp;lt;- ( Ywf - competition$Yield ) / Ywf * 100 

#nls fit
model &amp;lt;- nls(YL ~ NLS.YL(Dens, a, i), data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: YL ~ NLS.YL(Dens, a, i)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## a    8.207      1.187   6.914 1.93e-08 ***
## i   75.048      2.353  31.894  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 6.061 on 42 degrees of freedom
## 
## Number of iterations to convergence: 2 
## Achieved convergence tolerance: 2.685e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#drc fit
model &amp;lt;- drm(YL ~ Dens, fct = DRC.YL(), data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Yield-Loss function (Cousens, 1985) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## i:(Intercept)   8.2068     1.1715  7.0056 1.427e-08 ***
## A:(Intercept)  75.0492     2.3298 32.2133 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  6.060578 (42 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above fit constrains the yield loss to be 0 when weed density is 0. This is logical, but, it has the important consequence that the weed-free yield is constrained to be equal to the observed weed-free yield, which is not realistic. Therefore, we can reparameterise the yield-loss function, in order to use the observed yield as the dependent variable.&lt;/p&gt;
&lt;p&gt;Indeed, from the above equation we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_W  = Y_{WF}  - \frac{Y_L }{100}Y_{WF}  = Y_{WF} \left( {1 - \frac{Y_L }{100}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_W  = Y_{WF} \left( 1 - \frac{i\, X}{100 \left( 1 + \frac{i \, X}{a} \right) } \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function can be fit with ‘drm()’, by using the ‘DRC.cousens85()’ self starting function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Yield ~ Dens, fct = DRC.cousens85(), 
             data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Yield-Weed Density function (Cousens, 1985) (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## YWF:(Intercept) 30.47211    0.92763 32.8493 &amp;lt; 2.2e-16 ***
## i:(Intercept)    8.24038    1.36541  6.0351 3.857e-07 ***
## a:(Intercept)   75.07312    2.40366 31.2328 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  1.866311 (41 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sygmoidal-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sygmoidal curves&lt;/h1&gt;
&lt;p&gt;Sygmoidal curves are S-shaped and they may be increasing, decreasing, symmetric or non-simmetric around the inflection point. They are parameterised in countless ways, which may be often confusing. Therefore, we will show a common parameterisation, that is very useful in biological terms.&lt;/p&gt;
&lt;div id=&#34;logistic-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic curve&lt;/h2&gt;
&lt;p&gt;The logistic curve derives from the cumulative logistic distribution function; the curve is symmetric around the inflection point and it it may be parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + exp(b (X - e))}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the higher asymptote, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the lower asymptote, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value producing a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be positive or negative and, consequently, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may increase or decrease as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The above function is known as four-parameter logistic. If necessary, contraints can be put on parameter values, i.e. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; can be constrained to 0 (three-parameter logistic). Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be also contrained to 1 (two-parameter logistic).&lt;/p&gt;
&lt;p&gt;The four- and three-parameter logistic curves can be fit by ‘nls()’, respectively with the self-starting functions ‘SSfpl()’ and ‘SSlogis’ (‘nlme’ package). In these functions, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(scal = -1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With ‘drm()’, we can use the self-starting functions ‘L.4()’ and ‘L.3()’, The ‘L.2()’ function has been included in the ‘aomisc’ package.&lt;/p&gt;
&lt;p&gt;Logistic functions are very useful, e.g., for plant growth studies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(beetGrowth)
beetGrowth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    DAE weightInf weightFree
## 1   21   0.06000  0.0715091
## 2   21   0.06000  0.0662547
## 3   21   0.11000  0.0747931
## 4   27   0.20000  0.3368074
## 5   27   0.20000  0.3952256
## 6   27   0.21000  0.2520960
## 7   38   2.13000  2.3225072
## 8   38   3.03000  1.7163224
## 9   38   1.27000  1.2189231
## 10  49   6.13000 11.7761096
## 11  49   5.76000 13.6191507
## 12  49   7.78000 12.1462931
## 13  65  17.05000 33.1067720
## 14  65  22.48000 24.9648226
## 15  65  12.66000 34.6577561
## 16 186  21.51010 38.8329912
## 17 186  26.25887 27.8375016
## 18 186  27.67733 37.7165427&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(weightFree ~ DAE, fct = L.3(), data = beetGrowth)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Logistic (ED50 as parameter) with lower limit fixed at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.179393   0.039059 -4.5929 0.0003519 ***
## d:(Intercept) 34.532001   1.676430 20.5985 2.057e-12 ***
## e:(Intercept) 52.384788   1.580269 33.1493 1.838e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.970191 (15 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gompertz-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gompertz Curve&lt;/h2&gt;
&lt;p&gt;The Gompertz curve is parameterised in very many ways. We favour a parameterisation that resambles the one used for the logistic function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;were the parameters have the same meaning as those in the logistic function. The difference is that this curve is not symmetric around the inflection point. As for the logistic, we can have a four-, three- and two-parameter Gompertz functions, which can be fit by using ‘drm()’ and, respectively the ‘G.4()’, ‘G.3()’ and ‘G.2()’ sef-starters. The three-parameter Gompertz can also be fit with ‘nls()’, by using the ‘SSGompertz()’ self-starter in the ‘nlme’ package, although this is a different parameterisation.&lt;/p&gt;
&lt;p&gt;We give an example of the different shapes for the logistic (red) and Gompertz (black) functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-type-of-asimmetry&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another type of asimmetry&lt;/h2&gt;
&lt;p&gt;We have seen that, with respect to the logistic, the Gompertz shows a longer lag at the beginning, but raises steadily afterwards. We could describe a different pattern by changing the Gompertz function as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We add to the previous graph this function (in blue), to show how it differs from the logistic and Gompertz.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- 10; c &amp;lt;- 2; e &amp;lt;- 7; b &amp;lt;- - 0.5
curve( G.fun(x, b, c, d, e), xlim = c(0, 20) , xlab=&amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)
curve( L.fun(x, b, c, d, e), add = T, col = &amp;quot;red&amp;quot; )
curve( E.fun(x, b, c, d, e), add = T, col = &amp;quot;blue&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The self-starters for this function are not yet available, at least to the best of my knowledge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-logistic-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log-logistic curve&lt;/h2&gt;
&lt;p&gt;In many applications, the sigmoidal response curve is symmetric on the logarithm of x, which requires a log-logistic curve (a log-normal curve would be practically equivalent, but it is used far less often). For example, in biologic assays (but also in germination assays), the log-logistic curve is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \exp \left\{ b \left[ \log(X) - \log(e) \right] \right\} } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the parameters have the very same meanng as the logistic equationn given above. It is easy to see that the above equation is equivalent to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \left( \frac{X}{e} \right)^b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another possible parameterisation is the so-called Hill function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = \frac{a \, X^b}{ X^b + e^b} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{a \, X^b}{ X^b + e^b} =  \frac{a}{ \frac{X^b}{X^b} + \frac{c^b}{X^b}} = \frac{a}{ 1 + \left( \frac{c}{X} \right)^b} = \frac{a}{ 1 + \left( \frac{c}{X} \right)^b} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Log-logistic functions are used for crop growth, seed germination and bioassay work and they can have the same constraints as the logistic function. The four-parameter logistic is available as ‘LL.4()’ in the ‘drc’ package and as ‘SSfpl()’ in the ‘nlme’ package. This latter function replaces &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(scal = 1/b\)&lt;/span&gt;. Also in ‘drc’, we have ‘LL.3()’ (three-parameter logistic, with &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;) and ‘LL.2()’ (two-parameter logistic, with &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;). In ‘nlme’ we have ‘SSlogis()’, that is a three-parameter logistic with &lt;span class=&#34;math inline&#34;&gt;\(scal = 1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We show an example of a log-logistic fit, relating to a bioassay with &lt;em&gt;Brassica rapa&lt;/em&gt; treated at increasing dosages of an herbicide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)
model &amp;lt;- drm(FW ~ Dose, fct = LL.4(), data = brassica)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Log-logistic (ED50 as parameter) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.45113    0.24113  6.0181 1.743e-06 ***
## c:(Intercept)  0.34948    0.18580  1.8810   0.07041 .  
## d:(Intercept)  4.53636    0.20514 22.1140 &amp;lt; 2.2e-16 ***
## e:(Intercept)  2.46557    0.35111  7.0221 1.228e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4067837 (28 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-curve-type-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull curve (type 2)&lt;/h2&gt;
&lt;p&gt;The type 2 Weibull curve is for the Gompertz curve what the log-logistic curve is for the logistic curve. The equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ b \, (log(X) - log(e)) \right] \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the parameters have the very same meaning as the other sygmoidal curves given above.&lt;/p&gt;
&lt;p&gt;As for fitting, the ‘drc’ package contains the self-starting functions ‘W2.2()’, ‘W2.3()’ and ‘W2.4()’ that can be used to fit respectively the two-, three- and four-parameter type 2 Weibull functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-curve-type-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull curve (type 1)&lt;/h2&gt;
&lt;p&gt;The type 1 Weibull is similar to the type 2 Weibull, but describes a different type of asymmetry (see above):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (log(X) - log(e)) \right] \right\} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(FW ~ Dose, fct = W2.4(), data = brassica)
model2 &amp;lt;- drm(FW ~ Dose, fct = W1.4(), data = brassica)
plot(model)
plot(model2, add=T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About this site</title>
      <link>/aboutthis/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aboutthis/</guid>
      <description>


&lt;div id=&#34;the-broken-bridge-between-biologists-and-statisticians&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The broken bridge between biologists and statisticians&lt;/h1&gt;
&lt;p&gt;Hi, all!&lt;/p&gt;
&lt;p&gt;This website deals with statistic. “What’s new then?”, you might say. “There’s plenty of them, out there!”. Yes, that’s right. However, the new thing is that I am not a statistician. I am an applied biologist who happened to be fascinated by the idea that natural phenomena might be described, interpreted and understood by using the universal language of math. Since then, I’ve been mainly involved with experimental design and data analyses. Indeed, I turned myself into an applied statistician…&lt;/p&gt;
&lt;p&gt;My collegue Marcin Kozak, in one of his papers, noted that, although efficient communication between biologists and statisticians is fundamental for the development of knowledge, there is indeed a problem in this respect (the broken bridge)&lt;span class=&#34;citation&#34;&gt;(Kozak 2016)&lt;/span&gt;. I would like to help build such bridge.&lt;/p&gt;
&lt;p&gt;Indeed, I developed the awareness that biostatistic is a fundamental component of science: it helps reach more reliable conclusions. Now I’m going grey… I would like to pass some of my experience to my younger collegues and help them save a few headaches… Though my language might be rough and overly simplistic, I do hope it is useful to reach the above aim: spread a better awareness of the potential usefulness, beauty and limitations of biostatistic.&lt;/p&gt;
&lt;p&gt;This is an ongoing project. All blog posts are listed under the ‘Home’ directory of this website and they are searchable by date and tag. You will also find a series of tutorials and a web book (in italian), under the list pages of the single sections, i.e., &lt;a href=&#34;/tutorials/&#34;&gt;tutorials&lt;/a&gt; and &lt;a href=&#34;/_statbook/&#34;&gt;book&lt;/a&gt;. If you would like to contribute or comment, please, drop me a note to &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-KozakCommunicationagriculturalscientists2016&#34;&gt;
&lt;p&gt;&lt;span class=&#34;smallcaps&#34;&gt;Kozak, M&lt;/span&gt; (2016) Communication between agricultural scientists and statisticians: A broken bridge? &lt;em&gt;Scientia Agricola&lt;/em&gt; &lt;strong&gt;73&lt;/strong&gt;, 505–511&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tutorials and articles</title>
      <link>/tutorials/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/tutorials/</guid>
      <description>

&lt;h1 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/nonLinearRegression/&#34;&gt;Non-linear regression analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onofriandreapg.github.io/agriCensData/&#34;&gt;Interval-censored data in agriculture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/seedGermination/&#34;&gt;Analysing seed germination data with R: a tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;articles&#34;&gt;Articles&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Blogdow: &lt;a href=&#34;/articles/BlogdownSteps/&#34;&gt;My learning path with blogdown&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;other-articles-italian-language&#34;&gt;Other articles (Italian language)&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/PCA/&#34;&gt;L&amp;rsquo;analisi delle componenti principali&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/CVA/&#34;&gt;L&amp;rsquo;analisi delle variabili canoniche&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>My first experience with blogdown</title>
      <link>/2018/2018-11_15-first-day-with-blogdown/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-11_15-first-day-with-blogdown/</guid>
      <description>&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;

&lt;p&gt;I thought that it might be useful to write down a few notes, to summarise my steps ahead, during the learning process. I do not work with blogdown everyday and I tend to forget things quite easily. Therefore, these notes may help me recap how far I have come. And they might also help other beginners, to speed up their initial steps with such a powerful blogging platform.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll find my notes &lt;a href=&#34;/articles/blogdownSteps/&#34;&gt;here&lt;/a&gt;; I&amp;rsquo;ll try to keep them updated.&lt;/p&gt;

&lt;p&gt;Happy reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My learning path with blogdown</title>
      <link>/articles/blogdownsteps/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/articles/blogdownsteps/</guid>
      <description>

&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;

&lt;p&gt;I worked with R Studio and installed the library &amp;lsquo;blogdown&amp;rsquo;. I followed the instructions and used the &amp;lsquo;NEW PROJECT&amp;rsquo; menu and added a &amp;lsquo;NEW BLOGDOWN SITE&amp;rsquo; to my file system. I selected the &amp;lsquo;hugo-blog-jeffprod&amp;rsquo; theme. This created a bunch of directories in my working directory. If I use the &amp;lsquo;serve_site()&amp;rsquo; function in console, the Hugo server is called and the site is continuously updated, as soon as I make changes and save them. When the server is started, a message appears, saying: &amp;ldquo;Serving the directory PROJECTDIR at &lt;a href=&#34;http://127.0.0.1:7958&amp;quot;&#34;&gt;http://127.0.0.1:7958&amp;quot;&lt;/a&gt;. I copied the http address on Safari (I am working on Mac OS&amp;hellip;), so that I can check my changes there.&lt;/p&gt;

&lt;p&gt;Here are my steps to the learning the process.&lt;/p&gt;

&lt;h1 id=&#34;step-1-the-mathjax-problem&#34;&gt;STEP 1 - The MathJax problem&lt;/h1&gt;

&lt;p&gt;I added a post to the &amp;lsquo;content&amp;rsquo; directory. This was a trial post, with an equation in latex code. I immediately noted that there was no support to MathJax. Surfing the net, I found a solution for this. These were my steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The folder &amp;lsquo;themes/hugo-blog-jeffprod/layouts/partials&amp;rsquo; contains the template file &amp;lsquo;header.html&amp;rsquo;. The blogdown documentation suggests not to directly modify this file. Thus I created a copy on the &amp;lsquo;layouts/partials/&amp;rsquo; folder in my root directory. This folder did not exist at the beginning, so I created it.&lt;/li&gt;
&lt;li&gt;I opened the newly created file &amp;lsquo;layouts/partials/header.html&amp;rsquo; and added the following lines within the &lt;head&gt; &amp;hellip; &lt;/head&gt; section:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Equations seem to render ok, now.&lt;/p&gt;

&lt;h1 id=&#34;step-2-change-the-blog-name-and-author&#34;&gt;STEP 2 - Change the blog name and author&lt;/h1&gt;

&lt;p&gt;I modified the &amp;lsquo;config.toml&amp;rsquo; file in the root directory and added name and author of the site. An easy step&amp;hellip;&lt;/p&gt;

&lt;h1 id=&#34;step-3-clean-the-example-files&#34;&gt;STEP 3 - Clean the example files&lt;/h1&gt;

&lt;p&gt;There were a lot of example posts in the &amp;lsquo;content&amp;rsquo; directory. I removed them all, with no problems. I also had to remove the content of the &amp;lsquo;static&amp;rsquo; folder. These files were not needed, because they belonged to the example posts.&lt;/p&gt;

&lt;h1 id=&#34;step-4-where-do-i-put-my-post&#34;&gt;Step 4 - Where do I put my post?&lt;/h1&gt;

&lt;p&gt;New posts need to be put in the &amp;lsquo;content/post&amp;rsquo; directory. In order to add a post, I am using the function &amp;lsquo;blogdown:::new_post_addin()&amp;rsquo; (I like working in console&amp;hellip;), which creates a correct skeleton. Blog posts are listed under the home page of my blog, sorted by date and by tags.&lt;/p&gt;

&lt;h1 id=&#34;step-5-where-do-i-put-my-non-post-pages&#34;&gt;Step 5 - Where do I put my non-post pages?&lt;/h1&gt;

&lt;p&gt;I did not intend to create a pure blog. I wanted also to add static pages, such as articles, tutorials, a project page, links and other stuff. I noted that, when .md or .Rmd files are left in the &amp;lsquo;content&amp;rsquo; folder, a folder with the same name is created in &amp;lsquo;public&amp;rsquo; and the post is rendered within that folder. Therefore, I put a &amp;lsquo;links.md&amp;rsquo; file in the &amp;lsquo;content&amp;rsquo; folder, containing a few links to other web pages of interest. This file is not rendered as a post and it is not visible in my home page. However, a folder named &amp;lsquo;link&amp;rsquo; is created in the public directory, containing the &amp;lsquo;links.md&amp;rsquo; file, rendered as &amp;lsquo;index.html&amp;rsquo; file. Therefore, I can link this file by using the &amp;lsquo;/link/&amp;rsquo; address.&lt;/p&gt;

&lt;h1 id=&#34;step-6-how-do-i-create-further-entries-for-the-navigation-bar&#34;&gt;Step 6 - How do I create further entries for the navigation bar?&lt;/h1&gt;

&lt;p&gt;I just created the &amp;lsquo;/link/index.html&amp;rsquo; file in the &amp;lsquo;public&amp;rsquo; directory. I would like the reader to access this web page by clicking on an appropriate link at the top of the home-page, next to the &amp;lsquo;Home&amp;rsquo; and &amp;lsquo;About me&amp;rsquo; links. In order to add this link, I had to edit again the &amp;lsquo;layouts/partials/header.html&amp;rsquo; file. In the &amp;lsquo;nav&amp;rsquo; section, there was the line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;navbar-item&amp;quot; href=&amp;quot;{{ &amp;quot;about&amp;quot; | absURL }}&amp;quot;&amp;gt;About me&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line creates the &amp;lsquo;About me&amp;rsquo; link on the navigation bar. Below this line, I added another line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;navbar-item&amp;quot; href=&amp;quot;{{ &amp;quot;links&amp;quot; | absURL }}&amp;quot;&amp;gt;Links&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I guess I can use this approach to redirect readers to other web pages of interest.&lt;/p&gt;

&lt;h1 id=&#34;step-7-i-need-to-put-some-order-in-the-content-folder-4-1-2019&#34;&gt;Step 7 - I need to put some order in the &amp;lsquo;content&amp;rsquo; folder&amp;rsquo; (4/1/2019)!&lt;/h1&gt;

&lt;p&gt;After adding a few non-post pages to my content folder I wanted to avoid too much confusion. I discovered that I can add folders to the &amp;lsquo;content&amp;rsquo; folder. They are rendered as such in the public directory. For example, I created an &amp;lsquo;article&amp;rsquo; folder and put a &amp;lsquo;page1.md&amp;rsquo; file inside it. After rendering, a folder &amp;lsquo;article/page1/&amp;rsquo; is created in the &amp;lsquo;public&amp;rsquo; folder, containing the &amp;lsquo;index.html&amp;rsquo; page, which is the rendered version of the &amp;lsquo;page1.md&amp;rsquo; file.&lt;/p&gt;

&lt;p&gt;I think I start feeling slightly more confident with blogdown!&lt;/p&gt;

&lt;h1 id=&#34;step-8-literature-support-7-1-2019&#34;&gt;Step 8 - Literature support (7/1/2019)!&lt;/h1&gt;

&lt;p&gt;This problem took me a while to solve. Lately, I discovered that, in order to be able to insert citations, the post file needs to be &amp;lsquo;.Rmd&amp;rsquo; and not &amp;lsquo;.md.&amp;rsquo; Furthermore, the literature file (&amp;lsquo;.bib&amp;rsquo;) and the &amp;lsquo;.csl&amp;rsquo; file need to be in the &amp;lsquo;/content/&amp;rsquo; folder. A reference to both files need to be put in the YAML header, e.g., :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bibliography: myFile.bib
csl: myStyle.csl
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-9-google-analytics-6-2-2019&#34;&gt;Step 9 - Google Analytics (6/2/2019)&lt;/h1&gt;

&lt;p&gt;I would like to track the visits I receive. To do so I enabled Google Analytics and got my tracking id (&lt;a href=&#34;https://support.google.com/analytics/answer/1008080&#34;&gt;see here&lt;/a&gt;). I also worked on my &amp;lsquo;config.toml&amp;rsquo; file and added the following statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GoogleAnalytics = &amp;quot;UA-my_ID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In contrast to what I expected, Google Analytics did not work and no hits were recorded for my blog. Lately, I worked on the &amp;lsquo;layouts/partials/header.html&amp;rsquo; and added the following code, right after the &amp;lsquo;&lt;head&gt;&amp;rsquo; tag (just replaced &amp;lsquo;UA-my_ID&amp;rsquo; with my tracking id).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Global Site Tag (gtag.js) - Google Analytics --&amp;gt;
&amp;lt;script async src=&amp;quot;https://www.googletagmanager.com/gtag/js?id=UA-my_ID&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag(&#39;js&#39;, new Date());

  gtag(&#39;config&#39;, &#39;UA-my_ID&#39;);
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now Google Analytics works ok!&lt;/p&gt;

&lt;h1 id=&#34;step-10-add-an-rss-template&#34;&gt;Step 10 - Add an RSS Template&lt;/h1&gt;

&lt;p&gt;I wanted my RSS feed to contain the whole post and not only a description. I did the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Took the default template from  &lt;a href=&#34;https://gohugo.io/templates/rss/#the-embedded-rss-xml&#34;&gt;this site&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Replaced &amp;lsquo;{{ .Summary | html }}&amp;rsquo; with &amp;lsquo;{{ .Content | html }}&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Saved the file as &amp;lsquo;RSS.xml&amp;rsquo;under the &amp;lsquo;layouts&amp;rsquo; directory.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/about/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;my name is Andrea Onofri. I have been always fond of science. I got my degree in Agricultural Sciences in 1987 and I like to think of  myself as an applied biologist. When I started doing research (in 1990) I was very much fascinated by the idea that natural phenomena might be described, interpreted and understood by using the universal language of math. My PhD studies revolved around nonlinear regression analyses and, afterwards, I&amp;rsquo;ve been mainly involved with biostatistics, data analyses and modelling. I do think that biostatistics is fundamental in developing scientific thinking.&lt;/p&gt;

&lt;p&gt;Today, I am a Professor and I teach Experimental Methodology at the University of Perugia. My students are biologists and they do not have a very deep background in maths and statistic. Teaching them requires great care and a very simple language.&lt;/p&gt;

&lt;p&gt;If you want to make comments or contact me for any reasons, here are my whereabouts:&lt;/p&gt;

&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;br /&gt;
Email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;br /&gt;
My ORCID: &lt;a href=&#34;https://orcid.org/0000-0002-6603-329X&#34;&gt;https://orcid.org/0000-0002-6603-329X&lt;/a&gt;&lt;br /&gt;
SCOPUS preview: &lt;a href=&#34;https://www.scopus.com/authid/detail.uri?authorId=57193769604&#34;&gt;https://www.scopus.com/authid/detail.uri?authorId=57193769604&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample variance and population variance: which of the two?</title>
      <link>/2018/stat_general_variancesamplepop/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/stat_general_variancesamplepop/</guid>
      <description>


&lt;p&gt;Teaching experimental methodology in agriculture related master courses poses some peculiar problems. One of these is to explain the difference between sample variance and population variance. For the students it is usually easy to grasp the idea that, being the mean the ‘center’ of the dataset, it is relevant to measure the average distance to the mean for all individuals in the dataset. Of course, we need to take the sum of squared distances, otherwise negative and positive residuals cancel each other out.&lt;/p&gt;
&lt;p&gt;It is also very intuitive that the average of squared residuals (mean square or variance) is calculated by using the following expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - \mu )^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, this is not what R (and other software) does. R divides by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; and, at this stage, the students usually ask: “&lt;em&gt;why are you telling me that I have to divide the sum of squares by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;? I have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals, not &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;!&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;You may think that the answer is pretty obvious. I don’t think so. The concept of variance is introduced at the beginning of the course, within the frame of descriptive statistics. At this stage, the students know nothing about probability distributions, sampling, inference and all the related concepts. They do not even know anything about degrees of freedom, yet!&lt;/p&gt;
&lt;p&gt;If I were a mathematician and were facing students in math/stat, I could show the class how clever I am by giving a formal proof that &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; is the correct denominator in most circumstances. In particular, this is true when we have a sample taken from a population and we want to infere the variance of the population by using the sample. The formal proof can be found in most stat books and I am attaching it at the end of this post. However, I will never show this formal proof to my students at this stage. Indeed, such a proof assumes that the students know what a sampling distribution is, what the standard error is and how it is calculated. Furthermore, students in agriculture are usually very reluctant, when it comes to dealing with maths!&lt;/p&gt;
&lt;p&gt;Therefore, I usually refrain from trying to appear more clever than I am. My question: “is it possible to teach the difference between the population variance and the sample variance, without talking about degrees of freedom or other ‘difficult’ concepts?”. Here is how I try to put it.&lt;/p&gt;
&lt;p&gt;Let’s take a big, but finite population, i.e. an hectar of maize plants (roughly 70,000 plants). Let’s imagine that we know the individual yields for all plants in the population. With R we can get those yields by taking random values in the interval from 150 to 250 grams (this is reasonable… students in agriculture know these things pretty well! And, I am not referring to any density distribution… it would be too much at this stage).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
pop &amp;lt;- runif(70000, min = 150, max = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our population (I will not show it… lack of space!). It is a finite population, so we can easily calculate the mean and the variance, i.e. the mean squared distance of all 70,000 individuals to the mean. You see that I am using the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; underneath).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0893&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2 &amp;lt;- sum( ( pop - mean(pop) )^2 )/70000
sigma2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 835.184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now forget about the population. In the field, we would never be able to measure the yield for all the plants in one hectar, due to lack of resources (time and money). Therefore, let’s measure 10 randomly selected plants (one in 7000; samples are often small in agriculture!). We can do this easily in R by sampling the original population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(pop, 10)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 237.7442 242.4557 243.2478 152.7499 246.9411 154.4087 226.4784
##  [8] 236.2380 216.2226 187.3896&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now calculate the mean and the variance for the sample, by using the equation above (i.e. using &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; as the denominator).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 214.3876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2_s &amp;lt;- sum( ( x - mean(x) )^2 )/10
sigma2_s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1197.856&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not know the population but we know our sample. As usual, in lack of other information, we conclude that the population should have the same characteristics of our sample. This is usually seen as a reasonable guess; at least the most reasonable one. The procedure we use to make a guess is called ‘estimator’ and the guess in itself is an ‘estimate’. Have we used a good estimator?&lt;/p&gt;
&lt;p&gt;We can’t answer. Unless we repeat the sampling process for a lot of times and see how our estimates behave in the long run. Our estimator is good if, in the long run, it converges on the real value for the population. Let’s check this: we repeatedly take samples of 10 plants from our population and calculate the mean and variance as above. We repeat this process 10,000 times, storing the 10,000 means and variances in two vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanS &amp;lt;- c(); varS &amp;lt;- c()
for(i in 1:10000){
  x &amp;lt;- sample(pop, 10)
  meanS[i] &amp;lt;- mean(x)
  varS[i] &amp;lt;- sum( ( x - mean(x) )^2 )/10
}
mean(meanS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0309&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 753.4311&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the mean of the means is 200.03. We got really very close to the real mean of the population. Indeed, the mean of the sample is a good (unbiased) estimator of the mean of the population.&lt;/p&gt;
&lt;p&gt;On the other hand, the mean of the variances is 753.43. Please note that this is much smaller than the real variance of the population. It means that if we use the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; at the denominator) to calculate the variance for the sample, we do not have a good estimator of the variance for the whole population. We can now look at the ratio between the variance of the population and our guess in the long run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(pop)/mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.108523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that this is roughly equal to &lt;span class=&#34;math inline&#34;&gt;\(10/9\)&lt;/span&gt;. In other words, if we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the sample and calculate the variance by using the equation above,&lt;/li&gt;
&lt;li&gt;multiply by &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;divide by &lt;span class=&#34;math inline&#34;&gt;\(n - 1 = 9\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we can get a good estimator. This leads us to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - m )^2}{n - 1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is exactly the variance for the sample. This one we can easily calculate with R.&lt;/p&gt;
&lt;div id=&#34;which-of-the-two-then&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which of the two, then?&lt;/h1&gt;
&lt;p&gt;Just rememeber:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the variance (mean square) is calculated by dividing the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. This makes sense, because we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals.&lt;/li&gt;
&lt;li&gt;However, if we have a random sample taken from a population, and if our aim is to estimate the variance for the whole population, we need to divide the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;. Otherwise, we get an understimation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-formal-proof-look-how-clever-i-am&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The formal proof (look how clever I am!)&lt;/h1&gt;
&lt;p&gt;We have a population with mean equal to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Let’s take a sample with its mean &lt;span class=&#34;math inline&#34;&gt;\(m \neq \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m - \mu = \varepsilon\)&lt;/span&gt;. For the population, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu )^2  = \sum\limits_{i = 1}^n (X_ i - m + \varepsilon)^2 = \sum\limits_{i = 1}^n ({X_i} - m)^2 + n \varepsilon^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We already see that the sum of squared residuals is higher if we take &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, unless &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon = 0\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m = \mu\)&lt;/span&gt;. From the above, we derive&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  \sum\limits_{i = 1}^n (X_i - \mu )^2  - n ( {m - \mu } )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the first equation above (variance for the population) we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  n \sigma^2 - n( m - \mu  )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that the rightmost term of this equation (&lt;span class=&#34;math inline&#34;&gt;\(n ( m - \mu )^2\)&lt;/span&gt; is the sum of squared distances for the sampling distribution of the mean. That is &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times the squared standard error. Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } n{\sigma ^2} - n\left( {\frac{{{\sigma ^2}}}{n}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } (n - 1){\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go! Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{\sum\limits_{i = 1}^n (X_i - m)^2 }{n - 1} = {\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is R dangerous? Side effects of free software for biologists</title>
      <link>/2014/2014-06-08-the-danger-of-r/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/2014-06-08-the-danger-of-r/</guid>
      <description>


&lt;p&gt;When I started my career in the biological field (it’s already 25 years ago), only the luckiest of us had access to very advanced statistical software. Licenses were very expensive and it was not easy to convince the boss that they were really necessary: “Why do you need to spend so much money to perform an ANOVA?”. Indeed, simple one-way or two-ways ANOVAs were quite easy to perform and one of the people in my group had already built the appropriate routines for several designs, by using the GW-BASIC language. But I wanted more!&lt;/p&gt;
&lt;p&gt;My agriculture experiments often showed complex designs and split-plot, strip-plot, subsampling and repeated measures/experiments were much more than an exception. I decided to start writing several Quick-BASIC routines to implement those types of ANOVAs on my PC. At the beginning of the ’90s, nonlinear response models became in fashion and I had to programme my first Gauss-Newton optimiser, also with Quick-BASIC. GLMs were not yet widespread among biologists and I mainly relied on stabilising transformations for those cases where the basic assumptions for linear/nonlinear models were not met.&lt;/p&gt;
&lt;p&gt;This ‘poor and humble’ statistical life gave me an undeniable advantage: it forced me into thoroughly studying and understanding the principles of each new technique and algorithm, before I could be able to programme it. I had to become acquainted with all those strange mathematical objects, such as matrices, eigenvalues and determinants, which are not usually a part of the mathematical background of biologists (at least in Italy). And my BASIC routines, once completed, could only do what they were programmed to do; only one specific solution to one specific task, no further options and no error management. In one sentence: no general solutions.&lt;/p&gt;
&lt;p&gt;Nowadays I have R: it is free and everything is possible and smooth. A few lines of code and I can fit whatever model comes to my mind in a few minutes. I can try several options: which is the best one? Which is the one that makes my data tell the story I would like to tell? Biometry books have changed as well; they have taken a more ‘algorithmic’ approach and math is confined within boxes that may be easily skipped. I have to admit that I frequently skip them: code snippets are more than enough to do the trick and I can also find thousands of them in the Internet. In other words, why should I bother studying such an abstract thing called statistics when I have R?&lt;/p&gt;
&lt;p&gt;Obviously this is just an exaggeration. However, I have the feeling that there might be some drawbacks relating to the availability of such a powerful free software. Biologists (especially students) may mistake studying R for studying stats. I am very much surprised to see how many complex models are fit on these days, with hardly any biological and statistical justifications and with very little care about the basic assumptions that these models make. A few days ago a PhD student at my Department showed me the results of fitting a reduced rank regression to a biological dataset. He was very proud of how he mastered the R coding process: by using the correct option (found after a thorough search over the Internet). He had even managed to avoid a ‘pretty strange’ warning message. Unfortunately, that warning message had been misinterpreted and therefore the analysis was wrong from the very beginning.&lt;/p&gt;
&lt;p&gt;A warning message for all biologists (including myself): R is really wonderful, but it will not necessarily bring to sound data analyses. Let’s use R, but let’s study stats first!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>