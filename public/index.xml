<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Models are wrong on The broken bridge between biologists and statisticians</title>
    <link>/</link>
    <description>Recent content in Models are wrong on The broken bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2018, @AndreaOnofri</copyright>
    <lastBuildDate>Thu, 11 Feb 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Split-plot designs: the transition to mixed models for a dinosaur</title>
      <link>/2021/stat_lmm_splitplottransition/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/stat_lmm_splitplottransition/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Those who long ago took courses in ‘analysis of variance’ or ‘experimental design’ … would have learned methods … based on observed and expected mean squares and methods of testing based on ‘error strata’ (if you weren’t forced to learn this, consider yourself lucky). (&lt;a href=&#34;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&#34;&gt;Douglas Bates, 2006&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In a previous post, I already mentioned that, due to my age, I see myself as a dinosaur within the R-users community. I already mentioned how difficult it is, for a dinosaur, to adjust to new concepts and paradigms in data analysis, after having done things differently for a long time ( &lt;a href=&#34;https://www.statforbiology.com/2020/stat_r_tidyverse_columnwise/&#34;&gt;see this post here&lt;/a&gt; ). Today, I decided to sit and write a second post, relating to data analyses for split-plot designs. Some years ago, when switching to R, this topic required some adjustments to my usual workflow, which gave me a few headaches.&lt;/p&gt;
&lt;p&gt;Let’s start from a real-life example.&lt;/p&gt;
&lt;div id=&#34;a-split-plot-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A split-plot experiment&lt;/h1&gt;
&lt;p&gt;The dataset ‘beet.csv’ is available in a web repository. It was obtained from a split-plot experiment with two experimental factors: three tillage methods (shallow ploughing, deep ploughing and minimum tillage) and two weed control methods (total and partial, meaning that the herbicide was sprayed broadcast or only along crop rows). Tillage methods were allocated to main-plots, while weed control methods were allocated to sub-plots and the experiment was designed in four complete blocks. A typical split-plot field experiment, indeed. The code below can be used to load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/beet.csv&amp;quot;
dataset &amp;lt;- read_csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Tillage, WeedControl, Block), .fns = factor))
dataset
## # A tibble: 24 x 4
##    Tillage WeedControl Block Yield
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
##  1 MIN     TOT         1     11.6 
##  2 MIN     TOT         2      9.28
##  3 MIN     TOT         3      7.02
##  4 MIN     TOT         4      8.02
##  5 MIN     PART        1      5.12
##  6 MIN     PART        2      4.31
##  7 MIN     PART        3      8.94
##  8 MIN     PART        4      5.62
##  9 SP      TOT         1     10.0 
## 10 SP      TOT         2      8.69
## # … with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-traditional-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The traditional approach&lt;/h1&gt;
&lt;p&gt;Split-plot designs are very commonly used in field experiments and they have been in fashion for (at least) eighty years, long before that the mixed model platform with REML estimation was largely available. Whoever has taken a course in ‘experimental design’ at the end of the 80s has studied how to perform a split-plot ANOVA by hand-calculations, based on the method of moments. For the youngest readers, it might be useful to give a few hints on what I used to do thirty years ago with the above dataset:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;calculate the overall mean and the means for the levels of blocks, tillage, weed control and for the combined levels of tillage and weed control.&lt;/li&gt;
&lt;li&gt;Calculate the means for the combined levels of blocks and tillage, which would correspond to the means for the twelve main-plots.&lt;/li&gt;
&lt;li&gt;With all those means, calculate the deviances for all effects and interactions, as the sums of squared residuals with respect to the overall mean.&lt;/li&gt;
&lt;li&gt;Derive the related variance, by using the appropriate number of degrees of freedom for each effect.&lt;/li&gt;
&lt;li&gt;Calculate F ratios, based on the appropriate error stratum, i.e. the mean square for the ‘blocks ⨉ tillage’ combinations (so called: error A) and the residual mean square.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most relevant aspect in the approach outlined above is the ‘block by tillage’ interaction; the mean square for this effect was used as the denominator in the F ratio, to test for the significance of the tillage main effect.&lt;/p&gt;
&lt;p&gt;The above process was simple to teach and simple to grasp and I used to see it as a totally correct approach to balanced (orthogonal) split-plot data. Those of you who are experienced with SAS should probably remember that, before the advent of PROC MIXED in 1992, split-plot designs were analysed with PROC GLM, using the very same approach as outlined above.&lt;/p&gt;
&lt;p&gt;Considering the above background, let’s see what I did when I switched to R?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-step-aov&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First step: ‘aov()’&lt;/h1&gt;
&lt;p&gt;Having the method of moments in mind, my first line of attack was to use the &lt;code&gt;aov()&lt;/code&gt; function, as suggested in Venables and Ripley (2002) at pag. 283. Those authors make use of the nesting operator in the expression &lt;code&gt;Error(Block/Tillage)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov &amp;lt;- aov(Yield ~ Tillage*WeedControl +
                 Error(Block/Tillage), data = dataset)
summary(mod.aov)
## 
## Error: Block
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)
## Residuals  3   3.66    1.22               
## 
## Error: Block:Tillage
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)   
## Tillage    2 23.656   11.83    19.4 0.0024 **
## Residuals  6  3.658    0.61                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Error: Within
##                     Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  
## WeedControl          1   3.32   3.320   1.225 0.2972  
## Tillage:WeedControl  2  19.46   9.732   3.589 0.0714 .
## Residuals            9  24.40   2.711                 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above definition, the block effect is regarded as random, while, in the traditional approach, it is often regarded as fixed. Indeed, still today, there is no consensus among agricultural scientists on whether the block effect should be regarded as random or fixed (see Dixon, 2016); for the sake of this exercise, let me regard it as fixed. After a few attempts, I discovered that I could move the effect of blocks from the &lt;code&gt;Error()&lt;/code&gt; definition to the fixed effect formula and use the expression &lt;code&gt;Error(Block:Tillage)&lt;/code&gt; to specify the uppermost error stratum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov2 &amp;lt;- aov(Yield ~ Block + Tillage*WeedControl +
                 Error(Block:Tillage), data = dataset)
## Warning in aov(Yield ~ Block + Tillage * WeedControl + Error(Block:Tillage), :
## Error() model is singular
summary(mod.aov2)
## 
## Error: Block:Tillage
##           Df Sum Sq Mean Sq F value Pr(&amp;gt;F)   
## Block      3  3.660    1.22   2.001 0.2155   
## Tillage    2 23.656   11.83  19.399 0.0024 **
## Residuals  6  3.658    0.61                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Error: Within
##                     Df Sum Sq Mean Sq F value Pr(&amp;gt;F)  
## WeedControl          1   3.32   3.320   1.225 0.2972  
## Tillage:WeedControl  2  19.46   9.732   3.589 0.0714 .
## Residuals            9  24.40   2.711                 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the above code produces a warning message, the result is totally the same as I would have obtained by hand-calculations.&lt;/p&gt;
&lt;p&gt;For me, the &lt;code&gt;aov()&lt;/code&gt; function represented a safe harbour, mainly because the result was very much like what I would expect, considering my experience with mean squares and error strata. Unfortunately, I had to realise that there were several limitations to this approach and, finally, I had to switch to the mixed model platform.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-step-the-mixed-model-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Second step: the mixed model framework&lt;/h1&gt;
&lt;p&gt;When making this switch to mixed models, I had the expectation that I should be able to reproduce the results obtained with the &lt;code&gt;aov()&lt;/code&gt; function and, formerly, by hand-calculations.&lt;/p&gt;
&lt;p&gt;I started with the &lt;code&gt;lme()&lt;/code&gt; function in the ‘nlme’ package (Pinheiro et al., 2018) and I had the idea that I could simply replace the &lt;code&gt;Error(Block:Tillage)&lt;/code&gt; statement with &lt;code&gt;random = ~1|Block:Tillage&lt;/code&gt;. Unfortunately, using the &lt;code&gt;:&lt;/code&gt; operator in the &lt;code&gt;lme()&lt;/code&gt; function is not possible and I had to resort to using the nesting operator &lt;code&gt;‘Block/Tillage’&lt;/code&gt;. Consequently, I noted that the F test for the block effect was wrong (of course: the specification was wrong…). I could have removed the block from the fixed effect model, but I was so stupidly determined to reproduce my hand-calculations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
## 
## Attaching package: &amp;#39;nlme&amp;#39;
## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     collapse
mod.lme &amp;lt;- lme(Yield ~ Block + Tillage*WeedControl,
               random = ~1|Block/Tillage, data = dataset)
anova(mod.lme)
## Warning in pf(Fval[i], nDF[i], dDF[i]): NaNs produced
##                     numDF denDF   F-value p-value
## (Intercept)             1     9 120.85864  &amp;lt;.0001
## Block                   3     0   0.08045     NaN
## Tillage                 2     6   6.32281  0.0333
## WeedControl             1     9   1.77497  0.2155
## Tillage:WeedControl     2     9   5.20229  0.0315&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, I tried to switch to the &lt;code&gt;lmer()&lt;/code&gt; function in the ‘lme4’ package (Bates et al., 2015). With this platform, it was possible to include the ‘block by tillage’ interaction as a random effect, according to my usual workflow. Still, the results did not match to those obtained with the &lt;code&gt;aov()&lt;/code&gt; function: an error message was raised and F ratios were totally different. Furthermore, p-levels were not even displayed (yes, now I know that we can use the ‘lmerTest’ package, but, please, wait a few seconds).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
mod.lmer.split &amp;lt;- lmer(Yield ~ Block + WeedControl*Tillage +
                     (1|Block:Tillage), 
                     data=dataset)
anova(mod.lmer.split)
## Analysis of Variance Table
##                     npar  Sum Sq Mean Sq F value
## Block                  3  3.6596  1.2199  0.6521
## WeedControl            1  3.3205  3.3205  1.7750
## Tillage                2 23.6565 11.8282  6.3228
## WeedControl:Tillage    2 19.4641  9.7321  5.2023&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What’s wrong with that? Why was I not able to reproduce my hand-calculations with the mixed model platform?&lt;/p&gt;
&lt;p&gt;I investigated this matter and I found a very enlightening post by Douglas Bates (the author of ‘nlme’ and ‘lme4’), which is available at &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html&#34;&gt;this link&lt;/a&gt;. From there, it was clear to me that F ratios in mixed models are “&lt;em&gt;not based on expected mean squares and error strata&lt;/em&gt;”; further ahead, it is said that there is “&lt;em&gt;a problem with the assumption that the reference distribution for these F statistics should be an F distribution with a known numerator of degrees of freedom but a variable denominator degrees of freedom&lt;/em&gt;”. In the end, it was clear to me that, according to Douglas Bates, the traditional approach of calculating p-values from F ratios based on expected mean squares and error strata was not necessarily correct.&lt;/p&gt;
&lt;p&gt;I made some further research on this matter. Indeed, looking at the &lt;code&gt;aov()&lt;/code&gt; output above, I noted that the residual mean square was equal to 2.711, while the mean square for the ‘Block by Tillage’ interaction was 0.6097. My beloved method of moments brought me to a negative estimate of the variance component for the ‘block by tillage’ interaction, that is &lt;span class=&#34;math inline&#34;&gt;\((0.6097 - 2.711)/4 = -0.5254\)&lt;/span&gt;. I gasped: this was unreasonable and, at least, it would imply that the variance component for the ‘block by tillage’ random effect was not significantly different from zero. In other words, the mean square for the ‘block by tillage’ interaction and the mean square for the residuals were nothing but two separate estimates of the residual plot-to-plot error. I started being suspicious about my hand-calculations. Why did I use two estimates of the same amount as two different error strata?&lt;/p&gt;
&lt;p&gt;I tried a different line of attack: considering that the ‘block by tillage’ interaction was not significant, I removed it from the model. Afterwards I fitted a linear fixed effect model, where the two error strata had been pooled into the residual error term. I obtained the very same F ratios as those obtained from the ‘lmer’ fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.lm &amp;lt;- lm(Yield ~ Block + WeedControl*Tillage, data=dataset) 
anova(mod.lm)
## Analysis of Variance Table
## 
## Response: Yield
##                     Df  Sum Sq Mean Sq F value  Pr(&amp;gt;F)  
## Block                3  3.6596  1.2199  0.6521 0.59389  
## WeedControl          1  3.3205  3.3205  1.7750 0.20266  
## Tillage              2 23.6565 11.8282  6.3228 0.01020 *
## WeedControl:Tillage  2 19.4641  9.7321  5.2023 0.01922 *
## Residuals           15 28.0609  1.8707                  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In that precise moment when I noted such a result, it was clear to me that, even with simple and orthogonal split-plot designs, hand-calculations do not necessarily produce correct results and should never, ever be used as the reference to assess the validity of a mixed model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suggestions-for-dinosaurs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Suggestions for dinosaurs&lt;/h1&gt;
&lt;p&gt;If you are one of those who have never taken a lesson about expected mean squares and error strata, well, believe me, you are lucky! For us dinosaurs, switching to the mixed model platform may be a daunting task. We need to free up our minds and change our workflow; a few suggestions are following.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-1-change-model-building-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rule 1: change model building process&lt;/h1&gt;
&lt;p&gt;In principle, do no insist on including the ‘block by tillage’ interaction in the model. With split-plot experiments, the main-plot is to be regarded as a &lt;em&gt;grouping structure&lt;/em&gt;, wherein we take repeated measures in different sub-plots. These measures are correlated, as they are more alike than measures taken in different sub-plots.&lt;/p&gt;
&lt;p&gt;Therefore, for this grouping structure (and for all grouping structures in general) we need to code a &lt;em&gt;grouping factor&lt;/em&gt;, to uniquely identify the repeated measures in each main-plot. This factor must be included in the model, otherwise we violate the basic assumption of independence of model residuals. Consider that the main-plot represent the randomisation units to which the tillage treatments were allocated; therefore, the main plot factor needs to be included in the model as a random effect. Please refer to the good paper of Piepho et al. (2003) for further information on this model building approach.&lt;/p&gt;
&lt;p&gt;In the box below I created the main-plot factor by using &lt;code&gt;dplyr()&lt;/code&gt; to combine the levels of blocks and tillage methods. The difference with the traditional approach of using the ‘block by tillage’ interaction in the model is subtle, but, in this case, the &lt;code&gt;lme()&lt;/code&gt; function returns no error. Please, note that, having no interest in the estimation of variance components, I have fitted this model by maximum likelihood estimation: it is confirmed that the main-plot random effect is zero (see the output of the &lt;code&gt;VarCorr()&lt;/code&gt; function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(mainPlots = factor(Block:Tillage))
mod.lme2 &amp;lt;- lme(Yield ~ Block + Tillage * WeedControl,
               random = ~1|mainPlots, data = dataset,
               method = &amp;quot;ML&amp;quot;)
VarCorr(mod.lme2)
## mainPlots = pdLogChol(1) 
##             Variance     StdDev      
## (Intercept) 4.462849e-10 2.112546e-05
## Residual    1.169203e+00 1.081297e+00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rule-2-change-the-approach-to-hypotheses-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Rule 2: change the approach to hypotheses testing&lt;/h1&gt;
&lt;p&gt;In the agricultural sciences we have been very much familiar with ANOVA tables, showing all fixed effects along with their significance level. I am very much convinced that we should refrain from such a (possibly bad) habit. Indeed, there is no point in testing the significance of main effects before testing the significance of the ‘tillage by weed control’ interaction, as main effects are marginal to the interaction effect.&lt;/p&gt;
&lt;p&gt;At first, we need to concentrate on the interaction effect. According to maximum likelihood theory, it is very logic to think of a Likelihood Ratio Test (LRT), which consists of comparing the likelihoods of two alternative and nested models. In this case, the model above (‘mod.lme2’) can be compared with a ‘reduced’ model without the ‘tillage by weed control’ interaction term: if the two likelihoods are similar, that would be a sign that the interaction effect is not significant. The reduced model fit is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.lme3 &amp;lt;- lme(Yield ~ Block + Tillage + WeedControl,
               random = ~1|mainPlots, data = dataset,
               method = &amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logarithms of the two likelihoods show that the ‘full model’ (with the interaction term) is more ‘likely’ than the reduced model. The LRT is calculated as twice the difference between the two log-likelihoods (the logarithm of the ratio of two numbers is the difference of the logarithms, remember?).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll2 &amp;lt;- logLik(mod.lme2)
ll3 &amp;lt;- logLik(mod.lme3)
ll2; ll3
## &amp;#39;log Lik.&amp;#39; -35.93039 (df=11)
## &amp;#39;log Lik.&amp;#39; -42.25294 (df=9)
LRT &amp;lt;- - 2 * (as.numeric(ll3) - as.numeric(ll2))
LRT
## [1] 12.6451&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For large samples and under the null hypothesis that the two models are not significantly different, the LRT is distributed according to a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; with two degrees of freedom (i.e. the difference in the number of model parameters used by the two models). We could use such an assumption to obtain a p-level for the null, for example by way of the &lt;code&gt;anova()&lt;/code&gt; function, to which we pass the two model objects as arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.lme2, mod.lme3)
##          Model df       AIC      BIC    logLik   Test L.Ratio p-value
## mod.lme2     1 11  93.86078 106.8194 -35.93039                       
## mod.lme3     2  9 102.50589 113.1084 -42.25294 1 vs 2 12.6451  0.0018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, our experiment consists of only 24 observations and the large sample theory should not hold. Therefore, instead of relying on the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; distribution, we can build an empirical sampling distribution for the LRT with Monte Carlo simulation (parametric bootstrap). The process is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;simulate a new dataset under the reduced model, using the fitted parameter estimates and assuming normality for the errors and random effects;&lt;/li&gt;
&lt;li&gt;fit to this dataset both the full and the reduced model;&lt;/li&gt;
&lt;li&gt;compute the LRT statistic;&lt;/li&gt;
&lt;li&gt;repeat steps 1 to 3 many times (e.g., 10000);&lt;/li&gt;
&lt;li&gt;examine the distribution of the bootstrapped LRT values and compute the proportion of those exceeding 12.6451 (empirical p-value).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To this aim, we can use the &lt;code&gt;simulate()&lt;/code&gt; function in the ‘nlme’ package. We pass the reduced model object as the first argument, the full model as the argument ‘m2’, the number of simulations and the seed (if we intend to obtain reproducible results). The fit may take quite a few minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- simulate(mod.lme3, nsim = 10000, m2 = mod.lme2, method=&amp;quot;ML&amp;quot;,
               set.seed = 1234)
lrtSimT &amp;lt;- as.numeric(2*(y$alt$ML[,2] - y$null$ML[,2]))
length(lrtSimT[lrtSimT &amp;gt; 12.6451])/length(lrtSimT)
## [1] 0.0211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We conclude that the interaction is significant and we can go ahead with further analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;What is the take-home message for this post? When we have to analyse a dataset coming from a split-plot experiment, R forces us to use the mixed model platform. We should not necessarily expect to reproduce the approach and the results we were used to obtain when we made our hand-calculations based on least squares and the method of moments. On the contrary, we should adapt our model building and hypothesis testing process to such a very powerful platform, wherein the slit-plot is treated on equal footing to all other types of repeated measures designs.&lt;/p&gt;
&lt;p&gt;Hope this was fun! If you have any comments, drop me a line to the email below.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Douglas Bates, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. &lt;a href=&#34;doi:10.18637/jss.v067.i01&#34; class=&#34;uri&#34;&gt;doi:10.18637/jss.v067.i01&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2018). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-137, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Venables, W.N., Ripley, B.D., Venables, W.N., 2002. Modern applied statistics with S, 4th ed. ed, Statistics and computing. Springer, New York.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pairwise comparisons in nonlinear regression</title>
      <link>/2021/stat_nls_paircomp/</link>
      <pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/stat_nls_paircomp/</guid>
      <description>


&lt;p&gt;Pairwise comparisons are one of the most debated topic in agricultural research: they are very often used and, sometimes, abused, in literature. I have nothing against the appropriate use of this very useful technique and, for those who are interested, some colleagues and I have given a bunch of (hopefully) useful suggestions in a paper, a few years ago (&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/j.1365-3180.2009.00758.x&#34;&gt;follow this link here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Pairwise comparisons usually follow the application of some sort of linear or generalised linear model; in this setting, the ‘emmeans’ package (Lenth, 2020) is very handy, as it uses a very logical approach. However, we can find ourselves in the need of making pairwise comparisons between the elements of a vector, which does not came as the result of linear model fitting.&lt;/p&gt;
&lt;p&gt;For example, we may happen to have an old table of means with standard errors and have lost the original raw data. Or, we may happen to have a vector of parameters from a nonlinear regression model, fitted with the &lt;code&gt;nls()&lt;/code&gt; function. How do we make pairwise comparisons? Experienced users can make profit of the &lt;code&gt;glht()&lt;/code&gt; function in the ‘multcomp’ package, although this is not immediate and, at least for me, it takes always some attempts to recall the exact syntax.&lt;/p&gt;
&lt;p&gt;Therefore, I have built the &lt;code&gt;pairComp()&lt;/code&gt; wrapper, which is available within the ‘aomisc’ package, the accompanying package for this website. Let’s see how this function works by using a typical example.&lt;/p&gt;
&lt;div id=&#34;a-case-study&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A case-study&lt;/h1&gt;
&lt;p&gt;This is a real-life example, taken from a research published by Vischetti et al. in 1996 (we have used this example in other posts, before). That research considered three herbicides for weed control in sugar beet, i.e. metamitron (M), phenmedipham (P) and chloridazon (C). Four soil samples were contaminated, respectively with: (i) M alone, (ii) M + P (iii) M + C and (iv) M + P + C. The aim was to assess whether the degradation speed of metamitron in soil depended on the presence of co-applied herbicides. To reach this aim, the soil samples were incubated at 20°C and sub-samples were taken in different times after the beginning of the experiment. The concentration of metamitron in those sub-samples was measured by HPLC analyses, performed in triplicate. The resulting dataset is available within the ‘aomisc’ package.&lt;/p&gt;
&lt;p&gt;In the box below. we install the ‘aomisc’ package from gitHub (if necessary), load it and load the ‘metamitron’ dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
library(aomisc)
data(metamitron)
head(metamitron)
##   Time Herbicide   Conc
## 1    0         M  92.00
## 2    0         M 118.64
## 3    0         M  89.58
## 4    7         M  59.32
## 5    7         M  62.95
## 6    7         M  62.95
#...
#...
tail(metamitron)
##    Time Herbicide  Conc
## 91   55       MPC 35.75
## 92   55       MPC 37.83
## 93   55       MPC 27.41
## 94   67       MPC 23.38
## 95   67       MPC 28.41
## 96   67       MPC 18.92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first step we take is to fit a first-order degradation model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[C_{t, h} = A_h \, \exp \left(-k_h  \, t \right)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is the concentration at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; for metamitron in the &lt;span class=&#34;math inline&#34;&gt;\(h^{th}\)&lt;/span&gt; combination (M alone, M + P, M + C and M + P + C), &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the initial concentration for the metamitron in the &lt;span class=&#34;math inline&#34;&gt;\(h^{th}\)&lt;/span&gt; combination, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the degradation rate for metamitron in the &lt;span class=&#34;math inline&#34;&gt;\(h^{th}\)&lt;/span&gt; combination. This model is nonlinear and, therefore, we can use the &lt;code&gt;nls()&lt;/code&gt; function for nonlinear least squares regression. The code is given below: please, note that the two parameters are followed by the name of the factor variable in square brackets (i.e.: A[Herbicide] and k[Herbicide]). This is necessary, to fit a different parameter value for each level of the ‘Herbicide’ factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Fit nls grouped model
modNlin &amp;lt;- nls(Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time), 
               start=list(A=rep(100, 4), k=rep(0.06, 4)), 
               data=metamitron)
summary(modNlin)
## 
## Formula: Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## A1 9.483e+01  4.796e+00   19.77   &amp;lt;2e-16 ***
## A2 1.021e+02  4.316e+00   23.65   &amp;lt;2e-16 ***
## A3 9.959e+01  4.463e+00   22.31   &amp;lt;2e-16 ***
## A4 1.116e+02  4.184e+00   26.68   &amp;lt;2e-16 ***
## k1 4.260e-02  4.128e-03   10.32   &amp;lt;2e-16 ***
## k2 2.574e-02  2.285e-03   11.26   &amp;lt;2e-16 ***
## k3 3.034e-02  2.733e-03   11.10   &amp;lt;2e-16 ***
## k4 2.186e-02  1.822e-03   12.00   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 9.701 on 88 degrees of freedom
## 
## Number of iterations to convergence: 5 
## Achieved convergence tolerance: 7.136e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can retrieve the degradation rates for the four herbicides (&lt;em&gt;k1&lt;/em&gt;, &lt;em&gt;k2&lt;/em&gt;, &lt;em&gt;k3&lt;/em&gt; and &lt;em&gt;k4&lt;/em&gt;) together with standard errors and load them into two vectors, as shown in the box below. In order to make pairwise comparisons, we also need to retrieve an estimate of the residual degrees of freedom, which we can also extract from the model fit object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- summary(modNlin)
dRates &amp;lt;- tab$coef[5:8,1]
SEs &amp;lt;- tab$coef[5:8,2]
dfr = tab$df[2]
dRates
##         k1         k2         k3         k4 
## 0.04260044 0.02573512 0.03033803 0.02185935
SEs
##          k1          k2          k3          k4 
## 0.004128447 0.002284696 0.002733498 0.001822218
dfr
## [1] 88&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have one vector of estimates to be compared and one vector of standard errors. In this situation, we can make pairwise comparisons by using the &lt;code&gt;pairComp()&lt;/code&gt; function in the ‘aomisc’ package. We just have to pass the vector of model parameters, the vector of standard errors, and, optionally, the names of parameters (we do not need this, as ‘dRates’ is a named vector), the number of residual degrees of freedom (defaults to ‘Inf’) and the multiplicity adjustment method, as in the ‘multcomp’ package (defaults to “single-step”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cp &amp;lt;- pairComp(dRates, SEs, dfr = dfr, adjust = &amp;quot;holm&amp;quot;)
cp$pairs
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## k1-k2 == 0  0.016865   0.004718   3.574  0.00286 ** 
## k1-k3 == 0  0.012262   0.004951   2.477  0.04604 *  
## k1-k4 == 0  0.020741   0.004513   4.596 8.58e-05 ***
## k2-k3 == 0 -0.004603   0.003563  -1.292  0.37639    
## k2-k4 == 0  0.003876   0.002922   1.326  0.37639    
## k3-k4 == 0  0.008479   0.003285   2.581  0.04604 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- holm method)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also obtain a letter display, by taking the ‘Letters’ slot in the ‘cp’ object. In this case, we might like to change the yardstick protection level, by passing the ‘level’ argument in ‘pairComp()’, that defaults to 0.05.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cp$Letters
##          Mean          SE CLD
## k1 0.04260044 0.004128447   a
## k2 0.02573512 0.002284696  bc
## k3 0.03033803 0.002733498   b
## k4 0.02185935 0.001822218   c&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that the &lt;code&gt;pairComp()&lt;/code&gt; function can be flexibly used in every situation where we have a vector of estimates and a vector of standard errors. It yields correct results whenever the elements of the vector of estimates are uncorrelated. Hope this is useful. Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;
Blog &lt;a href=&#34;www.statforbiology.com&#34;&gt;www.statforbiology.com&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Torsten Hothorn, Frank Bretz and Peter Westfall (2008). Simultaneous Inference in General Parametric Models. Biometrical Journal 50(3), 346–363.&lt;/li&gt;
&lt;li&gt;Russell Lenth (2020). emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.5.0-5. &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Griffing&#39;s models (1956)</title>
      <link>/2021/stat_met_diallel_griffing/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/stat_met_diallel_griffing/</guid>
      <description>


&lt;p&gt;Diallel mating designs are often used by plant breeders to compare the possible crosses between a set of genotypes. In spite of such widespread usage, the process of data analysis in R is not yet strightforward and it is not clear which tool should be routinely used. We recently gave a small contribution by publishing a paper in Plant Breeding (&lt;a href=&#34;https://link.springer.com/article/10.1007/s00122-020-03716-8&#34;&gt;Onofri et al., 2020&lt;/a&gt; ), where we advocated the idea that models for diallel crosses are just a class of general linear models, that should be fit by Ordinary Least Squares (OLS) or REstricted Maximum Likelihood methods (REML).&lt;/p&gt;
&lt;p&gt;In that paper, we presented &lt;code&gt;lmDiallel&lt;/code&gt;, a new R package to fit diallel models, which we followed up with a series of three blog posts, giving more detail about the package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see here&lt;/a&gt;), about the Hayman’s models type 1 (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;) and type 2 (&lt;a href=&#34;https://www.statforbiology.com/2021/stat_met_diallel_hayman2/&#34;&gt;see here&lt;/a&gt;). These latter models can be used to describe the data from full diallel experiments.&lt;/p&gt;
&lt;p&gt;In this fourth post we are going to talk about a very flexible family of models, that was introduced by Griffing in 1956 and it is still very used in plant breeding, to estimate General Combining Ability (GCA) and Specific Combining Ability (SCAs). The equations take different forms, to account for all possible mating schemes.&lt;/p&gt;
&lt;p&gt;With full diallel experiments (including selfs and reciprocals; &lt;strong&gt;mating scheme 1&lt;/strong&gt;), the model is very similar to Hayman’s model type 1, except that reciprocal effects are not parted into RGCA and RSCA (Reciprocal General Combining Ability and Reciprocal Specific Combining Ability). The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{ts}_{ij} + r_{ij} + \varepsilon_{ijk} \quad\quad\quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the expected value (the overall mean, in the balanced case) and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual random error term for the observation in the &lt;span class=&#34;math inline&#34;&gt;\(k^{th}\)&lt;/span&gt; block and with the parentals &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. All the other terms correspond to genetic effects, namely:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_j\)&lt;/span&gt; terms are the General Combining Abilities (GCAs) of the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(ts_{ij}\)&lt;/span&gt; term is the total Specific Combining Ability (SCA) for the combination &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(r_{ij}\)&lt;/span&gt; term is the reciprocal effect for a specific &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt; combination.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When the reciprocal crosses are not available (&lt;strong&gt;mating scheme 2&lt;/strong&gt;), the term &lt;span class=&#34;math inline&#34;&gt;\(\textrm{r}_{ij}\)&lt;/span&gt; needs to be dropped, so that the model reduces to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{ts}_{ij} + \varepsilon_{ijk} \quad\quad\quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When the reciprocals are available, but selfs are missing (&lt;strong&gt;mating scheme 3&lt;/strong&gt;), the model is similar to equation 1, but the term &lt;span class=&#34;math inline&#34;&gt;\(\textrm{ts}_{ij}\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\textrm{s}_{ij}\)&lt;/span&gt; (we use a different symbol, because the design matrix is slightly different and needs a different coding):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{s}_{ij} + r_{ij} + \varepsilon_{ijk} \quad\quad\quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Finally, when neither selfs nor reciprocals are available (&lt;strong&gt;mating scheme 4&lt;/strong&gt;), the equation reduces to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{s}_{ij} + \varepsilon_{ijk} \quad\quad\quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s see how to fit the above models by using a set of examples with different mating schemes.&lt;/p&gt;
&lt;div id=&#34;example-1-a-full-diallel-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1: a full diallel experiment&lt;/h1&gt;
&lt;p&gt;The example in Hayman (1954) relates to a complete diallel experiment with eight parental lines, producing 64 combinations (8 selfs + 28 crosses with 2 reciprocals each). The R dataset is included in the ‘lmDiallel’ package; in the box below we load the data, after installing (if necessary) and loading the ‘lmDiallel’ package (see box below). For brevity, some R commands are shown but not executed (they are commented out)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
data(&amp;quot;hayman54&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this complete diallel experiment we can fit equation 1, by including GCAs, tSCAs and reciprocal effects. Please, note that excluding any of these effects results in unreliable estimates of the residual mean square. We can use either the &lt;code&gt;lm()&lt;/code&gt; or the &lt;code&gt;lm.diallel()&lt;/code&gt; functions, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contrasts(hayman54$Block) &amp;lt;- &amp;quot;contr.sum&amp;quot;
dMod &amp;lt;- lm(Ftime ~ Block + GCA(Par1, Par2) + tSCA(Par1, Par2) +
             REC(Par1, Par2), data = hayman54)
dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54, fct = &amp;quot;GRIFFING1&amp;quot;)
# summary(dMod2)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block        1    142     142  0.3416   0.56100    
## GCA          7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## SCA         28 102238    3651  8.7599 6.656e-13 ***
## Reciprocals 28  19112     683  1.6375   0.05369 .  
## Residuals   63  26260                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to obtain the full list of genetical parameters, we can use the &lt;code&gt;glht()&lt;/code&gt; function in the &lt;code&gt;multcomp&lt;/code&gt; package, together with the &lt;code&gt;diallel.eff()&lt;/code&gt; function in the ‘lmDiallel’ package. An excerpt of the results is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0  1.629e+02  1.805e+00  90.270  &amp;lt; 2e-16 ***
# g_A == 0        4.620e+01  3.376e+00  13.683 2.17e-13 ***
# g_B == 0       -2.459e+01  3.376e+00  -7.282 9.83e-08 ***
# g_C == 0        4.963e+01  3.376e+00  14.702 4.13e-14 ***
# g_D == 0        1.835e+01  3.376e+00   5.436 1.07e-05 ***
# g_E == 0       -2.093e+01  3.376e+00  -6.199 1.47e-06 ***
# g_F == 0        2.445e+00  3.376e+00   0.724 0.475340    
# g_G == 0       -4.471e+01  3.376e+00 -13.244 4.57e-13 ***
# g_H == 0       -2.640e+01  3.376e+00  -7.819 2.71e-08 ***
# ts_A:A == 0     3.371e+01  1.263e+01   2.669 0.012941 *  
# ts_A:B == 0    -3.151e+01  9.023e+00  -3.492 0.001731 ** 
# ...
# ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-no-reciprocals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2: no reciprocals&lt;/h1&gt;
&lt;p&gt;As an example of a diallel experiments with no reciprocals, we consider the data reported in Lonnquist and Gardner (1961) relating to the yield of 21 maize genotypes, obtained from six male and six female parentals. The dataset is available as &lt;code&gt;lonnquist61&lt;/code&gt; in the &lt;code&gt;lmDiallel&lt;/code&gt; package and the model fitting process is very similar to that shown before for the mating scheme 1, apart from the fact that we fit equation 2 instead of equation 1. In the ‘lm()’ call, we use the &lt;code&gt;GCA()&lt;/code&gt; and &lt;code&gt;tSCA()&lt;/code&gt; functions, while in the &lt;code&gt;lm.diallel()&lt;/code&gt; call, we set the argument ‘fct’ to “GRIFFING2”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
data(lonnquist61)
dMod &amp;lt;- lm(Yield ~ GCA(Par1, Par2) + tSCA(Par1, Par2), 
           data = lonnquist61)
dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = lonnquist61, fct = &amp;quot;GRIFFING2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the dataset has no replicates and, for the inferences, we need to provide an estimate of the residual mean square and degrees of freedom (see box below). If we have fitted the model by using the &lt;code&gt;lm()&lt;/code&gt; function, the resulting ‘lm’ object can be explored by using the &lt;code&gt;summary.diallel()&lt;/code&gt; and &lt;code&gt;anova.diallel()&lt;/code&gt; functions. Otherwise, if we have fitted the model with the &lt;code&gt;lm.diallel()&lt;/code&gt; function, the resulting ‘diallel’ object can be explored by using the &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; methods. See the box below for an example: the results are, obviously, the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summary.diallel(dMod, MSE = 7.1, dfr = 60)
anova.diallel(dMod, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##                  Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA(Par1, Par2)   5 234.23  46.846  6.5980 5.923e-05 ***
## tSCA(Par1, Par2) 15 238.94  15.929  2.2436   0.01411 *  
## Residuals        60          7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# summary(dMod2, MSE = 7.1, dfr = 60)
anova(dMod2, MSE = 7.1, dfr = 60)
## Analysis of Variance Table
## 
## Response: Yield
##           Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA        5 234.23  46.846  6.5980 5.923e-05 ***
## SCA       15 238.94  15.929  2.2436   0.01411 *  
## Residuals 60          7.100                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also for the diallel object, we can retrieve the full list of genetical parameters with the &lt;code&gt;glht()&lt;/code&gt; function, by using the same syntax as shown above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh &amp;lt;- glht(linfct = diallel.eff(dMod2, MSE = 7.1, dfr = 60))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3-no-selfs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3: no selfs&lt;/h1&gt;
&lt;p&gt;When the experimental design includes the reciprocal crosses but not the selfs, we can fit Equation 3. As an example, we take the same dataset as before (‘hayman54’), but we remove the selfs by using ‘dplyr’. The fitting process is the same as shown above and only the model specification is changed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
data(hayman54)
hayman54b &amp;lt;- hayman54  %&amp;gt;% 
  filter(Par1 != Par2)

dMod &amp;lt;- lm(Ftime ~ Block + GCA(Par1, Par2) + 
             SCA.G3(Par1, Par2) + REC.G3(Par1, Par2), 
           data = hayman54b)
dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54b, fct = &amp;quot;GRIFFING3&amp;quot;)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block        1    329   329.1  0.8367   0.36432    
## GCA          7 168923 24131.9 61.3479 &amp;lt; 2.2e-16 ***
## SCA         20  37289  1864.4  4.7398 2.318e-06 ***
## Reciprocals 28  19112   682.6  1.7352   0.04052 *  
## Residuals   55  21635                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4-no-reciprocals-no-selfs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 4: no reciprocals, no selfs&lt;/h1&gt;
&lt;p&gt;In this final example, we consider a mating scheme where neither the reciprocal crosses nor the selfs are included (mating scheme 4). The dataset is taken from the original Griffing’s paper (Griffing, 1956) and it is available as ‘Griffing56’ in the ‘lmDiallel’ package. The analysis proceeds in the very same fashion as above, apart from the fact that we fit Equation 4, instead of 3 and that we input the appropriate residual error term to obtain the correct inferences, as the original dataset does not contain the replicated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;griffing56&amp;quot;)

dMod &amp;lt;- lm(Yield ~ GCA(Par1, Par2) + SCA.G3(Par1, Par2), 
           data = griffing56)
anova.diallel(dMod, MSE = 21.05, dfr = 2558)
## Analysis of Variance Table
## 
## Response: Yield
##                      Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA(Par1, Par2)       8 18606.0 2325.75 110.487 &amp;lt; 2.2e-16 ***
## SCA.G3(Par1, Par2)   27  9164.9  339.44  16.125 &amp;lt; 2.2e-16 ***
## Residuals          2558           21.05                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2,
                    data = griffing56, fct = &amp;quot;GRIFFING4&amp;quot;)
anova(dMod2, MSE = 21.05, dfr = 2558)
## Analysis of Variance Table
## 
## Response: Yield
##             Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA          8 18606.0 2325.75 110.487 &amp;lt; 2.2e-16 ***
## SCA         27  9164.9  339.44  16.125 &amp;lt; 2.2e-16 ***
## Residuals 2558           21.05                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# summary(dMod2, MSE = 21.05, dfr = 2558)

gh &amp;lt;- glht(linfct = diallel.eff(dMod2, MSE = 21.05, dfr = 2558))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;If we intend to regard the genetic effects as random and to estimate variance components, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a bunch of dummy variables. In order to make things simpler for routine experiments, we have coded the &lt;code&gt;mmer.diallel()&lt;/code&gt; wrapper using the same syntax as the &lt;code&gt;lm.diallel()&lt;/code&gt; function. The exemplary code is given in the box below, relating to Equation 2, although the other equations can be fitted in a similar manner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random genetic effects
mod1m &amp;lt;- mmer.diallel(Yield ~ Par1 + Par2,
                      data = lonnquist61,
                      fct = &amp;quot;GRIFFING2&amp;quot;)
mod1m
##        VarComp VarCompSE
## GCA   3.863695  3.769373
## tSCA 15.930144  5.819217&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next post we will consider another important family of models, devised by Gardner and Eberarth in 1966, which accounts for heterotic effects. Please, stay tuned!&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Dr. Niccolò Terzaroli&lt;br /&gt;
Prof. Gino Russi&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744.&lt;/li&gt;
&lt;li&gt;Griffing, B., 1956. Concept of general and specific combining ability in relation to diallel crossing systems. Australian Journal of Biological Science 9, 463–493.&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Terzaroli, N., Russi, L., 2020. Linear models for diallel crosses: a review with R functions. Theor Appl Genet. &lt;a href=&#34;https://doi.org/10.1007/s00122-020-03716-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00122-020-03716-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Hayman&#39;s model (type 2)</title>
      <link>/2021/stat_met_diallel_hayman2/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/stat_met_diallel_hayman2/</guid>
      <description>


&lt;p&gt;This posts follows two other previously published posts, where we presented our new ‘lmDiallel’ package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see here&lt;/a&gt;) and showed how we can use it to fit the Hayman’s model type 1, as proposed in Hayman (1954) (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;). In this post, we will give a further example relating to another very widespread model from the same author, the Hayman’s model type 2. We apologise for some overlapping with previous posts: we think this is necessary so that each post can be read on its own.&lt;/p&gt;
&lt;p&gt;The model we are going to talk about is used to describe the results of full (complete) diallel experiments, where we have crosses + reciprocals + selfs. If you are not sure what a diallel experiment is, we suggest you go back to one of our previous posts on this sequence, where we give some preliminary information for beginners. Otherwise, we can proceed to the motivating example.&lt;/p&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The example&lt;/h1&gt;
&lt;p&gt;In this post we will use the same example as provided in the original Hayman’s paper (Hayman, 1954), relating to a complete diallel experiment with eight parental lines. The R dataset is included in the ‘lmDiallel’ package; in the box below we load the data, after installing (if necessary) and loading the ‘lmDiallel’ package (see box below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
data(&amp;quot;hayman54&amp;quot;)
head(hayman54)
##   Block Par1 Par2 Ftime
## 1     1    A    A   276
## 2     1    A    B   156
## 3     1    A    C   322
## 4     1    A    D   250
## 5     1    A    E   162
## 6     1    A    F   193&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-haymans-model-type-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Hayman’s model type 2&lt;/h1&gt;
&lt;p&gt;The Hayman’s model type 2 is derived from type 1 (see our previous post), by partitioning the tSCA effect in three additive components. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \left\{ {\begin{array}{ll}
\mu + \gamma_k + \textrm{g}_i + \textrm{g}_j + m + d_i + d_j + s_{ij} + rg^a_i + rg^b_j + rs_{ij} + \varepsilon_{ijk} &amp;amp; \textrm{for} \quad i \neq j\\
\mu + \gamma_k + 2\, \textrm{g}_i - (n - 1)m - (n - 2)d_i + \varepsilon_{ijk} &amp;amp; \textrm{for} \quad i = j \end{array}} \right.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the expected value (the overall mean, in the case of fully orthogonal designs), &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of parentals and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual random error term. All the other terms correspond to genetic effects, namely:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_j\)&lt;/span&gt; terms are the &lt;strong&gt;general combining abilities&lt;/strong&gt; (GCAs) of the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rg^a_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(rg^b_j\)&lt;/span&gt; terms are the &lt;strong&gt;reciprocal general combining abilities&lt;/strong&gt; (RGCAs) for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; term relates to the difference between the average value of all observations and the average values of crosses (&lt;strong&gt;Mean Dominance Deviation&lt;/strong&gt;; MDD).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(d_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(df_j\)&lt;/span&gt; terms relate to the differences between the yield of each selfed parent (&lt;span class=&#34;math inline&#34;&gt;\(Y_{ij}\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(i = j\)&lt;/span&gt;) and the average yield of all selfed parents (&lt;strong&gt;dominance deviation&lt;/strong&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; parent; DD).&lt;/li&gt;
&lt;li&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(s_{ij}\)&lt;/span&gt; is the SCA effect for the combination &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rs_{ij}\)&lt;/span&gt; term is the &lt;strong&gt;reciprocal specific combining ability&lt;/strong&gt; (RSCA) for a specific &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt; combination, that is the discrepancy between the performances of the two reciprocals (e.g, A &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; B vs. B &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; A)(&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly to type 1, the Hayman’s model type 2 considers the genetical effects as differences with respect to the intercept &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, that is the mean of all observations (when the design is orthogonal). However, with respect to type 1, this latter model permits the estimation of a higher number of genetic effects (GCAs, RGCAs, MDD, DDs, SCAs and RSCAs) and provides an approach to quantify heterotic effects. We should consider that, due to unbalance (the number of crosses is never equal to the number of selfs), it is necessary to introduce some coefficients (i.e. &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n - 2\)&lt;/span&gt; in Equation 1), which do not have an obvious meaning. In future posts we will see that other diallel models were proposed, which account for heterotic effects in a different manner (Gardner and Eberhart, 1966).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting with R&lt;/h1&gt;
&lt;p&gt;Let’s assume that all effects are fixed, apart from the residual error effect. Consequently, Equation 1 is a specific parameterisation of a general linear model, which we can fit by the usual &lt;code&gt;lm()&lt;/code&gt; function and related methods. However, we need to exploit some of the facilities in our new ‘lmDiallel’ extension package, which consist of the &lt;code&gt;GCA()&lt;/code&gt;, &lt;code&gt;MDD()&lt;/code&gt;, &lt;code&gt;DD()&lt;/code&gt;, &lt;code&gt;SCA()&lt;/code&gt;, &lt;code&gt;RGCA()&lt;/code&gt; and &lt;code&gt;RSCA()&lt;/code&gt; functions (see the box below). The resulting &lt;code&gt;lm&lt;/code&gt; object can be explored by the usual R methods, such as &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; (the output of the &lt;code&gt;summary()&lt;/code&gt; method is partly hidden, for brevity)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contrasts(hayman54$Block) &amp;lt;- &amp;quot;contr.sum&amp;quot;
dMod &amp;lt;- lm(Ftime ~ Block + GCA(Par1, Par2) + MDD(Par1, Par2) +
             DD(Par1, Par2) + SCA(Par1, Par2) +
             RGCA(Par1, Par2) + RSCA(Par1, Par2), data = hayman54)
summary(dMod)$coef[1:6,]
##                      Estimate Std. Error    t value     Pr(&amp;gt;|t|)
## (Intercept)        162.898437   1.804567 90.2700843 2.381071e-68
## Block1              -1.054688   1.804567 -0.5844545 5.610017e-01
## GCA(Par1, Par2)g_A  46.195312   3.376036 13.6832990 1.558468e-20
## GCA(Par1, Par2)g_B -24.585938   3.376036 -7.2824864 6.421946e-10
## GCA(Par1, Par2)g_C  49.632812   3.376036 14.7015049 4.900927e-22
## GCA(Par1, Par2)g_D  18.351563   3.376036  5.4358311 9.415231e-07
# ...
# ...
anova(dMod)
## Analysis of Variance Table
## 
## Response: Ftime
##                  Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block             1    142     142  0.3416   0.56100    
## GCA(Par1, Par2)   7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## MDD(Par1, Par2)   1  30797   30797 73.8840 3.259e-12 ***
## DD(Par1, Par2)    7  34153    4879 11.7050 1.957e-09 ***
## SCA(Par1, Par2)  20  37289    1864  4.4729 2.560e-06 ***
## RGCA(Par1, Par2)  7   6739     963  2.3097   0.03671 *  
## RSCA(Par1, Par2) 21  12373     589  1.4135   0.14668    
## Residuals        63  26260     417                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simplicity, we also built a wrapper function named &lt;code&gt;lm.diallel()&lt;/code&gt;, which can be used in the very same fashion as &lt;code&gt;lm()&lt;/code&gt;. The syntax is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm.diallel(formula, Block, Env, data, fct)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘formula’ specifies the response variable and the two variables for parentals (e.g., Yield ~ Par1 + Par2) and the two arguments ‘Block’ and ‘Env’ are used to specify optional variables, coding for blocks and environments, respectively. The argument ‘data’ is a ‘dataframe’ where to look for the explanatory variables and, finally, ‘fct’ is a string variable coding for the selected model (“HAYMAN2”, for this example; see below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod2 &amp;lt;- lm.diallel(Ftime ~ Par1 + Par2, Block = Block,
                    data = hayman54, fct = &amp;quot;HAYMAN2&amp;quot;)
# summary(dMod2)
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Ftime
##           Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block      1    142     142  0.3416   0.56100    
## MDD        1  30797   30797 73.8840 3.259e-12 ***
## GCA        7 277717   39674 95.1805 &amp;lt; 2.2e-16 ***
## DD         7  34153    4879 11.7050 1.957e-09 ***
## SCA       20  37289    1864  4.4729 2.560e-06 ***
## RGCA       7   6739     963  2.3097   0.03671 *  
## RSCA      21  12373     589  1.4135   0.14668    
## Residuals 63  26260                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function works very much like the &lt;code&gt;lm()&lt;/code&gt; function and makes use of the general purpose linear model solver &lt;code&gt;lm.fit()&lt;/code&gt;. Apart from simplicity, another advantage is that the call to &lt;code&gt;lm.diallel()&lt;/code&gt; returns an object of both ‘lm’ and ‘diallel’ classes. For this latter class, we built several specific S3 methods, such as the usual &lt;code&gt;anova()&lt;/code&gt;, &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;model.matrix()&lt;/code&gt; methods, partly shown in the box above.&lt;/p&gt;
&lt;p&gt;Considering that diallel models are usually fitted to determine genetical parameters, we also built the &lt;code&gt;glht.diallelMod()&lt;/code&gt; method and the &lt;code&gt;diallel.eff()&lt;/code&gt; function, which can be used with the ‘multcomp’ package, to retrieve the complete list of genetical parameters. An excerpt is shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
# summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;))
#    Simultaneous Tests for General Linear Hypotheses
# 
# Linear Hypotheses:
#                Estimate Std. Error t value Pr(&amp;gt;|t|)    
# Intercept == 0 162.8984     1.8046  90.270  &amp;lt; 2e-16 ***
# m == 0          -5.8627     0.6821  -8.596 4.48e-09 ***
# g_A == 0        46.1953     3.3760  13.683 2.17e-13 ***
# g_B == 0       -24.5859     3.3760  -7.282 9.83e-08 ***
# g_C == 0        49.6328     3.3760  14.702 4.13e-14 ***
# g_D == 0        18.3516     3.3760   5.436 1.07e-05 ***
# g_E == 0       -20.9297     3.3760  -6.199 1.47e-06 ***
# g_F == 0         2.4453     3.3760   0.724 0.475340    
# g_G == 0       -44.7109     3.3760 -13.244 4.57e-13 ***
# g_H == 0       -26.3984     3.3760  -7.819 2.71e-08 ***
# d_A == 0         1.2213     1.9492   0.627 0.536380    
# d_B == 0        -2.6224     1.9492  -1.345 0.190113    
# ...
# ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a previous post (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel_hayman1/&#34;&gt;see here&lt;/a&gt;) we have shown that, when diallel models are fitted to the genotype means (and thus we have no replicates), an appropriate estimate of residual mean square and degrees of freedom can be passed as arguments to the &lt;code&gt;summary()&lt;/code&gt;, &lt;code&gt;anova()&lt;/code&gt; and &lt;code&gt;diallel.eff()&lt;/code&gt; methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;If we intend to regard the genetic effects as random and to estimate variance components, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a bunch of dummy variables. In order to make things simpler for routine experiments, we have coded the &lt;code&gt;mmer.diallel()&lt;/code&gt; wrapper using the same syntax as the &lt;code&gt;lm.diallel()&lt;/code&gt; function. The exemplary code is given in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Random genetic effects
mod1m &amp;lt;- mmer.diallel(Ftime ~ Par1 + Par2, Block = Block, 
                      data = hayman54,
                      fct = &amp;quot;HAYMAN2&amp;quot;)
mod1m
##              VarComp   VarCompSE
## Block        0.00000    9.188298
## MDD       1783.96081 3118.893561
## GCA       1005.92052  574.893353
## RGCA        17.97898   19.920016
## DD         659.53567  468.205470
## SCA        351.74035  144.688653
## RSCA        32.02325   46.361581
## Residuals  412.54051   73.506382&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s all about the Hayman’s models; you may have noted that both models (type 1 and 2) were devised for full diallel experiments, which are not so widespread in ‘genetical’ literature. A few years later, in 1956, the Australian scientist B. Griffing made the relevant effort of creating a comprehensive set of models which can be fitted to all types of diallel experiments. We will talk about these models in a future post.&lt;/p&gt;
&lt;p&gt;Thanks for reading (and happy 2021!)&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Prof. Luigi Russi&lt;br /&gt;
Dr. Niccolò Terzaroli&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0156744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0156744&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hayman, B.I., 1954. The Analysis of Variance of Diallel Tables. Biometrics 10, 235. &lt;a href=&#34;https://doi.org/10.2307/3001877&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/3001877&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Terzaroli, N., Russi, L., 2020. Linear models for diallel crosses: a review with R functions. Theor Appl Genet. &lt;a href=&#34;https://doi.org/10.1007/s00122-020-03716-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00122-020-03716-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>General code to fit ANOVA models with JAGS and &#39;rjags&#39;</title>
      <link>/2020/stat_bayesian_anovamodels/</link>
      <pubDate>Wed, 23 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_bayesian_anovamodels/</guid>
      <description>


&lt;p&gt;One of the reasons why I like BUGS and all related dialects has been put nicely in a very good book, i.e. “Introduction to WinBUGS for ecologists” (Kery, 2010); at page 11, the author says: &lt;em&gt;“WinBUGS helps free the modeler in you”&lt;/em&gt;. Ultimately, that statement is true: when I have fully understood a model with all its components (and thus I have become a modeler), I can very logically translate it to BUGS code. The drawback is that, very often, the final coding appears to be rather ‘problem-specific’ and difficult to be reused in other situations, without an extensive editing work.&lt;/p&gt;
&lt;p&gt;For example, consider the ‘ANOVA models’ with all their ‘flavors’: one-way, two-ways with interactions, nested, and so on. These models are rather common in agricultural research and they are relatively easy to code in BUGS, following the suggestions provided in Kery’s book. However, passing from one model to the others requires some editing, which is often prone to errors. And errors in BUGS are difficult to spot in short time… Therefore, I have been wondering: “&lt;em&gt;Can I write general BUGS code, which can be used for all ANOVA models with no substantial editing?&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;Finally, I have found a solution and, as it took me awhile to sort things out, I thought I might share it, for the benefit of those who would like to fit ANOVA models in the Bayesian framework. It works with JAGS (JUST ANOTHER GIBBS SAMPLER), a BUGS dialect running also in Mac OS and developed by Marty Plummer. JAGS can be used from R, thanks to the ‘rjags’ package (Plummer, 2019), which I will use in this post.&lt;/p&gt;
&lt;p&gt;Let’s start from a working example.&lt;/p&gt;
&lt;div id=&#34;a-genotype-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A genotype experiment&lt;/h1&gt;
&lt;p&gt;The yields of seven wheat genotypes were compared in one experiment laid down in three randomised complete blocks. The data is available in an external repository as a ‘csv’ file and it can be loaded by using the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat2002.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName, header = T)
head(dataset)
##   Plot Block Genotype Yield
## 1   57     A COLOSSEO  4.31
## 2   61     B COLOSSEO  4.73
## 3   11     C COLOSSEO  5.64
## 4   60     A    CRESO  3.99
## 5   10     B    CRESO  4.82
## 6   42     C    CRESO  4.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the typical situation were we might be interested in fitting an ANOVA model, with the yield as the response variable and the blocks and genotypes as the explanatory factors. The ultimate aim is to estimate genotype means and credible intervals, which calls for the Bayesian approach.&lt;/p&gt;
&lt;p&gt;For the sake of simplicity, let’s take both the block and genotype effects as fixed; in matrix notation, a general linear fixed effects model can be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = X \, \beta + \varepsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the vector of the observed response, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the vector of estimated parameters, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the design matrix and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the vector of residuals, which we assume as gaussian and homoscedastic (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon \sim N(0, \sigma^2 I)\)&lt;/span&gt;; N is the multivariate gaussian distribution). The same model can also be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim N \left( X \, \beta, \sigma^2 I \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In JAGS (and maybe also in other BUGS dialects), we can code every linear model using the above specification, as long as we can provide the correct design matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Luckily, we will see that this is a rather simple task; but… let’s do it one step at a time!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specification-of-a-jags-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Specification of a JAGS model&lt;/h1&gt;
&lt;p&gt;First of all, we open R and code a general linear JAGS model, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Coding a JAGS model
modelSpec &amp;lt;- &amp;quot;
data {
n &amp;lt;- length(Y)
np &amp;lt;- dim(X)
nk &amp;lt;- dim(K)
}

model {
# Model 
for (i in 1:n) {
   expected[i] &amp;lt;- inprod(X[i,], beta)
   Y[i] ~ dnorm(expected[i], tau)
  }

# Priors
beta[1] ~ dunif(0, 1000000)
for (i in 2:np[2]){
  beta[i] ~ dnorm(0, 0.000001)
  }
sigma ~ dunif(0, 100)

# Derived quantities (model specific)
tau &amp;lt;- 1 / ( sigma * sigma)

# Contrasts of interest
for (i in 1:nk[1]) {
   mu[i] &amp;lt;- inprod(K[i,], beta)
  }
}&amp;quot;
writeLines(modelSpec, con=&amp;quot;ModelAOV.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see some more detail; you may notice that the code above consists of two fundamental parts, surrounded by curly brackets:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a ‘data’ part&lt;/li&gt;
&lt;li&gt;a ‘model’ part&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the data part, we create three variables, i.e. the number of data (&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;), the number of estimated parameters (&lt;span class=&#34;math inline&#34;&gt;\(np\)&lt;/span&gt;) and the number of contrasts (see later). All variables are used in successive model steps and they are obtained, respectively, by counting the number of observations in the vector &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, the number of columns in the design matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the number of rows in the contrast matrix &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the model part we have three further components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the model specification&lt;/li&gt;
&lt;li&gt;the priors&lt;/li&gt;
&lt;li&gt;the derived quantities&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model specification contains ‘deterministic’ and ‘stochastic’ statements (nodes). The ‘deterministic’ node returns the expected values for all observations, based on multiplying the design matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by the vector of estimated parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. In practice, we use a ‘for()’ loop and, for each i&lt;sup&gt;th&lt;/sup&gt; observation, we sum the products of all element in the i&lt;sup&gt;th&lt;/sup&gt; row of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; by the corresponding elements in the vector of estimated parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. This sum of products is accomplished by using the function &lt;code&gt;inprod(X[i,], beta)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the ‘stochastic’ node we specify that the observed values in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are sampled from a gaussian distribution (‘dnorm’), with mean equal to the expected value and precision equal to ‘tau’. In JAGS, WinBUGS and all related software, the normal distribution is parameterised by using the precision &lt;span class=&#34;math inline&#34;&gt;\(\tau = 1/\sigma^2\)&lt;/span&gt;, instead of standard deviation.&lt;/p&gt;
&lt;p&gt;Next, we have to define the priors for all the estimands. For those who are not very much into Bayesian inference, I will only say that priors represent our expectations about model parameters before looking at the data; in this example, we use very vague priors, meaning that, before looking at the data, we have no idea about the values of these unknown quantities. In detail, for the intercept we specify a uniform distribution from 0 to 10000 (&lt;code&gt;beta[1] ~ dunif(0, 1000000)&lt;/code&gt;), meaning that the overall mean might be included between 0 and 10000 and we have no preference for any values within that range (a very vague prior, indeed). For all other effects in the vector &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, our prior expectation is that they are normally distributed with a mean of 0 and very low precision (&lt;code&gt;beta[i] ~ dnorm(0, 0.000001)&lt;/code&gt;). For the residual standard deviation, we expect that it is uniformly distributed from 0 to 100. The selection of priors is central to Bayesian inference and, in other circumstances, you may like to adopt more informative priors. We do not discuss this important item here.&lt;/p&gt;
&lt;p&gt;In the end, we also specify some quantities that should be derived from estimated parameters. As we have put a prior on standard deviation, we need to derive the precision (&lt;code&gt;tau &amp;lt;- 1 / ( sigma * sigma)&lt;/code&gt;), that is necessary for the stochastic node in the specification of our linear model. Afterwards, we add a set of contrasts, which are specified by way of a matrix of contrast coefficients (&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;; one row per each contrast). This is useful to calculate, e.g., the means of treatment levels or pairwise differences between means as linear combinations of model parameters.&lt;/p&gt;
&lt;p&gt;The model definition in the box above is assigned to a text string (&lt;code&gt;modelSpec&lt;/code&gt;) and it is finally written to an external text file (‘modelAOV.txt’), using the function &lt;code&gt;writeLines()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I conclude this part by saying that, based on our model specification, JAGS requires three input ingredients: the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; vector of responses, the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix and the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; contrast matrix. Furthermore, JAGS requires initial values for all estimands, i.e. for all quantities for which we have specified our prior expectations (the ‘beta’ vector and the ‘sigma’ scalar).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-jags-model-from-within-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting the JAGS model from within R&lt;/h1&gt;
&lt;p&gt;JAGS models can be fitted from R by using the &lt;code&gt;rjags&lt;/code&gt; package (Plummer, 2019). However, we have some preliminary steps to accomplish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;loading the dataset (see the first box above);&lt;/li&gt;
&lt;li&gt;creating the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; vector of responses&lt;/li&gt;
&lt;li&gt;creating the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix&lt;/li&gt;
&lt;li&gt;creating the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrix&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first two steps are obvious. The third step can be accomplished by using the &lt;code&gt;model.matrix()&lt;/code&gt; function: the call is very similar to an ‘lm()’ call, although we do not need to explicitly indicate the response variable (see the box below). In order to create the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrix of contrasts, we might prefer to work with the sum-to-zero parameterisation (&lt;code&gt;options(contrasts=c(&amp;quot;contr.sum&amp;quot;, &amp;quot;contr.poly&amp;quot;))&lt;/code&gt;), so that the intercept represents the overall mean (for balanced designs) and the effects of blocks and genotypes represent differences with respect to the overall mean. In the box below we specify a set of eight contrasts returning the means for all genotypes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(contrasts=c(&amp;quot;contr.sum&amp;quot;, &amp;quot;contr.poly&amp;quot;))
Y &amp;lt;- dataset$Yield  
X &amp;lt;- model.matrix( ~ Block + Genotype, data = dataset)

k1 &amp;lt;- c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0)
k2 &amp;lt;- c(1, 0, 0, 0, 1, 0, 0, 0, 0, 0)
k3 &amp;lt;- c(1, 0, 0, 0, 0, 1, 0, 0, 0, 0)
k4 &amp;lt;- c(1, 0, 0, 0, 0, 0, 1, 0, 0, 0)
k5 &amp;lt;- c(1, 0, 0, 0, 0, 0, 0, 1, 0, 0)
k6 &amp;lt;- c(1, 0, 0, 0, 0, 0, 0, 0, 1, 0)
k7 &amp;lt;- c(1, 0, 0, 0, 0, 0, 0, 0, 0, 1)
k8 &amp;lt;- c(1, 0, 0,-1,-1,-1,-1,-1,-1,-1)
K &amp;lt;- rbind(k1, k2, k3, k4, k5, k6, k7, k8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need further explanation about the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrices and their role in the analysis, I have added an appendix below. Otherwise, we are ready to fit the model. To this aim, we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;load the &lt;code&gt;rjags&lt;/code&gt; library;&lt;/li&gt;
&lt;li&gt;create two lists: a list of all the data needed for the analysis (&lt;code&gt;dataList&lt;/code&gt;) and a list of the initial values for the parameters to be estimated (&lt;code&gt;initList&lt;/code&gt;). Initial values need not be particularly precise;&lt;/li&gt;
&lt;li&gt;send the model specification and the other data to JAGS, using the function &lt;code&gt;jags.model()&lt;/code&gt; from the &lt;code&gt;rjags&lt;/code&gt; package;&lt;/li&gt;
&lt;li&gt;start the sampler, using the &lt;code&gt;coda.samples()&lt;/code&gt; function. In this step, we specify which parameters we want to obtain estimates for and the number of samples we want to draw (&lt;code&gt;n.iter&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;obtain the number of required samples, using the &lt;code&gt;window()&lt;/code&gt; function. In this step, we specify how many samples should be discarded as &lt;code&gt;burn.in&lt;/code&gt;. These samples might have been produced before reaching the convergence, so they might not come from the correct posterior distribution and we need to get rid of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the posterior, we obtain the mean and median as measures of central tendency, the standard deviation as a measure of uncertainty and credible intervals, which are the Bayesian analog to confidence intervals. Due to our vague priors, the results are very similar to those obtained with a traditional frequentist analysis (see the appendix below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rjags)

# Create lists
dataList &amp;lt;- list(Y = Y, X = X, K = K)
initList &amp;lt;- list(beta = c(4.3, rep(0, 9)), sigma = 0.33)

# Start sampler
mcmc &amp;lt;- jags.model(&amp;quot;modelAOV.txt&amp;quot;, 
                   data = dataList, inits = initList, 
                   n.chains = 4, n.adapt = 100)
## Compiling data graph
##    Resolving undeclared variables
##    Allocating nodes
##    Initializing
##    Reading data back into data table
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 24
##    Unobserved stochastic nodes: 11
##    Total graph size: 451
## 
## Initializing model
# Get samples
res &amp;lt;- coda.samples(mcmc, variable.names = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;mu&amp;quot;),
                    n.iter = 1000)

out &amp;lt;- summary(window(res, start = 110))
res &amp;lt;- cbind(out$statistics[,1:2], out$quantiles[,c(1,5)])
res
##                 Mean         SD        2.5%       97.5%
## beta[1]   4.42459380 0.07450192  4.27331180  4.56808970
## beta[2]  -0.21830958 0.10451119 -0.42114931 -0.01073556
## beta[3]  -0.01067826 0.10489729 -0.22122939  0.19833336
## beta[4]   0.46390007 0.19802745  0.04693158  0.83344935
## beta[5]  -0.09827277 0.19450502 -0.48923642  0.27830973
## beta[6]  -0.18856790 0.19460922 -0.58070525  0.19744961
## beta[7]  -0.07897598 0.20104587 -0.46988505  0.32557805
## beta[8]   0.54737970 0.19813294  0.15200081  0.93771826
## beta[9]   0.07786131 0.19946009 -0.31222840  0.48406509
## beta[10] -1.09035526 0.19353015 -1.47151174 -0.70790010
## mu[1]     4.88849386 0.21213313  4.44865961  5.29774010
## mu[2]     4.32632102 0.20825090  3.91707525  4.71763694
## mu[3]     4.23602590 0.20766216  3.83381632  4.64905719
## mu[4]     4.34561782 0.21404163  3.93107938  4.77594175
## mu[5]     4.97197349 0.21222639  4.54813702  5.38971184
## mu[6]     4.50245511 0.21323351  4.09000675  4.93210416
## mu[7]     3.33423854 0.20600397  2.92628945  3.73812600
## mu[8]     4.79162462 0.21207498  4.36009497  5.21086914
## sigma     0.35632989 0.08036709  0.24276414  0.55078418&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reusing-the-code-for-a-multi-environment-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reusing the code for a multi-environment experiment&lt;/h1&gt;
&lt;p&gt;The JAGS model above is very general and can be easily reused for other situations. For example, if the above genotype experiment is replicated across years, we might like to fit an ANOVA model by considering the blocks (within years), the genotypes, the years and the ‘year by genotype’ interaction. The dataset is available in the same external repository, as the ‘WinterWheat.csv’ file.&lt;/p&gt;
&lt;p&gt;The JAGS specification for this multienvironment model does not change, we only need to update the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; matrices, as shown in the box below. In order to obtain the contrast matrix for the means of the ‘genotype x environment’ combinations we need to write some cumbersome code, as shown below (but, perhaps, some of you could suggest better alternatives…).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

# Loading the data
fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat.csv&amp;quot;
dataset &amp;lt;- read_csv(fileName)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Block, Year, Genotype), .fns = factor))
dataset
## # A tibble: 168 x 5
##     Plot Block Genotype Yield Year 
##    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;
##  1     2 1     COLOSSEO  6.73 1996 
##  2   110 2     COLOSSEO  6.96 1996 
##  3   181 3     COLOSSEO  5.35 1996 
##  4     2 1     COLOSSEO  6.26 1997 
##  5   110 2     COLOSSEO  7.01 1997 
##  6   181 3     COLOSSEO  6.11 1997 
##  7    17 1     COLOSSEO  6.75 1998 
##  8   110 2     COLOSSEO  6.82 1998 
##  9   256 3     COLOSSEO  6.52 1998 
## 10    18 1     COLOSSEO  7.18 1999 
## # … with 158 more rows
# Create input matrices
Y &amp;lt;- dataset$Yield 
X &amp;lt;- model.matrix(~ Genotype*Year +  Block:Year, data = dataset)

# Workaround to get K matrix
asgn &amp;lt;- attr(X, &amp;quot;assign&amp;quot;)
tmp1 &amp;lt;- expand.grid(Genotype = unique(dataset$Genotype),
                    Year = unique(dataset$Year))
K1 &amp;lt;- model.matrix(~ Genotype*Year, data = tmp1)
K2 &amp;lt;- matrix(0, nrow = nrow(K1), ncol = length(asgn[asgn==4]))
colnames(K2) &amp;lt;- colnames(X)[asgn==4]
K &amp;lt;- cbind(K1, K2)
row.names(K) &amp;lt;- with(tmp1, interaction(Genotype:Year))
# K

# Create lists
dataList &amp;lt;- list(Y = Y, X = X, K = K)
initList &amp;lt;- list(beta = c(4.3, rep(0, length(X[1,])-1)), sigma = 0.33)

# Start sampler
mcmc &amp;lt;- jags.model(&amp;quot;modelAOV.txt&amp;quot;, 
                   data = dataList, inits = initList, 
                   n.chains = 4, n.adapt = 100)
## Compiling data graph
##    Resolving undeclared variables
##    Allocating nodes
##    Initializing
##    Reading data back into data table
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 168
##    Unobserved stochastic nodes: 71
##    Total graph size: 16519
## 
## Initializing model
# Get samples
res &amp;lt;- coda.samples(mcmc, variable.names = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma&amp;quot;, &amp;quot;mu&amp;quot;),
                    n.iter = 1000)

out &amp;lt;- summary(window(res, start = 110))
res &amp;lt;- cbind(out$statistics[,1:2], &amp;#39;50%&amp;#39;=out$quantiles[,3],
             out$quantiles[,c(1, 5)])
head(res)
##               Mean         SD        50%         2.5%      97.5%
## beta[1]  6.2677640 0.03043537  6.2673982  6.208390004  6.3300080
## beta[2]  0.1464792 0.07854643  0.1461239 -0.009216115  0.2979934
## beta[3] -0.2947195 0.07764813 -0.2955634 -0.446178189 -0.1401161
## beta[4]  0.3248028 0.07746617  0.3246501  0.174748219  0.4733587
## beta[5] -0.1843623 0.08221827 -0.1854063 -0.349242266 -0.0245657
## beta[6]  0.4269969 0.07989058  0.4260559  0.272364897  0.5842687
#....
tail(res)
##             Mean         SD       50%      2.5%    97.5%
## mu[52] 4.3419716 0.22876493 4.3422616 3.8980334 4.792529
## mu[53] 4.9626900 0.22571023 4.9634626 4.5132893 5.392722
## mu[54] 4.5076303 0.22951179 4.5094695 4.0518930 4.955052
## mu[55] 3.3378495 0.22618643 3.3342110 2.9010714 3.769008
## mu[56] 4.7907886 0.22765164 4.7854894 4.3385675 5.241458
## sigma  0.3915724 0.02904505 0.3893148 0.3428075 0.454776&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The discovery of the &lt;code&gt;inprod()&lt;/code&gt; function was a very big hit for me: the above approach is very flexible and lend itself to a lot of potential uses, including fitting mixed models. I will show some examples in future posts.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy 2021! Let’s hope we finally get back to normality!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Kery, M., 2010. Introduction to WinBUGS for ecologists. A Bayesian approach to regression, ANOVA, mixed models and related analyses. Academic Press, Burlington, MA (USA).&lt;/li&gt;
&lt;li&gt;Plummer M. (2019). rjags: Bayesian Graphical Models using MCMC. R package version 4-10. &lt;a href=&#34;https://CRAN.R-project.org/package=rjags&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rjags&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;I feel that it may be useful to take a look at the results of traditional model fitting with the &lt;code&gt;lm()&lt;/code&gt; function and to explore the role of the matrices &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. Let’s go back to the first example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/WinterWheat2002.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName, header = T)
mod.aov &amp;lt;- lm(Yield ~ Block + Genotype, data = dataset)
summary(mod.aov)
## 
## Call:
## lm(formula = Yield ~ Block + Genotype, data = dataset)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.38458 -0.12854 -0.08271  0.20396  0.51875 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  4.424583   0.065697  67.348  &amp;lt; 2e-16 ***
## Block1      -0.218333   0.092910  -2.350  0.03397 *  
## Block2      -0.009583   0.092910  -0.103  0.91931    
## Genotype1    0.468750   0.173818   2.697  0.01737 *  
## Genotype2   -0.097917   0.173818  -0.563  0.58212    
## Genotype3   -0.184583   0.173818  -1.062  0.30624    
## Genotype4   -0.084583   0.173818  -0.487  0.63406    
## Genotype5    0.538750   0.173818   3.100  0.00784 ** 
## Genotype6    0.078750   0.173818   0.453  0.65745    
## Genotype7   -1.084583   0.173818  -6.240 2.16e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3218 on 14 degrees of freedom
## Multiple R-squared:  0.8159, Adjusted R-squared:  0.6976 
## F-statistic: 6.895 on 9 and 14 DF,  p-value: 0.0007881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s also look at the first row of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix, which I can also retrieve from the fitted model object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.matrix(mod.aov)[1,]
## (Intercept)      Block1      Block2   Genotype1   Genotype2   Genotype3 
##           1           1           0           1           0           0 
##   Genotype4   Genotype5   Genotype6   Genotype7 
##           0           0           0           0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the vector of estimated parameters and the first row in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; have 10 elements: if we multiply them and sum, we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 1 \cdot 4.425 + 1 \cdot -0.218 + 0 \cdot -0.0096 + 1 \cdot 0.469 + 0 \cdot ... = 4.675\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is exactly the first fitted value (first genotype in first block):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted(mod.aov)[1]
##     1 
## 4.675&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we do this for all rows in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, we obtain all fitted values and such an operation is most quickly done by using matrix multiplication.&lt;/p&gt;
&lt;p&gt;Likewise, if we multiply the elements in ‘beta’ for the corresponding elements in the first row of the ‘K’ matrix and sum, we get the mean for the first genotype (COLOSSEO) and if we do so for all rows in &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; we get all the genotype means.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 1 \cdot 4.425 + 0 \cdot -0.218 + 0 \cdot -0.0096 + 1 \cdot 0.469 + 0 \cdot ... = 4.893\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emmeans::emmeans(mod.aov, ~Genotype) 
##  Genotype emmean    SE df lower.CL upper.CL
##  COLOSSEO   4.89 0.186 14     4.49     5.29
##  CRESO      4.33 0.186 14     3.93     4.73
##  DUILIO     4.24 0.186 14     3.84     4.64
##  GRAZIA     4.34 0.186 14     3.94     4.74
##  IRIDE      4.96 0.186 14     4.56     5.36
##  SANCARLO   4.50 0.186 14     4.10     4.90
##  SIMETO     3.34 0.186 14     2.94     3.74
##  SOLEX      4.79 0.186 14     4.39     5.19
## 
## Results are averaged over the levels of: Block 
## Confidence level used: 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this is useful!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>From &#39;&#39;for()&#39;&#39; loops to the &#39;&#39;split-apply-combine&#39;&#39; paradigm for column-wise tasks: the transition for a dinosaur</title>
      <link>/2020/stat_r_tidyverse_columnwise/</link>
      <pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_r_tidyverse_columnwise/</guid>
      <description>


&lt;p&gt;I have been involved with data crunching for 30 years, and, due to my age, I see myself as a dinosaur within the R-users community. I must admit, I’m rather slow to incorporate new paradigms in my programming workflow … I’m pretty busy and the time I save today is often more important than the time I could save in the future, by picking up new techniques. However, resisting to progress is not necessarily a good idea and, from time to time, also a dinosaur feels like living more dangerously and exploring new ideas and views.&lt;/p&gt;
&lt;p&gt;Today, I want to talk about column-wise tasks in relatively big datasets. These tasks are rather common with agricultural and biological experiments, where we have several subjects in different treatment groups and we record, for each subject, a high number of traits. In these conditions, the very first step to data analyses is to calculate several descriptive stats for each group of subjects and each variable. What is the best method to write simple and reusable code? As I will show later, the most natural approach to me may not necessarily be the most fashionable one.&lt;/p&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The example&lt;/h1&gt;
&lt;p&gt;A few days ago, I met a colleague from the plant pathology group, who asked my co-operation for an experiment where he studied the content in 62 mycotoxins in wheat caryopses, as recorded in 70 samples coming from different Italian regions. The dataset was structured as 70 x 64 table, as shown in the scheme below: there is one row for each wheat sample, while the first column represents the sample id, the second column represents the region from where that sample was collected and the other columns represent the concentrations of the 62 toxins (one column per toxin).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/_Figures/Stat_general_tidyverse.png&#34; width=&#34;75%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That colleague wanted me to calculate, for each toxin and for each region, the following stats:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;number of collected samples&lt;/li&gt;
&lt;li&gt;mean concentration value&lt;/li&gt;
&lt;li&gt;maximum concentration value&lt;/li&gt;
&lt;li&gt;standard error&lt;/li&gt;
&lt;li&gt;number of contaminated samples (concentration higher than 0)&lt;/li&gt;
&lt;li&gt;percentage of contaminated samples&lt;/li&gt;
&lt;li&gt;mean value for contaminated samples&lt;/li&gt;
&lt;li&gt;standard error for contaminated samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;He also wanted me to return to him a list of 62 tables (one per toxin), with all the regions along with their descriptive stats.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;i-have-already-done-something-like-this&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;I have already done something like this!&lt;/h1&gt;
&lt;p&gt;When facing this task, my first reaction was to search in my ‘computer’s attic’, looking for previously written codes to accomplish the very same task. As I said, this is a fairly common task in my everyday work and I could find several examples of old codes. Looking at those, I realised that, when the number of variables is high, I have so far made an extensive use of &lt;code&gt;for&lt;/code&gt; loops to repeat the same task across columns. This is indeed no wonder, as I came across R in 2000, after an extensive usage of general purpose languages, such as Quick Basic and Visual Basic.&lt;/p&gt;
&lt;p&gt;In practise, most of my earliest R codes were based on the &lt;code&gt;tapply()&lt;/code&gt; function to calculate statistics for grouped data and &lt;code&gt;for&lt;/code&gt; loops to iterate over columns. In the box below I show an example of such an approach, by using a factitious dataset, with the very same structure as my colleague’s dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
dataset &amp;lt;- read.csv(&amp;quot;https://casaonofri.it/_datasets/Mycotoxins.csv&amp;quot;, header = T)
# str(dataset)
# &amp;#39;data.frame&amp;#39;: 70 obs. of  64 variables:
#  $ Sample : int  1 2 3 4 5 6 7 8 9 10 ...
#  $ Region : chr  &amp;quot;Lombardy&amp;quot; &amp;quot;Lombardy&amp;quot; &amp;quot;Lombardy&amp;quot; &amp;quot;Lombardy&amp;quot; ...
#  $ DON    : num  8.62 16.2 18.19 27.08 10.97 ...
#  $ DON3G  : num  21.7 28.4 34.7 26.9 26.4 ...
#  $ NIV    : num  23.5 25.3 22.6 27.8 29.4 ...
#  $ NIVG   : num  38.6 26.2 13.5 21.4 15.4 ...
#  $ T2     : num  23.9 23.6 19.7 25.7 21.7 ...
#  $ HT2    : num  19 37.6 32.1 22.3 25.6 ...
#  $ HT2G   : num  13.7 25.7 25.8 33.4 32.7 ...
#  $ NEO    : num  29.06 7.56 27.52 32.91 24.49 ...

returnList &amp;lt;- list()
for(i in 1:(length(dataset[1,]) - 3)){
  y &amp;lt;- as.numeric( unlist(dataset[, i + 3]) )
  Count &amp;lt;- tapply(y, dataset$Region, length)
  Mean &amp;lt;- tapply(y, dataset$Region, mean)
  Max &amp;lt;- tapply(y, dataset$Region, max)
  SE &amp;lt;- tapply(y, dataset$Region, sd)/sqrt(Count)
  nPos &amp;lt;- tapply(y != 0, dataset$Region, sum)
  PercPos &amp;lt;- tapply(y != 0, dataset$Region, mean)*100
  muPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA), dataset$Region, mean, na.rm = T)
  muPos[is.na(muPos)] &amp;lt;- 0
  sdPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA), dataset$Region, sd, na.rm = T)
  SEpos &amp;lt;- sdPos/sqrt(nPos)
  returnList[[i]] &amp;lt;- data.frame(cbind(Count, Mean, Max, SE, nPos, PercPos, muPos, SEpos))
  names(returnList)[[i]] &amp;lt;- colnames(dataset)[i + 3]
}
print(returnList$CRV, digits = 2)
##                 Count Mean Max   SE nPos PercPos muPos SEpos
## Abruzzo             4   28  30 0.85    4     100    28  0.85
## Apulia              9   25  40 2.67    9     100    25  2.67
## Campania            2   20  21 0.74    2     100    20  0.74
## Emilia Romagna      8   23  33 2.80    8     100    23  2.80
## Latium              7   20  33 2.76    7     100    20  2.76
## Lombardy            4   25  32 5.12    4     100    25  5.12
## Molise              1   18  18   NA    1     100    18    NA
## Sardinia            6   25  38 3.32    6     100    25  3.32
## Sicily              6   21  30 2.94    6     100    21  2.94
## The Marche          5   19  23 1.58    5     100    19  1.58
## Tuscany             5   30  34 1.22    5     100    30  1.22
## Umbria              9   21  32 2.83    9     100    21  2.83
## Veneto              4   23  28 1.99    4     100    23  1.99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I must admit that the above code is ugly: first, &lt;code&gt;for&lt;/code&gt; loops in R are not very efficient and, second, reusing that code requires quite a bit of editing and it is prone to errors. Therefore, I asked myself how I could write more efficient code …&lt;/p&gt;
&lt;p&gt;First of all, I thought I should use &lt;code&gt;apply()&lt;/code&gt; instead of the &lt;code&gt;for&lt;/code&gt; loop. Thus I wrote a function to calculate the required stats for each column and &lt;code&gt;apply()-ed&lt;/code&gt; that function to all columns of my data-frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funBec &amp;lt;- function(y, group){
  # y &amp;lt;- as.numeric( unlist(dataset[, i + 3]) )
  Count &amp;lt;- tapply(y, group, length)
  Mean &amp;lt;- tapply(y, group, mean)
  Max &amp;lt;- tapply(y, group, max)
  SE &amp;lt;- tapply(y, group, sd)/sqrt(Count)
  nPos &amp;lt;- tapply(y != 0, group, sum)
  PercPos &amp;lt;- tapply(y != 0, group, mean)*100
  muPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA),group, mean, na.rm = T)
  muPos[is.na(muPos)] &amp;lt;- 0
  sdPos &amp;lt;- tapply(ifelse(y &amp;gt; 0, y, NA), group, sd, na.rm = T)
  SEpos &amp;lt;- sdPos/sqrt(nPos)
  data.frame(cbind(Count, Mean, Max, SE, nPos, PercPos, muPos, SEpos))
}
returnList2 &amp;lt;- apply(dataset[3:length(dataset[1,])], 2,
                     function(col) funBec(col, dataset$Region))
# kable(returnList2$CRV, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am pretty happy with the above approach; it feels ‘natural’, to me and the coding appears to be pretty clear and easily reusable. However, it makes only use of the &lt;code&gt;base&lt;/code&gt; R implementation and, therefore, it looks a bit out-fashioned… a dinosaur’s code…&lt;/p&gt;
&lt;p&gt;What could I possibly do to write more ‘modern’ code? A brief survey of posts from the R world suggested the ultimate solution: “I must use the ‘tidyverse’!”. Therefore, I tried to perform my column-wise task by using &lt;code&gt;dplyr&lt;/code&gt; and related packages and I must admit that I could not immediately find an acceptable solution. After some careful thinking, it was clear that the tidyverse requires a strong change in programming habits, which is not always simple for those who are over a certain age. Or, at least, that is not simple for me…&lt;/p&gt;
&lt;p&gt;Eventually, I thought I should switch from a &lt;code&gt;for&lt;/code&gt; attitude to a &lt;code&gt;split-apply-combine&lt;/code&gt; attitude; indeed, I discovered that, if I ‘melted’ the dataset so that the variables were stacked one above the other, I could, consequently:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;split the dataset in several groups, consisting of one toxin and one region,&lt;/li&gt;
&lt;li&gt;calculate the stats for each group, and&lt;/li&gt;
&lt;li&gt;combine the results for all groups.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’d like to share the final code: I used the &lt;code&gt;pivot_longer()&lt;/code&gt; function to melt the dataset, the &lt;code&gt;group_by()&lt;/code&gt; function to create an ‘internal’ grouping structure by regions and toxins, the &lt;code&gt;summarise()&lt;/code&gt; and &lt;code&gt;across()&lt;/code&gt; functions to calculate the required stats for each group. In the end, I obtained a tibble, that I split into a list of tibbles, by using the &lt;code&gt;group_split()&lt;/code&gt; function. The code is shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
returnList6 &amp;lt;- dataset %&amp;gt;%
  select(-Sample) %&amp;gt;% 
  pivot_longer(names_to = &amp;quot;Toxin&amp;quot;, values_to = &amp;quot;Conc&amp;quot;,
               cols = c(3:length(dataset[1,]) - 1)) %&amp;gt;% 
  group_by(Toxin, Region) %&amp;gt;% 
  summarise(across(&amp;quot;Conc&amp;quot;, .fns =
               list(Mean = mean,
                    Max = max,
                    SE = function(df) sd(df)/sqrt(length(df)),
                    nPos = function(df) length(df[df &amp;gt; 0]),
                    percPos = function(df) length(df[df &amp;gt; 0])/length(df)*100,
                    muPos =  function(df) mean(df[df &amp;gt; 0]),
                    SEpos =  function(df) sd(df[df &amp;gt; 0])/sqrt(length(df[df &amp;gt; 0]))
                    ))) %&amp;gt;% 
  ungroup() %&amp;gt;%
  group_split(Toxin, .keep = F)
names(returnList6) &amp;lt;- names(dataset)[3:64]
# returnList6$CRV&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, I can say that with the above code I have, at least, tried not to be a dinosaur… but, I am sure that many young scientists out there can suggest better solutions. If so, please drop me a line at the email address below. Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;[UPDATE: 13/12/2020]&lt;/p&gt;
&lt;p&gt;Bryan Hutchinson from the UK proposed an alternative solution based on the ‘data.table’ and ‘DescTools’ libraries. I have never used these libraries before, but the code looks very efficient and elegant. At the end, some examples of how the dataset can be filtered are given. Thanks, Bryan!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(DescTools)
 
dataset &amp;lt;- fread(&amp;quot;https://casaonofri.it/_datasets/Mycotoxins.csv&amp;quot;, header = T)
tnames &amp;lt;- names(dataset[,-c(1:2)])
 
sum_stats &amp;lt;- function(var) {
  # convert integer to double
  var &amp;lt;- as.numeric(var)
  list(
    mean = mean(var, na.rm = TRUE),
    max = max(var, na.rm = TRUE),
    se = sd(var, na.rm = TRUE) / sqrt(length(var)),
    nPos = length(var &amp;gt;0),
    percPos = length(var &amp;gt;0)/length(var)*100,
    muPos =  mean(var &amp;gt; 0),
    SEpos =  sd(var &amp;gt; 0)/sqrt(length(var &amp;gt; 0))
  )
}
 
df_long &amp;lt;- melt(dataset,
                measure.vars = list(tnames),
                variable.name = &amp;quot;Toxin&amp;quot;,
                value.name = &amp;quot;Conc&amp;quot;)
 
 
# df_long[, sum_stats(Conc), .(Toxin, Region)]
# df_long[Toxin == &amp;quot;FLAG&amp;quot;, sum_stats(Conc), .(Toxin, Region)]
# df_long[Region == &amp;quot;Lombardy&amp;quot;, sum_stats(Conc), .(Region, Toxin)]&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;[UPDATE: 14/12/2020]&lt;/p&gt;
&lt;p&gt;A younger collegue (Renzo Bonifazi) suggested some changes to the code, which I am happy to share here. Setting the range of columns at the beginning seems to be wise and removing the ‘ungroup()’ function is correct. I did not know about the ‘setNames()’ function, which was used at the end. Thanks, Renzo!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define here the range of cols to apply upon the code
range_cols &amp;lt;- c(3:length(dataset[1,]))

results &amp;lt;- dataset %&amp;gt;%
  select(-Sample) %&amp;gt;% 
  pivot_longer(names_to = &amp;quot;Toxin&amp;quot;, values_to = &amp;quot;Conc&amp;quot;,
               cols = range_cols - 1) %&amp;gt;% # use user defined range of cols
  group_by(Toxin, Region) %&amp;gt;% 
  summarise(across(&amp;quot;Conc&amp;quot;, .fns =
                     list(Mean = mean,
                          Max = max,
                          SE = function(df) sd(df)/sqrt(length(df)),
                          nPos = function(df) length(df[df &amp;gt; 0]),
                          percPos = function(df) length(df[df &amp;gt; 0])/length(df)*100,
                          muPos =  function(df) mean(df[df &amp;gt; 0]),
                          SEpos =  function(df) sd(df[df &amp;gt; 0])/sqrt(length(df[df &amp;gt; 0]))
                     ))) %&amp;gt;%
   group_split(.keep = F) %&amp;gt;% # This fun by default seems to split based on the first defined group var, i.e. &amp;quot;Toxin&amp;quot; in this case
setNames(names(dataset)[range_cols]) # define the names internally&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for the experimental design in linear/nonlinear regression analyses</title>
      <link>/2020/stat_nlmm_designconstraints/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_nlmm_designconstraints/</guid>
      <description>


&lt;p&gt;In this post, I am going to talk about an issue that is often overlooked by agronomists and biologists. The point is that field experiments are very often laid down in blocks, using split-plot designs, strip-plot designs or other types of designs with grouping factors (blocks, main-plots, sub-plots). We know that these grouping factors should be appropriately accounted for in data analyses: ‘analyze them as you have randomized them’ is a common saying attributed to Ronald Fisher. Indeed, observations in the same group are correlated, as they are more alike than observations in different groups. What happens if we neglect the grouping factors? We break the independence assumption and our inferences are invalid (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;To my experience, field scientists are totally aware of this issue when they deal with ANOVA-type models (e.g., see Jensen et al., 2018). However, a brief survey of literature shows that there is not the same awareness, when field scientists deal with linear/nonlinear regression models. Therefore, I decided to sit down and write this post, in the hope that it may be useful to obtain more reliable data analyses.&lt;/p&gt;
&lt;div id=&#34;an-example-with-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with linear regression&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the ‘yieldDensity.csv’ dataset, that is available on gitHub. It represents an experiment where sunflower was tested with increasing weed densities (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 10 blocks. a swift plot shows that yield is linearly related to weed density, which calls for linear regression analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(lattice)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/yieldDensityB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yield
## 1     1       0 29.90
## 2     2       0 34.23
## 3     3       0 37.12
## 4     4       0 26.37
## 5     5       0 34.48
## 6     6       0 33.70
plot(yield ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We might be tempted to neglect the block effect and run a linear regression analysis of yield against density. This is clearly wrong (I am violating the independence assumption) and inefficient, as any block-to-block variability goes into the residual error term, which is, therefore, inflated.&lt;/p&gt;
&lt;p&gt;Some of my collegues would take the means for densities and use those to fit a linear regression model (two-steps analysis). By doing so, block-to-block variability is cancelled out and the analysis becomes more efficient. However, such a solution is not general, as it is not feasible, e.g., when we have unbalanced designs and heteroscedastic data. With the appropriate approach, sound analyses can also be made in two-steps (Damesa et al., 2017). From my point of view, it is reasonable to search for more general solutions to deal with one-step analyses.&lt;/p&gt;
&lt;p&gt;Based on our experience with traditional ANOVA models, we might think of taking the block effect as fixed and fit it as and additive term. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg &amp;lt;- lm(yield ~ block + density, data=dataset)
summary(mod.reg)
## 
## Call:
## lm(formula = yield ~ block + density, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6062 -0.8242 -0.3315  0.7505  4.6244 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 29.10462    0.57750  50.397  &amp;lt; 2e-16 ***
## block2       4.57750    0.74668   6.130 4.81e-08 ***
## block3       7.05875    0.74668   9.453 4.49e-14 ***
## block4      -3.98000    0.74668  -5.330 1.17e-06 ***
## block5       6.17625    0.74668   8.272 6.37e-12 ***
## block6       5.92750    0.74668   7.938 2.59e-11 ***
## block7       1.23750    0.74668   1.657  0.10199    
## block8       1.25500    0.74668   1.681  0.09733 .  
## block9       2.34875    0.74668   3.146  0.00245 ** 
## block10      2.25125    0.74668   3.015  0.00359 ** 
## density     -0.26744    0.00701 -38.149  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.493 on 69 degrees of freedom
## Multiple R-squared:  0.9635, Adjusted R-squared:  0.9582 
## F-statistic: 181.9 on 10 and 69 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With regression, this solution is not convincing. Indeed, the above model assumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected. Is this a reasonable assumption? I vote no.&lt;/p&gt;
&lt;p&gt;Let’s check this by fitting a different regression model per block (ten different slopes + ten different intercepts):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.reg2 &amp;lt;- lm(yield ~ block/density + block, data=dataset)
anova(mod.reg, mod.reg2)
## Analysis of Variance Table
## 
## Model 1: yield ~ block + density
## Model 2: yield ~ block/density + block
##   Res.Df    RSS Df Sum of Sq      F  Pr(&amp;gt;F)  
## 1     69 153.88                              
## 2     60 115.75  9    38.135 2.1965 0.03465 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-level confirms that the block had a significant effect both on the intercept and on the slope. To describe such an effect we need 20 parameters in the model, which is not very parsimonious. And above all: which regression line do we use for predictions? Taking the block effect as fixed is clearly sub-optimal with regression models.&lt;/p&gt;
&lt;p&gt;The question is: can we fit a simpler and clearer model? The answer is: yes. Why don’t we take the block effect as random? This is perfectly reasonable. Let’s do it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = ~ density|block, data=dataset)
summary(modMix.1)
## Linear mixed-effects model fit by REML
##  Data: dataset 
##        AIC      BIC    logLik
##   340.9166 355.0569 -164.4583
## 
## Random effects:
##  Formula: ~density | block
##  Structure: General positive-definite, Log-Cholesky parametrization
##             StdDev     Corr  
## (Intercept) 3.16871858 (Intr)
## density     0.02255249 0.09  
## Residual    1.38891957       
## 
## Fixed effects: yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0370844 69  30.65311       0
## density     -0.26744 0.0096629 69 -27.67704       0
##  Correlation: 
##         (Intr)
## density -0.078
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9923722 -0.5657555 -0.1997103  0.4961675  2.6699060 
## 
## Number of Observations: 80
## Number of Groups: 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above fit shows that the random effects (slope and intercept) are sligthly correlated (r = 0.091). We might like to try a simpler model, where random effects are independent. To do so, we need to consider that the above model is equivalent to the following model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modMix.1 &amp;lt;- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s just two different ways to code the very same model. However, this latter coding, based on a ‘pdMat’ structure, can be easily modified to remove the correlation. Indeed, ‘pdSymm’ specifies a totally unstructured variance-covariance matrix for random effects and it can be replaced by ‘pdDiag’, which specifies a diagonal matrix, where covariances (off-diagonal terms) are constrained to 0. The coding is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix.2 &amp;lt;- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)
summary(modMix.2)
## Linear mixed-effects model fit by REML
##  Data: dataset 
##       AIC      BIC   logLik
##   338.952 350.7355 -164.476
## 
## Random effects:
##  Formula: ~density | block
##  Structure: Diagonal
##         (Intercept)    density Residual
## StdDev:    3.198267 0.02293222 1.387148
## 
## Fixed effects: yield ~ density 
##                Value Std.Error DF   t-value p-value
## (Intercept) 31.78987 1.0460282 69  30.39102       0
## density     -0.26744 0.0097463 69 -27.44020       0
##  Correlation: 
##         (Intr)
## density -0.139
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -1.9991174 -0.5451478 -0.1970267  0.4925092  2.6700388 
## 
## Number of Observations: 80
## Number of Groups: 10
anova(modMix.1, modMix.2)
##          Model df      AIC      BIC    logLik   Test    L.Ratio p-value
## modMix.1     1  6 340.9166 355.0569 -164.4583                          
## modMix.2     2  5 338.9520 350.7355 -164.4760 1 vs 2 0.03535079  0.8509&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model could be further simplified. For example, the code below shows how we could fit models with either random intercept or random slope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Model with only random intercept
modMix.3 &amp;lt;- lme(yield ~ density, random = list(block = ~1), data=dataset)

#Alternative
#random = ~ 1|block

#Model with only random slope
modMix.4 &amp;lt;- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)

#Alternative
#random = ~density - 1 | block&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with nonlinear regression&lt;/h1&gt;
&lt;p&gt;The problem may become trickier if we have a nonlinear relationship. Let’s have a look at another similar dataset (‘YieldLossB.csv’), that is also available on gitHub. It represents another experiment where sunflower was grown with the same increasing densities of another weed (0, 14, 19, 28, 32, 38, 54, 82 plants per &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), on a randomised complete block design, with 8 blocks. In this case, the yield loss was recorded and analysed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/YieldLossB.csv&amp;quot;,
  header=T)
dataset$block &amp;lt;- factor(dataset$block)
head(dataset)
##   block density yieldLoss
## 1     1       0     1.532
## 2     2       0    -0.661
## 3     3       0    -0.986
## 4     4       0    -0.697
## 5     5       0    -2.264
## 6     6       0    -1.623
plot(yieldLoss ~ density, data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nlmm_DesignConstraints_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A swift plot shows that the relationship between density and yield loss is not linear. Literature references (Cousens, 1985) show that this could be modelled by using a rectangular hyperbola:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[YL = \frac{i \, D}{1 + \frac{i \, D}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(YL\)&lt;/span&gt; is the yield loss, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is weed density, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the slope at the origin of axes and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum asymptotic yield loss. This function, together with self-starters, is available in the ‘NLS.YL()’ function in the ‘aomisc’ package, which is the accompanying package for this blog. If you do not have this package, please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this link&lt;/a&gt; to download it.&lt;/p&gt;
&lt;p&gt;The problem is the very same as above: the block effect may produce random fluctuations for both model parameters. The only difference is that we need to use the ‘nlme()’ function instead of ‘lme()’. With nonlinear mixed models, I strongly suggest you use a ‘groupedData’ object, which permits to avoid several problems. The second line below shows how to turn a data frame into a ‘groupedData’ object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
datasetG &amp;lt;- groupedData(yieldLoss ~ 1|block, dataset)
nlin.mix &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                        fixed = list(i ~ 1, A ~ 1),
            random = i + A ~ 1|block)
summary(nlin.mix)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##  Data: datasetG 
##        AIC      BIC    logLik
##   474.8228 491.5478 -231.4114
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite, Log-Cholesky parametrization
##          StdDev    Corr 
## i        0.1112839 i    
## A        4.0444538 0.195
## Residual 1.4142272      
## 
## Fixed effects: list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23238 0.0382246 104 32.24038       0
## A 68.52305 1.9449745 104 35.23082       0
##  Correlation: 
##   i     
## A -0.408
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4416770 -0.7049388 -0.1805690  0.3385458  2.8788981 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly to linear mixed models, the above coding implies correlated random effects (r = 0.194). Alternatively, the above model can be coded by using a ’pdMat construct, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix2 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdSymm(list(i ~ 1, A ~ 1)))
summary(nlin.mix2)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##  Data: datasetG 
##        AIC      BIC    logLik
##   474.8225 491.5475 -231.4113
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: General positive-definite
##          StdDev    Corr 
## i        0.1112839 i    
## A        4.0466971 0.194
## Residual 1.4142009      
## 
## Fixed effects: list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23242  0.038225 104 32.24107       0
## A 68.52068  1.945173 104 35.22600       0
##  Correlation: 
##   i     
## A -0.409
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.4414051 -0.7049356 -0.1805322  0.3385275  2.8787362 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can try to simplify the model, for example by excluding the correlation between random effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlin.mix3 &amp;lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG, 
                              fixed = list(i ~ 1, A ~ 1),
                  random = pdDiag(list(i ~ 1, A ~ 1)))
summary(nlin.mix3)
## Nonlinear mixed-effects model fit by maximum likelihood
##   Model: yieldLoss ~ NLS.YL(density, i, A) 
##  Data: datasetG 
##        AIC      BIC    logLik
##   472.9076 486.8451 -231.4538
## 
## Random effects:
##  Formula: list(i ~ 1, A ~ 1)
##  Level: block
##  Structure: Diagonal
##                 i        A Residual
## StdDev: 0.1172791 4.389173 1.408963
## 
## Fixed effects: list(i ~ 1, A ~ 1) 
##      Value Std.Error  DF  t-value p-value
## i  1.23243 0.0393514 104 31.31852       0
## A 68.57655 1.9905549 104 34.45097       0
##  Correlation: 
##   i     
## A -0.459
## 
## Standardized Within-Group Residuals:
##        Min         Q1        Med         Q3        Max 
## -2.3577291 -0.6849962 -0.1785860  0.3255925  2.8592764 
## 
## Number of Observations: 120
## Number of Groups: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a little fantasy, we can easily code several alternative models to represent alternative hypotheses about the observed data. Obviously, the very same method can be used (and SHOULD be used) to account for other grouping factors, such as main-plots in split-plot designs or plots in repeated measure designs.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Cousens, R., 1985. A simple model relating yield loss to weed density. Annals of Applied Biology 107, 239–252. &lt;a href=&#34;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/j.1744-7348.1985.tb01567.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jensen, S.M., Schaarschmidt, F., Onofri, A., Ritz, C., 2018. Experimental design matters for statistical analysis: how to handle blocking: Experimental design matters for statistical analysis. Pest Management Science 74, 523–534. &lt;a href=&#34;https://doi.org/10.1002/ps.4773&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1002/ps.4773&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. The Hayman&#39;s model (type 1)</title>
      <link>/2020/stat_met_diallel_hayman1/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_met_diallel_hayman1/</guid>
      <description>


&lt;p&gt;In a previous post we have presented our new ‘lmDiallel’ package (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_met_diallel1/&#34;&gt;see this link here&lt;/a&gt; and see also the original paper in &lt;a href=&#34;https://rdcu.be/caxZh&#34;&gt;Theoretical and Applied Genetics&lt;/a&gt;). This package provides an extensions to fit a class of linear models of interest for plant breeders or geneticists, the so-called diallel models. In this post and other future posts we would like to present some examples of how to use this package: please, sit back and relax and, if you have comments, let us know, using the email link at the bottom of this post.&lt;/p&gt;
&lt;div id=&#34;but-what-is-a-diallel&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;But… what is a ‘diallel’?&lt;/h1&gt;
&lt;p&gt;If you are not a plant breeder or a geneticist in general, you may be asking this question. From the ancient Greek language, the ‘diallel’ word means ‘reciprocating’ and a diallel cross is a set of several possible crosses and selfs between some parental lines. For example, if we take the male lines A, B and C together with the same female lines A, B and C and we imagine to cross those lines with one another, we obtain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the selfs A&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;A, B&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;B and C&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;C,&lt;/li&gt;
&lt;li&gt;the crosses A&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;B, A&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;C and B&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;C,&lt;/li&gt;
&lt;li&gt;and, in some instances, the reciprocals B&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;A, C&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;A and C&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;B (where the father and mother are swapped).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The performances of crosses and/or selfs and/or reciprocals can be compared by planning field experiments, usually known as &lt;strong&gt;diallel experiments&lt;/strong&gt; and designed as randomised complete blocks with 3-4 replicates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The example&lt;/h1&gt;
&lt;p&gt;Depending on how the experiment is planned, we can have four experimental methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Crosses + reciprocals + selfs (complete diallel)&lt;/li&gt;
&lt;li&gt;Crosses and reciprocals (no selfs)&lt;/li&gt;
&lt;li&gt;Crosses and selfs (no reciprocals)&lt;/li&gt;
&lt;li&gt;Only crosses (no selfs, no reciprocals)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post we will concentrate on the first design (complete diallel) and we will use a simple example with three parental lines (A, B and C). The csv file (‘diallel1.csv’) is available in an external repository; in the box below we load the data and we use the &lt;code&gt;group_by()&lt;/code&gt; function in the ‘dplyr’ package to obtain the means for all crosses and selfs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
rm(list = ls())
df &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/diallel1.csv&amp;quot;)
df$Block &amp;lt;- factor(df$Block)
dfM &amp;lt;- df %&amp;gt;% 
  group_by(Par1, Par2) %&amp;gt;% 
  summarise(YieldM = mean(Yield), SEs = sd(Yield/sqrt(4)))
dfM
## # A tibble: 9 x 4
## # Groups:   Par1 [3]
##   Par1  Par2  YieldM   SEs
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 A     A         12 0.740
## 2 A     B         13 0.600
## 3 A     C         14 0.498
## 4 B     A         11 1.00 
## 5 B     B         15 0.332
## 6 B     C         21 0.273
## 7 C     A         17 0.295
## 8 C     B         16 0.166
## 9 C     C         19 1.90&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-model-do-we-use&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What model do we use?&lt;/h1&gt;
&lt;p&gt;In order to describe the above dataset, we might think of a two-way ANOVA model, where the ‘father’ and ‘mother’ lines (the ‘Par1’ and ‘Par2’ variables, respectively) are used as the explanatory factors.&lt;/p&gt;
&lt;p&gt;This is a very tempting solution, but we should resist: a two way ANOVA model regards the ‘father’ and ‘mother’ effects as two completely different series of treatments, neglecting the fact that they are, indeed, the same genotypes in different combinations. That is exactly why we need specific &lt;strong&gt;diallel models&lt;/strong&gt; to describe the results of diallel experiments!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-haymans-model-type-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Hayman’s model type 1&lt;/h1&gt;
&lt;p&gt;The first diallel model was proposed by Hayman (1954) and it was devised for complete diallel experiments, where reciprocals are available. Neglecting the design effects (blocks and/or environments), the Hayman’s model is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y _{ijk} = \mu + \textrm{g}_i + \textrm{g}_j + \textrm{ts}_{ij} + \textrm{rg}^a_{i} + \textrm{rg}^b_{j} + rs_{ij} + \varepsilon_{ijk} \quad\quad\quad (Eq. 1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is expected value (the overall mean, in the balanced case) and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual random error terms for the observation in the &lt;span class=&#34;math inline&#34;&gt;\(k^{th}\)&lt;/span&gt; block and with the parentals &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. All the other terms correspond to genetic effects, namely:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{g}_j\)&lt;/span&gt; terms are the &lt;strong&gt;general combining abilities&lt;/strong&gt; (GCAs) of the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents. Each term relates to the average performances of a parental line in all its hybrid combination, under the sum-to-zero constraint (i.e. the sum of &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; values for all parentals must be zero). For example, with our balanced experiment, the overall mean is &lt;span class=&#34;math inline&#34;&gt;\(\mu = 15.33\)&lt;/span&gt;, while the mean for the A parent when used as the ‘father’ is &lt;span class=&#34;math inline&#34;&gt;\(\mu_{A.} = 13\)&lt;/span&gt; and the mean for the same parent A when used as the ‘mother’ is &lt;span class=&#34;math inline&#34;&gt;\(\mu_{.A} = 13.33\)&lt;/span&gt;. Consequently:
&lt;span class=&#34;math display&#34;&gt;\[g_A = \left(13 + 13.33 \right)/2 - 15.33 = -2.167\]&lt;/span&gt; Analogously, it is &lt;span class=&#34;math inline&#34;&gt;\(g_B = -0.167\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rg^a_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(rg^b_j\)&lt;/span&gt; terms are the &lt;strong&gt;reciprocal general combining abilities&lt;/strong&gt; (RGCAs) for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents. Each term relates to the discrepancy between the effect of a parent when it is used as father/mother and its average effect in all its combinations. For example, considering the parent A, the term &lt;span class=&#34;math inline&#34;&gt;\(rg^a_A\)&lt;/span&gt; is: &lt;span class=&#34;math display&#34;&gt;\[rg^a_A = \mu_{A.} - \frac{\mu_{A.} + \mu_{.A}}{2} = 13 - 13.167 = -0.167\]&lt;/span&gt; Obviously, it must be &lt;span class=&#34;math inline&#34;&gt;\(rg^a_A = - rg^b_B\)&lt;/span&gt; and it must also be that the sum of all &lt;span class=&#34;math inline&#34;&gt;\(rg^a\)&lt;/span&gt; terms is zero (sum-to-zero constraint).&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\textrm{ts}_{ij}\)&lt;/span&gt; term is the total &lt;strong&gt;specific combining ability&lt;/strong&gt; (tSCA) for the combination between the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; parents. It relates to the discrepancy from additivity for a specific combination of two parentals. For example, considering the ‘A &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; B’ cross, the expected yield under additivity would be: &lt;span class=&#34;math display&#34;&gt;\[\mu_{A:B} = \mu + \textrm{g}_A + \textrm{g}_B +\textrm{rg}^a_{A} + \textrm{rg}^b_{B} =\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[ = 15.33 - 2.167 - 0.167 - 0.167 - 0.5 = 12.333\]&lt;/span&gt; while the observed yield is 13, with a with a difference of &lt;span class=&#34;math inline&#34;&gt;\(-0.667\)&lt;/span&gt;. On the other hand, considering the ‘B &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; A’ reciprocal cross, the expected yield under additivity would be: &lt;span class=&#34;math display&#34;&gt;\[\mu_{A:B} = \mu + \textrm{g}_A + \textrm{g}_B +\textrm{rg}^a_{B} + \textrm{rg}^b_{A} =\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= 15.33 - 2.167 - 0.167 + 0.167 + 0.5 = 13.667\]&lt;/span&gt; while the observed yield is 11, with a difference of &lt;span class=&#34;math inline&#34;&gt;\(2.667\)&lt;/span&gt;. The tSCA for the cross between A and B (regardless of the reciprocal) is the average difference, that is &lt;span class=&#34;math inline&#34;&gt;\(\textrm{ts}_{AB} = (-0.667 + 2.667)/2 = 1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(rs_{ij}\)&lt;/span&gt; term is the &lt;strong&gt;reciprocal specific combining ability&lt;/strong&gt; (RSCA) for a specific &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt; combination, that is the discrepancy between the performances of the two reciprocals (e.g, A &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; B vs. B &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; A). For example, the &lt;span class=&#34;math inline&#34;&gt;\(\textrm{rs}_{AB}\)&lt;/span&gt; term is equal to &lt;span class=&#34;math inline&#34;&gt;\(-0.667 - 1 = -1.667\)&lt;/span&gt;, that is the opposite of &lt;span class=&#34;math inline&#34;&gt;\(\textrm{rs}_{BA}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting with R&lt;/h1&gt;
&lt;p&gt;Hands-calculations based on means may be useful to understand the meaning of genetical effects, although they are biased with unbalanced designs and, above all, they are totally uninteresting from a practical point of view: we’d rather fit the model by using a statistical software.&lt;/p&gt;
&lt;p&gt;Let’s assume that all effects are fixed, apart from the residual standard error. This is a reasonable assumption, as we have a very low number of parentals, which would make the estimation of variance components totally unreliable. We clearly see that the Hayman’s model above is a specific parameterisation of a general linear model and we should be able to fit it by the usual &lt;code&gt;lm()&lt;/code&gt; function and related methods. We can, indeed, do so by using our ‘lmDiallel’ extension package, that provides the facilities to generate the correct design matrices for the Hayman’s model (and for other diallel models, as we will show in future posts).&lt;/p&gt;
&lt;p&gt;At the beginning, we have to install (if necessary) and load the ‘lmDiallel’ package (see box below). Model fitting can be performed by using the &lt;code&gt;GCA()&lt;/code&gt;, &lt;code&gt;tSCA()&lt;/code&gt;, &lt;code&gt;RGCA()&lt;/code&gt; and &lt;code&gt;RSCA()&lt;/code&gt; functions as shown in the box below: the resulting &lt;code&gt;lm&lt;/code&gt; object can be explored by the usual R methods, such as &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools) # Install if necessary
# install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)
library(lmDiallel)
## Loading required package: multcomp
## Loading required package: mvtnorm
## Loading required package: survival
## Loading required package: TH.data
## Loading required package: MASS
## 
## Attaching package: &amp;#39;MASS&amp;#39;
## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select
## 
## Attaching package: &amp;#39;TH.data&amp;#39;
## The following object is masked from &amp;#39;package:MASS&amp;#39;:
## 
##     geyser
## Loading required package: plyr
## ------------------------------------------------------------------------------
## You have loaded plyr after dplyr - this is likely to cause problems.
## If you need functions from both plyr and dplyr, please load plyr first, then dplyr:
## library(plyr); library(dplyr)
## ------------------------------------------------------------------------------
## 
## Attaching package: &amp;#39;plyr&amp;#39;
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     arrange, count, desc, failwith, id, mutate, rename, summarise,
##     summarize
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     compact
## Loading required package: sommer
## Loading required package: Matrix
## 
## Attaching package: &amp;#39;Matrix&amp;#39;
## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack
## Loading required package: lattice
## Loading required package: crayon
## 
## Attaching package: &amp;#39;crayon&amp;#39;
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%
dMod &amp;lt;- lm(Yield ~ Block + GCA(Par1, Par2) + tSCA(Par1, Par2) +
              RGCA(Par1, Par2) + RSCA(Par1, Par2), data = df)
summary(dMod)
## 
## Call:
## lm(formula = Yield ~ Block + GCA(Par1, Par2) + tSCA(Par1, Par2) + 
##     RGCA(Par1, Par2) + RSCA(Par1, Par2), data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.3500 -0.5644  0.0606  0.4722  2.7911 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)             1.558e+01  5.780e-01  26.962  &amp;lt; 2e-16 ***
## Block2                 -3.772e-01  8.174e-01  -0.461   0.6486    
## Block3                 -3.011e-01  8.174e-01  -0.368   0.7158    
## Block4                 -3.261e-01  8.174e-01  -0.399   0.6935    
## GCA(Par1, Par2)g_A     -2.167e+00  2.890e-01  -7.497 9.77e-08 ***
## GCA(Par1, Par2)g_B     -1.667e-01  2.890e-01  -0.577   0.5695    
## tSCA(Par1, Par2)ts_A:A  1.000e+00  5.780e-01   1.730   0.0965 .  
## tSCA(Par1, Par2)ts_A:B -1.000e+00  4.570e-01  -2.188   0.0386 *  
## tSCA(Par1, Par2)ts_B:B  1.230e-16  5.780e-01   0.000   1.0000    
## RGCA(Par1, Par2)rg_A   -1.667e-01  2.890e-01  -0.577   0.5695    
## RGCA(Par1, Par2)rg_B    5.000e-01  2.890e-01   1.730   0.0965 .  
## RSCA(Par1, Par2)        1.667e+00  3.540e-01   4.709 8.71e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.734 on 24 degrees of freedom
## Multiple R-squared:  0.8269, Adjusted R-squared:  0.7476 
## F-statistic: 10.42 on 11 and 24 DF,  p-value: 1.129e-06
anova(dMod)
## Analysis of Variance Table
## 
## Response: Yield
##                  Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block             3   0.784   0.261  0.0869    0.9665    
## GCA(Par1, Par2)   2 244.000 122.000 40.5743 1.999e-08 ***
## tSCA(Par1, Par2)  3  24.000   8.000  2.6606    0.0710 .  
## RGCA(Par1, Par2)  2   9.333   4.667  1.5520    0.2323    
## RSCA(Par1, Par2)  1  66.667  66.667 22.1717 8.710e-05 ***
## Residuals        24  72.164   3.007                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simplicity, we also built a wrapper function named &lt;code&gt;lm.diallel()&lt;/code&gt;, which can be used in the very same fashion as &lt;code&gt;lm()&lt;/code&gt;. The syntax is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm.diallel(formula, Block, Env, data, fct)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘formula’ specifies the response variable and the two variables for parentals (e.g., Yield ~ Par1 + Par2) and the two arguments ‘Block’ and ‘Env’ are used to specify optional variables, coding for blocks and environments, respectively. The argument ‘data’ is a ‘dataframe’ where to look for the explanatory variables and, finally, ‘fct’ is a string variable coding for the selected model (“HAYMAN1”, for this example; see below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod2 &amp;lt;- lm.diallel(Yield ~ Par1 + Par2, Block = Block,
                    data = df, fct = &amp;quot;HAYMAN1&amp;quot;)
summary(dMod2)
## 
## Call:
## lm.diallel(formula = Yield ~ Par1 + Par2, Block = Block, fct = &amp;quot;HAYMAN1&amp;quot;, 
##     data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.3500 -0.5644  0.0606  0.4722  2.7911 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept  1.533e+01  2.890e-01  53.056  &amp;lt; 2e-16 ***
## Block1     2.511e-01  5.006e-01   0.502   0.6205    
## Block2    -1.261e-01  5.006e-01  -0.252   0.8032    
## Block3    -5.000e-02  5.006e-01  -0.100   0.9213    
## g_A       -2.167e+00  2.890e-01  -7.497 9.77e-08 ***
## g_B       -1.667e-01  2.890e-01  -0.577   0.5695    
## ts_A:A     1.000e+00  5.780e-01   1.730   0.0965 .  
## ts_A:B    -1.000e+00  4.570e-01  -2.188   0.0386 *  
## ts_B:B     6.152e-16  5.780e-01   0.000   1.0000    
## rg_A      -1.667e-01  2.890e-01  -0.577   0.5695    
## rg_B       5.000e-01  2.890e-01   1.730   0.0965 .  
## rs_A:B     1.667e+00  3.540e-01   4.709 8.71e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.734 on 24 degrees of freedom
## Multiple R-squared:  0.8269, Adjusted R-squared:  0.7476 
## F-statistic: 10.42 on 11 and 24 DF,  p-value: 1.129e-06
anova(dMod2)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Block      3   0.784   0.261  0.0869    0.9665    
## GCA        2 244.000 122.000 40.5743 1.999e-08 ***
## tSCA       3  24.000   8.000  2.6606    0.0710 .  
## RGCA       2   9.333   4.667  1.5520    0.2323    
## RSCA       1  66.667  66.667 22.1717 8.710e-05 ***
## Residuals 24  72.164                              
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function works very much like the &lt;code&gt;lm()&lt;/code&gt; function and makes use of the general purpose linear model solver &lt;code&gt;lm.fit()&lt;/code&gt;. Apart from simplicity, another advantage is that the call to &lt;code&gt;lm.diallel()&lt;/code&gt; returns an object of both ‘lm’ and ‘diallel’ classes. For this latter class, we built several specific S3 methods, such as the usual &lt;code&gt;anova()&lt;/code&gt;, &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;model.matrix()&lt;/code&gt; methods, partly shown in the box above.&lt;/p&gt;
&lt;p&gt;Considering that diallel models are usually fitted to determine genetical parameters, we also built the &lt;code&gt;glht.diallelMod()&lt;/code&gt; method and the &lt;code&gt;diallel.eff()&lt;/code&gt; function, which can be used with the ‘multcomp’ package, to retrieve the complete list of genetical parameters, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
gh &amp;lt;- glht(linfct = diallel.eff(dMod2))
summary(gh, test = adjusted(type = &amp;quot;none&amp;quot;)) 
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept == 0  1.533e+01  2.890e-01  53.056  &amp;lt; 2e-16 ***
## g_A == 0       -2.167e+00  2.890e-01  -7.497 5.85e-08 ***
## g_B == 0       -1.667e-01  2.890e-01  -0.577   0.5691    
## g_C == 0        2.333e+00  2.890e-01   8.074 1.49e-08 ***
## ts_A:A == 0     1.000e+00  5.780e-01   1.730   0.0955 .  
## ts_A:B == 0    -1.000e+00  4.570e-01  -2.188   0.0378 *  
## ts_A:C == 0     1.443e-15  4.570e-01   0.000   1.0000    
## ts_B:A == 0    -1.000e+00  4.570e-01  -2.188   0.0378 *  
## ts_B:B == 0     6.152e-16  5.780e-01   0.000   1.0000    
## ts_B:C == 0     1.000e+00  4.570e-01   2.188   0.0378 *  
## ts_C:A == 0     1.443e-15  4.570e-01   0.000   1.0000    
## ts_C:B == 0     1.000e+00  4.570e-01   2.188   0.0378 *  
## ts_C:C == 0    -1.000e+00  5.780e-01  -1.730   0.0955 .  
## j_A == 0       -1.667e-01  2.890e-01  -0.577   0.5691    
## j_B == 0        5.000e-01  2.890e-01   1.730   0.0955 .  
## j_C == 0       -3.333e-01  2.890e-01  -1.153   0.2592    
## rs_A:B == 0     1.667e+00  3.540e-01   4.709 7.25e-05 ***
## rs_A:C == 0    -1.667e+00  3.540e-01  -4.709 7.25e-05 ***
## rs_B:A == 0    -1.667e+00  3.540e-01  -4.709 7.25e-05 ***
## rs_B:C == 0     1.667e+00  3.540e-01   4.709 7.25e-05 ***
## rs_C:A == 0     1.667e+00  3.540e-01   4.709 7.25e-05 ***
## rs_C:B == 0    -1.667e+00  3.540e-01  -4.709 7.25e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- none method)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting-in-two-steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting in two steps&lt;/h1&gt;
&lt;p&gt;In some cases, the analysis is performed in two steps and a diallel model is fitted to the means of selfs and crosses, which are calculated in the first step. Under the assumption of variance homogeneity and equal number of replicates, we can fit the Hayman’s model by using the &lt;code&gt;lm.diallel()&lt;/code&gt; function without the ‘Block’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dMod3 &amp;lt;- lm.diallel(YieldM ~ Par1 + Par2, 
                    data = dfM, fct = &amp;quot;HAYMAN1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we have no reliable estimate of residual error, but the &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;anova()&lt;/code&gt; methods have been enhanced to give us the possibility of passing some information from the first step, i.e. an appropriate estimate of the residual mean square and degrees of freedom; the residual mean square from the first step needs to be appropriately weighted for the number of replicates (i.e., for this example, MSE = 3.007/4 with 24 degrees of freedom).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(dMod3, MSE = 3.007/4, dfr = 24)
## Analysis of Variance Table
## 
## Response: YieldM
##           Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## GCA        2 61.000 30.5000 40.5720 2.000e-08 ***
## tSCA       3  6.000  2.0000  2.6605   0.07101 .  
## RGCA       2  2.333  1.1667  1.5519   0.23236    
## RSCA       1 16.667 16.6667 22.1705 8.713e-05 ***
## Residuals 24         0.7518                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
summary(dMod3, MSE = 3.007/4, dfr = 24)
##                Estimate        SE       t value     Pr(&amp;gt;|t|)
## Intercept  1.533333e+01 0.2890117  5.305436e+01 2.157713e-26
## g_A       -2.166667e+00 0.2890117 -7.496812e+00 9.771159e-08
## g_B       -1.666667e-01 0.2890117 -5.766779e-01 5.695269e-01
## ts_A:A     1.000000e+00 0.5780235  1.730034e+00 9.646589e-02
## ts_A:B    -1.000000e+00 0.4569677 -2.188339e+00 3.861373e-02
## ts_B:B     2.417819e-15 0.5780235  4.182908e-15 1.000000e+00
## rg_A      -1.666667e-01 0.2890117 -5.766779e-01 5.695269e-01
## rg_B       5.000000e-01 0.2890117  1.730034e+00 9.646589e-02
## rs_A:B     1.666667e+00 0.3539656  4.708555e+00 8.712864e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The genetical parameters can be obtained by using the &lt;code&gt;glht()&lt;/code&gt; function and passing the information from the first step within the call to the &lt;code&gt;diallel.eff()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gh2 &amp;lt;- glht(linfct = diallel.eff(dMod3, MSE = 3.007/4, dfr = 24))
summary(gh2, test = adjusted(type = &amp;quot;none&amp;quot;)) 
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Linear Hypotheses:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)    
## Intercept == 0  1.533e+01  2.890e-01  53.054  &amp;lt; 2e-16 ***
## g_A == 0       -2.167e+00  2.890e-01  -7.497 5.85e-08 ***
## g_B == 0       -1.667e-01  2.890e-01  -0.577   0.5691    
## g_C == 0        2.333e+00  2.890e-01   8.073 1.49e-08 ***
## ts_A:A == 0     1.000e+00  5.780e-01   1.730   0.0955 .  
## ts_A:B == 0    -1.000e+00  4.570e-01  -2.188   0.0378 *  
## ts_A:C == 0    -8.882e-16  4.570e-01   0.000   1.0000    
## ts_B:A == 0    -1.000e+00  4.570e-01  -2.188   0.0378 *  
## ts_B:B == 0     2.418e-15  5.780e-01   0.000   1.0000    
## ts_B:C == 0     1.000e+00  4.570e-01   2.188   0.0378 *  
## ts_C:A == 0    -8.882e-16  4.570e-01   0.000   1.0000    
## ts_C:B == 0     1.000e+00  4.570e-01   2.188   0.0378 *  
## ts_C:C == 0    -1.000e+00  5.780e-01  -1.730   0.0955 .  
## j_A == 0       -1.667e-01  2.890e-01  -0.577   0.5691    
## j_B == 0        5.000e-01  2.890e-01   1.730   0.0955 .  
## j_C == 0       -3.333e-01  2.890e-01  -1.153   0.2593    
## rs_A:B == 0     1.667e+00  3.540e-01   4.709 7.25e-05 ***
## rs_A:C == 0    -1.667e+00  3.540e-01  -4.709 7.25e-05 ***
## rs_B:A == 0    -1.667e+00  3.540e-01  -4.709 7.25e-05 ***
## rs_B:C == 0     1.667e+00  3.540e-01   4.709 7.25e-05 ***
## rs_C:A == 0     1.667e+00  3.540e-01   4.709 7.25e-05 ***
## rs_C:B == 0    -1.667e+00  3.540e-01  -4.709 7.25e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- none method)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-variance-components-random-genetic-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of variance components (random genetic effects)&lt;/h1&gt;
&lt;p&gt;In some cases, genetic effects are regarded as random and the aim is to estimate variance components. For this, we can use the &lt;code&gt;mmer()&lt;/code&gt; function in the ‘sommer’ package (Covarrubias-Pazaran, 2016), although we need to code a few dummy variables, which may make the task difficult for practitioners. Therefore, we coded a wrapper for the &lt;code&gt;mmer()&lt;/code&gt; function (&lt;code&gt;mmer.diallel()&lt;/code&gt;)that uses the same syntax as &lt;code&gt;lm.diallel()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It would make no sense to estimate the variance components for genetic effects with a diallel experiment based on three parentals and, therefore, we give an example based on the ‘hayman54’ dataset, as available in the ‘lmDiallel’ package and relating to a complete diallel experiment with eight parentals (Hayman, 1954).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
data(hayman54)
mod.ran &amp;lt;- mmer.diallel(Ftime ~ Par1 + Par2, Block = Block,
                        data = hayman54, fct = &amp;quot;HAYMAN1&amp;quot;)
mod.ran
##              VarComp  VarCompSE
## Block        0.00000   9.321698
## GCA       1276.73142 750.174164
## RGCA        17.97647  19.909911
## tSCA      1110.99398 330.172943
## RSCA        30.53937  46.467163
## Residuals  418.47875  74.563526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do hope that you enjoyed this post; if you are interested in diallel models, please, stay tuned: we have other examples on the way.&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Prof. Luigi Russi&lt;br /&gt;
Dr. Niccolò Terzaroli&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Send comments to: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Covarrubias-Pazaran, G., 2016. Genome-Assisted Prediction of Quantitative Traits Using the R Package sommer. PLOS ONE 11, e0156744. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0156744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0156744&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hayman, B.I., 1954. The Analysis of Variance of Diallel Tables. Biometrics 10, 235. &lt;a href=&#34;https://doi.org/10.2307/3001877&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2307/3001877&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Möhring, J., Melchinger, A.E., Piepho, H.P., 2011b. REML-Based Diallel Analysis. Crop Science 51, 470–478. &lt;a href=&#34;https://doi.org/10.2135/cropsci2010.05.0272&#34; class=&#34;uri&#34;&gt;https://doi.org/10.2135/cropsci2010.05.0272&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Terzaroli, N., Russi, L., 2020. Linear models for diallel crosses: a review with R functions. Theoretical Applied Genetics, &lt;a href=&#34;https://doi.org/10.1007/s00122-020-03716-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s00122-020-03716-8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>lmDiallel: a new R package to fit diallel models. Introduction</title>
      <link>/2020/stat_met_diallel1/</link>
      <pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_met_diallel1/</guid>
      <description>


&lt;p&gt;Together with some colleagues from the plant breeding group, we have just published a new paper, where we presented a bunch of R functions to analyse the data from diallel experiments. The paper is titled ‘&lt;em&gt;Linear models for diallel crosses: a review with R functions&lt;/em&gt;’ and it is published in the ‘&lt;em&gt;Theoretical and Applied Genetics&lt;/em&gt;’ Journal. If you are interested, you can take a look &lt;a href=&#34;https://rdcu.be/caxZh&#34;&gt;here at this link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Diallel experiments are based on a set of possible crosses between some homozygous (inbred) lines. For example, if we have the male lines A, B and C and the female lines A, B and C (same lines used, alternatively, as male and female), we would have the following selfed parents: AA, BB and CC and the following crosses: AB, AC, BC. In some instances, we might also have the reciprocals BA, CA and CB. Selfed parents and crosses are compared on a Randomised Complete Block Design, usually replicated across seasons and/or locations.&lt;/p&gt;
&lt;p&gt;For these diallel experiments, six main diallel models are available in literature, to quantify genetic effects, such as general combining ability (GCA), specific combining ability (SCA), reciprocal (maternal) effects and heterosis. If you are an expert in plant breeding, you do not need any other explanation; if you are not an expert, well… you are like me: we only need to know that these effects are determined as linear combinations of means for crosses, means for selfed parents and reciprocals. However, as I recently discovered, fitting diallel models to experimental data from diallel experiments is a relevant task for plant breeders.&lt;/p&gt;
&lt;p&gt;When I started dealing with diallel models, I was very surprised by the fact that they are often presented as separate entities, to be fitted by using specialised software; indeed, to the eyes of a biostatistician, it would appear that all diallel models are only different parameterisations of the same general linear model (Mohring et al., 2011). Therefore, it seemed to me very strange that we could not fit diallel models by simply using the &lt;code&gt;lm()&lt;/code&gt; function in R and related platform.&lt;/p&gt;
&lt;p&gt;A deeper diving in this subject showed me that the main implementation problem was that certain effects, such as the GCA effect, require the definition of unconventional design matrices, which were not yet available in R. Indeed, the packages ‘asreml-R’ and ‘sommer’ permit, e.g., the overlay of design matrices (function &lt;code&gt;and()&lt;/code&gt; in ‘asreml’ and &lt;code&gt;overlay()&lt;/code&gt; in ‘sommer’), which is useful to code GCA effects, but none of the two packages played well with the &lt;code&gt;lm()&lt;/code&gt; function in R. Therefore, together with Niccolò and Luigi, we decided to enhance the &lt;code&gt;model.matrix()&lt;/code&gt; function in R, building a handful of new R functions, aimed at producing the correct design matrices for all types of diallel models. All these functions are available within the ‘lmDiallel’ package, which is available on gitHub; it can be installed by using the ‘install_github()’ function, as available in the ‘devtools’ package. Therefore, if necessary, install this package first. The code is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;) # Only at first instance
library(devtools)
install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The core functions for ‘lmDiallel’ are named after the corresponding genetic effects, i.e.: &lt;code&gt;GCA()&lt;/code&gt; (general combining ability), &lt;code&gt;tSCA()&lt;/code&gt; (total Specific Combining Ability), &lt;code&gt;RGCA()&lt;/code&gt; (reciprocal general combining ability), &lt;code&gt;RSCA()&lt;/code&gt; (reciprocal specific combining ability), &lt;code&gt;REC()&lt;/code&gt; (RECiprocal effects = RGCA + RSCA), &lt;code&gt;DD()&lt;/code&gt; (Dominance Deviation), &lt;code&gt;MDD()&lt;/code&gt; (Mean Dominance Deviation), &lt;code&gt;H.BAR()&lt;/code&gt; (Average Heterosis), &lt;code&gt;Hi()&lt;/code&gt; (Average hetorosis for one parent), &lt;code&gt;VEi()&lt;/code&gt; (Variety Effect), &lt;code&gt;SP()&lt;/code&gt; (effect of Selfed Parents) and &lt;code&gt;GCAC()&lt;/code&gt; (GCA for parents in their crosses). The usage of these functions is very simple. For example, let’s assume that we have the two variables ‘Par1’ and ‘Par2’ in a dataset, to represent the two parental lines (father and mother); the GCA effect is coded as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GCA(Par1, Par2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the SCA effect is coded as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SCA(Par1, Par2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using these R functions as building blocks, we can fit all diallel models inside the &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;lme()&lt;/code&gt; functions. For example, the following line of code fits a diallel model containing the GCA and SCA effects, to the data contained in the ‘df’ dataframe:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm(yield ~ GCA(Par1, Par2) + SCA(Par1, Par2), data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the effect of reciprocals and random blocks can be introduced by the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lme(yield ~ GCA(Par1, Par2) + SCA(Par1, Par2) +
            REC(Par1, Par2),
            random = ~1|Block, data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model building process outlined above is clearly rooted in the frame of general linear models, although we recognise that plant breeders usually refer to certain relevant parameterisations of diallel models by using the name of the authors. In this respect, it is very common to use the terms “HAYMAN1”, “GRIFFING1”, “GRIFFING2”, “HAYMAN2”, “GE2” and “GE3” to refer to the main six diallel models available in literature (see Hayman, 1954; Griffing, 1956; Gardner and Eberhart, 1966). Although these models can be built and fit by using the above method, we thought it might be useful to simplify the whole process. For this reason, we also built a wrapper function named &lt;code&gt;lm.diallel()&lt;/code&gt;, which can be used in the very same fashion as &lt;code&gt;lm()&lt;/code&gt;. The syntax is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lm.diallel(formula, Block, Env, data, fct)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘formula’ uses the regular R syntax to specify the response variable and the two variables for parentals (e.g., Yield ~ Par1 + Par2). The two arguments ‘Block’ and ‘Env’ are used to specify optional variables, coding for blocks and environments, respectively. The argument ‘data’ is a ‘dataframe’ where to look for explanatory variables. Finally, ‘fct’ is a string variable coding for the selected model, i.e. “HAYMAN1”, “GRIFFING1”, “GRIFFING2”, “HAYMAN2”, “GE2”, “GE3”, according to the existing literature.&lt;/p&gt;
&lt;p&gt;We have also built the &lt;code&gt;summary()&lt;/code&gt;, &lt;code&gt;vcov(),&lt;/code&gt; &lt;code&gt;anova()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; methods for ‘lm.diallel’ objects, in order to obey to some peculiar aspects of diallel models.&lt;/p&gt;
&lt;p&gt;In our paper (&lt;a href=&#34;https://rdcu.be/caxZh&#34;&gt;‘Linear models for diallel crosses: a review with R functions’&lt;/a&gt;) we have reviewed diallel models and gave examples on how they can be fitted with our new package ‘lmDiallel’. We have also shown how the facilities we provide can be used to fit random effects diallel models with ‘jags’. We intend to provide a more lengthy documentation for our package in a coming series of posts; thus, if you are interested, please, stay tuned.&lt;/p&gt;
&lt;p&gt;I believe that increasing the usability of existing packages that have gained a wide popularity may be an advantageous programming strategy, compared to the usual strategy of building brand new platforms. From the point of view of the developer, it is efficient, as it requires a minor programming effort. From the point of view of the users (professionals, technicians and students), it is handy to be put in the conditions of making statistical analyses, without the need of learning new softwares and/or languages and/or syntaxes. Due to its open-source nature, the R environment is often overwhelming for users, that are confused by the extremely wide availability of alternative methods to perform the same task. In this regard, a programming strategy aimed at supporting some existing reference platforms might help build a more comfortable environment for statistical analyses.&lt;/p&gt;
&lt;p&gt;Thanks for reading and, please, stay tuned! If you have comments, please, drop me a line at the email address below. Best wishes,&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Covarrubias-Pazaran G (2016) Genome-assisted prediction of quantitative traits using the R package sommer. PLoS ONE 11:e0156744.&lt;/li&gt;
&lt;li&gt;Gardner CO, Eberhart SA (1966) Analysis and interpretation of the variety cross diallel and related populations. Biometrics 22:439–452.&lt;/li&gt;
&lt;li&gt;Gilmoure A, Gogel BJ, Cullis BR, Whelam SJ, Thompson R (2015) ASReml user guide release 4.1 structural specification. VSN International Ltd, Hemel Hempstead, HP1 1ES, UK&lt;/li&gt;
&lt;li&gt;Griffing B (1956) Concept of general and specific combining ability in relation to diallel crossing systems. Aust J Biol Sci 9:463–493&lt;/li&gt;
&lt;li&gt;Möhring J, Melchinger AE, Piepho HP (2011) REML-based diallel analysis. Crop Sci 51:470–478.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>QQ-plots and Box-Whisker plots: where do they come from?</title>
      <link>/2020/stat_general_percentiles/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_general_percentiles/</guid>
      <description>


&lt;div id=&#34;for-the-most-curious-students&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;For the most curious students&lt;/h1&gt;
&lt;p&gt;QQ-plots and Box-Whisker plots usually become part of the statistical toolbox for the students attending my course of ‘Experimental methods in agriculture’. Most of them learn that the QQ-plot can be used to check for the basic assumption of gaussian residuals in linear models and that the Box-Whisker plot can be used to describe the experimental groups, when their size is big enough and we do not want to assume a gaussian distribution. Furthermore, most students learn to use the &lt;code&gt;plot()&lt;/code&gt; method on an ‘lm’ object and the &lt;code&gt;boxplot()&lt;/code&gt; function in the base ‘graphic’ package and concentrate on the interpretation of the R output. To me, in practical terms, this is enough; however, there is at least a couple of students per year who think that this is not enough and they want to know more. What is the math behind those useful plots? Can we draw them by hand?&lt;/p&gt;
&lt;p&gt;If I were to give further detail about these two types of graphs, I should give a more detailed explanation of percentiles, at the very beginning of my course. I usually present the concept, which is rather easy to grasp, for students and I also give an example of how to calculate the 50th percentile (i.e. the median). So far, so good. But, what about the 25th and 75th percentiles, which are needed to build the Box-Whisker plot? As a teacher, I must admit I usually skip this aspect; I resort to showing the use of the &lt;code&gt;quantile()&lt;/code&gt; function and the students are happy, apart the same couple per year, who asks: “what is this function doing in the background?”.&lt;/p&gt;
&lt;p&gt;Usually, there is no time to satisfy the above thirst for knowledge. First of all, I need time to introduce other important concepts for agricultural student; second, giving more detail would imply a high risk of being ‘beaten’ by all the other, less eager, students. Therefore, I decided to put my explanation here, to the benefit of the most curious of my students.&lt;/p&gt;
&lt;p&gt;As an example, we will use 11 observations, randomly selected from a uniform distribution. First of all, we sort them out in increasing order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
y &amp;lt;- sort( runif(11))
y
##  [1] 0.009495756 0.113703411 0.232550506 0.514251141 0.609274733 0.622299405
##  [7] 0.623379442 0.640310605 0.666083758 0.693591292 0.860915384&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-p-values-of-observations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The P-values of observations&lt;/h1&gt;
&lt;p&gt;Let’s try to imagine that our eleven observations are percentiles, although we do not know what their percentage point is. Well, we know something, at least; we have an odd number of observations and we know that the central value (0.622299405) is the median, i.e. the 50th percentile. But, what about the other values? In other words, we are looking for the P-values associated to each observation (probability points).&lt;/p&gt;
&lt;p&gt;In order to determine the P-values, we divide the whole scale from 0 to 100% into as many intervals as there are values in our sample, that is eleven intervals. Each interval contains &lt;span class=&#34;math inline&#34;&gt;\(1 / 11 \times 100 = 9.09\%\)&lt;/span&gt; of the whole scale: the first interval goes from 0% to 9.09%, the second goes from 9.09% to 18.18% and so on, until the 11th interval, that goes from 90.91% to 100%.&lt;/p&gt;
&lt;p&gt;Now, each value in the ordered sample is associated to one probability interval. The problem is: where do we put the value, within each interval? A possible line of attack, that is the default in R with &lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; 10\)&lt;/span&gt;, is to put the value in the middle of the interval, as shown in the Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.casaonofri.it/_Figures/PercentagePoints.png&#34; style=&#34;width:95.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Consequently, each point corresponds to the following P-values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1:11 - 0.5)/11
##  [1] 0.04545455 0.13636364 0.22727273 0.31818182 0.40909091 0.50000000
##  [7] 0.59090909 0.68181818 0.77272727 0.86363636 0.95454545&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{i - 0.5}{n} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of values and &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the index of each value in the sorted vector. In other words, the first value is the 4.5th percentile, the second value is the 13.64th percentile and so on, until the 11th value that is the 95.45th percentile.&lt;/p&gt;
&lt;p&gt;Unfortunately, the calculation of P-values is not unique and there are other ways to locate the values within each interval. A second possibility, is to put the value close to the beginning of each interval. In this case, the corresponding P-values can be calculated as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1:11 - 1)/(11 - 1)
##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{i - 1}{n - 1} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;According to this definition, the first value is the 0th percentile, the second is the 10th percentile and so on.&lt;/p&gt;
&lt;p&gt;A third possibility, is to put the value at the end of each interval. In this case, each point corresponds to the following P-values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:11/11
##  [1] 0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455
##  [7] 0.63636364 0.72727273 0.81818182 0.90909091 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{i}{n} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although it is not the default in R, this third approach is, perhaps, the most understandable, as it is more closely related to the definition of percentiles. It is clear that there is 9.09% subjects equal to or lower than the first one and that there is 18.18% of subjects equal to or lower than the second one.&lt;/p&gt;
&lt;p&gt;Several other possibilities exist, but we will not explore them here. However, the most common function in R to calculate probability points is the &lt;code&gt;ppoints()&lt;/code&gt; function, which gives different results, according to the selection of the ‘a’ argument. In detail, if &lt;span class=&#34;math inline&#34;&gt;\(a = 0.5\)&lt;/span&gt; (the default) we obtain the first series of P-values (where each of the original values is put in the middle of each interval) while, if &lt;span class=&#34;math inline&#34;&gt;\(a = 1\)&lt;/span&gt;, we obtain the second series of P-values (where each of the original values is put near to the beginning of each interval).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ppoints(y)
##  [1] 0.04545455 0.13636364 0.22727273 0.31818182 0.40909091 0.50000000
##  [7] 0.59090909 0.68181818 0.77272727 0.86363636 0.95454545
ppoints(y, a = 1)
##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;from-ppoints-to-the-qq-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;From &lt;code&gt;ppoints()&lt;/code&gt; to the QQ-plot&lt;/h1&gt;
&lt;p&gt;P-values are used to draw quantile-quantile plots. Let’s imagine that we want to know whether the eleven values in the vector ‘y’ can be regarded as a sample from a gaussian distribution. To this aim, we can standardise them and plot them against the corresponding percentiles of a gaussian distribution, which we derive from the above determined P-values, by the &lt;code&gt;pnorm()&lt;/code&gt; function. The output is exactly the same as that produced by a call to the &lt;code&gt;qqnorm()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.coord &amp;lt;- qnorm(ppoints(y))
y.coord &amp;lt;- scale(y, scale = T) 
plot(y.coord ~ x.coord, main = &amp;quot;Manual QQ-plot&amp;quot;,
     xlab = &amp;quot;Theoretical quantiles&amp;quot;, ylab = &amp;quot;Standardised values&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_General_percentiles_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;percentiles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Percentiles&lt;/h1&gt;
&lt;p&gt;Percentiles can be calculated by solving the three equations above for &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. For example, from the first equation, with simple math, we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[i = n p_i + 0.5\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which gives the index for the required percentile. If we look for the 30th percentile, the index is &lt;span class=&#34;math inline&#34;&gt;\(11 \times 0.30 + 0.5 = 3.8\)&lt;/span&gt;, i.e. this percentile is between the 3rd and the 4th value, that are, respectively, 0.232550506 and 0.514251141. We can find the exact percentile by a sort of ‘weighted average’ of these two values, considering the decimal part of the index (0.8):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1 - 0.8) * y[3] + 0.8 * y[4]
## [1] 0.457911&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get the same result, by using the &lt;code&gt;quantile()&lt;/code&gt; function in R and setting the ‘type = 5’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(y, 0.3, type = 5)
##      30% 
## 0.457911&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we start from the second equation, we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[i = p \times (n - 1) + 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the 30th percentile, the index is &lt;span class=&#34;math inline&#34;&gt;\(0.3 \times (11 - 1) + 1 = 4\)&lt;/span&gt;. Thus, the required percentile is exactly in 4th position (0.5142511). With R, we can use the same &lt;code&gt;quantile()&lt;/code&gt; function, but we have to set the ‘type = 7’ argument, that is the default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(y, 0.3)
##       30% 
## 0.5142511&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take the third equation above, we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[i = n \times p\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and thus the index for the 30th percentile is: &lt;span class=&#34;math inline&#34;&gt;\(11 \times 0.3 = 3.3\)&lt;/span&gt;. We use the same kind of ‘weighted average’ to retrieve the exact percentile:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1 - 0.3) * y[3] + 0.3 * y[4]
## [1] 0.3170607&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(y, 0.3, type = 4)
##       30% 
## 0.3170607&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the above math to derive the 25th, the 50th and the 75th percentile, we are ready to draw our boxplot by hand. Please, consider that the &lt;code&gt;boxplot()&lt;/code&gt; function in R uses the default quantiles (‘type = 7’).&lt;/p&gt;
&lt;p&gt;It’s all, for today. If you have any comments, please, drop me a note at &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;. Best wishes,&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building ANOVA-models for long-term experiments in agriculture</title>
      <link>/2020/stat_lte_modelbuilding/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_lte_modelbuilding/</guid>
      <description>


&lt;p&gt;This is the follow-up of a manuscript that we (some colleagues and I) have published in 2016 in the European Journal of Agronomy (Onofri et al., 2016). I thought that it might be a good idea to rework some concepts to make them less formal, simpler to follow and more closely related to the implementation with R. Please, be patient: this lesson may be longer than usual.&lt;/p&gt;
&lt;div id=&#34;what-are-long-term-experiments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are long-term experiments?&lt;/h1&gt;
&lt;p&gt;Agricultural experiments have to deal with long-term effects of cropping practices. Think about fertilisation: certain types of organic fertilisers may give effects on soil fertility, which are only observed after a relatively high number of years (say: 10-15). In order to observe those long-term effects, we need to plan Long Term Experiments (LTEs), wherein each plot is regarded as a small cropping system, with the selected combination of rotation, fertilisation, weed control and other cropping practices. Due to the fact that yield and other relevant variables are repeatedly recorded over time, LTEs represent a particular class of multi-environment experiments with repeated measures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-main-problem-with-ltes-lack-of-independence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The main problem with LTEs: lack of independence&lt;/h1&gt;
&lt;p&gt;We know that, with linear models, once the effects of experimental factors have been accounted for, the residuals must be independent. Otherwise, inferences are invalid.&lt;/p&gt;
&lt;p&gt;With LTEs, observations are repeatedly taken on the same plot and, therefore, the residuals cannot be independent. Indeed, all measurements taken on one specific plot will be affected by the peculiar characteristics of that plot and they will be more alike than measurements taken in different plots. Thus, there will be a ‘plot’ effect, which will induce a within-plot correlation. The problem is: how do we restore the necessary independence of residuals?&lt;/p&gt;
&lt;p&gt;At the basic level, the main way to account for the ‘plot’ effect is by including a random term in the model; in this way, we recognise that there is a plot-to-plot variability, following a gaussian distribution, with mean equal to 0 and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B\)&lt;/span&gt; (‘plot’ error). This plot-to-plot variability is additional to the usual residual variability (within-plot error), that is also gaussian with mean equal to 0 and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As the result, if we take one observation, the variance will be equal to the sum &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B + \sigma^2\)&lt;/span&gt;. If we take two observations in different plots, they will have different random ‘plot’ effects and, thus, they will be independent. Otherwise, if we take two observations in the same plot, they will share the same random plot effect and they will ‘co-vary’, showing a positive covariance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_B\)&lt;/span&gt;. The correlation among observations in the same plot will be quantified by the ratio &lt;span class=&#34;math inline&#34;&gt;\(\rho = \sigma^2_B / (\sigma^2_B + \sigma^2)\)&lt;/span&gt; (intra-class correlation).&lt;/p&gt;
&lt;p&gt;In simple words, adding a random plot effect to the model accounts for the fact that observations in the same plot are correlated. This would be similar to a split-plot design (indeed, we talk about split-plot in time) with the important difference that the sub-plot factor (year) is not randomised. The correlation of observations in one plot will always be the same, independent from the year, which is known as Compound Symmetry (CS) correlation structure. Be careful: &lt;strong&gt;it may be more reasonable to assume that observations close in time are more correlated than observations distant in time&lt;/strong&gt;, but we will address this point elsewhere.&lt;/p&gt;
&lt;p&gt;It is necessary to remember that, apart from plots, the experimental design may be characterised by other grouping structures, such as blocks or main plots. All these grouping structures must be appropriately referenced in the model, to account for intra-group correlation. I’ll be back into this in a few moments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-problem-with-ltes-rotation-treatments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Another problem with LTEs: rotation treatments&lt;/h1&gt;
&lt;p&gt;In many instances, the aim of LTEs is to compare different cropping systems, which are allocated to different plots, possibly in different blocks. When the different cropping systems involve rotations, we need to consider a very important rule, that was pointed out by W.G. Cochran in a seminal paper dating back to 1937: “&lt;em&gt;The most important rule about rotation experiments is that each crop in the rotation must be grown every year&lt;/em&gt;”. If we do not follow this rule, “&lt;em&gt;the experiment has to last longer to obtain equal information on the long-term effects of the treatments&lt;/em&gt;” and “&lt;em&gt;the effects of the treatments on the separate crops are obtained under different seasonal conditions, so that a compact summary of the results of the experiment as a whole is made exceedingly difficult&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;The above rule has important consequences: first of all, with, e.g., a three year rotation, we need three plots per treatment and per block in each year, which increases the size of the experiment (that’s why some LTEs are designed without within-year replicates; Patterson, 1964). Secondly, if we want to consider only one crop in the rotation, the experiment becomes unbalanced, as not all plots contribute useful data in each one year. Last, but not least, in long rotations the test crop may return onto the same plot after a relatively long period of time, which may create a totally different correlation structure, compared to short rotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-build-anova-like-models-for-ltes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How do we build ‘ANOVA-like’ models for LTEs?&lt;/h1&gt;
&lt;p&gt;For the reasons explained above, building ANOVA-like models for data analyses may be a daunting task and it is useful to follow a structured procedure. First of all, we need to remember that ANOVA models are based on classification variables, commonly known as factors. There are three types of factors:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;treatment factors, which are randomly allocated to randomisation units (e.g. rotations, fertilisations, management of crop residues);&lt;/li&gt;
&lt;li&gt;block factors, which group the observations according to some ‘innate’ (not randomly allocated) criterion (e.g. by position), such as the blocks, the locations, the main-plots, the sub-plots and so on. Block factors may represent the randomisation units, to which treatments are randomly allocated;&lt;/li&gt;
&lt;li&gt;repeated factors, which relate to time and thus cannot be randomised (e.g. years, cycles …).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to build a model, the starting point is to list all factors (treatment, grouping and repeated) and their relationships. We can follow the general method proposed by Piepho et al. (2003), which we have slightly modified, to make it more ‘R-centric’. The relationships among factors can be specified by using the following operators:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the ‘colon’ operator denotes an interaction of crossed effects (e.g. A:B means that A and B are crossed factors);&lt;/li&gt;
&lt;li&gt;the ‘nesting’ operator denotes nested effects (e.g. A/B means that B is nested within A and it is equal to A + A:B);&lt;/li&gt;
&lt;li&gt;the ‘crossing’ operator denotes the full factorial model for two terms (A*B = A + B + A:B).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When building models we need to pay attention to properly code interactions. Let’s have a look at a simple two-way ANOVA, with the ‘JohnsonGrass.csv’ dataset. In this case we have the two crossed effects Length and Timing and we could build a model as: ‘RIZOMEWEIGHT ~ LENGTH + TIMING + LENGTH:TIMING’ that is shortened as: ’RIZOMEWEIGHT ~ LENGTH*TIMING’. However, if we build our model as: ‘RIZOMEWEIGHT ~ LENGTH:TIMING’, the two main effects are ‘absorbed’ by the term ‘LENGTH:TIMING’, which is no longer an interaction. The code below may clear up what I mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- readr::read_csv(&amp;quot;https://www.casaonofri.it/_datasets/JohnsonGrass.csv&amp;quot;)

mod1 &amp;lt;- lm(RizomeWeight ~ Length + Timing + Length:Timing, data = dataset)
anova(mod1)
## Analysis of Variance Table
## 
## Response: RizomeWeight
##               Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Length         2  1762.2   881.1  9.4795 0.0002961 ***
## Timing         5 16927.9  3385.6 36.4241 3.896e-16 ***
## Length:Timing 10   952.7    95.3  1.0250 0.4354263    
## Residuals     54  5019.2    92.9                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
mod2 &amp;lt;- lm(RizomeWeight ~ Length:Timing, data = dataset)
anova(mod2)
## Analysis of Variance Table
## 
## Response: RizomeWeight
##               Df  Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## Length:Timing 17 19642.8 1155.46  12.431 4.766e-13 ***
## Residuals     54  5019.2   92.95                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-to-model-building&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Steps to model building&lt;/h1&gt;
&lt;p&gt;The steps to model building may be summarised as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Select the repeated factor.&lt;/li&gt;
&lt;li&gt;Consider one fixed level of the repeated factor and build a treatment model for the randomized treatment factors.&lt;/li&gt;
&lt;li&gt;Consider one fixed level of the repeated factor and build a block model for block factors.&lt;/li&gt;
&lt;li&gt;Check whether randomised treatment factors might interact with block effects: if such an interaction is to be expected it should be added to the model.&lt;/li&gt;
&lt;li&gt;Include the unrandomised repeated factor into the model.&lt;/li&gt;
&lt;li&gt;Combine treatment model and repeated factor model, by crossing or nesting as appropriate.&lt;/li&gt;
&lt;li&gt;Consider which effects in the block model reference randomisation units, i.e. those units which receive the levels of a factor or factor combination by a randomisation process. It should be clear that randomisation units can be seen as randomly selected from a wider population. Therefore, the corresponding terms should be assigned a separate random effect, as explicitly recommended in Piepho (2004).&lt;/li&gt;
&lt;li&gt;Excluding the terms for randomisation units, nest the repeated factor in all the other terms in the block model.&lt;/li&gt;
&lt;li&gt;Combine random effects for randomisation units with the repeated factor, by using the colon operator, in order to derive the correct error terms to accommodate correlation structures.&lt;/li&gt;
&lt;li&gt;Apart from randomisation units (see #7), decide which factors are random and which are fixed. In our examples, the random model will include all random terms for randomisation units (terms at steps 7 and 9), while the fixed model will include all the other terms. Several extensions/changes to this basic approach are possible.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key idea for the above approach is that for a properly designed experiment, valid analyses should be possible for the data at each single level of the repeated factor. Such a basic requirement should never be taken for granted, but it should be carefully checked before the beginning of the model building process (see later for Example 3).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-further-definitions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some further definitions&lt;/h1&gt;
&lt;p&gt;It is perhaps important to clear up some definitions, which we will use afterwards. Each crop component in a rotation is usually known as a phase; e.g., in the rotation Maize-Wheat-Wheat, Maize is phase 1, Wheat is phase 2 and Wheat is phase 3. The number of phases defines the period (duration) of the rotation. All phases need to be contemporarily present in any one year and, therefore, we can define the so-called sequences: i.e. each of the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; possible arrangements for a rotation of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; years, having the same crop ordering, but different initial phases (e.g Maize - Wheat - Wheat, Wheat - Wheat - Maize and Wheat - Maize - Wheat). Each sequences is uniquely identified by its starting phase, which needs to be randomly allocated to each plot at the start of the experiment.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;In order to give a practical demonstration, we have selected five exemplary datasets, relating to LTEs with different designs. If you are in a hurry, you can follow the links below to jump directly to the example that is most relevant for you.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Example 1. &lt;a href=&#34;#example-1-ltes-with-monocultures-or-perennial-crops&#34;&gt;LTEs to compare monocultures or perennial crops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 2. &lt;a href=&#34;#example-2.-ltes-with-different-rotations-of-the-same-length-and-one-test-crop-per-rotation-cycle&#34;&gt;LTEs to compare rotations of the same length and one test crop per rotation cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 3. &lt;a href=&#34;#example-3.-ltes-with-a-fixed-rotation-one-test-crop-per-cycle-and-different-treatments&#34;&gt;LTEs with a fixed rotation (one test crop per cycle) and different treatments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 4. &lt;a href=&#34;#example-4.-lte-with-a-fixed-rotation-different-treatments-and-more-than-one-phase-per-crop-and-cycle&#34;&gt;LTE with a fixed rotation, different treatments and more than one phase per crop and cycle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example 5. &lt;a href=&#34;#example-5-ltes-with-several-rotations-of-different-lengths-and-different-number-of-phases-per-crop-and-rotation-cycle&#34;&gt;LTEs with several rotations of different lengths and different number of phases per crop and rotation cycle&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will analyse all examples, using the ‘tidyverse’ (Wickham, 2019) for data management and the ‘nlme’ package to fit random effect models (Pinheiro et al., 2019). We will also use the ‘asreml-R’ (Butler, 2019) package, for those of you who own a licence. Let’s load those packages in the R environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())
library(tidyverse)
library(nlme)
# library(asreml)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;example-1-ltes-with-monocultures-or-perennial-crops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: LTEs with monocultures or perennial crops&lt;/h2&gt;
&lt;p&gt;Wheat is grown in continuous cropping from 1983 to 2012, with three fertilisation levels (150, 200 and 250 kg N ha&lt;span class=&#34;math inline&#34;&gt;\(^{-1}\)&lt;/span&gt;), randomly assigned to three plots in each of three blocks. In all, there are nine plots with yearly sampling, with a total of 270 wheat yield observations in 30 years. The following figure shows the design for one block: the spatial split for each plot represents the actual temporal split.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/_Figures/Stat_lte_ModelBuildingFigure1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this example, the repeated factor is the year (YEAR). In one year, the treatment factor is nitrogen fertilisation (N) and there are two block factors, i.e. the blocks (BLOCK) and the plots within each block (PLOT). Therefore, the block model is BLOCK + BLOCK:PLOT.&lt;/p&gt;
&lt;p&gt;We now introduce the repeated factor YEAR and combine it with the treatment model, by including N*YEAR = N + YEAR + N:YEAR. The term BLOCK:PLOT references the randomisation units and receives a random effect. As the year might interact with the block, we add the term BLOCK:YEAR. We also combine the year with the random effect for plots (BLOCK:PLOT:YEAR), although this residual term does not need to be explicitly coded when implementing the model. The final model is (the operator ~ means ‘is modelled as’):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ N + BLOCK + YEAR + N:YEAR + BLOCK:YEAR
RANDOM = BLOCK:PLOT + BLOCK:PLOT:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use the above notation in R, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE1.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(Block, Plot, Year, N), factor))
head(dataset)
## # A tibble: 6 x 5
##   Plot  N     Year  Block Yield
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
## 1 33    fn150 1983  1      4.54
## 2 98    fn150 1983  3      4.18
## 3 162   fn150 1983  2      3.7 
## 4 33    fn150 1984  1      4.57
## 5 98    fn150 1984  3      5.04
## 6 162   fn150 1984  2      5.06
# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Year + N + N:Year + Block:Year,
           random = ~ 1|Plot, data = dataset)
anova(mod)
##             numDF denDF  F-value p-value
## (Intercept)     1   116 5434.427  &amp;lt;.0001
## Block           2     4    0.300  0.7564
## Year           29   116   36.496  &amp;lt;.0001
## N               2     4    1.304  0.3664
## Year:N         58   116    1.663  0.0105
## Block:Year     58   116    2.545  &amp;lt;.0001
# Implementation with asreml (the residual statement is unnecessary, here)
# Need to sort the data according to the residual statement
# datasetS &amp;lt;- dataset %&amp;gt;% 
#  arrange(Plot, Year)
# mod &amp;lt;- asreml(Yield ~ Block + Year + N + N:Year + Block:Year,
#           random = ~ Plot, 
#           residual = ~ Plot:Year, data = datasetS)
# wald(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is worth to notice that the same model may be fitted in an alternative way, i.e. by dropping the BLOCK:PLOT random effects and using the residual term BLOCK:PLOT:YEAR to accommodate the CS structure into the model. This may be done very intuitively with ‘asreml’, by using the following notation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Code not run
# mod &amp;lt;- asreml(Yield ~ Block + Year + N + N:Year + Block:Year,
#           residual = ~ Plot:cor(Year), data = datasetS)
# summary(mod)$varcomp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the final command returns the correlation between observations in the same plot. With ‘lme’ package, the notation is different, as we have to switch from the ‘lme()’ to the ‘gls()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- gls(Yield ~ Block + Year + N + N:Year + Block:Year,
           correlation = corCompSymm(form = ~1|Plot), data = dataset)
mod$modelStruct$corStruct
## Correlation structure of class corCompSymm representing
##       Rho 
## 0.1269864&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This alternative coding can be used to implement different correlation structures for the cases when a simple CS correlation structure is not satisfactory. For example, when the observations close in time are more correlated than those distant in time, we can implement a serial correlation structure by appropriately changing the ‘residual’ argument in ‘asreml()’ or the ‘correlation’ argument in ‘lme()’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2.-ltes-with-different-rotations-of-the-same-length-and-one-test-crop-per-rotation-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2. LTEs with different rotations of the same length and one test crop per rotation cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in five types of two-year rotations, with either pea (&lt;em&gt;Pisum sativum&lt;/em&gt; L), grain sorghum (&lt;em&gt;Sorghum bicolor&lt;/em&gt; (L.) Moench), sugar beet (&lt;em&gt;Beta vulgaris&lt;/em&gt; L. subsp. &lt;em&gt;saccharifera&lt;/em&gt;), sunflower (&lt;em&gt;Helianthus annuus&lt;/em&gt; L.) and faba bean (&lt;em&gt;Vicia faba&lt;/em&gt; L. subsp. &lt;em&gt;minor&lt;/em&gt;). For each rotation, there are two possible sequences (wheat in odd years and wheat in even years) and the ten combinations (five rotations by two sequences) are completely randomised to ten plots per each of three blocks (Figure 2). Therefore, five wheat plots out of the available ten plots are used from each block and year, for a total of 450 observations, from 1983 to 2012. The experimental design for one block is shown in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/_Figures/Stat_lte_ModelBuildingFigure2.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example we have two crops in a rotation and both crops are grown in different plots in the same year. Thus, for each rotation, we have two sequences in time (e.g., maize-sunflower and sunflower-maize). If we consider only one of the two crops, the main difference with respect to Example 1 is that the data obtained in two consecutive years for the same treatment and block are independent, in the sense that they are obtained in different plots. Otherwise, data obtained in a two-year interval (on different rotation cycles) on the same block are correlated, as they originate from the same plot.&lt;/p&gt;
&lt;p&gt;Which is the repeated factor? Indeed, if we look only at one phase in the rotation (in this case wheat), we note that observations are repeated every second year on the same plot (Figure above), according to the sequence they belong to. In other words, observations are repeated on each rotation cycle (CYC; two years) in the same plot, while there is neither a within-cycle repetition nor a within-cycle phase difference: we have only one observation per plot per cycle. Therefore, it is natural to take the rotation cycle as the repeated factor (CYC instead of YEAR).&lt;/p&gt;
&lt;p&gt;As the next step, we should look at what happens in one fixed level of CYC: what did we randomize to the ten plots in a two-years time slot? It is clear that, considering only wheat, we randomised each combination of rotation (ROT) and positioning in the sequence (SEQ; i.e. wheat as the first crop of the sequence and wheat as the second crop of the sequence). Therefore, the treatment factors are ROT and SEQ. Now we can cross the repeated factor with the treatment factors. Accordingly, The model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ SEQ*ROT + BLOCK + CYC + ROT:CYC + BLOCK:CYC
RANDOM: BLOCK:PLOT + BLOCK:PLOT:CYC &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code below shows how to fit the model with R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE2.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor))
head(dataset)
## # A tibble: 6 x 8
##   Block Main  Plot  Rot   Year  Sequence Cycle Yield
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;
## 1 1     1_1   4     SBW   1983  1        1      5.1 
## 2 3     3_1   70    SBW   1983  1        1      4.5 
## 3 2     2_1   135   SBW   1983  1        1      4.53
## 4 1     1_0   27    SBW   1984  0        1      5.83
## 5 3     3_0   95    SBW   1984  0        1      6.26
## 6 2     2_0   160   SBW   1984  0        1      6.22
# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Rot*Sequence + Block:Sequence +
             Cycle + Cycle:Sequence + Rot:Cycle + 
             Rot:Sequence:Cycle + Cycle:Block +
             Cycle:Block:Sequence,
             random = ~1|Plot,
           data = dataset)
anova(mod)
##                      numDF denDF   F-value p-value
## (Intercept)              1   224 115809.71  &amp;lt;.0001
## Block                    2    16     42.68  &amp;lt;.0001
## Rot                      4    16      4.19  0.0165
## Sequence                 1    16     26.14  0.0001
## Cycle                   14   224    133.43  &amp;lt;.0001
## Rot:Sequence             4    16      3.60  0.0283
## Block:Sequence           2    16      2.24  0.1384
## Sequence:Cycle          14   224    119.86  &amp;lt;.0001
## Rot:Cycle               56   224      1.74  0.0025
## Block:Cycle             28   224      8.77  &amp;lt;.0001
## Rot:Sequence:Cycle      56   224      1.61  0.0081
## Block:Sequence:Cycle    28   224      6.85  &amp;lt;.0001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach is commonly suggested in literature (see Yates, 1954) and it is convenient, mainly because the resulting model is orthogonal and may be fitted by ordinary least squares. Indeed, for Dataset 2 (and similar experiments), there is only one observation for each block, treatment, cycle, sequence and no missing data (in our case: 3 blocks x 5 rotations x 2 sequences x 15 cycles = 450 observations). The phase should not enter into this model, as we are looking only at one of the two crops (only one phase).&lt;/p&gt;
&lt;p&gt;However, the drawback is that such an approach cannot be immediately extended to the other more complex examples (e.g. rotations with different lengths and/or with a different number of test-crops). Furthermore, the effect of years is partitioned into three components, i.e. ‘cycles’, ‘sequences’ and ‘cycle x sequences’, which might make modeling possible ‘fertility’ trends over time less immediate. In this respect, we should note that possible differences between sequences for a given cycle (wheat as the first crop of the sequence and wheat as the second crop of the sequence, i.e. wheat in even years and wheat in odd years) do not carry any meaning that helps understand the behaviour of rotations.&lt;/p&gt;
&lt;p&gt;An alternative and more natural approach is to take the year as the repeated factor; indeed, for Example 2, the year effect is totally confounded with the factorial combination of ‘cycle’ and ‘sequence’ (15 cycles x 2 sequences = 30 years). If we consider the YEAR as the repeated factor, in one year the treatment model is composed only by the rotation (ROT), that is allocated to plots (PLOT), within blocks. The block model for one year is BLOCK/PLOT = BLOCK + BLOCK:PLOT. We can combine the treatment model with the repeated factor (ROT:YEAR) and add the term BLOCK:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ ROT + BLOCK + YEAR + ROT:YEAR + BLOCK:YEAR
RANDOM: BLOCK:PLOT + BLOCK:PLOT:YEAR &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from random effects for randomisation units, this model is totally similar to the one used for multi-environment genotype experiments and, represents a convenient and clear platform for the analyses of LTE data. We remind the reader that the residual term (BLOCK:PLOT:YEAR) does not need to be explicitly coded when implementing the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Implementation with lme
mod &amp;lt;- lme(Yield ~ Block + Rot + Year + Rot:Year + Block:Year,
             random = ~1|Plot,
           data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us a good common modelling platform for all datasets, although it may be argued that the models are no longer orthogonal, as not all plots produce data in all years. It should be recognised, however, that the lack of orthogonality can easily be accommodated within mixed models.&lt;/p&gt;
&lt;p&gt;The situation is totally different if we look at both the phases of the rotation (e.g. wheat and sunflower): in this case, we have a phase difference within each cycle and, considering one level of the repeated factor year, the treatment model should contain both the rotation and the phase, together with their interaction. When we introduce the year (steps 5 and 6 above), we also introduce the interactions ‘year x rotation’, ‘year x phase’ and ‘year x rotation x phase’, which are all meaningful when studying the behaviour of rotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3.-ltes-with-a-fixed-rotation-one-test-crop-per-cycle-and-different-treatments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3. LTEs with a fixed rotation (one test crop per cycle) and different treatments&lt;/h2&gt;
&lt;p&gt;Durum wheat (&lt;em&gt;Triticum durum&lt;/em&gt; L.) is grown in a two-year rotation with a spring crop and nine cropping systems, consisting of the factorial combination of three soil tillage methods (CT: conventional 40 cm deep ploughing; M: scarification at 25 cm; S: sod seeding with chemical desiccation and chopping) and three N-fertilisation levels (N0, N90 and N180, corresponding to 0, 90 and 180 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;). The two possible rotation sequences (wheat-spring crop and spring crop-wheat) are arranged in two adjacent fields, which therefore host the two different crops of the rotation in the same year. Within the two fields, there are two independent randomisations, each with two blocks, tillage levels randomised to main-plots (1500 &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;) and N levels randomised to sub-plots (500 &lt;span class=&#34;math inline&#34;&gt;\(m^2\)&lt;/span&gt;), according to a split-plot design with two replicates. The design is taken from Seddaiu et al. (2016), while the data have been simulated by using Monte Carlo methods.&lt;/p&gt;
&lt;p&gt;This type of LTE is very similar to the previous one, though we have a different experimental layout:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;there are two experimental treatments, laid out in a split-plot design;&lt;/li&gt;
&lt;li&gt;the two sequences are accommodated in two fields.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The experimental design for Dataset 3, for each of two fields in one year is reported in the figure below. The position of wheat and spring crop is exchanged in the following year (CT: conventional ploughing; M: scarification; S: sod seeding; 0: no nitrogen fertilisation; 1: 90 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;; 2: 180 kg N &lt;span class=&#34;math inline&#34;&gt;\(ha^{-1}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/_Figures/Stat_lte_ModelBuildingFigure3.jpg&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before proceeding to model building for Example 3, we need to discuss whether valid analyses are possible at each single level of the repeated factor. Indeed, this is clearly true if we take the year as the repeated factor and consider only one of the two crops in the rotation (wheat, in this case). However, if we intended to consider both crops and compare e.g. their yields, the crop effect would be confounded with the field effect within a single year and, therefore, valid within-year analyses would not be possible. In this case, we should resort to taking the rotation cycle as the repeated factor.&lt;/p&gt;
&lt;p&gt;Dealing only with wheat, we can therefore take the YEAR as the repeated factor and consider that, in one year, the randomised treatment factors are tillage (T) and nitrogen fertilisation (N) and the treatment model is T + N + T:N.&lt;/p&gt;
&lt;p&gt;The block factors are the FIELDS, the BLOCKS within fields, the MAIN plots within blocks and the subplots (SUB) within main plots. The block model (for one year) is FIELD + FIELD:BLOCK + FIELD:BLOCK:MAIN + FIELD:BLOCK:MAIN:SUB.&lt;/p&gt;
&lt;p&gt;The treatment and repeated model can be combined as: (T + N + T:N)*YEAR = T + N + T:N + YEAR + T:YEAR + N:YEAR + T:N:YEAR.&lt;/p&gt;
&lt;p&gt;At this stage, the FIELD main effect needs to be removed, as it is totally confounded with the years. We assign a random effect to the other randomisation units, i.e. FIELD:BLOCK, FIELD:BLOCK:MAIN and FIELD:BLOCK:MAIN:SUB and combine these random terms with the repeated factor YEAR, by using the colon operator, which leads us to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ T + N + T:N + YEAR + T:YEAR + N:YEAR + T:N:YEAR
RANDOM: FIELD:BLOCK + FIELD:BLOCK:MAIN + FIELD:BLOCK:MAIN:SUB + FIELD:BLOCK:YEAR + FIELD:BLOCK:MAIN:YEAR + FIELD:BLOCK:MAIN:SUB:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual, the last term (residual) does not need to be explicitly included when implementing the model, but it can be used, together with the two previous ones (FIELD:BLOCK:YEAR + FIELD:BLOCK:MAIN:YEAR) to accommodate possible serial correlation structures into the model, by allowing year-specifity of all design effects and the residuals. For this types of models with several crossed random effects, the coding of ‘lme()’ is not straightforward and it does not always lead to a flexible implementation of correlation structures.&lt;/p&gt;
&lt;p&gt;In the code below we need to build dummy variables for all random effects (five variables, excluding the residual error term, which is not needed). Afterwards, we have to code the random effects as a list; R expects that the element of such list are nested and, therefore, we need to work around this by coding an additional variable, which takes the value of ‘1’ for all subjects, so that the nesting structure is only artificial. We use the ‘pdIdent’ construct to say that each random effect is homoscedastic and uncorrelated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE3.csv&amp;quot;)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:9), factor)) %&amp;gt;% 
  mutate(FB = factor(Block:Field),
         FBM = factor(FB:Main),
         FBMS = factor(FBM:Sub),
         FBY = factor(FB:Year),
         FBMY = factor(FBM:Year),
         one = 1L)

mod &amp;lt;- lme(Yield ~ T + N + N:T + 
                 Year + Year:T + Year:N + Year:N:T,
                 random = list(one = pdIdent(~FB - 1),
                               one = pdIdent(~FBM - 1),
                               one = pdIdent(~FBMS - 1),
                               one = pdIdent(~FBY - 1),
                               one = pdIdent(~FBMY - 1)),
               data = dataset, na.action = na.omit)

# Fixed effects tested by using LRT
library(car)
Anova(mod)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##             Chisq Df Pr(&amp;gt;Chisq)    
## T          5.1169  2    0.07743 .  
## N        804.1717  2  &amp;lt; 2.2e-16 ***
## Year     446.6794 17  &amp;lt; 2.2e-16 ***
## T:N        6.2105  4    0.18397    
## T:Year   142.5132 34  3.065e-15 ***
## N:Year   246.5584 34  &amp;lt; 2.2e-16 ***
## T:N:Year 159.3230 68  2.705e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fit takes a long time and it is not easy to manipulate the model to introduce correlation structures for the random effects. However, it is not difficult to introduce the correlation of residuals, by using the ‘correlation’ argument (see above).&lt;/p&gt;
&lt;p&gt;Coding the same model with ‘asreml()’ is easier and so is to introduce patterned correlations structures. However, the design has to be balanced and, therefore, we need to introduce NAs for missing observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Asreml fit (not run)
# datasetS &amp;lt;- dataset %&amp;gt;% 
#  arrange(Sub, Year)
# mod2 &amp;lt;- asreml(Yield ~ T + N + N:T + 
#                 Year + Year:T + Year:N + Year:N:T,
#                 random = ~FB + FB:Main + FB:Main:Sub + 
#                 FB:Year + FB:Main:Year,
#               residual = ~ Sub:Year,
#               data = datasetS)
# summary(mod2)$varcomp&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4.-lte-with-a-fixed-rotation-different-treatments-and-more-than-one-phase-per-crop-and-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4. LTE with a fixed rotation, different treatments and more than one phase per crop and cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in a three-year rotation maize-wheat-wheat, under two types of management of crop residues (burial and removal), which are randomised to main plots, while the three possible rotation sequences are randomised to subplots. This experiment has 18 plots (three sequences x two treatment levels x three blocks) and, in every year, 12 of those are cropped with wheat and six with maize.&lt;/p&gt;
&lt;p&gt;Also in this case, the response variable is wheat yield from 1983 to 2012, i.e. twelve observations per year and 360 observations in total. Data obtained in the same plot in different years belong to two different phases (wheat after maize and wheat after wheat; the experimental design for one block is shown in the figure below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/_Figures/Stat_lte_ModelBuildingFigure4.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With respect to Example 3, the situation becomes more complex, because we have two distinct phases in each rotation cycle (phase difference: wheat after maize and wheat after wheat). As usual, we start by regarding the YEAR as the repeated factor. In one year, the treatment factors are the management of soil residues (RES, that is randomly allocated to main-plots) and the phases (P; randomly allocated to subplots); the treatment model is indeed: RES*P.&lt;/p&gt;
&lt;p&gt;In one year, the block model is: BLOCK/MAIN/SUB = BLOCK + BLOCK:MAIN + BLOCK:MAIN:SUB. Introducing the YEAR as repeated factor, we can combine the treatment model with the repeated model as: RES + P + &lt;a href=&#34;RES:P&#34; class=&#34;uri&#34;&gt;RES:P&lt;/a&gt; + YEAR + &lt;a href=&#34;RES:YEAR&#34; class=&#34;uri&#34;&gt;RES:YEAR&lt;/a&gt; + P:YEAR + &lt;a href=&#34;RES:P:YEAR&#34; class=&#34;uri&#34;&gt;RES:P:YEAR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The terms BLOCK:MAIN and BLOCK:MAIN:SUB reference randomisation units and should receive random effects. The blocks may interact with the years (BLOCK:YEAR), while the random effects for randomisation units can be made year-specific by adding BLOCK:MAIN:YEAR and the residual term BLOCK:MAIN:SUB:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ RES + P + RES:P + YEAR + RES:YEAR + P:YEAR + RES:P:YEAR + BLOCK + BLOCK:YEAR
RANDOM: BLOCK:MAIN + BLOCK:MAIN:SUB + BLOCK:MAIN:YEAR + BLOCK:MAIN:SUB:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE4.csv&amp;quot;)
dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor)) %&amp;gt;% 
  mutate(Main = factor(Block:Res),
         Sub = factor(Main:Sub))

mod &amp;lt;- lme(Yield ~ Block + Res*P + 
              Year + Year:Res + Year:P + Year:Res:P +
              Block:Year, 
           random = list(Main = pdIdent(~1),
                         Main = pdIdent(~Sub - 1),
                         Main = pdIdent(~Year - 1)),
           data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-5-ltes-with-several-rotations-of-different-lengths-and-different-number-of-phases-per-crop-and-rotation-cycle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 5: LTEs with several rotations of different lengths and different number of phases per crop and rotation cycle&lt;/h2&gt;
&lt;p&gt;Wheat is grown in five maize (M) - wheat (W) rotations of different lengths, i.e. M-W, M-W-W, M-W-W-W, M-W-W-W-W, M-W-W-W-W-W. For all rotations, all phases are contemporarily present in each year, for a total of 20 plots (one for each of the possible sequences, i.e. 2 + 3 + 4 + 5 + 6 = 20) in each of three blocks. Considering wheat yield as the response variable, we find that only 15 observations are obtained in each year, for a total of 1350 data, from 1983 to 2012.&lt;/p&gt;
&lt;p&gt;Experiments of this type represent a high degree of complexity. Indeed, in contrast to all other examples, after 30 years there are plots with: (i) a different number of observations for the same test crop; (ii) a different number of cycles (in some cases the last cycle is also incomplete); (iii) a different number of phases for wheat.&lt;/p&gt;
&lt;p&gt;In some cases, it is necessary to compare several rotations with different characteristics (e.g. a different duration and/or a different number of tests crops and /or a different number of phases per crop), which may create a complex design with some degree of non-orthogonality.&lt;/p&gt;
&lt;p&gt;The repeated factor is again the YEAR. In one year, the treatment factors are the rotation system (ROT) and the rotation phase (P), which are randomly allocated to plots. As there is a different number of phases for each rotation, we nest the phase within the rotation, leading to the following treatment model: ROT/P = ROT + P:ROT.&lt;/p&gt;
&lt;p&gt;In one year, the block model is BLOCK/PLOT = BLOCK + BLOCK:PLOT. The repeated factor is included and combined with the treatment model, by introducing YEAR + ROT:YEAR + ROT:P:YEAR.&lt;/p&gt;
&lt;p&gt;The term BLOCK:PLOT references randomisation units and needs to receive a random effect, while the term YEAR:BLOCK can be added to the model, together with the residual term BLOCK:PLOT:YEAR. The final model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YIELD ~ ROT + P:ROT + YEAR + ROT:YEAR + ROT:P:YEAR
RANDOM: BLOCK:PLOT + BLOCK:PLOT:YEAR&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the implementation below we did not succeed reaching convergence with ‘lme()’, but we were successful ‘with asreml()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) 
dataset &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/LTE5.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;% 
  mutate(across(c(1:7), factor))

# mod &amp;lt;- asreml(Yield ~ Block + Rot + Rot:P + 
#                 Year + Year:Rot + Year:Rot:P + Block:Year,
#               random = ~Plot,
#               data = dataset)

# mod &amp;lt;- lme(Yield ~ Block + Rot + Rot:P + 
#                Year + Year:Rot + Year:Rot:P + Block:Year, 
#                random=~1|Plot,
#              data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;warning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Warning!&lt;/h1&gt;
&lt;p&gt;Models 1 to 5 are fairly similar and they are very closely related to those used for multi-environment experiments. However, there are some peculiar aspects which needs to be taken under consideration.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Apart from Dataset 1, there is always a certain degree of unbalance, as plots do not produce data every year. Testing for fixed effects requires great care.&lt;/li&gt;
&lt;li&gt;In all cases, the model formulations shown above induce a compound symmetry correlation structure for observations taken in the same plot over time (‘split-plot in time’). This is seldom appropriate and, thus, more complex correlations structures should be considered.&lt;/li&gt;
&lt;li&gt;In the above formulations, only randomisation units have been given a random effect, while all the other effects have been regarded as fixed. Obviously, depending on the aims of the analyses, it might be convenient and appropriate to regard some of these effects (e.g., the year main effect and interactions) as random.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It’s all, thanks for reading this far. If you have any comments, please, drop me a note at &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;. Best wishes,&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Brien, C.J., Demetrio, C.G.B., 2009. Formulating mixed models for experiments, including longitudinal experiments. Journal of Agricultural, Biological and Environmental Statistics 14, 253–280.&lt;/li&gt;
&lt;li&gt;David Butler (2019). asreml: Fits the Linear Mixed Model. R package version 4.1.0.110. www.vsni.co.uk&lt;/li&gt;
&lt;li&gt;Cochran, W.G., 1939. Long-Term Agricultural Experiments. Supplement to the Journal of the Royal Statistical Society 6, 104–148.&lt;/li&gt;
&lt;li&gt;Onofri, A., Seddaiu, G., Piepho, H.-P., 2016. Long-Term Experiments with cropping systems: Case studies on data analysis. European Journal of Agronomy 77, 223–235.&lt;/li&gt;
&lt;li&gt;Patterson, H.D., 1964. Theory of Cyclic Rotation Experiments. Journal of the Royal Statistical Society. Series B (Methodological) 26, 1–45.&lt;/li&gt;
&lt;li&gt;Payne, R.W., 2015. The design and analysis of long-term rotation experiments. Agronomy Journal 107, 772–785.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Richter, C., 2004. A Mixed Modelling Approach for Randomized Experiments with Repeated Measures. Journal of Agronomy and Crop Science 190, 230–247.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2019). &lt;em&gt;nlme: Linear and Nonlinear Mixed Effects Models&lt;/em&gt;. R package version 3.1-142, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Seddaiu, G., Iocola, I., Farina, R., Orsini, R., Iezzi, G., Roggero, P.P., 2016. Long term effects of tillage practices and N fertilization in rainfed Mediterranean cropping systems: durum wheat, sunflower and maize grain yield. European Journal of Agronomy 77, 166–178. &lt;a href=&#34;https://doi.org/10.1016/j.eja.2016.02.008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.eja.2016.02.008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34; class=&#34;uri&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting complex mixed models with nlme. Example #5</title>
      <link>/2020/stat_met_jointreg/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_met_jointreg/</guid>
      <description>


&lt;div id=&#34;a-joint-regression-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Joint Regression model&lt;/h1&gt;
&lt;p&gt;Let’s talk about a very old, but, nonetheless, useful technique. It is widely known that the yield of a genotype in different environments depends on environmental covariates, such as the amount of rainfall in some critical periods of time. Apart from rain, also temperature, wind, solar radiation, air humidity and soil characteristics may concur to characterise a certain environment as good or bad and, ultimately, to determine yield potential.&lt;/p&gt;
&lt;p&gt;Early in the 60s, several authors proposed that the yield of genotypes is expressed as a function of an environmental index &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt;, measuring the yield potential of each environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (Finlay and Wilkinson, 1963; Eberhart and Russel, 1966; Perkins and Jinks, 1968). For example, for a genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, we could write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu_i + \beta_i e_j\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the yield &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in a certain environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is expressed as a linear function of the environmental index &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is the slope, which expresses how the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; responds to the environment.&lt;/p&gt;
&lt;p&gt;A graphical example may be useful; in the figure below we have two genotypes tested in 10 environments. The yield of the first genotype (red) increases as the environmental index increases, with slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 0.81\)&lt;/span&gt;. On the other hand, the yield of the second genotype (blue) does not change much with the environment (&lt;span class=&#34;math inline&#34;&gt;\(\beta_2 = -0.08)\)&lt;/span&gt;. Clearly, a high value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; demonstrates that the genotype is responsive to the environment and makes profit of favorable conditions. Otherwise, a low &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; value (close to 0) demonstrates that the genotype is not responsive and tends to give more or less the same yield in all environments (static stability; Wood, 1976).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_met_JointReg_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By now, it should be clear that &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a relevant measure of stability. Now, the problem is: how do we determine such value from a multi-environment genotype experiment? As usual, let’s start from a meaningful example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-multi-environment-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A multi-environment experiment&lt;/h1&gt;
&lt;p&gt;Let’s take the data in Sharma (2006; Statistical And Biometrical Techniques In Plant Breeding, New Age International ltd. New Delhi, India). They refer to a multi-environment experiment with 7 genotypes, 6 environments and 3 blocks; let’s load the data in the dataframe ‘dataFull’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(nlme)
library(emmeans)
## Welcome to emmeans.
## NOTE -- Important change from versions &amp;lt;= 1.41:
##     Indicator predictors are now treated as 2-level factors by default.
##     To revert to old behavior, use emm_options(cov.keep = character(0))
Block &amp;lt;- factor(rep(c(1:3), 42))
Var &amp;lt;- factor(rep(LETTERS[1:7],each=18))
Loc &amp;lt;- factor(rep(rep(letters[1:6], each=3), 7))
P1 &amp;lt;- factor(Loc:Block)
Yield &amp;lt;- c(60,65,60,80,65,75,70,75,70,72,82,90,48,45,50,50,40,40,
           80,90,83,70,60,60,85,90,90,70,85,80,40,40,40,38,40,50,
           25,28,30,40,35,35,35,30,30,40,35,35,35,25,20,35,30,30,
           50,65,50,40,40,40,48,50,52,45,45,50,50,50,45,40,48,40,
           52,50,55,55,54,50,40,40,60,48,38,45,38,30,40,35,40,35,
           22,25,25,30,28,32,28,25,30,26,28,28,45,50,45,50,50,50,
           30,30,25,28,34,35,40,45,35,30,32,35,45,35,38,44,45,40)
dataFull &amp;lt;- data.frame(Block, Var, Loc, Yield)
rm(Block, Var, Loc, P1, Yield)
head(dataFull)
##   Block Var Loc Yield
## 1     1   A   a    60
## 2     2   A   a    65
## 3     3   A   a    60
## 4     1   A   b    80
## 5     2   A   b    65
## 6     3   A   b    75&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-an-environmental-index&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is an environmental index?&lt;/h1&gt;
&lt;p&gt;First of all, we need to define an environmental index, which can describe the yield potential in each of the seven environments. Yates and Cochran (1937) proposed that we use the mean of all observations in each environment, expressed as the difference between the environmental mean yield &lt;span class=&#34;math inline&#34;&gt;\(\mu_j\)&lt;/span&gt; and the overall mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (i.e. &lt;span class=&#34;math inline&#34;&gt;\(e_j = \mu_j - \mu\)&lt;/span&gt;). Let’s do it; in the box below we use the package ‘dplyr’ to augment the dataset with a new variable, representing the environmental indices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
dataFull &amp;lt;- dataFull %&amp;gt;%
  group_by(Loc) %&amp;gt;% 
  mutate(ej = mean(Yield) - mean(dataFull$Yield))
head(dataFull)
## # A tibble: 6 x 5
## # Groups:   Loc [2]
##   Block Var   Loc   Yield    ej
##   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 1     A     a        60 1.45 
## 2 2     A     a        65 1.45 
## 3 3     A     a        60 1.45 
## 4 1     A     b        80 0.786
## 5 2     A     b        65 0.786
## 6 3     A     b        75 0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This step is ok with balanced data and it is clear that a high environmental index identifies the favorable environments, while a low (negative) environmental index identifies unfavorable environments. It is necessary to keep in mind that we have unwillingly put a constraint on &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; values, that have to sum up to zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-model-definition-equation-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Full model definition (Equation 1)&lt;/h1&gt;
&lt;p&gt;Now, it is possible to regress the yield data for each genotype against the environmental indices, according to the following joint regression model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \gamma_{jk} + \mu_i + \beta_i e_j + d_{ij} + \varepsilon_{ijk} \quad\quad\quad \textrm{(Equation 1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where: &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; is the yield for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of blocks within environments and &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the average yield for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. As we have seen in the figure above, the average yield of a genotype in each environment cannot be exactly described by the regression against the environmental indices (in other words: the observed means do not lie along the regression line). As the consequence, we need the random term &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; to represent the deviation from the regression line for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Finally, the random elements &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; represent the deviations between the replicates for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (within-trial errors). As I said, &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; are random, with variances equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;According to Finlay-Wilkinson, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; is assumed to be equal for all genotypes. Otherwise, according to Eberarth-Russel, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d}\)&lt;/span&gt; may assume a different value for each genotype (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d(i)}\)&lt;/span&gt;) and may become a further measure of stability: if this is small, a genotype does not show relevant variability of yield, apart from that due to the regression against the environmental indices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;We can start the analyses by fitting a traditional ANOVA model, to keep as a reference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.aov &amp;lt;- lm(Yield ~ Loc/Block + Var*Loc, data = dataFull)
anova(mod.aov)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Loc        5  1856.0   371.2  17.9749 1.575e-11 ***
## Var        6 20599.2  3433.2 166.2504 &amp;lt; 2.2e-16 ***
## Loc:Block 12   309.8    25.8   1.2502    0.2673    
## Loc:Var   30 12063.6   402.1  19.4724 &amp;lt; 2.2e-16 ***
## Residuals 72  1486.9    20.7                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we said, Equation 1 is a mixed model, which calls for the use of the ‘lme()’ function. For better understanding, it is useful to start by augmenting the previous ANOVA model with the regression term (‘Var/ej’). We use the nesting operator, to have different regression lines for each level of ‘Var’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Augmented ANOVA model
mod.aov2 &amp;lt;- lm(Yield ~ Loc/Block + Var/ej + Loc:Var, data=dataFull)
anova(mod.aov2)
## Analysis of Variance Table
## 
## Response: Yield
##           Df  Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Loc        5  1856.0   371.2  17.9749 1.575e-11 ***
## Var        6 20599.2  3433.2 166.2504 &amp;lt; 2.2e-16 ***
## Loc:Block 12   309.8    25.8   1.2502    0.2673    
## Var:ej     6  9181.2  1530.2  74.0985 &amp;lt; 2.2e-16 ***
## Loc:Var   24  2882.5   120.1   5.8159 2.960e-09 ***
## Residuals 72  1486.9    20.7                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the GE interaction in the ANOVA model has been decomposed into two parts: the regression term (‘Var/ej’) and the deviation from regression (‘Loc:Var’), with 6 and 24 degrees of freedom, respectively. This second term corresponds to &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; in Equation 1 (please, note that the two terms ‘Var/ej’ and ‘Loc:Var’ are partly confounded).&lt;/p&gt;
&lt;p&gt;The above analysis is only useful for teaching purposes, but it is unsatisfactory, because the &lt;span class=&#34;math inline&#34;&gt;\(d_{ij}\)&lt;/span&gt; terms have been regarded as fixed, which is pretty illogical. Therefore, we change the fixed effect model into a mixed model, where we include the random ‘genotype by environment’ interaction. We also change the fixed block effect into a random effect and remove the intercept, to more strictly adhere to the parameterisation of Equation 1. The two random effects ‘Loc:Block’ and ‘Loc:Var’ are not nested into each other and we need to code them by using ‘pdMat’ constructs, which are not straightforward. You can use the code in the box below as a guidance to fit a Finlay-Wilkinson model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finlay-Wilkinson model
modFull1 &amp;lt;- lme(Yield ~ Var/ej - 1, 
                random = list(Loc = pdIdent(~ Var - 1),
                              Loc = pdIdent(~ Block - 1)), 
                data=dataFull)
summary(modFull1)$tTable
##              Value Std.Error  DF    t-value      p-value
## VarA    63.1666667 2.4017164 107 26.3006350 1.624334e-48
## VarB    66.1666667 2.4017164 107 27.5497417 2.135264e-50
## VarC    31.8333333 2.4017164 107 13.2544097 2.599693e-24
## VarD    47.1111111 2.4017164 107 19.6156012 3.170228e-37
## VarE    44.7222222 2.4017164 107 18.6209421 2.378452e-35
## VarF    34.2777778 2.4017164 107 14.2722004 1.614127e-26
## VarG    35.8888889 2.4017164 107 14.9430169 6.028635e-28
## VarA:ej  3.2249875 0.6257787 107  5.1535588 1.176645e-06
## VarB:ej  4.7936139 0.6257787 107  7.6602379 8.827229e-12
## VarC:ej  0.4771074 0.6257787 107  0.7624219 4.474857e-01
## VarD:ej  0.3653064 0.6257787 107  0.5837629 5.606084e-01
## VarE:ej  1.2369950 0.6257787 107  1.9767291 5.064533e-02
## VarF:ej -2.4316943 0.6257787 107 -3.8858692 1.770611e-04
## VarG:ej -0.6663160 0.6257787 107 -1.0647790 2.893729e-01
VarCorr(modFull1)
##          Variance           StdDev   
## Loc =    pdIdent(Var - 1)            
## VarA     27.5007919         5.2441197
## VarB     27.5007919         5.2441197
## VarC     27.5007919         5.2441197
## VarD     27.5007919         5.2441197
## VarE     27.5007919         5.2441197
## VarF     27.5007919         5.2441197
## VarG     27.5007919         5.2441197
## Loc =    pdIdent(Block - 1)          
## Block1    0.4478291         0.6692003
## Block2    0.4478291         0.6692003
## Block3    0.4478291         0.6692003
## Residual 20.8781458         4.5692610&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the variance component &lt;span class=&#34;math inline&#34;&gt;\(\sigma_d\)&lt;/span&gt; (27.50) is the same for all genotypes; if we want to let a different value for each genotype (Eberarth-Russel model), we need to change the ‘pdMat’ construct for the ‘Loc:Var’ effect, turning from ‘pdIdent’ to ‘pdDiag’, as shown in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eberhart-Russel model
modFull2 &amp;lt;- lme(Yield ~ Var/ej - 1, 
                random = list(Loc = pdDiag(~ Var - 1),
                              Loc = pdIdent(~ Block - 1)), 
                data=dataFull)
summary(modFull2)$tTable
##              Value Std.Error  DF    t-value      p-value
## VarA    63.1666667 3.0507629 107 20.7052032 3.221930e-39
## VarB    66.1666667 2.7818326 107 23.7852798 1.604422e-44
## VarC    31.8333333 1.7240721 107 18.4640387 4.753742e-35
## VarD    47.1111111 2.3526521 107 20.0246824 5.564350e-38
## VarE    44.7222222 2.4054296 107 18.5921974 2.699536e-35
## VarF    34.2777778 1.9814442 107 17.2993906 8.947485e-33
## VarG    35.8888889 2.2617501 107 15.8677515 7.076551e-30
## VarA:ej  3.2249875 0.7948909 107  4.0571447 9.466174e-05
## VarB:ej  4.7936139 0.7248198 107  6.6135249 1.522848e-09
## VarC:ej  0.4771074 0.4492152 107  1.0620909 2.905857e-01
## VarD:ej  0.3653064 0.6129948 107  0.5959372 5.524757e-01
## VarE:ej  1.2369950 0.6267462 107  1.9736777 5.099652e-02
## VarF:ej -2.4316943 0.5162748 107 -4.7100774 7.473942e-06
## VarG:ej -0.6663160 0.5893098 107 -1.1306718 2.607213e-01
VarCorr(modFull2)
##          Variance           StdDev   
## Loc =    pdDiag(Var - 1)             
## VarA     48.7341240         6.9809830
## VarB     39.3227526         6.2707856
## VarC     10.7257438         3.2750181
## VarD     26.1010286         5.1089166
## VarE     27.6077467         5.2543074
## VarF     16.4479246         4.0556041
## VarG     23.5842788         4.8563648
## Loc =    pdIdent(Block - 1)          
## Block1    0.4520678         0.6723599
## Block2    0.4520678         0.6723599
## Block3    0.4520678         0.6723599
## Residual 20.8743411         4.5688446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the regression slopes we see that the genotypes A and B are the most responsive to the environment (&lt;span class=&#34;math inline&#34;&gt;\(\beta_A = 3.22\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_B = 4.79\)&lt;/span&gt;, respectively), while the genotypes C and D are stable in a static sense, although their average yield is pretty low.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-joint-regression-model-in-two-steps-equation-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a joint regression model in two-steps (Equation 2)&lt;/h1&gt;
&lt;p&gt;In the previous analyses we used the plot data to fit a joint regression model. In order to reduce the computational burden, it may be useful to split the analyses in two-steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;we analyse the plot data, to retrieve the means for the ‘genotype by environment’ combinations;&lt;/li&gt;
&lt;li&gt;we fit the joint regression model to those means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results of the two approaches are not necessarily the same, as some information in the first step is lost in the second. Several weighing schemes have been proposed to make two-steps fitting more reliable (Möhring and Piepho, 2009); in this example, I will show an unweighted two-steps analyses, which is simple, but not necessarily the best way to go.&lt;/p&gt;
&lt;p&gt;A model for the second step is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu_i + \beta_i e_j + f_{ij} \quad\quad\quad \textrm{(Equation 2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the residual random component &lt;span class=&#34;math inline&#34;&gt;\(f_{ij}\)&lt;/span&gt; is assumed as normally distributed, with mean equal to zero and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f\)&lt;/span&gt;. In general, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_f &amp;gt; \sigma^2_d\)&lt;/span&gt;, as the residual sum of squares from Model 2 also contains a component for within trial errors. Indeed, for a balanced experiment, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_{f} = \sigma^2_d + \frac{\sigma^2}{k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the within-trial error, which needs to be obtained from the first step. In the previous analyses we have already fitted an anova model to the whole dataset (‘mod.aov’). In the box below, we make use of the ‘emmeans’ package to retrieve the least squares means for the seven genotypes in all locations. Subsequently, the environmental means are calculated and centered, by subtracting the overall mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emmeans)
muGE &amp;lt;- as.data.frame( emmeans(mod.aov, ~Var:Loc) )[,1:3]
names(muGE) &amp;lt;- c(&amp;quot;Var&amp;quot;, &amp;quot;Loc&amp;quot;, &amp;quot;Yield&amp;quot;)
muGE &amp;lt;- muGE %&amp;gt;% 
  group_by(Loc) %&amp;gt;% 
  mutate(ej = mean(Yield) - mean(muGE$Yield))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we fit Equation 2 to the means. In the code below we assume homoscedasticity and, thus, we are fitting the Finlay-Wilkinson model. The variance component &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_d\)&lt;/span&gt; is obtained by subtracting a fraction of the residual variance from the first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Finlay-Wilkinson model
modFinlay &amp;lt;- lm(Yield ~ Var/ej - 1, data=muGE)
summary(modFinlay)
## 
## Call:
## lm(formula = Yield ~ Var/ej - 1, data = muGE)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.3981 -3.5314 -0.8864  3.7791 11.2045 
## 
## Coefficients:
##         Estimate Std. Error t value Pr(&amp;gt;|t|)    
## VarA     63.1667     2.3915  26.413  &amp;lt; 2e-16 ***
## VarB     66.1667     2.3915  27.668  &amp;lt; 2e-16 ***
## VarC     31.8333     2.3915  13.311 1.24e-13 ***
## VarD     47.1111     2.3915  19.699  &amp;lt; 2e-16 ***
## VarE     44.7222     2.3915  18.701  &amp;lt; 2e-16 ***
## VarF     34.2778     2.3915  14.333 2.02e-14 ***
## VarG     35.8889     2.3915  15.007 6.45e-15 ***
## VarA:ej   3.2250     0.6231   5.176 1.72e-05 ***
## VarB:ej   4.7936     0.6231   7.693 2.22e-08 ***
## VarC:ej   0.4771     0.6231   0.766 0.450272    
## VarD:ej   0.3653     0.6231   0.586 0.562398    
## VarE:ej   1.2370     0.6231   1.985 0.056998 .  
## VarF:ej  -2.4317     0.6231  -3.902 0.000545 ***
## VarG:ej  -0.6663     0.6231  -1.069 0.294052    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 5.858 on 28 degrees of freedom
## Multiple R-squared:  0.9905, Adjusted R-squared:  0.9857 
## F-statistic: 208.3 on 14 and 28 DF,  p-value: &amp;lt; 2.2e-16
sigmaf &amp;lt;- summary(modFinlay)$sigma^2 
sigma2 &amp;lt;- summary(mod.aov)$sigma^2 
sigmaf - sigma2/3 #sigma2_d
## [1] 27.43169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the box below, we allow for different variances for each genotype and, therefore, we fit the Eberarth-Russel model. As before, we can retrieve the variance components &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{d(i)}\)&lt;/span&gt; from the fitted model object, by subtracting the within-trial error obtained in the first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eberarth-Russel model
modEberarth &amp;lt;- gls(Yield ~ Var/ej - 1, 
              weights=varIdent(form=~1|Var), data=muGE)
coefs &amp;lt;- summary(modEberarth)$tTable
coefs
##              Value Std.Error    t-value      p-value
## VarA    63.1666667 3.0434527 20.7549360 1.531581e-18
## VarB    66.1666667 2.7653537 23.9270177 3.508778e-20
## VarC    31.8333333 1.7165377 18.5450822 2.912238e-17
## VarD    47.1111111 2.3344802 20.1805574 3.204306e-18
## VarE    44.7222222 2.3899219 18.7128381 2.304763e-17
## VarF    34.2777778 1.9783684 17.3262868 1.685683e-16
## VarG    35.8888889 2.2589244 15.8876005 1.537133e-15
## VarA:ej  3.2249875 0.7929862  4.0668898 3.511248e-04
## VarB:ej  4.7936139 0.7205262  6.6529352 3.218756e-07
## VarC:ej  0.4771074 0.4472521  1.0667527 2.951955e-01
## VarD:ej  0.3653064 0.6082600  0.6005761 5.529531e-01
## VarE:ej  1.2369950 0.6227056  1.9864844 5.684599e-02
## VarF:ej -2.4316943 0.5154734 -4.7174004 6.004832e-05
## VarG:ej -0.6663160 0.5885736 -1.1320862 2.672006e-01
sigma &amp;lt;- summary(modEberarth)$sigma
sigma2fi &amp;lt;- (c(1, coef(modEberarth$modelStruct$varStruct, uncons = FALSE)) * sigma)^2
names(sigma2fi)[1] &amp;lt;- &amp;quot;A&amp;quot;
sigma2fi - sigma2/3 #sigma2_di
##        A        B        C        D        E        F        G 
## 48.69203 38.99949 10.79541 25.81519 27.38676 16.60005 23.73284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting in two steps we obtain the very same result as with fitting in one step, but it ain’t necessarily so.&lt;/p&gt;
&lt;p&gt;I would like to conclude by saying that a joint regression model, the way I have fitted it here, is simple and intuitively appealing, although it has been criticized for a number of reasons. In particular, it has been noted that the environmental indices &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; are estimated from the observed data and, therefore, they are not precisely known. On the contrary, linear regression makes the assumption that the levels of explanatory variables are precisely known and not sampled. As the consequence, our estimates of the slopes &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; may be biased. Furthermore, in our construction we have put some arbitrary constraints on the environmental indices (&lt;span class=&#34;math inline&#34;&gt;\(\sum{e_j} = 0\)&lt;/span&gt;) and on the regression slopes (&lt;span class=&#34;math inline&#34;&gt;\(\sum({\beta_i})/G = 1\)&lt;/span&gt;; where G is the number of genotypes), which are not necessarily reasonable.&lt;/p&gt;
&lt;p&gt;Alternative methods of fitting joint regression models have been proposed (see Piepho, 1998), but they are slightly more complex and I will deal with them in a future post.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Eberhart, S.A., Russel, W.A., 1966. Stability parameters for comparing verieties. Crop Science 6, 36–40.&lt;/li&gt;
&lt;li&gt;Finlay, K.W., Wilkinson, G.N., 1963. The analysis of adaptation in a plant-breeding programme. Australian Journal of Agricultural Research 14, 742–754.&lt;/li&gt;
&lt;li&gt;Möhring, J., Piepho, H.-P., 2009. Comparison of Weighting in Two-Stage Analysis of Plant Breeding Trials. Crop Science 49, 1977–1988.&lt;/li&gt;
&lt;li&gt;Perkins, J.M., Jinks, J.L., 1968. Environmental gentype-environmental components of variability. III. Multiple lines and crosses. Heredity 23, 339–356.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1998. Methods for comparing the yield stability of cropping systems - A review. Journal of Agronomy and Crop Science 180, 193–213.&lt;/li&gt;
&lt;li&gt;Wood, J., 1976. The use of environmental variables in the interpretation of genotype-environment interaction. Heredity 37, 1–7.&lt;/li&gt;
&lt;li&gt;Yates, F., and Cochran G., 1938. The analysis of groups of experiments. Journal of Agricultural Sciences, 28, 556—580.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>AMMI analyses for GE interactions</title>
      <link>/2020/stat_met_ammi/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_met_ammi/</guid>
      <description>


&lt;p&gt;The CoViD-19 situation in Italy is little by little improving and I feel a bit more optimistic. It’s time for a new post! I will go back to a subject that is rather important for most agronomists, i.e. the selection of crop varieties.&lt;/p&gt;
&lt;p&gt;All farmers are perfectly aware that crop performances are affected both by the genotype and by the environment. These two effects are not purely additive and they often show a significant interaction. By this word, we mean that a genotype can give particularly good/bad performances in some specific environmental situations, which we may not expect, considering its average behaviour in other environmental conditions. The Genotype by Environment (GE) interaction may cause changes in the ranking of genotypes, depending on the environment and may play a key role in varietal recommendation, for a given mega-environment.&lt;/p&gt;
&lt;p&gt;GE interactions are usually studied by way of Multi-Environment Trials (MET), where experiments are repeated across several years, locations or any combinations of those. Traditional techniques of data analyses, such as two-way ANOVA, give very little insight on the stability/reliability of genotypes across environments and, thus, other specialised techniques are necessary to shed light on interaction effects. I have already talked about stability analyses in other posts, such as &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/&#34;&gt;in this post about the stability variance&lt;/a&gt; or in this &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_environmentalvariance/&#34;&gt;other post about the environmental variance&lt;/a&gt;. Now, I would like to propose some simple explanations about AMMI analysis. AMMI stands for: &lt;strong&gt;Additive Main effect Multiplicative Interaction&lt;/strong&gt; and it has become very much in fashion in the last 20-25 years.&lt;/p&gt;
&lt;p&gt;Let’s start with a real MET example.&lt;/p&gt;
&lt;div id=&#34;a-met-with-faba-bean&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A MET with faba bean&lt;/h1&gt;
&lt;p&gt;This experiment consists of 12 faba bean genotypes (well, it was, indeed, 6 genotypes in two sowing dates; but, let’s disregard this detail from now on) in four blocks, two locations and three years (six environments, in all). The dataset is online available as ‘fabaBean.csv’. It has been published by Stagnari et al. (2007).&lt;/p&gt;
&lt;p&gt;First of all, let’s load the dataset and transform the block variable into a factor. Let’s also inspect the two-way table of means, together with the marginal means for genotypes and environments, which will be useful later. In this post, we will make use of the packages ‘dplyr’ (Wickham &lt;em&gt;et al&lt;/em&gt;., 2020), ‘emmeans’ (Lenth, 2020) and ‘aomisc’; this latter is the companion package for this website and must have been installed as detailed in this &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;page here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# options(width = 70)

rm(list=ls())
# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
library(reshape)
library(emmeans)
library(aomisc)

fileName &amp;lt;- &amp;quot;https://www.casaonofri.it/_datasets/fabaBean.csv&amp;quot;
dataset &amp;lt;- read.csv(fileName, header=T)
dataset &amp;lt;- transform(dataset, Block = factor(Block),
                     Genotype = factor(Genotype),
                     Environment = factor(Environment))
head(dataset)
##      Genotype Block Environment Yield
## 1    Chiaro_A     1       bad_1  4.36
## 2    Chiaro_P     1       bad_1  2.76
## 3 Collameno_A     1       bad_1  3.01
## 4 Collameno_P     1       bad_1  2.50
## 5    Palomb_A     1       bad_1  3.85
## 6    Palomb_P     1       bad_1  2.21
#
# Two-ways table of means
GEmedie &amp;lt;- cast(Genotype ~ Environment, data = dataset,
                value = &amp;quot;Yield&amp;quot;, fun=mean)
GEmedie
##       Genotype  bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## 1     Chiaro_A 4.1050 2.3400 4.1250 4.6325 2.4100 3.8500
## 2     Chiaro_P 2.5075 1.3325 4.2025 3.3225 1.4050 4.3175
## 3  Collameno_A 3.2500 2.1150 4.3825 3.8475 2.2325 4.0700
## 4  Collameno_P 1.9075 0.8475 3.8650 2.5200 0.9850 4.0525
## 5     Palomb_A 3.8400 2.0750 4.2050 5.0525 2.6850 4.6675
## 6     Palomb_P 2.2500 0.9725 3.2575 3.2700 0.8825 4.0125
## 7      Scuro_A 4.3700 2.1050 4.1525 4.8625 2.1275 4.2050
## 8      Scuro_P 3.0500 1.6375 3.9300 3.7200 1.7475 4.5125
## 9    Sicania_A 3.8300 1.9450 4.5050 3.9550 2.2350 4.2350
## 10   Sicania_P 3.2700 0.9900 3.7300 4.0475 0.8225 3.8950
## 11   Vesuvio_A 4.1375 2.0175 4.0275 4.5025 2.2650 4.3225
## 12   Vesuvio_P 2.1225 1.1800 3.5250 3.0950 0.9375 3.6275
#
# Marginal means for genotypes
apply(GEmedie, 1, mean)
##    Chiaro_A    Chiaro_P Collameno_A Collameno_P    Palomb_A 
##    3.577083    2.847917    3.316250    2.362917    3.754167 
##    Palomb_P     Scuro_A     Scuro_P   Sicania_A   Sicania_P 
##    2.440833    3.637083    3.099583    3.450833    2.792500 
##   Vesuvio_A   Vesuvio_P 
##    3.545417    2.414583
#
# Marginal means for environments
apply(GEmedie, 2, mean)
##    bad_1    bad_2    bad_3    pap_1    pap_2    pap_3 
## 3.220000 1.629792 3.992292 3.902292 1.727917 4.147292
#
# Overall mean
mean(as.matrix(GEmedie))
## [1] 3.103264&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What model could we possibly fit to the above data? The basic two-way ANOVA model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{ijk} = \mu + \gamma_{jk} + g_i + e_j + ge_{ij} + \varepsilon_{ijk} \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the yield &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for given block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is described as a function of the effects of blocks within environments (&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;), genotypes (&lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;), environments (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and GE interaction (&lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt;). The residual error term &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is assumed to be normal and homoscedastic, with standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Let’s also assume that both the genotype and environment effects are fixed: this is useful for teaching purposes and it is reasonable, as we intend to study the behaviour of specific genotypes in several specific environments.&lt;/p&gt;
&lt;p&gt;The interaction effect &lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt;, under some important assumptions (i.e. balanced data, no missing cells and homoscedastic errors), is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ge_{ij} = Y_{ij.} - \left( \mu + g_i + e_j \right) = Y_{ij.} - Y_{i..} - Y_{.j.} + \mu \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij.}\)&lt;/span&gt; is the mean of the combination between the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y_{i..}\)&lt;/span&gt; is the mean for the genotype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y_{.j.}\)&lt;/span&gt; is the mean for the environment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. For example, for the genotype ‘Chiaro_A’ in the environment ‘bad_1’, the interaction effect was:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;4.1050 - 3.577 - 3.22 + 3.103
## [1] 0.411&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the interaction was positive, in the sense that ‘Chiaro_A’, gave 0.411 tons per hectare more than we could have expected, considering its average performances across environments and the average performances of all genotypes in ‘bad_1’.&lt;/p&gt;
&lt;p&gt;More generally, the two-way table of interaction effects can be obtained by doubly centring the matrix of means, as shown in the following box.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GE &amp;lt;- as.data.frame(t(scale( t(scale(GEmedie, center=T,
 scale=F)), center=T, scale=F)))
print(round(GE, 3))
##              bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## Chiaro_A     0.411  0.236 -0.341  0.256  0.208 -0.771
## Chiaro_P    -0.457 -0.042  0.466 -0.324 -0.068  0.426
## Collameno_A -0.183  0.272  0.177 -0.268  0.292 -0.290
## Collameno_P -0.572 -0.042  0.613 -0.642 -0.003  0.646
## Palomb_A    -0.031 -0.206 -0.438  0.499  0.306 -0.131
## Palomb_P    -0.308  0.005 -0.072  0.030 -0.183  0.528
## Scuro_A      0.616 -0.059 -0.374  0.426 -0.134 -0.476
## Scuro_P     -0.166  0.011 -0.059 -0.179  0.023  0.369
## Sicania_A    0.262 -0.032  0.165 -0.295  0.160 -0.260
## Sicania_P    0.361 -0.329  0.048  0.456 -0.595  0.058
## Vesuvio_A    0.475 -0.054 -0.407  0.158  0.095 -0.267
## Vesuvio_P   -0.409  0.239  0.221 -0.119 -0.102  0.169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that the overall mean for all elements in ‘GE’ is zero and the sum of squares is equal to a fraction of the interaction sum of squares in ANOVA (that is &lt;span class=&#34;math inline&#34;&gt;\(RMSE/r\)&lt;/span&gt;; where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the number of blocks).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(unlist(GE))
## [1] 6.914424e-18
sum(GE^2)
## [1] 7.742996
mod &amp;lt;- lm(Yield ~ Environment/Block + Genotype*Environment, data = dataset)
anova(mod)
## Analysis of Variance Table
## 
## Response: Yield
##                       Df Sum Sq Mean Sq  F value    Pr(&amp;gt;F)    
## Environment            5 316.57  63.313 580.9181 &amp;lt; 2.2e-16 ***
## Genotype              11  70.03   6.366  58.4111 &amp;lt; 2.2e-16 ***
## Environment:Block     18   6.76   0.375   3.4450 8.724e-06 ***
## Environment:Genotype  55  30.97   0.563   5.1669 &amp;lt; 2.2e-16 ***
## Residuals            198  21.58   0.109                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
30.97/4
## [1] 7.7425&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;decomposing-the-ge-matrix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Decomposing the GE matrix&lt;/h1&gt;
&lt;p&gt;It would be nice to be able to give a graphical summary of the GE matrix; in this regard, we could think of using Principal Component Analysis (PCA) via Singular Value Decomposition (SVD). This has been shown by Zobel &lt;em&gt;et al&lt;/em&gt;. (1988) and, formerly, by Gollob (1968). May I just remind you a few things about PCA and SVD? No overwhelming math detail, I promise!&lt;/p&gt;
&lt;p&gt;Most matrices (and our GE matrix) can be decomposed as the product of three matrices, according to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = U D V^T \quad \quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the matrix to be decomposed, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is the matrix of the first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is the matrix of the first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(X^T X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the diagonal matrix of the first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; singular values of &lt;span class=&#34;math inline&#34;&gt;\(XX^T\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(X^T X\)&lt;/span&gt;; it does not matter, they are the same).&lt;/p&gt;
&lt;p&gt;Indeed, if we want to decompose our GE matrix, it is more clever (and more useful to our purposes), to write the following matrices:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_g = U D^{1/2} \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S_e = V D^{1/2} \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[GE = S_g \, S_e^T \quad \quad (6)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; is the matrix of row-scores (genotype scores) and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt; is the matrix of column scores (environment scores). Let me give you an empirical proof, in the box below. In order to find &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;, I will use a mathematical operation that is known as Singular Value Decomposition (SVD):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;U &amp;lt;- svd(GE)$u
V &amp;lt;- svd(GE)$v
D &amp;lt;- diag(svd(GE)$d)
Sg &amp;lt;- U %*% sqrt(D)
Se &amp;lt;- V %*% sqrt(D)
row.names(Sg) &amp;lt;- levels(dataset$Genotype)
row.names(Se) &amp;lt;- levels(dataset$Environment)
colnames(Sg) &amp;lt;- colnames(Se) &amp;lt;- paste(&amp;quot;PC&amp;quot;, 1:6, sep =&amp;quot;&amp;quot;)
round(Sg %*% t(Se), 3)
##              bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## Chiaro_A     0.411  0.236 -0.341  0.256  0.208 -0.771
## Chiaro_P    -0.457 -0.042  0.466 -0.324 -0.068  0.426
## Collameno_A -0.183  0.272  0.177 -0.268  0.292 -0.290
## Collameno_P -0.572 -0.042  0.613 -0.642 -0.003  0.646
## Palomb_A    -0.031 -0.206 -0.438  0.499  0.306 -0.131
## Palomb_P    -0.308  0.005 -0.072  0.030 -0.183  0.528
## Scuro_A      0.616 -0.059 -0.374  0.426 -0.134 -0.476
## Scuro_P     -0.166  0.011 -0.059 -0.179  0.023  0.369
## Sicania_A    0.262 -0.032  0.165 -0.295  0.160 -0.260
## Sicania_P    0.361 -0.329  0.048  0.456 -0.595  0.058
## Vesuvio_A    0.475 -0.054 -0.407  0.158  0.095 -0.267
## Vesuvio_P   -0.409  0.239  0.221 -0.119 -0.102  0.169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s have a look at &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;: they are two interesting entities. I will round up a little to make them smaller, and less scaring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(Sg, 3)
##                PC1    PC2    PC3    PC4    PC5 PC6
## Chiaro_A    -0.607 -0.384  0.001  0.208 -0.063   0
## Chiaro_P     0.552  0.027 -0.081  0.045  0.164   0
## Collameno_A  0.084 -0.542 -0.006  0.176  0.057   0
## Collameno_P  0.807 -0.066 -0.132 -0.172  0.079   0
## Palomb_A    -0.321  0.110  0.591 -0.083  0.389   0
## Palomb_P     0.281  0.346  0.282  0.042 -0.253   0
## Scuro_A     -0.626  0.139 -0.163  0.017 -0.080   0
## Scuro_P      0.230  0.077  0.182 -0.207 -0.242   0
## Sicania_A   -0.063 -0.324 -0.355 -0.280  0.090   0
## Sicania_P   -0.214  0.683 -0.402  0.148  0.151   0
## Vesuvio_A   -0.438 -0.008  0.020 -0.300 -0.177   0
## Vesuvio_P    0.316 -0.058  0.063  0.405 -0.114   0
round(Se, 3)
##          PC1    PC2    PC3    PC4    PC5 PC6
## bad_1 -0.831  0.095 -0.467 -0.317 -0.151   0
## bad_2  0.044 -0.418  0.070  0.371 -0.403   0
## bad_3  0.670 -0.130 -0.525  0.171  0.298   0
## pap_1 -0.661  0.513  0.289  0.314  0.221   0
## pap_2 -0.069 -0.627  0.420 -0.294  0.208   0
## pap_3  0.846  0.567  0.213 -0.244 -0.173   0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both matrices have 6 columns. Why six, are you asking? I promised I would not go into math detail; it’s enough to know that the number of columns is always equal to the minimum value between the number of genotypes and the number of environments. The final column is irrelevant (all elements are 0). &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; has 12 rows, one per genotype; these are the so called genotype scores: each genotype has six scores. &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt; has six rows, one per environment (environment scores).&lt;/p&gt;
&lt;p&gt;You may have some ‘rusty’ memories about matrix multiplication; however, what we have discovered in the code box above is that the GE interaction for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; genotype and the &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; environment can be obtained as the product of genotype scores and environments scores. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ge_{ij} = \sum_{z = 1}^n \left[ S_g(iz) \cdot S_e(jz) \right] \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of columns (number of principal components). An example is in order, at this point; again, let’s consider the first genotype and the first environment. The genotype and environments scores are in the first columns of &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;; if we multiply the elements in the same positioning (1st with 1st, 2nd with 2nd, and so on) and sum up, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.607 * -0.831 +
-0.384 *  0.095 +
 0.001 * -0.467 +
 0.208 * -0.317 + 
-0.063 * -0.151 +
     0 * 0
## [1] 0.411047&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s done: we have transformed the interaction effect into the sum of multiplicative terms. If we replace Equation 7 into the ANOVA model above (Equation 1), we obtain an &lt;em&gt;Additive Main effects Multiplicative Interaction&lt;/em&gt; model, i.e. an AMMI model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reducing-the-rank&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reducing the rank&lt;/h1&gt;
&lt;p&gt;In this case we took all available columns in &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;. For the sake of simplicity, we could have taken only a subset of those columns. The Eckart-Young (1936) theorem says that, if we take &lt;span class=&#34;math inline&#34;&gt;\(m &amp;lt; 6\)&lt;/span&gt; columns, we obtain the best possible approximation of GE in reduced rank space. For example, let’s use the first two columns of &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt; (the first two principal component scores):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PC &amp;lt;- 2
Sg2 &amp;lt;- Sg[,1:PC]
Se2 &amp;lt;- Se[,1:PC]
GE2 &amp;lt;- Sg2 %*% t(Se2)
print ( round(GE2, 3) )
##              bad_1  bad_2  bad_3  pap_1  pap_2  pap_3
## Chiaro_A     0.468  0.134 -0.357  0.205  0.282 -0.732
## Chiaro_P    -0.456  0.013  0.367 -0.351 -0.055  0.482
## Collameno_A -0.122  0.230  0.127 -0.334  0.334 -0.236
## Collameno_P -0.676  0.063  0.549 -0.567 -0.014  0.645
## Palomb_A     0.277 -0.060 -0.230  0.269 -0.047 -0.209
## Palomb_P    -0.201 -0.132  0.144 -0.009 -0.236  0.434
## Scuro_A      0.534 -0.086 -0.438  0.486 -0.044 -0.451
## Scuro_P     -0.184 -0.022  0.144 -0.113 -0.064  0.238
## Sicania_A    0.022  0.133  0.000 -0.124  0.207 -0.237
## Sicania_P    0.243 -0.295 -0.232  0.492 -0.414  0.206
## Vesuvio_A    0.363 -0.016 -0.293  0.286  0.035 -0.375
## Vesuvio_P   -0.268  0.038  0.219 -0.239  0.015  0.234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;GE2 is not equal to GE, but it is a close approximation. A close approximation in what sense?… you may wonder. Well, the sum of squared elements in GE2 is as close as possible (with &lt;span class=&#34;math inline&#34;&gt;\(n = 2\)&lt;/span&gt;) to the sum of squared elements in GE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(GE2^2)
## [1] 6.678985&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the sum of squares in GE2 is 86% of the sum of squares in GE. A very good approximation, isn’t it? It means that the variability of yield across environments is described well enough by using a relatively low number of parameters (scores). However, the multiplicative part of our AMMI model needs to be modified:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ge_{ij} = \sum_{z = 1}^m \left[ s_{g(iz)} \cdot s_{e(jz)} \right] + \xi_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed, a residual term &lt;span class=&#34;math inline&#34;&gt;\(\xi_{ij}\)&lt;/span&gt; is necessary, to account for the fact that the sum of multiplicative terms is not able to fully recover the original matrix GE. Another example? For the first genotype and the first environment the multiplicative interaction is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.607 * -0.831 + -0.384 * 0.095
## [1] 0.467937&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the residual term &lt;span class=&#34;math inline&#34;&gt;\(\xi_{11}\)&lt;/span&gt; is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.41118056 -0.607 * -0.831 + -0.384 * 0.095
## [1] 0.8791176&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, the residual terms need to be small enough to be negligible, otherwise the approximation in reduced rank space is not good enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-this-useful&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why is this useful?&lt;/h1&gt;
&lt;p&gt;Did you get lost? Hope you didn’t, but let’s make a stop and see where we are standing now. We started from the interaction matrix GE and found a way to decompose it as the product of two matrices, i.e. &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;, a matrix of genotype scores and a matrix of environment scores. We discovered that we could obtain a good approximation of GE by working in reduced rank space and we only used two genotypic scores and two environment scores, in place of the available six.&lt;/p&gt;
&lt;p&gt;This is great! Now we have the ability of drawing a biplot, i.e. we can plot both genotypic scores and environmental scores in a dispersion graph (biplot: two plots in one), as we see below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot(Sg[,1:2], Se[,1:2], xlim = c(-1, 1), ylim = c(-1, 1),
       xlab = &amp;quot;PC 1&amp;quot;, ylab = &amp;quot;PC 2&amp;quot;)
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_met_AMMI_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph provides a very effective description of GE interaction effects. I will not go into detail, here. Just a few simple comments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;genotypes and environments lying close to the origin of the axes do not interact with each other (the product of scores would be close to 0)&lt;/li&gt;
&lt;li&gt;genotype and environments lying far away from the origin of axes show very big interaction and, therefore, high yield instability. Someone says that the euclidean distance from the origin should be taken as a measure of instability&lt;/li&gt;
&lt;li&gt;the interaction is positive, when genotypes and environments are close to each other. If two objects are close, their scores (co-ordinates) will have the same signs and thus their product will be positive.&lt;/li&gt;
&lt;li&gt;the interaction is negative, when genotypes and environments are far away from each other. If two objects are distant, their scores (co-ordinates) will have opposte signs and thus their product will be negative.&lt;/li&gt;
&lt;li&gt;For instance, ‘Palomb_P’, ‘Scuro_P’, ‘Chiaro_P’ and ‘Collameno_P’ gave particularly good yields in the environments ‘pap_3’ and ‘bad_3’, while ‘Scuro_A’, ‘Palomb_A’ and ‘Vesuvio_A’ gave particularly good yields (compared to their average) in the environments ‘pap_1’ and ‘bad_1’. ‘Sicania_A’ and ‘Collameno_A’ gave good yields in ‘bad_2’ and ‘pap_2’.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;how-many-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How many components?&lt;/h2&gt;
&lt;p&gt;In my opinion, AMMI analysis is mainly a visualisation method. Therefore, we should select as many components (columns in &lt;span class=&#34;math inline&#34;&gt;\(S_g\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S_e\)&lt;/span&gt;) as necessary to describe a main part of the interaction sum of squares. In our example, two components are enough, as they represent 86% of the interaction sum of squares.&lt;/p&gt;
&lt;p&gt;However, many people (and reviewers) are still very concerned with formal hypothesis testing. Therefore, we could proceed in a sequential fashion, and introduce the components one by one.&lt;/p&gt;
&lt;p&gt;The first component has a sum of squares equal to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PC &amp;lt;- 1
Sg2 &amp;lt;- Sg[,1:PC]
Se2 &amp;lt;- Se[,1:PC]
GE2 &amp;lt;- Sg2 %*% t(Se2)
sum(GE2^2)
## [1] 5.290174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have seen that the second component has an additional sum of squares equal to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6.678985 - 5.290174
## [1] 1.388811&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can go further ahead and get the sum of squares for all components. According to Zobel (1988), the degrees of freedom for each component are equal to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ df_n = i + j - 1 - 2m \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is the number of genotypes, &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is the number of environments, and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of the selected components. In our case, the first PC has 15 DF, the second one has 13 DF and so on.&lt;/p&gt;
&lt;p&gt;If we can have a reliable estimate of the pure error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; (see above), we can test the significance of each component by using F tests (although some authors argue that this is too a liberal approach; see Cornelius, 1993).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-ammi-analysis-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple AMMI analysis with R&lt;/h1&gt;
&lt;p&gt;We have seen that AMMI analysis, under the hood, is a sort of PCA. Therefore, it could be performed, in R by using one of the available packages for PCA. For the sake of simplicity, I have coded a couple of functions, i.e. ‘AMMI()’ and ‘AMMImeans()’ and they are both available in the ‘aomisc’ package. I have described the first one a few years ago in an R news paper (&lt;a href=&#34;https://www.researchgate.net/publication/289419258_Using_R_to_perform_the_AMMI_analysis_on_agriculture_variety_trials&#34;&gt;Onofri and Ciriciofolo, 2007&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Here, I will describe the second one, which permits to handle a small degree of unbalance (a few plots, missing at random). The analysis proceeds in two steps.&lt;/p&gt;
&lt;div id=&#34;first-step-on-raw-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First step on raw data&lt;/h2&gt;
&lt;p&gt;During the first step we need to obtain a reliable matrix of means for the ‘genotype x environment’ combinations. If the environment is fixed, we can use least squares means, which are unbiased, also when some observations are missing. If the environment effect is random, we could use the BLUPs, but we will not consider such an option here.&lt;/p&gt;
&lt;p&gt;In the box below we take the ‘mod’ object from a two way ANOVA fit and derive the residual mean square (RMSE), which we divide by the number of blocks. This will be our error term to test the significance of components. Later, we pass the ‘mod’ object to the ‘emmeans()’ function, to retrieve the expected marginal means for the ‘genotype by environment’ combinations and proceed to the second step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMSE &amp;lt;- summary(mod)$sigma^2 / 4
dfr &amp;lt;- mod$df.residual
ge.lsm &amp;lt;- emmeans(mod, ~Genotype:Environment)
ge.lsm &amp;lt;- data.frame(ge.lsm)[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;second-step-on-least-square-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second step on least square means&lt;/h2&gt;
&lt;p&gt;This second step assumes that the residual variances for all environments are homogeneous. If so (we’d better check this), we can take the expected marginal means (‘ge.lsm’) and submit them to AMMI analysis, by using the ‘AMMImeans()’ function. The syntax is fairly obvious; we also pass to it the RMSE and its degrees of freedom. The resulting object can be explored, by using the appropriate slots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AMMIobj &amp;lt;- AMMImeans(yield = ge.lsm$emmean, 
                     genotype = ge.lsm$Genotype, 
                     environment = ge.lsm$Environment, 
                     MSE = RMSE, dfr = dfr)
#
AMMIobj$genotype_scores
##                     PC1          PC2
## Chiaro_A    -0.60710888 -0.383732821
## Chiaro_P     0.55192742  0.026531045
## Collameno_A  0.08444877 -0.542185666
## Collameno_P  0.80677055 -0.065752971
## Palomb_A    -0.32130513  0.110117240
## Palomb_P     0.28104959  0.345909298
## Scuro_A     -0.62638795  0.139185954
## Scuro_P      0.22961347  0.076555540
## Sicania_A   -0.06286803 -0.323857285
## Sicania_P   -0.21433211  0.683296898
## Vesuvio_A   -0.43786742 -0.007914342
## Vesuvio_P    0.31605973 -0.058152890
#
AMMIobj$environment_scores
##               PC1         PC2
## bad_1 -0.83078550  0.09477362
## bad_2  0.04401963 -0.41801637
## bad_3  0.67043214 -0.12977423
## pap_1 -0.66137357  0.51268429
## pap_2 -0.06863235 -0.62703224
## pap_3  0.84633965  0.56736492
#
round(AMMIobj$summary, 4)
##   PC Singular_value  PC_SS Perc_of_Total_SS cum_perc
## 1  1         2.3000 5.2902          68.3220  68.3220
## 2  2         1.1785 1.3888          17.9364  86.2584
## 3  3         0.8035 0.6456           8.3375  94.5959
## 4  4         0.5119 0.2621           3.3846  97.9806
## 5  5         0.3954 0.1564           2.0194 100.0000
## 6  6         0.0000 0.0000           0.0000 100.0000
#
round(AMMIobj$anova, 4)
##   PC     SS DF     MS       F P.value
## 1  1 5.2902 15 0.3527 12.9437  0.0000
## 2  2 1.3888 13 0.1068  3.9208  0.0000
## 3  3 0.6456 11 0.0587  2.1539  0.0184
## 4  4 0.2621  9 0.0291  1.0687  0.3876
## 5  5 0.1564  7 0.0223  0.8198  0.5718
## 6  6 0.0000  5 0.0000  0.0000  1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In detail, we can retrieve the genotype and environment scores, the proportion of the GE variance explained by each component and the significance of PCs.&lt;/p&gt;
&lt;p&gt;Just to show you, the box below reports the code for AMMI analysis on raw data. Please, note that this only works with balanced data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AMMIobj2 &amp;lt;- AMMI(yield = dataset$Yield, 
                 genotype = dataset$Genotype,
                 environment = dataset$Environment, 
                 block = dataset$Block)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I agree, these functions are not very ambitious. However, they are simple enough to be usable and give reliable results, as long as the basic assumptions for the method are respected. You may also consider to explore other more comprehensive R packages, such as ‘agricolae’ (de Mendiburu, 2020).&lt;/p&gt;
&lt;p&gt;Thank you for reading, so far, and… happy coding!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;literature-references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Literature references&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Annichiarico, P. (1997). Additive main effects and multiplicative interaction (AMMI) analysis of genotype-location interaction in variety trials repeated over years. Theoretical applied genetics, 94, 1072-1077.&lt;/li&gt;
&lt;li&gt;Ariyo, O. J. (1998). Use of additive main effects and multiplicative interaction model to analyse multilocation soybean varietal trials. J. Genet. and Breed, 129-134.&lt;/li&gt;
&lt;li&gt;Cornelius, P. L. (1993). Statistical tests and retention of terms in the Additive Main Effects and Multiplicative interaction model for cultivar trials. Crop Science, 33,1186-1193.&lt;/li&gt;
&lt;li&gt;Crossa, J. (1990). Statistical Analyses of multilocation trials. Advances in Agronomy, 44, 55-85.&lt;/li&gt;
&lt;li&gt;Gollob, H. F. (1968). A statistical model which combines features of factor analytic and analysis of variance techniques. Psychometrika, 33, 73-114.&lt;/li&gt;
&lt;li&gt;Lenth R., 2020. emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.4.6. &lt;a href=&#34;https://github.com/rvlenth/emmeans&#34; class=&#34;uri&#34;&gt;https://github.com/rvlenth/emmeans&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;de Mendiburu F., 2020. agricolae: Statistical Procedures for Agricultural Research. R package version 1.3-2. &lt;a href=&#34;https://CRAN.R-project.org/package=agricolae&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=agricolae&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Onofri, A., Ciriciofolo, E., 2007. Using R to perform the AMMI analysis on agriculture variety trials. R NEWS 7, 14–19.&lt;/li&gt;
&lt;li&gt;Stagnari F., Onofri A., Jemison J., Monotti M. (2006). Multivariate analyses to discriminate the behaviour of faba bean (Vicia faba L. var. minor) varieties as affected by sowing time in cool, low rainfall Mediterranean environments. Agronomy For Sustainable Development, 27, 387–397.&lt;/li&gt;
&lt;li&gt;Hadley Wickham, Romain François, Lionel Henry and Kirill Müller, 2020. dplyr: A Grammar of Data Manipulation. R package version 0.8.5. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Zobel, R. W., Wright, M.J., and Gauch, H. G. (1988). Statistical analysis of a yield trial. Agronomy Journal, 388-393.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Seed germination: fitting hydro-time models with R</title>
      <link>/2020/stat_survival_ht1step/</link>
      <pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_survival_ht1step/</guid>
      <description>


&lt;p&gt;I am locked at home, due to the COVID-19 emergency in Italy. Luckily I am healthy, but there is not much to do, inside. I thought it might be nice to spend some time to talk about seed germination models and the connections with survival analysis.&lt;/p&gt;
&lt;p&gt;We all know that seeds need water to germinate. Indeed, the absorption of water activates the hydrolytic enzymes, which break down food resources stored in seeds and provide energy for germination. As the consequence, there is a very close relationship between water content in the substrate and germination velocity: the higher the water content the quickest the germination, as long as the availability of oxygen does not become a problem (well, water and oxygen in soil may compete for space and a high water content may result in oxygen shortage).&lt;/p&gt;
&lt;p&gt;Indeed, it is relevant to build germination models, linking the proportion of germinated seeds to water availability in the substrate; these models are usually known as hydro-time (HT) models. The starting point is the famous equation of Bradford (1992), where the germination rate (GR) for the &lt;span class=&#34;math inline&#34;&gt;\(i-th\)&lt;/span&gt; seed in the lot is expressed as a linear function of water potential in the substrate (&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ GR_i = \textrm{min} \left( \frac{\Psi - \Psi_{b(i)}}{\theta_H}; 0 \right) \quad \quad \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In that equation, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b(i)}\)&lt;/span&gt; is the base water potential for the &lt;span class=&#34;math inline&#34;&gt;\(i-th\)&lt;/span&gt; seed and &lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt; is the hydro-time constant, expressed as &lt;em&gt;MPa day&lt;/em&gt; or &lt;em&gt;MPa hour&lt;/em&gt;. The concept is relatively simple: we just need to remember that the water can only move from a position with a higher water potential to a position with a lower water potential. Therefore, a seed cannot germinate when its base water potential is higher than the water potential in the substrate.&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(\Psi &amp;gt; \Psi_b(i)\)&lt;/span&gt;, the germination rate of the &lt;span class=&#34;math inline&#34;&gt;\(i-th\)&lt;/span&gt; seed is linearly related to &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;: the higher this latter value, the higher the germination rate. Now we should consider that the germination rate is the inverse of the germination time (&lt;span class=&#34;math inline&#34;&gt;\(GR = 1/t\)&lt;/span&gt;); thus, the higher the GR, the shortest the germination time. Germination is achieved at the time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; when:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ t \, \left( \Psi - \Psi_{b(i)} \right) = \theta_H \quad \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Elsewhere in this website, I show that Equation 1 can be fitted to germination data in a two-steps fashion. In this page we will see how we can embed Equation 1 into a germination model, to predict the proportion of germinated seeds, depending on time and water content in the substrate. As usual, let’s start from a practical example.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;the-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The dataset&lt;/h1&gt;
&lt;p&gt;The germination of rapeseed (&lt;em&gt;Brassica napus&lt;/em&gt; L. var. &lt;em&gt;oleifera&lt;/em&gt;, cv. Excalibur) was tested at thirteen different water potentials (-0.03, -0.15, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1, -1.1, -1.2, -1.5 MPa), which were created by using a polyethylene glycol solution (PEG 6000). For each water potential level, three replicated Petri dishes with 50 seeds were incubated at 20°C. Germinated seeds were counted and removed every 2-3 days for 14 days.&lt;/p&gt;
&lt;p&gt;The dataset was published by Pace et al. (2012). It is available as &lt;code&gt;rape&lt;/code&gt; in the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, which needs to be installed from github (see below). The following code loads the necessary packages, loads the dataset &lt;code&gt;rape&lt;/code&gt; and shows the first six lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)
library(drc)
library(drcSeedGerm)
library(lmtest)
library(sandwich)
data(rape)
head(rape)
##   Psi Dish timeBef timeAf nSeeds nCum propCum
## 1   0    1       0      3     49   49    0.98
## 2   0    1       3      4      0   49    0.98
## 3   0    1       4      5      0   49    0.98
## 4   0    1       5      7      0   49    0.98
## 5   0    1       7     10      0   49    0.98
## 6   0    1      10     14      0   49    0.98&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data are grouped by assessment interval: ‘timeAf’ represents the moment when germinated seeds were counted, while ’timeBef’ represents the previous inspection time (or the beginning of the assay). The column ’nSeeds’ is the number of seeds that germinated during the time interval between ‘timeBef’ and ‘timeAf. The ’propCum’ column contains the cumulative proportions of germinated seeds and it is not necessary for time-to-event models. The ‘drcSeedGerm’ package contains some service functions which might help prepare the dataset in this form (see the documentation for the functions ‘makeDrm()’ and ‘makeDrm2()’).&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;building-hydro-time-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building hydro-time models&lt;/h1&gt;
&lt;div id=&#34;models-based-on-the-distribution-of-germination-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models based on the distribution of germination time&lt;/h2&gt;
&lt;p&gt;How can we rework Equation 1 to predict the proportion of germinated seeds, as a function of time and water potential? One line of attack follows the proposal we made in a relatively recent paper (Onofri at al., 2018). We started from the idea that the time course of the proportion of germinated seeds (&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;) is expected to increase over time, according to a S-shaped curve, such as the usual log-logistic cumulative probability function (other cumulative distribution functions can be used; see our original paper):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{ P_{MAX} }{1 + \exp \left\{ b \left[ \log(t) - \log(t_{50} ) \right] \right\} } \quad \quad \quad (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t_{50}\)&lt;/span&gt; is the median germination time, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point and &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt; is the maximum germinated proportion. Considering that the germination rate is the inverse of germination time, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{ P_{MAX} }{1 + \exp \left\{ b \left[ \log(t) - \log(1 / GR_{50} ) \right] \right\} } \quad \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt; is the median germination rate in the population.&lt;/p&gt;
&lt;p&gt;We can now express &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt; as linear/nonlinear functions of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; (temperature and other environmental variables can be included as well. See our original paper). In our paper, for &lt;span class=&#34;math inline&#34;&gt;\(GR_{50}\)&lt;/span&gt;, we used the Equation 1 above. For &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt;, we used a shifted exponential distribution, which implies that germination capability is fully determined by the distribution of base water potential within the population and no germinations occur at &lt;span class=&#34;math inline&#34;&gt;\(\Psi \leq \Psi_b\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P_{MAX} = h_1(\Psi ) = \textrm{min} \left\{ G \, \left[ 1 - \exp \left( \frac{ \Psi - \Psi_b }{\sigma_{\Psi_b}} \right) \right]; 0 \right\} \quad \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above equation, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi_b}\)&lt;/span&gt; represents the variability of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population, which determines the steepness of the increase in &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; increases. &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the germinable fraction, accounting for the fact that &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt; may not reach 1, regardless of time and water potential.&lt;/p&gt;
&lt;p&gt;The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; was assumed to be constant and independent on &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;. In the end, our hydro-time model is composed by four sub-models:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a cumulative probability function (log-logistic, in our example), based on the three parameters &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(GR50\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(P_{MAX}\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(GR50\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;a sub-model expressing &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;, although, this was indeed a simple identity model &lt;span class=&#34;math inline&#34;&gt;\(b(\Psi) = b\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(t) = \frac{ h_1(\Psi) }{1 + \exp \left\{ b \left[ \log(t) - \log(1 / \left[ GR_{50}(\Psi) \right] ) \right] \right\} } \quad \quad \quad (6)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This hydro-time model was implemented in R as the &lt;code&gt;HTE1()&lt;/code&gt; function, and it is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package, together with the appropriate self-starting routine. It can be fitting by using the &lt;code&gt;drm()&lt;/code&gt; function in the &lt;code&gt;drc&lt;/code&gt; package. Please, note that the argument &lt;code&gt;type&lt;/code&gt; has to be set to “event”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modHTE &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi, 
                data = rape, fct = HTE1(), type = &amp;quot;event&amp;quot;)
summary(modHTE)
## 
## Model fitted: Hydro-time model with shifted exponential for Pmax and linear model for GR50
## 
## Parameter estimates:
## 
##                         Estimate Std. Error  t-value   p-value    
## G:(Intercept)          0.9577943  0.0063663  150.448 &amp;lt; 2.2e-16 ***
## Psib:(Intercept)      -1.0397178  0.0047014 -221.152 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.1108836  0.0087593   12.659 &amp;lt; 2.2e-16 ***
## thetaH:(Intercept)     0.9060853  0.0301585   30.044 &amp;lt; 2.2e-16 ***
## b:(Intercept)          4.0272972  0.1960877   20.538 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As seeds are clustered in Petri dishes, in order not to violate the independence assumption, it is preferable to get cluster robust standard errors. One possibility is to use the grouped version of the sandwich estimator, as available in the &lt;code&gt;sandwich package&lt;/code&gt; (Berger, 2017). The function &lt;code&gt;coeftest&lt;/code&gt; is available in the &lt;code&gt;lmtest&lt;/code&gt; package (Zeileis, 2002):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeftest(modHTE, vcov = vcovCL, cluster = rape$Dish)
## 
## t test of coefficients:
## 
##                         Estimate Std. Error   t value  Pr(&amp;gt;|t|)    
## G:(Intercept)          0.9577943  0.0080918  118.3661 &amp;lt; 2.2e-16 ***
## Psib:(Intercept)      -1.0397178  0.0047067 -220.9003 &amp;lt; 2.2e-16 ***
## sigmaPsib:(Intercept)  0.1108836  0.0121872    9.0983 &amp;lt; 2.2e-16 ***
## thetaH:(Intercept)     0.9060853  0.0410450   22.0754 &amp;lt; 2.2e-16 ***
## b:(Intercept)          4.0272972  0.1934579   20.8174 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative way to obtain cluster robust standard errors is to use the delete-a-group jackknife technique, which I described in one of my previous papers (Onofri et al., 2014). This is available in the &lt;code&gt;jackGroupSE()&lt;/code&gt; function in the &lt;code&gt;drcSeedGerm&lt;/code&gt; package. It takes quite a bit of computing time, so you may need to be patient, especially if you have a lot of Petri dishes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jack &amp;lt;- jackGroupSE(modHTE, data = rape, cluster = rape$Dish)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the model is fitted, we may be interested in using the fitted to curve to retrieve some biologically relevant information. For example, it may be interesting to retrieve the germination rates for some selected percentiles (e.g., the 30th, 20th and 10th percentiles). This is possible using the &lt;code&gt;GRate()&lt;/code&gt; function, that is a wrapper for the original &lt;code&gt;ED()&lt;/code&gt; function in the package &lt;code&gt;drc&lt;/code&gt;. It reverses the behavior of the &lt;code&gt;ED()&lt;/code&gt; function, in the sense that it considers, by default, the percentiles for the whole population, including the ungerminated fraction, which is, in our opinion, the most widespread interpretation of germination rates in seed science. The &lt;code&gt;GRate()&lt;/code&gt; function works very much like the &lt;code&gt;ED()&lt;/code&gt; function, although additional variables, such as the selected &lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt; level can be specified by using the argument &lt;code&gt;x2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Naive standard errors
GRate(modHTE, x2 = -1, respLev = c(30, 20, 10))
##           Estimate         SE
## GR:1:30 0.00000000 0.00000000
## GR:1:20 0.03578644 0.01515519
## GR:1:10 0.05129734 0.01546193&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we see that the &lt;span class=&#34;math inline&#34;&gt;\(GR_{30}\)&lt;/span&gt; cannot be calculated, as the germination capacity did not reach 30% at the selected water potential level (&lt;span class=&#34;math inline&#34;&gt;\(-1 \,\, MPa\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;As we said, cluster robust standard errors are recommended. The &lt;code&gt;GRate()&lt;/code&gt; function allows entering a user-defined variance-covariance matrix, that is obtained by using the &lt;code&gt;vcovCL()&lt;/code&gt; function in the &lt;code&gt;sandwich&lt;/code&gt; package. If necessary, germination times can be obtained in a similar way, by using the &lt;code&gt;GTime()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Cluster robust standard errors
sc &amp;lt;- vcovCL(modHTE, cluster = rape$Dish)
GRate(modHTE, x2 = -1, respLev = c(30, 20, 10), vcov.=sc)
##           Estimate          SE
## GR:1:30 0.00000000 0.000000000
## GR:1:20 0.03578644 0.005452517
## GR:1:10 0.05129734 0.005870701
#Germination times
GTime(modHTE, x2 = -1, respLev = c(30, 20, 10), vcov.=sc)
##        Estimate       SE
## T:1:30      Inf       NA
## T:1:20 27.94355 4.257553
## T:1:10 19.49419 2.231004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Last, but not least, we can predict the proportion of germinated seeds at given time and water potential level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictSG(modHTE, se.fit=T, vcov. = vcovCL,
        newdata = data.frame(Time=c(10, 10, 10), 
                             Psi=c(-1.5, -0.75, 0))
        )
##      Prediction         SE
## [1,]  0.0000000 0.00000000
## [2,]  0.8794104 0.03907374
## [3,]  0.9576615 0.01493018&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;models-based-on-the-distribution-of-psi_b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models based on the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Another type of hydro-time model was proposed by Bradford (2002) and later extended by Mesgaran et al (2013). This approach starts always from Equation 1; from that equation, considering that the germination time is the inverse of the GR, we can easily get to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Psi_b = \Psi - \frac{\theta_H}{t} \quad \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the germination time. What does this equation tell us? Let’s assume that the hydro-time to germination is 10 &lt;span class=&#34;math inline&#34;&gt;\(MPa \, d\)&lt;/span&gt; and the environmental water potential is -1 &lt;span class=&#34;math inline&#34;&gt;\(MPa\)&lt;/span&gt;. A single seed germinates in exactly one day, if its base water potential is &lt;span class=&#34;math inline&#34;&gt;\(-1 - 10/1 = -11\)&lt;/span&gt;. If the base water potential is higher, germination will take more than one day; if it is lower, germination will take less than one day. But now, the following questions come: how many seeds in a population will be able to germinate in one day? And in two days? And in &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; days?&lt;/p&gt;
&lt;p&gt;We know that the seeds within a population do not germinate altogether in the same moment, as they have different individual values of base water potential. If the population is big enough, we can describe the variation of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population by using some density function, possibly parameterised by way of a location (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and a scale (&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;) parameter:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \Psi_b \sim \phi \left( \frac{\Psi_b - \mu}{\sigma} \right) \quad \quad \quad (8)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is easier to understand if we make a specific example. Let’s assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; values within the population is gaussian, with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = -9\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 1\)&lt;/span&gt;. Let’s also assume that the hydro-time parameter (&lt;span class=&#34;math inline&#34;&gt;\(\theta_H\)&lt;/span&gt;) is constant within the population. We have the situation depicted in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_HT1step_files/figure-html/distribPsi-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The red left tail represents the proportion of seeds that germinate during the first day, as they have base water potentials equal to or lower than -11. By using the gaussian cumulative distribution function we can easily see that that proportion is 0.228:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(-1 - 10/1, mean = -9, sd = 1)
## [1] 0.02275013&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More generally, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \Phi \left\{ \frac{\Psi - (\theta_H / t) -\mu }{\sigma} \right\} \quad \quad \quad (9)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is the selected cumulative distribution function. The above model returns the proportion of germinated seeds (G), as a function of time and water potential in the substrate. According to Bradford (2002), &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; is cumulative gaussian.&lt;/p&gt;
&lt;p&gt;Let’s think more deeply about Equation 9 (Bradford, 2002). This function was built to represent the cumulative distribution function of base water potential within the population. However, &lt;strong&gt;it can be as well taken to represent the cumulative distribution function of germination time within the population&lt;/strong&gt;. Obviously, while the first distribution is gaussian, the second one is not: indeed, the germination time appears at the denominator of the expression &lt;span class=&#34;math inline&#34;&gt;\(\theta_H / t\)&lt;/span&gt;. It doesn’t matter: every cumulative distribution function for germination time can be fit by using time-to-event methods!&lt;/p&gt;
&lt;p&gt;We implemented this model in R as the function &lt;code&gt;HTnorm()&lt;/code&gt; that is available within the &lt;code&gt;drcSeedGerm&lt;/code&gt; package and it is meant to be used with the &lt;code&gt;drm()&lt;/code&gt; function, in the &lt;code&gt;drc&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Mesgaran et al (2013) suggested that the distribution of base water potential within the population may not be gaussian and proposed several alternatives, which we have all implemented within the package. In all, &lt;code&gt;drcSeedGerm&lt;/code&gt; contains six possible distributions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;gaussian distribution (function &lt;code&gt;HTnorm()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;logistic distribution (function &lt;code&gt;HTL()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Gumbel (function &lt;code&gt;HTG()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;log-logistic (function &lt;code&gt;HTLL()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Weibull (Type I) (function &lt;code&gt;HTW1()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Weibull (Type II) (function &lt;code&gt;HTW2()&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The equations are given at the end of this page; for gaussian, logistic and log-logistic distributions, &lt;span class=&#34;math inline&#34;&gt;\(\Psi_{b(50)}\)&lt;/span&gt; is the median base water potential within the population. For the gaussian distribution, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\Psi b}\)&lt;/span&gt; corresponds to the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\Psi_b\)&lt;/span&gt; within the population.&lt;/p&gt;
&lt;p&gt;Distributions based on logarithms (the log-logistic and all other distributions thereafter) are only defined for positive amounts. On the contrary, we know that base water potential is mostly negative. Therefore, shifted distributions need to be used, by introducing a shifting parameter &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; which ‘moves’ the distribution to the left, along the x-axis, so that negative values are possible (see Mesgaran et al., 2013).&lt;/p&gt;
&lt;p&gt;Let’s fit the above functions to the ‘rape’ dataset. But, before, let me highlight that providing starting values is not necessary, as self-starting routines are already implemented for all models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
mod1 &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi, data = rape, 
            fct = HTnorm(), type = &amp;quot;event&amp;quot;)
mod2 &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi, data = rape,
            fct = HTL(), type = &amp;quot;event&amp;quot;)
mod3 &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi, data = rape,
            fct = HTG(), type = &amp;quot;event&amp;quot;)
mod4 &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTLL(), type = &amp;quot;event&amp;quot;)
mod5 &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTW1(), type = &amp;quot;event&amp;quot;)
mod6 &amp;lt;- drm(nSeeds ~ timeBef + timeAf + Psi,
            data = rape, fct = HTW2(), type = &amp;quot;event&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the best model for this dataset? Let’s use the Akaike’s Information Criterion (AIC) to decide:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod1, mod2, mod3, mod4, mod5, mod6, modHTE)
##         df      AIC
## mod1   291 3516.914
## mod2   291 3300.824
## mod3   291 3097.775
## mod4   290 2886.609
## mod5   290 2889.307
## mod6   290 2998.915
## modHTE 289 2832.481&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first model &lt;code&gt;modHTE&lt;/code&gt; considers explicitly the distribution of germination times and it is the best fitting of all. The other models consider explicitly the distribution of base water potential, while the distribution of germination times is indirectly included. Among these models, the gaussian is the worse fitting, while the log-logistic is the best one (&lt;code&gt;mod4&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;For this latter model, we take a look at the value of estimated parameters. Cluster robust standard errors can be obtained as before, by way of the sandwich estimator or a fully iterated delete-a-group jackknife estimator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sand &amp;lt;- coeftest(mod4, vcov = vcovCL, cluster = rape$Dish)
# jack &amp;lt;- jackGroupSE(mod4, data = rape, cluster = rape$Dish)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sand
## 
## t test of coefficients:
## 
##                     Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## thetaH:(Intercept)  0.677472   0.072902   9.2930  &amp;lt; 2e-16 ***
## delta:(Intercept)   1.136963   0.102440  11.0988  &amp;lt; 2e-16 ***
## Psib50:(Intercept) -0.948101   0.020341 -46.6097  &amp;lt; 2e-16 ***
## sigma:(Intercept)   0.372172   0.173360   2.1468  0.03264 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
# jack&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Germination rates and times for a certain percentile (e.g. GR50, GR30), can be obtained by using the &lt;code&gt;GRate()&lt;/code&gt; and &lt;code&gt;GTime()&lt;/code&gt; function in &lt;code&gt;drcSeedGerm&lt;/code&gt;. Again, the use of cluster-robust standard errors is highly recommended.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GRate(mod4, respLev=c(30, 50, 70), x2 = 0, vcov. = vcovCL)
##         Estimate        SE
## GR:1:30 1.474866 0.1796681
## GR:1:50 1.399469 0.1648390
## GR:1:70 1.296120 0.1561949
GTime(mod4, respLev=c(30, 50, 70), x2 = 0, vcov. = vcovCL)
##         Estimate         SE
## T:1:30 0.6780277 0.08259731
## T:1:50 0.7145569 0.08416538
## T:1:70 0.7715337 0.09297725&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also make predictions about the germinated proportion for a certain time and water potential level. The code below returns the maximum germinated proportions at -1.5, -0.75, and 0 MPa.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictSG(mod4, se.fit=T, vcov. = vcovCL,
        newdata = data.frame(Time=c(10, 10, 10), 
                             Psi=c(-1.5, -0.75, 0))
        )
##        Prediction           SE
## [1,] 6.658526e-15 1.124604e-13
## [2,] 8.038034e-01 5.594629e-02
## [3,] 9.906059e-01 9.465305e-03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use the &lt;code&gt;predictSG()&lt;/code&gt; function to plot the ‘modHTE’ and ‘mod4’ objects together in the same graph.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_HT1step_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Berger, S., Graham, N., Zeileis, A., 2017. Various versatile variances: An object-oriented implementation of clustered covariances in R. Faculty of Economics and Statistics, University of Innsbruck, Innsbruck.&lt;/li&gt;
&lt;li&gt;Bradford, K.J., 2002. Applications of hydrothermal time to quantifying and modeling seed germination and dormancy. Weed Science 50, 248–260.&lt;/li&gt;
&lt;li&gt;Mesgaran, M.B., Mashhadi, H.R., Alizadeh, H., Hunt, J., Young, K.R., Cousens, R.D., 2013. Importance of distribution function selection for hydrothermal time models of seed germination. Weed Research 53, 89–101. &lt;a href=&#34;https://doi.org/10.1111/wre.12008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Benincasa, P., Mesgaran, M.B., Ritz, C., 2018. Hydrothermal-time-to-event models for seed germination. European Journal of Agronomy 101, 129–139.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Neve, P., Cousens, R.D., 2014. Experimental design and parameter estimation for threshold models in seed germination. Weed Research 54, 425–435. &lt;a href=&#34;https://doi.org/10.1111/wre.12095&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/wre.12095&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pace, R., Benincasa, P., Ghanem, M.E., Quinet, M., Lutts, S., 2012. Germination of untreated and primed seeds in rapeseed (brassica napus var oleifera del.) under salinity and low matric potential. Experimental Agriculture 48, 238–251.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C. (2019) Dose-Response Analysis Using R CRC Press.
Achim Zeileis, Torsten Hothorn (2002). Diagnostic Checking in Regression Relationships. R News 2(3), 7-10. URL: &lt;a href=&#34;https://CRAN.R-project.org/doc/Rnews/&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/doc/Rnews/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;some-further-detail&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some further detail&lt;/h1&gt;
&lt;p&gt;Let us conclude this page by giving some detail on all equations.&lt;/p&gt;
&lt;p&gt;The equation for the model &lt;code&gt;HTnorm()&lt;/code&gt;. Here, we show all other equations, as implemented in our package.&lt;/p&gt;
&lt;div id=&#34;htl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTL()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \frac{1}{1 + exp \left[ - \frac{  \Psi  - \left( \theta _H/t \right) - \Psi_{b(50)} } {\sigma}  \right] }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htg&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTG()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \exp \left\{ { - \exp \left[ { - \left( {\frac{{\Psi - (\theta _H / t ) - \mu }}{\sigma }} \right)} \right]} \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htll&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTLL()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = \frac{1}{1 + \exp \left\{ \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b50} + \delta)  }{\sigma}\right\} }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htw1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTW1()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = exp \left\{ - \exp \left[ - \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b50} + \delta)  }{\sigma}\right] \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;htw2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;HTW2()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ G(t, \Psi) = 1 - exp \left\{ - \exp \left[ \frac{ \log \left[ \Psi  - \left( \frac{\theta _H}{t} \right) + \delta \right] - \log(\Psi_{b50} + \delta)  }{\sigma}\right] \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A collection of self-starters for nonlinear regression in R</title>
      <link>/2020/stat_nls_usefulfunctions/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_nls_usefulfunctions/</guid>
      <description>


&lt;p&gt;Usually, the first step of every nonlinear regression analysis is to select the function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, which best describes the phenomenon under study. The next step is to fit this function to the observed data, possibly by using some sort of nonlinear least squares algorithms. These algorithms are iterative, in the sense that they start from some initial values of model parameters and repeat a sequence of operations, which continuously improve the initial guesses, until the least squares solution is approximately reached.&lt;/p&gt;
&lt;p&gt;This is the main problem: we need to provide initial values for all model parameters! It is not irrelevant; indeed, if our guesses are not close enough to least squares estimates, the algorithm may freeze during the estimation process and may not reach convergence. Unfortunately, guessing good initial values for model parameters is not always easy, especially for students and practitioners. This is where self-starters come in handy.&lt;/p&gt;
&lt;p&gt;Self-starter functions can automatically calculate initial values for any given dataset and, therefore, they can make nonlinear regression analysis as smooth as linear regression analysis. From a teaching perspective, this means that the transition from linear to nonlinear models is immediate and hassle-free!&lt;/p&gt;
&lt;p&gt;In a recent post (&lt;a href=&#34;https://www.statforbiology.com/2020/stat_nls_selfstarting/&#34;&gt;see here&lt;/a&gt;) I gave detail on how self-starters can be built, both for the ‘nls()’ function in the ‘stats’ package and for the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2019). Both ‘nls()’ and ‘drm()’ can be used to fit nonlinear regression models in R and the respective packages already contain several robust self-starting functions. I am a long-time user of both ‘nls()’ and ‘drm()’ and I have little-by-little built a rather wide knowledge base of self-starters for both. I’ll describe them in this post; they are available within the package ‘aomisc’, that is the accompanying package for this blog.&lt;/p&gt;
&lt;p&gt;First of all, we need to install this package (if necessary) and load it, by using the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#installing package, if not yet available
# library(devtools)
# install_github(&amp;quot;onofriandreapg/aomisc&amp;quot;)

# loading package
library(aomisc)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;functions-and-curve-shapes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functions and curve shapes&lt;/h1&gt;
&lt;p&gt;Nonlinear regression functions are usually classified according to the shape they show when they are plotted in a XY-graph. Such an approach is taken, e.g., in the great book of Ratkowsky (1990). The following classification is heavily based on that book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomials
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#straight-line-function&#34;&gt;Straight line function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-polynomial-function&#34;&gt;Quadratic polynomial function&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Concave/Convex curves (no inflection)
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#exponential-function&#34;&gt;Exponential function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#asymptotic-function&#34;&gt;Asymptotic function / Negative exponential function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#power-function&#34;&gt;Power curve function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logarithmic-function&#34;&gt;Logarithmic function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rectangular-hyperbola&#34;&gt;Rectangular hyperbola&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Sigmoidal curves
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-function&#34;&gt;Logistic function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gompertz-function&#34;&gt;Gompertz function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modified-gompertz-function&#34;&gt;Modified Gompertz function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#log-logistic-function&#34;&gt;Log-logistic function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weibull-function-type-1&#34;&gt;Weibull (type 1) function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weibull-function-type-2&#34;&gt;Weibull (type 2) function&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Curves with maxima/minima
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#bragg-function&#34;&gt;Bragg function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lorentz-function&#34;&gt;Lorentz function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beta-function&#34;&gt;Beta function&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s go through these functions and see how we can fit them both with ‘nls()’ and ‘drm()’, by using the appropriate self-starters. There are many functions and, therefore, the post is rather long… however, you can look at the graph below to spot the function you are interested in and use the link above to reach the relevant part in this web page.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-2-1.png&#34; alt=&#34;The shapes of the most important functions&#34; width=&#34;95%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The shapes of the most important functions
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomials&lt;/h1&gt;
&lt;p&gt;Polynomials are a group on their own, as they are characterized by very flexible shapes, which can be used to describe several different biological processes. They are simple and, although they may be curvilinear, they are always linear in the parameters and can be fitted by using least squares methods. One disadvantage is that they cannot describe asymptotic processes, which are very common in biology. Furthermore, polynomials are prone to overfitting, as we may be tempted to add terms to improve the fit, with little care for biological realism.&lt;/p&gt;
&lt;p&gt;Nowadays, thanks to the wide availability of nonlinear regression algorithms, the use of polynomials has sensibly decreased; linear or quadratic polynomials are mainly used when we want to approximate the observed response within a narrow range of a quantitative predictor. On the other hand, higher order polynomials are very rarely seen, in practice.&lt;/p&gt;
&lt;div id=&#34;straight-line-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Straight line function&lt;/h2&gt;
&lt;p&gt;Among the polynomials, we should cite the straight line. Obviously, this is not a curve, although it deserves to be mentioned here. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1 \, X \quad \quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is the slope, i.e. the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The Y increases as X increases when &lt;span class=&#34;math inline&#34;&gt;\(b_1 &amp;gt; 0\)&lt;/span&gt;, otherwise it decreases. Straight lines are the most common functions for regression and they are most often used to approximate biological phenomena within a small range for the predictor.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-polynomial-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic polynomial function&lt;/h2&gt;
&lt;p&gt;The quadratic polynomial is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1\, X + b_2 \, X^2 \quad \quad \quad (2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt;, taken separately, lack a clear biological meaning. However, it is interesting to consider the first derivative, which measures the rate at which the value &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; changes with respect to the change of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Calculating a derivative may be tricky for biologists; however, we can make use of the available facilities in R, represented by the D() function, which requires an expression as the argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a + b*X + c*X^2), &amp;quot;X&amp;quot;)
## b + c * (2 * X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the first derivative is not constant, but it changes according to the level of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The stationary point, i.e. the point at which the derivative is zero, is &lt;span class=&#34;math inline&#34;&gt;\(X_m = - b_1 / 2 b_2\)&lt;/span&gt;; this point is a maximum when &lt;span class=&#34;math inline&#34;&gt;\(b_2 &amp;gt; 0\)&lt;/span&gt;, otherwise it is a minimum.&lt;/p&gt;
&lt;p&gt;The maximum/minimum value is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_m = \frac{4\,b_0\,b_2 - b_1^2}{4\,b_2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-fitting-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polynomial fitting in R&lt;/h2&gt;
&lt;p&gt;In R, polynomials are fitted by using ‘lm()’. In a couple of cases, I found myself in the need of fitting a polynomial by using nonlinear regression with ‘nls()’ o ‘drm()’. I know, this is not efficient…, but some methods for ‘drc’ objects are rather handy. For these unusual cases, we can use the &lt;code&gt;NLS.linear()&lt;/code&gt;, &lt;code&gt;NLS.poly2()&lt;/code&gt;, &lt;code&gt;DRC.linear()&lt;/code&gt; and &lt;code&gt;DRC.poly2()&lt;/code&gt; self-starting functions, as available in the ‘aomisc’ package. An example of usage is given below: in this case, the polynomial has been used to describe a concave increasing trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- seq(5, 50, 5)
Y &amp;lt;- c(12.6, 74.1, 157.6, 225.5, 303.4, 462.8, 
       669.9, 805.3, 964.2, 1169)

# nls fit
model &amp;lt;- nls(Y ~ NLS.poly2(X, a, b, c))

#drc fit
model &amp;lt;- drm(Y ~ X, fct = DRC.poly2())
summary(model)
## 
## Model fitted: Second Order Polynomial (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value  p-value    
## a:(Intercept) -23.515000  31.175139 -0.7543  0.47528    
## b:(Intercept)   5.466470   2.604011  2.0993  0.07395 .  
## c:(Intercept)   0.371561   0.046141  8.0527 8.74e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  26.50605 (7 degrees of freedom)
plot(model, log = &amp;quot;&amp;quot;, main = &amp;quot;2nd order polynomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concaveconvex-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concave/Convex curves&lt;/h1&gt;
&lt;div id=&#34;exponential-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential function&lt;/h2&gt;
&lt;p&gt;The exponential function describes an increasing/decreasing trend, with constant relative rate. The most common equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  e^{k X} \quad \quad \quad (3) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Two additional and equivalent parameterisations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  b^X  =  e^{d + k X} \quad \quad \quad (4)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Equations 3 and the two equations 4 are equivalent, as proved by setting &lt;span class=&#34;math inline&#34;&gt;\(b = e^k\)&lt;/span&gt; e &lt;span class=&#34;math inline&#34;&gt;\(a = e^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  b^X  = a  (e^k)^{X} =  a  e^{kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  e^{kX} = e^d \cdot e^{kX} =  e^{d + kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The meaning of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is clear: this is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;. In order to understand the meaning of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, we can calculate the first derivative of the exponential function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * exp(k * X)), &amp;quot;X&amp;quot;)
## a * (exp(k * X) * k)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we see that the ratio of increase/decrease of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{dY}{dX} = k \, a \, e^{k \, X} = k \, Y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, the relative ratio of increase/decrease is constant and equal to &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{dY}{dX} \frac{1}{Y} = k\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases if &lt;span class=&#34;math inline&#34;&gt;\(k &amp;gt; 0\)&lt;/span&gt; (exponential growth), while it decreases when &lt;span class=&#34;math inline&#34;&gt;\(k &amp;lt; 0\)&lt;/span&gt; (exponential decay). This curve is used to describe the growth of populations in non-limiting environmental conditions, or to describe the degradation of xenobiotics in the environment (first-order degradation kinetic).&lt;/p&gt;
&lt;p&gt;Another slightly different parameterisation exists, which is common in bioassay work and it is mainly used as an exponential decay model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \exp(-x/e) \quad \quad \quad (5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the same as &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in the model above and &lt;span class=&#34;math inline&#34;&gt;\(e = - 1/k\)&lt;/span&gt;. For all the aforementioned exponential decay equations &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. In the above curve, a lower asymptote &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt; can also be included, for those situations where the phenomenon under study does not approach 0 when the independent variable approaches infinity:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d -c) \exp(-x/e) \quad \quad \quad (6)\]&lt;/span&gt;
The exponential function is nonlinear in &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and needs to be fitted by using ‘nls()’ or ‘drm()’. Several self-starting functions are available: in the package &lt;code&gt;aomisc&lt;/code&gt; you can find &lt;code&gt;NLS.expoGrowth()&lt;/code&gt;, &lt;code&gt;NLS.expoDecay()&lt;/code&gt;, &lt;code&gt;DRC.expoGrowth()&lt;/code&gt; and &lt;code&gt;DRC.expoDecay()&lt;/code&gt;, which can be used to fit the Equation 3, respectively with ‘nls()’ and ‘drm()’. The ‘drc’ package also contains the functions &lt;code&gt;EXD.2()&lt;/code&gt; and &lt;code&gt;EXD.3()&lt;/code&gt;, which can be used to fit, respectively, the Equations 5 and 6. Examples of usage are given below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(degradation)

# nls fit
model &amp;lt;- nls(Conc ~ NLS.expoDecay(Time, a, k),
             data = degradation)

# drm fit
model &amp;lt;- drm(Conc ~ Time, fct = DRC.expoDecay(),
             data = degradation)
summary(model)
## 
## Model fitted: Exponential Decay Model (2 parms)
## 
## Parameter estimates:
## 
##                    Estimate Std. Error t-value   p-value    
## init:(Intercept) 99.6349312  1.4646680  68.026 &amp;lt; 2.2e-16 ***
## k:(Intercept)     0.0670391  0.0019089  35.120 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.621386 (22 degrees of freedom)
plot(model, log = &amp;quot;&amp;quot;, main = &amp;quot;Exponential decay&amp;quot;, ylim = c(0, 110))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotic function&lt;/h2&gt;
&lt;p&gt;The asymptotic function can be used to model the growth of a population/individual, where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; approaches an horizontal asymptote as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; tends to infinity. This function is used in several different parameterisations and it is also known as the monomolecular growth function, the Mitscherlich law or the von Bertalanffy law. Due to its biological meaning, the most widespread parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a - (a - b) \, \exp (- c  X) \quad \quad \quad (7)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum attainable value for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (plateau), &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the initial &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; value (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is proportional to the relative rate of increase for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. Indeed, we can see that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a - (a - b) * exp (- c * X)), &amp;quot;X&amp;quot;)
## (a - b) * (exp(-c * X) * c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;that is, the absolute ratio of increase of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at a given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is not constant, but depends on the attained value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{dY}{dX} = c \, (a - Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the relative rate of increase of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{dY}{dX} \frac{1}{Y} = c \, \frac{(a - Y)}{Y}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It means that the relative rate of increase of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is maximum at the beginning and approaches 0 when &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; approaches the plateau &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In order to fit the asymptotic function, the ‘aomisc’ package contains the self-starting routines &lt;code&gt;NLS.asymReg()&lt;/code&gt; and &lt;code&gt;DRC.asymReg()&lt;/code&gt;, which can be used, respectively, with ‘nls()’ and ‘drm()’. The ‘drc’ package contains the function &lt;code&gt;AR.3()&lt;/code&gt;, that is a similar parameterisation where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named &lt;code&gt;SSasymp()&lt;/code&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(1, 3, 5, 7, 9, 11, 13, 20)
Y &amp;lt;- c(8.22, 14.0, 17.2, 16.9, 19.2, 19.6, 19.4, 19.6)

# nls fit
model &amp;lt;- nls(Y ~ NLS.asymReg(X, init, m, plateau) )

# drm fit
model &amp;lt;- drm(Y ~ X, fct = DRC.asymReg())
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Asymptotic regression&amp;quot;, 
     ylim = c(0,25), xlim = c(0,20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If we take the asymptotic function and set &lt;span class=&#34;math inline&#34;&gt;\(b = 0\)&lt;/span&gt;, we get the negative exponential function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a [1 -  \exp (- c  X) ] \quad \quad \quad (8)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function shows a similar shape as the asymptotic function, but &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is 0 when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is 0 (the curve passes through the origin). It is often used to model the absorbed Photosintetically Active Radiation (&lt;span class=&#34;math inline&#34;&gt;\(Y = PAR_a\)&lt;/span&gt;) as a function of Leaf Area Index (X = LAI). In this case, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the incident PAR (&lt;span class=&#34;math inline&#34;&gt;\(a = PAR_i\)&lt;/span&gt;), and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; represents the extinction coefficient.&lt;/p&gt;
&lt;p&gt;In order to fit the Equation 8, we can use the self-starters &lt;code&gt;NLS.negExp()&lt;/code&gt; with ‘nls()’ and &lt;code&gt;DRC.negExp()&lt;/code&gt; with ‘drm()’; both self-starters are available within the ‘aomisc’ package. The ‘drc’ package contains the function &lt;code&gt;AR.2()&lt;/code&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named &lt;code&gt;SSasympOrig()&lt;/code&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;power-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power function&lt;/h2&gt;
&lt;p&gt;The power function is also known as Freundlich function or allometric function and the most common parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a \, X^b \quad \quad \quad (9)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is perfectly equivalent to an exponential curve on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a\,X^b  = a\, e^{\log( X^b )}  = a\,e^{b \, \log(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve does not have an asymptote for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The slope (first derivative) of the curve is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * X^b), &amp;quot;X&amp;quot;)
## a * (X^(b - 1) * b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both parameters relate to the ‘slope’ of the curve and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates its shape. If &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; b &amp;lt; 1\)&lt;/span&gt;, the response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases and the curve is convex up. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. Otherwise, if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 1\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. The three curve types are shown in Figure 1, above.&lt;/p&gt;
&lt;p&gt;The power function (Freundlich equation) is often used in agricultural chemistry, e.g. to model the sorption of xenobiotics in soil. It is also used to model the number of plant species as a function of sampling area (Muller-Dumbois method). The following example uses the &lt;code&gt;DRC.powerCurve()&lt;/code&gt; and &lt;code&gt;NLS.powerCurve()&lt;/code&gt; self starters in the ‘aomisc’ package to fit a species-area curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(speciesArea)

#nls fit
model &amp;lt;- nls(numSpecies ~ NLS.powerCurve(Area, a, b),
             data = speciesArea)

# drm fit
model &amp;lt;- drm(numSpecies ~ Area, fct = DRC.powerCurve(),
             data = speciesArea)
summary(model)
## 
## Model fitted: Power curve (Freundlich equation) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 4.348404   0.337197  12.896 3.917e-06 ***
## b:(Intercept) 0.329770   0.016723  19.719 2.155e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.9588598 (7 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;logarithmic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logarithmic function&lt;/h2&gt;
&lt;p&gt;This is indeed a linear model on log-transformed &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = a + b \, \log(X) \quad \quad \quad (10)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to the logarithmic function, it must be &lt;span class=&#34;math inline&#34;&gt;\(X &amp;gt; 0\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates the shape: if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 0\)&lt;/span&gt;, the curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, as shown in Figure 1 above.&lt;/p&gt;
&lt;p&gt;The logarithmic equation can be fit by using ‘lm()’. If necessary, it can also be fit by using ‘nls()’ and ‘drm()’: the self-starting functions &lt;code&gt;NLS.logCurve()&lt;/code&gt; and &lt;code&gt;DRC.logCurve()&lt;/code&gt; are available within the ‘aomisc’ package. We show an example of their usage in the box below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(1,2,4,5,7,12)
Y &amp;lt;- c(1.97, 2.32, 2.67, 2.71, 2.86, 3.09)

# lm fit
model &amp;lt;- lm(Y ~ log(X) )

# nls fit
model &amp;lt;- nls(Y ~ NLS.logCurve(X, a, b) )

# drm fit
model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;rectangular-hyperbola&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rectangular hyperbola&lt;/h2&gt;
&lt;p&gt;This function is also known as the Michaelis-Menten function and it is often parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X} \quad \quad \quad (11)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, up to a plateau level. The parameter &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the higher asymptote (for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;), while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the X value giving a response equal to &lt;span class=&#34;math inline&#34;&gt;\(a/2\)&lt;/span&gt;. Indeed, it is easily shown that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a}{2} = \frac{a\,X_{50} } {b + X_{50} } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which leads to &lt;span class=&#34;math inline&#34;&gt;\(b = x_{50}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The slope (first derivative) is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression( (a*X) / (b + X) ), &amp;quot;X&amp;quot;)
## a/(b + X) - (a * X)/(b + X)^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we can see that the initial slope (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) is $i = a/b $. The equation 11 is not defined for &lt;span class=&#34;math inline&#34;&gt;\(X = b\)&lt;/span&gt; and it makes no biological sense for &lt;span class=&#34;math inline&#34;&gt;\(X &amp;lt; b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An alternative parameterisation is obtained considering that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X} = \frac{a}{ \frac{b}{X} + \frac{X}{X}} = \frac{a}{1 + \frac{b}{X}} \quad \quad \quad (12)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This parameterisation is sometimes used in bioassays and it is parameterised with &lt;span class=&#34;math inline&#34;&gt;\(d = a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = b\)&lt;/span&gt;. From a strict mathematical point of view, the equation 12 is not defined for &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, although it approaches 0 when &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Michaelis-Menten function is used in pesticide chemistry, enzyme kinetics and in weed competition studies, to model yield losses as a function of weed density. In R, this model can be fit by using ‘nls()’ and the self-starting functions &lt;code&gt;SSmicmen()&lt;/code&gt;, within the package ‘nlme’. If you prefer a ‘drm()’ fit, you can use the &lt;code&gt;MM.2()&lt;/code&gt; function in the ‘drc’ package, which uses the parameterisation in Equation 12.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(0, 5, 7, 22, 28, 39, 46, 200)
Y &amp;lt;- c(12.74, 13.66, 14.11, 14.43, 14.78, 14.86, 14.78, 14.91)

#drm fit
model &amp;lt;- drm(Y ~ X, fct = MM.2())
summary(model)
## 
## Model fitted: Michaelis-Menten (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value  p-value   
## d:(Intercept) 14.93974    2.86695  5.2110 0.001993 **
## e:(Intercept)  0.45439    2.24339  0.2025 0.846183   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  5.202137 (6 degrees of freedom)
plot(model, log = &amp;quot;&amp;quot;, main = &amp;quot;Michaelis-Menten function&amp;quot;, 
     ylim = c(12, 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sigmoidal-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sigmoidal curves&lt;/h1&gt;
&lt;p&gt;Sigmoidal curves are S-shaped and they may be increasing, decreasing, symmetric or non-symmetric around the inflection point. They are parameterised in countless ways, which may often be confusing. I will show some widespread parameterisations, that are very useful for bioassays or growth studies. All curves can be turned from increasing to decreasing (and vice-versa) by reversing the sign of the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter.&lt;/p&gt;
&lt;div id=&#34;logistic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic function&lt;/h2&gt;
&lt;p&gt;The logistic curve derives from the cumulative logistic distribution function; the curve is symmetric around the inflection point and it may be parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + exp(- b (X - e))} \quad \quad \quad (13)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the higher asymptote, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the lower asymptote, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value at the inflection point, while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at the inflection point. As the curve is symmetric, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; represents also the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value producing a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; (usually known as the ED50, in biological assay). The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be positive or negative and, consequently, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may increase or decrease as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The above function is known as the four-parameter logistic. If necessary, constraints can be put on parameter values, i.e. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; can be constrained to 0 (three-parameter logistic) and, additionally, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be constrained to 1 (two-parameter logistic).&lt;/p&gt;
&lt;p&gt;In the ‘aomisc’ package, the three logistic curves (four-, three- and two-parameters) are available as &lt;code&gt;NLS.L4()&lt;/code&gt;, &lt;code&gt;NLS.L3()&lt;/code&gt; and &lt;code&gt;NLS.L2()&lt;/code&gt;, respectively. With ‘drm()’, we can use the self-starting functions &lt;code&gt;L.4()&lt;/code&gt; and &lt;code&gt;L.3()&lt;/code&gt; in the package ‘drc’, while the &lt;code&gt;L.2()&lt;/code&gt; function for the two-parameter logistic has been included in the ‘aomisc’ package. The only difference between the self-starters for ‘drm()’ and the self-starters for ‘nls()’ is that, in the former, the sign for the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter is reversed, i.e. it is negative for increasing curves and positive for decreasing curves.&lt;/p&gt;
&lt;p&gt;The four- and three-parameter logistic curves can also be fit with ‘nls()’, respectively with the self-starting functions &lt;code&gt;SSfpl()&lt;/code&gt; and &lt;code&gt;SSlogis()&lt;/code&gt;, in the ‘nlme’ package. In these functions, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(scal = -1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the box below, I show an example, regarding the growth of a sugar-beet crop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(beetGrowth)

# nls fit
model &amp;lt;- nls(weightInf ~ NLS.L3(DAE, b, d, e), data = beetGrowth)
model.2 &amp;lt;- nls(weightInf ~ NLS.L4(DAE, b, c, d, e), data = beetGrowth)
model.3 &amp;lt;- nls(weightInf/max(weightInf) ~ NLS.L2(DAE, b, e), data = beetGrowth)

# drm fit
model &amp;lt;- drm(weightInf ~ DAE, fct = L.3(), data = beetGrowth)
model.2 &amp;lt;- drm(weightInf ~ DAE, fct = L.4(), data = beetGrowth)
model.3 &amp;lt;- drm(weightInf/max(weightInf) ~ DAE, fct = L.2(), data = beetGrowth)
summary(model)
## 
## Model fitted: Logistic (ED50 as parameter) with lower limit fixed at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.118771   0.018319 -6.4835 1.032e-05 ***
## d:(Intercept) 25.118357   1.279417 19.6327 4.127e-12 ***
## e:(Intercept) 58.029764   1.834414 31.6340 3.786e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.219389 (15 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Logistic function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;gompertz-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gompertz function&lt;/h2&gt;
&lt;p&gt;The Gompertz curve is parameterised in very many ways. I tend to prefer a parameterisation that resembles the one used for the logistic function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ - b \, (X - e) \right] \right\} \quad \quad \quad (14)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difference between the logistic and Gompertz functions is that this latter is not symmetric around the inflection point: it shows a longer lag at the beginning, but raises steadily afterwards. The parameters have the same meaning as those in the logistic function, apart from the fact that &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, i.e. the abscissa of the inflection point, does not give a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. As for the logistic, we can have four-, three- and two-parameter Gompertz functions, which can be used to describe plant growth or several other biological processes.&lt;/p&gt;
&lt;p&gt;In R, the Gompertz equation can be fit by using ‘drm()’ and, respectively the &lt;code&gt;G.4()&lt;/code&gt;, &lt;code&gt;G.3()&lt;/code&gt; and &lt;code&gt;G.2()&lt;/code&gt; self-starters. With ‘nls’, the ‘aomisc’ package contains the corresponding functions &lt;code&gt;NLS.G4()&lt;/code&gt;, &lt;code&gt;NLS.G3()&lt;/code&gt; and &lt;code&gt;NLS.G2()&lt;/code&gt;. As for the logistic, the only difference between the self starters for ‘drm()’ and the self starters for ‘nls()’ is that, in the former, the sign for the &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; parameter is reversed, i.e. it is negative for increasing curves and positive for decreasing curves.&lt;/p&gt;
&lt;p&gt;The three-parameter Gompertz can also be fit with ‘nls()’, by using the &lt;code&gt;SSGompertz()&lt;/code&gt; self-starter in the ‘nlme’ package, although this is a different parameterisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# nls fit
model &amp;lt;- nls(weightFree ~ NLS.G3(DAE, b, d, e), data = beetGrowth)
model.2 &amp;lt;- nls(weightFree ~ NLS.G4(DAE, b, c, d, e), data = beetGrowth)
model.3 &amp;lt;- nls(weightFree/max(weightFree) ~ NLS.G2(DAE, b, e), data = beetGrowth)

# drm fit
model &amp;lt;- drm(weightFree ~ DAE, fct = G.3(), data = beetGrowth)
model.2 &amp;lt;- drm(weightFree ~ DAE, fct = G.4(), data = beetGrowth)
model.3 &amp;lt;- drm(weightFree/max(weightFree) ~ DAE, fct = G.2(), data = beetGrowth)
summary(model)
## 
## Model fitted: Gompertz with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.122255   0.029938 -4.0836 0.0009783 ***
## d:(Intercept) 35.078529   1.668665 21.0219 1.531e-12 ***
## e:(Intercept) 49.008075   1.165191 42.0601 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.995873 (15 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Gompertz function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;modified-gompertz-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modified Gompertz function&lt;/h2&gt;
&lt;p&gt;We have seen that the logistic curve is symmetric around the inflection point, while the Gompertz shows a longer lag at the beginning and raises steadily afterwards. We can describe a different pattern by modifying the Gompertz function as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \right\} \quad \quad \quad (15)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The resulting curve increases steadily at the beginning, but slows down later on. Also for this function, we can put constraints on &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and/or &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;, so that we have four-parameter, three-parameter and two-parameter modified Gompertz equations; these models can be fit by using ‘nls()’ and the self starters &lt;code&gt;NLS.E4()&lt;/code&gt;, &lt;code&gt;NLS.E3()&lt;/code&gt; and &lt;code&gt;NLS.E2()&lt;/code&gt; in the ‘aomisc’ package. Likewise, the modified Gompertz equations can be fit with ‘drm()’ and the self starters &lt;code&gt;E.4()&lt;/code&gt;, &lt;code&gt;E.3()&lt;/code&gt; and &lt;code&gt;E.2()&lt;/code&gt;, also available in the ‘aomisc’ package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# nls fit
model &amp;lt;- nls(weightInf ~ NLS.E3(DAE, b, d, e), data = beetGrowth)
model.2 &amp;lt;- nls(weightFree ~ NLS.E4(DAE, b, c, d, e), data = beetGrowth)
model.3 &amp;lt;- nls(I(weightFree/max(weightFree)) ~ NLS.E2(DAE, b, e), data = beetGrowth)


# drm fit
model &amp;lt;- drm(weightInf ~ DAE, fct = E.3(), data = beetGrowth)
model.2 &amp;lt;- drm(weightFree ~ DAE, fct = E.4(), data = beetGrowth)
model.3 &amp;lt;- drm(weightFree/max(weightFree) ~ DAE, fct = E.2(), data = beetGrowth)
summary(model)
## 
## Model fitted: Modified Gompertz equation (3 parameters) (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept)  0.092508   0.013231  6.9917 4.340e-06 ***
## d:(Intercept) 25.107004   1.304379 19.2482 5.493e-12 ***
## e:(Intercept) 63.004111   1.747087 36.0624 4.945e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.253878 (15 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Modified Gompertz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;log-logistic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log-logistic function&lt;/h2&gt;
&lt;p&gt;The log-logistic curve is symmetric on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (a log-normal curve would be practically equivalent, but it is used far less often). For example, in biologic assays (but also in germination assays), the log-logistic curve is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \exp \left\{ - b \left[ \log(X) - \log(e) \right] \right\} } \quad \quad \quad (16)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters have the same meaning as the logistic equation given above. In particular, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; represents the X-level which gives a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (ED50). It is easy to see that the above equation is equivalent to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \left( \frac{X}{e} \right)^{-b}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Log-logistic functions are used for crop growth, seed germination and bioassay work and they can have the same constraints as the logistic function (four- three- and two-parameter log-logistic). They can be fit by ‘drm()’ and the self starters &lt;code&gt;LL.4()&lt;/code&gt; (four-parameter log-logistic), &lt;code&gt;LL.3()&lt;/code&gt; (three-parameter log-logistic, with &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;) and &lt;code&gt;LL.2()&lt;/code&gt; (two-parameter log-logistic, with &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;), as available in the ‘drc’ package. With respect to Equation 16, in these self-starters the sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is reversed, i.e. it is negative for the increasing log-logistic curve and positive for the decreasing curve. In the package ‘aomisc’, the corresponding self starters for ‘nls()’ are &lt;code&gt;NLS.LL4()&lt;/code&gt;, &lt;code&gt;NLS.LL3()&lt;/code&gt; and &lt;code&gt;NLS.LL2()&lt;/code&gt;, which are all derived from Equation 15 (i.e. the sign of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is positive for increasing curves and negative for decreasing curves).&lt;/p&gt;
&lt;p&gt;We show an example of a log-logistic fit, relating to a bioassay with &lt;em&gt;Brassica rapa&lt;/em&gt; treated at increasing dosages of an herbicide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)

model &amp;lt;- nls(FW ~ NLS.LL4(Dose, b, c, d, e), data = brassica)
model &amp;lt;- nls(FW ~ NLS.LL3(Dose, b, d, e), data = brassica)
model &amp;lt;- nls(FW/max(FW) ~ NLS.LL2(Dose, b, e), data = brassica)

model &amp;lt;- drm(FW ~ Dose, fct = LL.4(), data = brassica)
summary(model)
## 
## Model fitted: Log-logistic (ED50 as parameter) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.45113    0.24113  6.0181 1.743e-06 ***
## c:(Intercept)  0.34948    0.18580  1.8810   0.07041 .  
## d:(Intercept)  4.53636    0.20514 22.1140 &amp;lt; 2.2e-16 ***
## e:(Intercept)  2.46557    0.35111  7.0221 1.228e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4067837 (28 degrees of freedom)
plot(model, ylim = c(0,6), main = &amp;quot;Log-logistic function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-function-type-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull function (type 1)&lt;/h2&gt;
&lt;p&gt;The type 1 Weibull function corresponds to the Gompertz, but it is based on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ - b \, (\log(X) - \log(e)) \right] \right\} \quad \quad \quad (17)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameters have the same meaning as the other sigmoidal curves given above, apart from the fact that &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the abscissa of the inflection point, but it is not the ED50.&lt;/p&gt;
&lt;p&gt;The four-parameters, three-parameters and two-parameters Weibull functions can be fit by using ‘drm()’ and the self-starters available within the ‘drc’ package, i.e. &lt;code&gt;W1.4()&lt;/code&gt;, &lt;code&gt;W1.3()&lt;/code&gt; and &lt;code&gt;W1.2()&lt;/code&gt;. The ‘aomisc’ package contains the corresponding self-starters &lt;code&gt;NLS.W1.4()&lt;/code&gt;, &lt;code&gt;NLS.W1.3()&lt;/code&gt; and &lt;code&gt;NLS.W1.2()&lt;/code&gt;, which can be used with ‘nls()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)

model &amp;lt;- nls(FW ~ NLS.W1.4(Dose, b, c, d, e), data = brassica)
model.2 &amp;lt;- nls(FW ~ NLS.W1.3(Dose, b, d, e), data = brassica)
model.3 &amp;lt;- nls(FW/max(FW) ~ NLS.W1.2(Dose, b, e), data = brassica)

model &amp;lt;- drm(FW ~ Dose, fct = W1.4(), data = brassica)
model.2 &amp;lt;- drm(FW ~ Dose, fct = W1.3(), data = brassica)
model.3 &amp;lt;- drm(FW/max(FW) ~ Dose, fct = W1.2(), data = brassica)
summary(model)
## 
## Model fitted: Weibull (type 1) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.01252    0.15080  6.7143 2.740e-07 ***
## c:(Intercept)  0.50418    0.14199  3.5507  0.001381 ** 
## d:(Intercept)  4.56137    0.19846 22.9841 &amp;lt; 2.2e-16 ***
## e:(Intercept)  3.55327    0.45039  7.8894 1.359e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.3986775 (28 degrees of freedom)
plot(model, ylim = c(0,6), main = &amp;quot;Weibull type 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-function-type-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull function (type 2)&lt;/h2&gt;
&lt;p&gt;The type 2 Weibull is similar to the type 1 Weibull, but describes a different type of asymmetry, corresponding to the modified Gompertz function above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (\log(X) - \log(e)) \right] \right\} \right\} \quad \quad \quad (18)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As for fitting, the ‘drc’ package contains the self-starting functions &lt;code&gt;W2.4()&lt;/code&gt;, &lt;code&gt;W2.3()&lt;/code&gt; and &lt;code&gt;W2.2()&lt;/code&gt;, which can be used with ‘drm()’ (be careful: the sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is reversed, with respect to Equation 18). The ‘aomisc’ package contains the corresponding self-starters &lt;code&gt;NLS.W2.4()&lt;/code&gt;, &lt;code&gt;NLS.W2.3()&lt;/code&gt; and &lt;code&gt;NLS.W2.2()&lt;/code&gt;, which can be used with ‘nls()’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)

model &amp;lt;- nls(FW ~ NLS.W2.4(Dose, b, c, d, e), data = brassica)
model.1 &amp;lt;- nls(FW ~ NLS.W2.3(Dose, b, d, e), data = brassica)
model.2 &amp;lt;- nls(FW/max(FW) ~ NLS.W2.2(Dose, b, e), data = brassica)

model &amp;lt;- drm(FW ~ Dose, fct = W2.4(), data = brassica)
summary(model)
## 
## Model fitted: Weibull (type 2) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.96191    0.17684 -5.4395 8.353e-06 ***
## c:(Intercept)  0.18068    0.25191  0.7173    0.4792    
## d:(Intercept)  4.53804    0.21576 21.0328 &amp;lt; 2.2e-16 ***
## e:(Intercept)  1.66342    0.25240  6.5906 3.793e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4215551 (28 degrees of freedom)
plot(model, ylim = c(0,6), main = &amp;quot;Weibull type 2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;curves-with-maximaminima&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Curves with maxima/minima&lt;/h1&gt;
&lt;p&gt;It is sometimes necessary to describe phenomena where the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; variable reaches a maximum value at a certain level of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; variable, and drops afterwords. For example, growth or germination rates are higher at optimal temperature levels and lower at supra-optimal or sub-optimal temperature levels. Another example relates to bioassays: in some cases, low doses of toxic substances induce a stimulation of growth (hormesis), which needs to be described by an appropriate model. Let’s see some functions which may turn out useful in these circumstances.&lt;/p&gt;
&lt;div id=&#34;bragg-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bragg function&lt;/h2&gt;
&lt;p&gt;This function is connected to the normal (Gaussian) distribution and has a symmetric shape with a maximum equal to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, that is reached when &lt;span class=&#34;math inline&#34;&gt;\(X = e\)&lt;/span&gt; and two inflection points. In this model, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; relates to the slope at the inflection points; the response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; approaches 0 when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; approaches &lt;span class=&#34;math inline&#34;&gt;\(\pm \infty\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \, \exp \left[ - b (X - e)^2 \right] \quad \quad \quad (19)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we would like to have lower asymptotes different from 0, we should add the parameter &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \, \exp \left[ - b (X - e)^2 \right] \quad \quad \quad (20)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two Bragg curves have been used in applications relating to the science of carbon materials. We can fit them with ‘drm()’, by using the self starters &lt;code&gt;DRC.bragg.3()&lt;/code&gt; (Equation 19) and &lt;code&gt;DRC.bragg.4&lt;/code&gt; (Equation 20), in the ‘aomisc’ package. With ‘nls()’, you can use the corresponding self-starters &lt;code&gt;NLS.bragg.3()&lt;/code&gt; and &lt;code&gt;NLS.bragg.4&lt;/code&gt;, also in the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
Y1 &amp;lt;- c(0.1, 2, 5.7, 9.3, 19.7, 28.4, 20.3, 6.6, 1.3, 0.1)
Y2 &amp;lt;- Y1 + 2

# nls fit
mod.nls &amp;lt;- nls(Y1 ~ NLS.bragg.3(X, b, d, e) )
mod.nls2 &amp;lt;- nls(Y2 ~ NLS.bragg.4(X, b, c, d, e) )

# drm fit
mod.drc &amp;lt;- drm(Y1 ~ X, fct = DRC.bragg.3() )
mod.drc2 &amp;lt;- drm(Y2 ~ X, fct = DRC.bragg.4() )
plot(mod.drc, ylim = c(0, 30), log = &amp;quot;&amp;quot;) 
plot(mod.drc2, add = T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lorentz-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lorentz function&lt;/h2&gt;
&lt;p&gt;The Lorentz function is similar to the Bragg function, although it has worse statistical properties (Ratkowsky, 1990). The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = \frac{d} { 1 + b (X - e)^2 } \quad \quad \quad (21)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also allow for lower asymptotes different from 0, by adding a further parameter:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + \frac{d - c} { 1 + b (X - e)^2 } \quad \quad \quad (22)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two Lorentz functions can be fit with ‘drm()’, by using the self-starters &lt;code&gt;DRC.lorentz.3()&lt;/code&gt; (Equation 21) and &lt;code&gt;DRC.lorentz.4&lt;/code&gt; (Equation 22), in the ‘aomisc’ package. With ‘nls()’, you can use the corresponding self-starters &lt;code&gt;NLS.lorentz.3()&lt;/code&gt; and &lt;code&gt;NLS.lorentz.4&lt;/code&gt;, also in the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
Y1 &amp;lt;- c(0.1, 2, 5.7, 9.3, 19.7, 28.4, 20.3, 6.6, 1.3, 0.1)
Y2 &amp;lt;- Y1 + 2

# nls fit
mod.nls &amp;lt;- nls(Y1 ~ NLS.lorentz.3(X, b, d, e) )
mod.nls2 &amp;lt;- nls(Y2 ~ NLS.lorentz.4(X, b, c, d, e) )

# drm fit
mod.drc &amp;lt;- drm(Y1 ~ X, fct = DRC.lorentz.3() )
mod.drc2 &amp;lt;- drm(Y2 ~ X, fct = DRC.lorentz.4() )
plot(mod.drc, ylim = c(0, 30), log = &amp;quot;&amp;quot;) 
plot(mod.drc2, add = T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beta-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beta function&lt;/h2&gt;
&lt;p&gt;The beta function derives from the beta density function and it has been adapted to describe phenomena taking place only within a minimum and a maximum threshold value (threshold model). One typical example is seed germination, where the germination rate (GR, i.e. the inverse of germination time) is 0 below the base temperature level and above the cutoff temperature level. Between these two extremes, the GR increases with temperature up to a maximum level, that is reached at the optimal temperature level.&lt;/p&gt;
&lt;p&gt;The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \,\left\{  \left( \frac{X - X_b}{X_o - X_b} \right) \left( \frac{X_c - X}{X_c - X_o} \right) ^ {\frac{X_c - X_o}{X_o - X_b}} \right\}^b \quad \quad \quad (23)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum level for the expected response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_c\)&lt;/span&gt; are, respectively, the minumum and maximum threshold levels, &lt;span class=&#34;math inline&#34;&gt;\(X_o\)&lt;/span&gt; is the abscissa at the maximum expected response level and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is a shape parameter. The above function is only defined for &lt;span class=&#34;math inline&#34;&gt;\(X_b &amp;lt; X &amp;lt; X_c\)&lt;/span&gt; and it returns 0 elsewhere.&lt;/p&gt;
&lt;p&gt;In R, the beta function can be fitted either with ‘drm()’ and the self-starter &lt;code&gt;DRC.beta()&lt;/code&gt;, or with ‘nls()’ and the self-starter &lt;code&gt;NLS.beta()&lt;/code&gt;. Both the self-starters are available within the ‘aomisc’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
Y &amp;lt;- c(0, 0, 0, 7.7, 12.3, 19.7, 22.4, 20.3, 6.6, 0, 0)

model &amp;lt;- nls(Y ~ NLS.beta(X, b, d, Xb, Xo, Xc))
model &amp;lt;- drm(Y ~ X, fct = DRC.beta())
summary(model)
## 
## Model fitted: Beta function
## 
## Parameter estimates:
## 
##                Estimate Std. Error  t-value   p-value    
## b:(Intercept)   1.29834    0.26171   4.9609 0.0025498 ** 
## d:(Intercept)  22.30117    0.53645  41.5715 1.296e-08 ***
## Xb:(Intercept)  9.41770    1.15826   8.1309 0.0001859 ***
## Xo:(Intercept) 31.14068    0.58044  53.6504 2.815e-09 ***
## Xc:(Intercept) 40.47294    0.33000 122.6455 1.981e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.7251948 (6 degrees of freedom)
plot(model, log=&amp;quot;&amp;quot;, main = &amp;quot;Beta function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_usefulFunctions_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are; I have discussed more than 20 models, which are commonly used for nonlinear regression. These models can be found in several different parameterisations and flavours; they can also be modified and combined to suit the needs of several disciplines in biology, chemistry and so on. However, this will require another post.&lt;/p&gt;
&lt;p&gt;Thanks for reading&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-readings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further readings&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Miguez, F., Archontoulis, S., Dokoohaki, H., Glaz, B., Yeater, K.M., 2018. Chapter 15: Nonlinear Regression Models and Applications, in: ACSESS Publications. American Society of Agronomy, Crop Science Society of America, and Soil Science Society of America, Inc.&lt;/li&gt;
&lt;li&gt;Ratkowsky, D.A., 1990. Handbook of nonlinear regression models. Marcel Dekker Inc., New York, USA.&lt;/li&gt;
&lt;li&gt;Ritz, C., Jensen, S. M., Gerhard, D., Streibig, J. C.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2019&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dose-Response Analysis Using R. CRC Press&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-starting routines for nonlinear regression models</title>
      <link>/2020/stat_nls_selfstarting/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_nls_selfstarting/</guid>
      <description>


&lt;p&gt;In R, the &lt;code&gt;drc&lt;/code&gt; package represents one of the main solutions for nonlinear regression and dose-response analyses (Ritz et al., 2015). It comes with a lot of nonlinear models, which are useful to describe several biological processes, from plant growth to bioassays, from herbicide degradation to seed germination. These models are provided with self-starting functions, which free the user from the hassle of providing initial guesses for model parameters. Indeed, getting these guesses may be a tricky task, both for students and for practitioners.&lt;/p&gt;
&lt;p&gt;Obviously, we should not expect that all possible models and parameterisations are included in the ‘drc’ package; therefore, sooner or later, we may all find ourselves in the need of building a user-defined function, for some peculiar tasks of nonlinear regression analysis. I found myself in that position several times in the past and it took me awhile to figure out a solution.&lt;/p&gt;
&lt;p&gt;In this post, I’ll describe how we can simply build self-starters for our nonlinear regression analyses, to be used in connection with the ‘drm()’ function in the ‘drc’ package. In the end, I will also extend the approach to work with the ‘nls()’ function in the ‘stats’ package.&lt;/p&gt;
&lt;p&gt;Let’s consider the following dataset, depicting the relationship between temperature and growth rate. We may note that the response reaches a maximum value around 30°C, while it is lower below and above such an optimal value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
Temp &amp;lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)
RGR &amp;lt;- c(0.1, 2, 5.7, 9.3, 19.7, 28.4, 20.3, 6.6, 1.3, 0.1)
plot(RGR ~ Temp, xlim = c(5, 50), 
     xlab = &amp;quot;Temperature&amp;quot;, ylab = &amp;quot;Growth rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_selfStarting_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Bragg equation can be a good candidate model for such a situation. It is characterized by a bell-like shape:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \, \exp \left[- b \, (X - e)^2 \right] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the observed growth rate, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the temperature, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum level for the expected response, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the abscissa at which such maximum occurs and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection points (the curve is bell-shaped and shows two inflection points around the maximum value). Unfortunately, such an equation is not already available within the &lt;code&gt;drc&lt;/code&gt; package. What should we do, then?&lt;/p&gt;
&lt;p&gt;First of all, let’s write this function in the usual R code. In my opinion, this is more convenient than writing it directly within the &lt;code&gt;drc&lt;/code&gt; framework; indeed, the usual R coding is not specific to any packages and can be used with all other nonlinear regression and plotting facilities, such as &lt;code&gt;nls()&lt;/code&gt;, or &lt;code&gt;nlme()&lt;/code&gt;. Let’s call this new function &lt;code&gt;bragg.3.fun()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Definition of Bragg function
bragg.3.fun &amp;lt;- function(X, b, d, e){
  d * exp(- b * (X - e)^2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to transport ‘bragg.3.fun()’ into the &lt;code&gt;drc&lt;/code&gt; platform, we need to code a function returning a list of (at least) three components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a response function (fct)&lt;/li&gt;
&lt;li&gt;a self-starting routine (ssfct)&lt;/li&gt;
&lt;li&gt;a vector with the names of parameters (names)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Optionally, we can also include a descriptive text, the derivatives and other useful information. This is the skeleton code, which I use as the template.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MyNewDRCfun &amp;lt;- function(){

  fct &amp;lt;- function(x, parm) {
      # function code here
  }
  ssfct &amp;lt;- function(data){
     # Self-starting code here
  }
  names &amp;lt;- c()
  text &amp;lt;- &amp;quot;Descriptive text&amp;quot;
    
  ## Returning the function with self starter and names
  returnList &amp;lt;- list(fct = fct, ssfct = ssfct, names = names, text = text)
  class(returnList) &amp;lt;- &amp;quot;drcMean&amp;quot;
  invisible(returnList)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two functions &lt;code&gt;fct()&lt;/code&gt; and &lt;code&gt;ssfct()&lt;/code&gt; are called internally by the &lt;code&gt;drm()&lt;/code&gt; function and, therefore, the list of arguments must be defined exactly as shown above. In detail, &lt;code&gt;fct()&lt;/code&gt; receives two arguments as inputs: the predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the dataframe of parameters, with one row and as many columns as there are parameters in the model. The predictor and parameters are used to return the vector of responses; in the code below, I am calling the function &lt;code&gt;bragg.3.fun()&lt;/code&gt; from within the function &lt;code&gt;fct()&lt;/code&gt;. Alternatively, the Bragg function can be directly coded within &lt;code&gt;fct()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fct &amp;lt;- function(x, parm) {
  bragg.3.fun(x, parm[,1], parm[,2], parm[,3])
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;ssfct()&lt;/code&gt; receives one argument as input, that is a dataframe with the predictor in the first column and the observed response in the second. These two variables can be used to calculate the starting values for model parameters. In order to get a starting value for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, we could take the maximum value for the observed response, by using the function &lt;code&gt;max()&lt;/code&gt;. Likewise, to get a starting value for &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, we could take the positioning of the maximum value in the observed response and use it to index the predictor. Once we have obtained a starting value for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, we can note that, from the Bragg equation, with simple math, we can derive the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \log \left( \frac{Y}{d} \right) = - b \left( X - e\right)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if we transform the observed response and the predictor as above, we can use polynomial regression to estimate a starting value for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. In the end, this self starting routine can be coded as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ssftc &amp;lt;- function(data){
  # Get the data     
  x &amp;lt;- data[, 1]
  y &amp;lt;- data[, 2]
  
  d &amp;lt;- max(y)
  e &amp;lt;- x[which.max(y)]
  
  ## Linear regression on pseudo-y and pseudo-x
  pseudoY &amp;lt;- log( y / d )
  pseudoX &amp;lt;- (x - e)^2
  coefs &amp;lt;- coef( lm(pseudoY ~ pseudoX - 1) )
  b &amp;lt;- coefs[1]
  return( c(b, d, e) )
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It may be worth to state that the self-starting function may be simply skipped by specifying starting values for model parameters, right inside &lt;code&gt;ssfct()&lt;/code&gt; (see Kniss et al., 2011).&lt;/p&gt;
&lt;p&gt;Now, let’s ‘encapsulate’ all components within the skeleton function given above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DRC.bragg.3 &amp;lt;- function(){
  fct &amp;lt;- function(x, parm) {
    bragg.3.fun(x, parm[,1], parm[,2], parm[,3])
  }
  ssfct &amp;lt;- function(data){
    # Get the data     
    x &amp;lt;- data[, 1]
    y &amp;lt;- data[, 2]
    
    d &amp;lt;- max(y)
    e &amp;lt;- x[which.max(y)]
    
    ## Linear regression on pseudo-y and pseudo-x
    pseudoY &amp;lt;- log( y / d )
    pseudoX &amp;lt;- (x - e)^2
    coefs &amp;lt;- coef( lm(pseudoY ~ pseudoX - 1) )
    b &amp;lt;- - coefs[1]
    start &amp;lt;- c(b, d, e)
    return( start )
  }
  names &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;)
  text &amp;lt;- &amp;quot;Bragg equation&amp;quot;
    
  ## Returning the function with self starter and names
  returnList &amp;lt;- list(fct = fct, ssfct = ssfct, names = names, text = text)
  class(returnList) &amp;lt;- &amp;quot;drcMean&amp;quot;
  invisible(returnList)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the &lt;code&gt;DRC.bragg.3()&lt;/code&gt; function is ready, it can be used as the value for the argument &lt;code&gt;fct&lt;/code&gt; in the &lt;code&gt;drm()&lt;/code&gt; function call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- drm(RGR ~ Temp, fct = DRC.bragg.3())
summary(mod)
## 
## Model fitted: Bragg equation
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## b:(Intercept)  0.0115272  0.0014506  7.9466 9.513e-05 ***
## d:(Intercept) 27.4122086  1.4192874 19.3141 2.486e-07 ***
## e:(Intercept) 29.6392304  0.3872418 76.5393 1.710e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  1.71838 (7 degrees of freedom)
plot(mod, log = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_selfStarting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;and-what-about-nls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And… what about nls()?&lt;/h1&gt;
&lt;p&gt;Yes, I know, some of you may prefer using the function &lt;code&gt;nls()&lt;/code&gt;, within the &lt;code&gt;stats&lt;/code&gt; package. In that platform, we can directly use &lt;code&gt;bragg.3.fun()&lt;/code&gt; as the response model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.nls &amp;lt;- nls(RGR ~ bragg.3.fun(Temp, b, d, e),
               start = list (b = 0.01, d = 27, e = 30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we are forced to provide starting values for all estimands, which might be a tricky task, unless we build a self-starting routine, as we did before for the &lt;code&gt;drc&lt;/code&gt; platform. This is not an impossible task and we can also re-use part of the code we have already written above. We have to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;build a self-starting function by using the appropriate coding (see below). In this step we should be careful to the command &lt;code&gt;sortedXyData(mCall[[&amp;quot;X&amp;quot;]], LHS, data)&lt;/code&gt;. The part in quotation marks (“X”) should correspond to the name of the predictor in the &lt;code&gt;bragg.3.fun()&lt;/code&gt; function definition.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;selfStart()&lt;/code&gt; function to combine the function with its self-starting routine.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bragg.3.init &amp;lt;- function(mCall, LHS, data) {
    xy &amp;lt;- sortedXyData(mCall[[&amp;quot;X&amp;quot;]], LHS, data)
    x &amp;lt;-  xy[, &amp;quot;x&amp;quot;]; y &amp;lt;- xy[, &amp;quot;y&amp;quot;]
    
    d &amp;lt;- max(y)
    e &amp;lt;- x[which.max(y)]

    ## Linear regression on pseudo-y and pseudo-x
    pseudoY &amp;lt;- log( y / d )
    pseudoX &amp;lt;- (x - e)^2
    coefs &amp;lt;- coef( lm(pseudoY ~ pseudoX - 1) )
    b &amp;lt;- - coefs[1]
    start &amp;lt;- c(b, d, e)
    names(start) &amp;lt;- mCall[c(&amp;quot;b&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;)]
    start
}

NLS.bragg.3 &amp;lt;- selfStart(bragg.3.fun, bragg.3.init, parameters=c(&amp;quot;b&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can use the &lt;code&gt;NLS.bragg.3()&lt;/code&gt; function in the &lt;code&gt;nls()&lt;/code&gt; call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.nls &amp;lt;- nls(RGR ~ NLS.bragg.3(Temp, b, d, e) )
summary(mod.nls)
## 
## Formula: RGR ~ NLS.bragg.3(Temp, b, d, e)
## 
## Parameters:
##    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## b  0.011527   0.001338   8.618 5.65e-05 ***
## d 27.411715   1.377361  19.902 2.02e-07 ***
## e 29.638976   0.382131  77.562 1.56e-11 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.718 on 7 degrees of freedom
## 
## Number of iterations to convergence: 8 
## Achieved convergence tolerance: 5.203e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have been building a lot of self-starters, both for &lt;code&gt;drm()&lt;/code&gt; and for &lt;code&gt;nls()&lt;/code&gt; and I have shared them within my &lt;code&gt;aomisc&lt;/code&gt; package. Therefore, should you need to fit some unusual nonlinear regression model, it may be worth to take a look at that package, to see whether you find something suitable.&lt;/p&gt;
&lt;p&gt;That’s it, thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Kniss, A.R., Vassios, J.D., Nissen, S.J., Ritz, C., 2011. Nonlinear Regression Analysis of Herbicide Absorption Studies. Weed Science 59, 601–610. &lt;a href=&#34;https://doi.org/10.1614/WS-D-11-00034.1&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1614/WS-D-11-00034.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J. C., Gerhard, D. (2015) Dose-Response Analysis Using R PLOS ONE, 10(12), e0146021&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some everyday data tasks: a few hints with R (revisited)</title>
      <link>/2020/stat_r_shapingdata2/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_r_shapingdata2/</guid>
      <description>


&lt;p&gt;One year ago, I published a post titled ‘Some everyday data tasks: a few hints with R’. In that post, I considered four data tasks, that we all need to accomplish daily, i.e.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;subsetting&lt;/li&gt;
&lt;li&gt;sorting&lt;/li&gt;
&lt;li&gt;casting&lt;/li&gt;
&lt;li&gt;melting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In that post, I used the methods I was more familiar with. And, as a long-time R user, I have mainly incorporated in my workflow all the functions from the base R implementation.&lt;/p&gt;
&lt;p&gt;But now, the tidyverse is with us! Well, as far as I know, the tidyverse has been around long before my post. However, for a long time, I did not want to surrender to such a new paradygm. I am no longer a young scientist and, therefore, picking up new techniques is becoming more difficult: why should I abandon my effective workflow in favour of new techniques, which I am not familiar with? Yes I know, the young scientists are thinking that I am just an old dinosaur, who is trying to resist to progress by all means… It is a good point! I see that reading the code produced by my younger collegues is becoming difficult, due to the massive use of the tidyverse and the pipes. I still have a few years to go, before retirement and I do not yet fell like being set aside. Therefore, a few weeks ago I finally surrendered and ‘embraced’ the tidyverse. Here is how I revisited my previous post.&lt;/p&gt;
&lt;div id=&#34;subsetting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subsetting the data&lt;/h1&gt;
&lt;p&gt;Subsetting means selecting the records (rows) or the variables (columns) which satisfy certain criteria. Let’s take the ‘students.csv’ dataset, which is available on one of my repositories. It is a database of student’s marks in a series of exams for different subjects and, obviously, I will use the ‘readr’ package to read it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(dplyr)
library(tidyr)
students &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/students.csv&amp;quot;)
students
## # A tibble: 232 x 6
##       Id Subject  Date        Mark  Year HighSchool 
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      
##  1     1 AGRONOMY 10/06/2002    30  2001 HUMANITIES 
##  2     2 AGRONOMY 08/07/2002    24  2001 AGRICULTURE
##  3     3 AGRONOMY 24/06/2002    30  2001 AGRICULTURE
##  4     4 AGRONOMY 24/06/2002    26  2001 HUMANITIES 
##  5     5 AGRONOMY 23/01/2003    30  2001 HUMANITIES 
##  6     6 AGRONOMY 09/09/2002    28  2001 AGRICULTURE
##  7     7 AGRONOMY 24/02/2003    26  2001 HUMANITIES 
##  8     8 AGRONOMY 09/09/2002    26  2001 SCIENTIFIC 
##  9     9 AGRONOMY 09/09/2002    23  2001 ACCOUNTING 
## 10    10 AGRONOMY 08/07/2002    27  2001 HUMANITIES 
## # … with 222 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the usual &lt;code&gt;read.csv&lt;/code&gt; function I saved some typing, as I did not need to specify the ‘header = T’ argument. Furthermore, printing the tibble only shows the first ten rows, which makes the ‘head()’ function no longer needed.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and try to subset this tibble: we want to extract the good students, with marks higher than 28. In my previous post, I used the ‘subset()’ function. Now, I will use the ‘filter()’ function in the ‘dplyr’ package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subData &amp;lt;- subset(students, Mark &amp;gt;= 28)
subData &amp;lt;- filter(students, Mark &amp;gt;= 28)
subData
## # A tibble: 87 x 6
##       Id Subject  Date        Mark  Year HighSchool  
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1     1 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  2     3 AGRONOMY 24/06/2002    30  2001 AGRICULTURE 
##  3     5 AGRONOMY 23/01/2003    30  2001 HUMANITIES  
##  4     6 AGRONOMY 09/09/2002    28  2001 AGRICULTURE 
##  5    11 AGRONOMY 09/09/2002    28  2001 SCIENTIFIC  
##  6    17 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  7    18 AGRONOMY 10/06/2002    30  2001 AGRICULTURE 
##  8    19 AGRONOMY 09/09/2002    30  2001 AGRICULTURE 
##  9    20 AGRONOMY 09/09/2002    30  2001 OTHER SCHOOL
## 10    22 AGRONOMY 23/01/2003    30  2001 ACCOUNTING  
## # … with 77 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have noted that all other subsetting examples in my previous post can be solved by simply replacing ‘subset()’ with ‘filter()’, with no other changes. However, differences appear when I try to select the columns. Indeed, ‘dplyr’ has a specific function ‘select()’, which should be used for this purpose. Therefore, in the case that I want to select the students with marks ranging from 26 to 28 in Maths or Chemistry and, at the same time, I want to report only the three columns ‘Subject’, ‘Mark’ and ‘Date’, I need to split the process in two steps (filter and, then, select):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
#                     Subject == &amp;quot;MATHS&amp;quot; | 
#                     Subject == &amp;quot;CHEMISTRY&amp;quot;,
#                   select = c(Subject, Mark, HighSchool))
subData1 &amp;lt;- filter(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;)
subData &amp;lt;- select(subData1, c(Subject, Mark, HighSchool))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the above two-steps process I could easily understand the meaning of the pipe operator: it simply replaces the word ‘then’ between the two steps (&lt;code&gt;filter&lt;/code&gt; and then &lt;code&gt;select&lt;/code&gt; is translated into &lt;code&gt;filter %&amp;gt;% select&lt;/code&gt;). Here is the resulting code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students %&amp;gt;%
  filter(Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;) %&amp;gt;%
  select(c(Subject, Mark, HighSchool))
subData
## # A tibble: 50 x 3
##    Subject    Mark HighSchool  
##    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1 CHEMISTRY    20 AGRICULTURE 
##  2 CHEMISTRY    21 HUMANITIES  
##  3 CHEMISTRY    21 HUMANITIES  
##  4 CHEMISTRY    18 AGRICULTURE 
##  5 CHEMISTRY    28 OTHER SCHOOL
##  6 CHEMISTRY    23 ACCOUNTING  
##  7 CHEMISTRY    26 ACCOUNTING  
##  8 CHEMISTRY    27 AGRICULTURE 
##  9 CHEMISTRY    27 SCIENTIFIC  
## 10 CHEMISTRY    23 ACCOUNTING  
## # … with 40 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the end: there is not much difference between ‘subset()’ and ‘filter()’. However, I must admit I am seduced by the ‘pipe’ operator… my younger collegues may be right: it should be possible to chain several useful data management steps, producing highly readable code. But… how about debugging?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sorting the data&lt;/h1&gt;
&lt;p&gt;In my previous post I showed how to sort a data frame by using the ‘order()’ function. Now, I can use the ‘arrange()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sortedData &amp;lt;- students[order(-students$Mark, students$Subject), ]
# head(sortedData)
sortedData &amp;lt;- arrange(students, desc(Mark), Subject)
sortedData
## # A tibble: 232 x 6
##       Id Subject  Date        Mark  Year HighSchool  
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1     1 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  2     3 AGRONOMY 24/06/2002    30  2001 AGRICULTURE 
##  3     5 AGRONOMY 23/01/2003    30  2001 HUMANITIES  
##  4    17 AGRONOMY 10/06/2002    30  2001 HUMANITIES  
##  5    18 AGRONOMY 10/06/2002    30  2001 AGRICULTURE 
##  6    19 AGRONOMY 09/09/2002    30  2001 AGRICULTURE 
##  7    20 AGRONOMY 09/09/2002    30  2001 OTHER SCHOOL
##  8    22 AGRONOMY 23/01/2003    30  2001 ACCOUNTING  
##  9    38 BIOLOGY  28/02/2002    30  2001 AGRICULTURE 
## 10    42 BIOLOGY  28/02/2002    30  2001 ACCOUNTING  
## # … with 222 more rows
# sortedData &amp;lt;- students[order(-students$Mark, -xtfrm(students$Subject)), ]
# head(sortedData)
sortedData &amp;lt;- arrange(students, desc(Mark), desc(Subject))
sortedData
## # A tibble: 232 x 6
##       Id Subject Date        Mark  Year HighSchool  
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       
##  1   116 MATHS   01/07/2002    30  2001 OTHER SCHOOL
##  2   117 MATHS   18/06/2002    30  2001 ACCOUNTING  
##  3   118 MATHS   09/07/2002    30  2001 AGRICULTURE 
##  4   121 MATHS   18/06/2002    30  2001 ACCOUNTING  
##  5   123 MATHS   09/07/2002    30  2001 HUMANITIES  
##  6   130 MATHS   07/02/2002    30  2001 SCIENTIFIC  
##  7   131 MATHS   09/07/2002    30  2001 AGRICULTURE 
##  8   134 MATHS   26/02/2002    30  2001 AGRICULTURE 
##  9   135 MATHS   11/02/2002    30  2001 AGRICULTURE 
## 10   143 MATHS   04/02/2002    30  2001 ACCOUNTING  
## # … with 222 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for sorting, there is no competition! The ‘arrange()’ function, together with the ‘desc()’ function for descending order, represents a much clearer way to sort the data, with respect to the traditional ‘order()’ function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;casting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Casting the data&lt;/h1&gt;
&lt;p&gt;When we have a dataset in the LONG format, we might be interested in reshaping it into the WIDE format. This is the same as what the ‘pivot table’ function in Excel does. For example, take the ‘rimsulfuron.csv’ dataset in my repository. This contains the results of a randomised block experiment, where we have 16 herbicides in four blocks. The dataset is in the LONG format, with one row per plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rimsulfuron &amp;lt;- read_csv(&amp;quot;https://www.casaonofri.it/_datasets/rimsulfuron.csv&amp;quot;)
## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Herbicide = col_character(),
##   Plot = col_double(),
##   Code = col_double(),
##   Block = col_double(),
##   Box = col_double(),
##   WeedCover = col_double(),
##   Yield = col_double()
## )
rimsulfuron
## # A tibble: 64 x 7
##    Herbicide                              Plot  Code Block   Box WeedCover Yield
##    &amp;lt;chr&amp;gt;                                 &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Rimsulfuron (40)                          1     1     1     1      27.8  85.9
##  2 Rimsulfuron (45)                          2     2     1     1      27.8  93.0
##  3 Rimsulfuron (50)                          3     3     1     1      23    86.9
##  4 Rimsulfuron (60)                          4     4     1     1      42.8  53.0
##  5 Rimsulfuron (50+30 split)                 5     5     1     2      15.1  71.4
##  6 Rimsulfuron + thyfensulfuron              6     6     1     2      22.9  75.3
##  7 Rimsulfuron + hoeing                      7     7     1     2      17.7  73.2
##  8 Pendimethalin (pre) + rimsulfuron (p…     8     8     1     2      10.2  65.5
##  9 Pendimethalin (post) + rimsuulfuron …     9     9     1     1       5.4  94.8
## 10 Rimsulfuron + Atred                      10    10     1     1      40.3  94.1
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s put this data frame in the WIDE format, with one row per herbicide and one column per block. In my previous post, I used to the ‘cast()’ function in the ‘reshape’ package. Now I can use the ‘pivot_wider()’ function in the ‘tidyr’ package: the herbicide goes in the first column, the blocks (B1, B2, B3, B4) should go in the next four columns, and each unique level of yield should go in each cell, at the crossing of the correct herbicide row and block column. The ‘Height’ variable is not needed and it should be removed. Again, a two steps process, that is made easier by using the pipe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(reshape)
# castData &amp;lt;- cast(Herbicide ~ Block, data = rimsulfuron,
#      value = &amp;quot;Yield&amp;quot;)
# head(castData)

castData &amp;lt;- rimsulfuron %&amp;gt;%
  select(-Plot, - Code, -Box, - WeedCover) %&amp;gt;%
  pivot_wider(names_from = Block, values_from = Yield)
castData
## # A tibble: 16 x 5
##    Herbicide                                    `1`   `2`   `3`   `4`
##    &amp;lt;chr&amp;gt;                                      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Rimsulfuron (40)                            85.9  91.1 111.   93.2
##  2 Rimsulfuron (45)                            93.0 105    89.2  79.0
##  3 Rimsulfuron (50)                            86.9 106.  110.   89.1
##  4 Rimsulfuron (60)                            53.0 103.  101.   97.0
##  5 Rimsulfuron (50+30 split)                   71.4  77.6 116.   92.2
##  6 Rimsulfuron + thyfensulfuron                75.3  82.6  95.0  85.8
##  7 Rimsulfuron + hoeing                        73.2  86.1 118.   98.3
##  8 Pendimethalin (pre) + rimsulfuron (post)    65.5  88.7  95.5  82.4
##  9 Pendimethalin (post) + rimsuulfuron (post)  94.8  87.7 102.  102. 
## 10 Rimsulfuron + Atred                         94.1  89.9 104.   99.6
## 11 Thifensulfuron                              78.5  42.3  62.5  24.3
## 12 Metolachlor + terbuthylazine (pre)          51.8  52.1  49.5  34.7
## 13 Alachlor + terbuthylazine                   12.1  49.6  41.3  16.4
## 14 Hand-Weeded                                 77.6  92.1  86.6  99.6
## 15 Unweeded 1                                  10.9  31.8  23.9  20.8
## 16 Unweeded 2                                  27.6  51.6  25.1  38.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I am not clear with which it is more advantageous than which. Simply, I do not see much difference: none of the two methods is as clear as I would expect it to be!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;melting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Melting the data&lt;/h1&gt;
&lt;p&gt;In this case we do the reverse: we transform the dataset from WIDE to LONG format. In my previous post I used the ‘melt()’ function in the ‘reshape2’ package; now, I will use the ‘pivot_longer()’ function in ‘tidyr’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(reshape2)
# castData &amp;lt;- as.data.frame(castData)
# mdati &amp;lt;- melt(castData,
#               variable.name = &amp;quot;Block&amp;quot;,
#               value.name = &amp;quot;Yield&amp;quot;,
#               id=c(&amp;quot;Herbicide&amp;quot;))
# 
# head(mdati)
# 
pivot_longer(castData, names_to = &amp;quot;Block&amp;quot;, values_to = &amp;quot;Yield&amp;quot;,
             cols = c(2:5))
## # A tibble: 64 x 3
##    Herbicide        Block Yield
##    &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Rimsulfuron (40) 1      85.9
##  2 Rimsulfuron (40) 2      91.1
##  3 Rimsulfuron (40) 3     111. 
##  4 Rimsulfuron (40) 4      93.2
##  5 Rimsulfuron (45) 1      93.0
##  6 Rimsulfuron (45) 2     105  
##  7 Rimsulfuron (45) 3      89.2
##  8 Rimsulfuron (45) 4      79.0
##  9 Rimsulfuron (50) 1      86.9
## 10 Rimsulfuron (50) 2     106. 
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before with casting, neither ‘melt()’, nor ‘pivot_longer()’ let me completely satisfied.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidyverse-or-not-tidyverse&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tidyverse or not tidyverse?&lt;/h1&gt;
&lt;p&gt;This post is the result of using some functions coming from the ‘tidyverse’ and related packages, to replace other functions from more traditional packages, which I was more accustomed to, as a long-time R user. What’s my feeling about this change? Let me try to figure it out.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First of all, it didn’t take much time to adjust. I need to thank the authors of ‘tidyverse’ for being very respectful of tradition.&lt;/li&gt;
&lt;li&gt;In one case (ordering), adjusting to the new paradigm brought to a easier coding. In all other cases, the ease of coding was not affected.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Will I stick to the new paradigm or will I go back to my familiar approaches? Should I only consider the simple tasks above, my answer would be: “I’ll go back!”. However, this would be an unfair answer. Indeed, my data tasks are not as simple as those above. More frequently, my data tasks are made of several different steps. For example:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Take the ‘students’ dataset&lt;/li&gt;
&lt;li&gt;Filter the marks included between 26 and 28&lt;/li&gt;
&lt;li&gt;Remove the ‘Id’, ‘date’ and ‘high-school’ columns&lt;/li&gt;
&lt;li&gt;Calculate the mean mark for each subject in each year&lt;/li&gt;
&lt;li&gt;Spread those means along Years&lt;/li&gt;
&lt;li&gt;Get the overall mean for each subject across years&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s try to accomplish this task by using both a ‘base’ approach and a ‘tidyverse’ approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Traditional approach
library(reshape)
students2 &amp;lt;- subset(students, Mark &amp;gt;= 26 | Mark &amp;lt;= 28, 
                    select = c(-Id, -Date, -HighSchool))
mstudents2 &amp;lt;- cast(Subject ~ Year, data = students2,
      value = &amp;quot;Mark&amp;quot;, fun.aggregate = mean)
mstudents2$Mean &amp;lt;- apply(mstudents2[,2:3], 1, mean)
mstudents2
##       Subject     2001     2002     Mean
## 1    AGRONOMY 26.69565 26.25000 26.47283
## 2     BIOLOGY 26.48000 26.41379 26.44690
## 3   CHEMISTRY 24.21429 22.19048 23.20238
## 4   ECONOMICS 27.73077 27.11111 27.42094
## 5 FRUIT TREES      NaN 26.92857      NaN
## 6       MATHS 26.59259 25.00000 25.79630
# Tidyverse approach
students %&amp;gt;%
  filter(Mark &amp;gt;= 26 | Mark &amp;lt;= 28) %&amp;gt;%
  select(c(-Id,-Date,-HighSchool)) %&amp;gt;%
  group_by(Subject, Year) %&amp;gt;%
  summarise(Mark = mean(Mark)) %&amp;gt;%
  pivot_wider(names_from = Year, values_from = Mark) %&amp;gt;%
  mutate(Mean = (`2001` + `2002`)/2)
## # A tibble: 6 x 4
## # Groups:   Subject [6]
##   Subject     `2001` `2002`  Mean
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 AGRONOMY      26.7   26.2  26.5
## 2 BIOLOGY       26.5   26.4  26.4
## 3 CHEMISTRY     24.2   22.2  23.2
## 4 ECONOMICS     27.7   27.1  27.4
## 5 FRUIT TREES   NA     26.9  NA  
## 6 MATHS         26.6   25    25.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I must admit the second piece of code flows much more smooothly and it is much closer to my natural way of thinking. A collegue of mine said that, when it comes to operating on big tables and making really complex operations, the tidyverse is currently considered ‘the most powerful tool in the world’. I will have to dedicate another post to such situations. So far, I have started to reconsider my attitute towards the tidyverse.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonlinear combinations of model parameters in regression</title>
      <link>/2020/stat_nls_gnlht/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/stat_nls_gnlht/</guid>
      <description>


&lt;p&gt;Nonlinear regression plays an important role in my research and teaching activities. While I often use the ‘drm()’ function in the ‘drc’ package for my research work, I tend to prefer the ‘nls()’ function for teaching purposes, mainly because, in my opinion, the transition from linear models to nonlinear models is smoother, for beginners. One problem with ‘nls()’ is that, in contrast to ‘drm()’, it is not specifically tailored to the needs of biologists or students in biology. Therefore, now and then, I have to build some helper functions, to perform some specific tasks; I usually share these functions within the ‘aomisc’ package, that is available on github (&lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;see this link&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, I would like to describe one of these helper functions, i.e. ‘gnlht()’, which is aimed at helping students (and practitioners; why not?) with one of their tasks, i.e. making some simple manipulations of model parameters, to obtain relevant biological information. Let’s see a typical example.&lt;/p&gt;
&lt;div id=&#34;motivating-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example&lt;/h1&gt;
&lt;p&gt;This is a real-life example, taken from a research published by Vischetti et al. in 1996. That research considered three herbicides for weed control in sugar beet, i.e. metamitron (M), phenmedipham (P) and cloridazon (C). Four soil samples were contaminated, respectively with: (i) M alone, (ii) M + P (iii) M + C and (iv) M + P + C. The aim was to assess whether the degradation speed of metamitron in soil depended on the presence of co-applied herbicides. To reach this aim, the soil samples were incubated at 20°C and sub-samples were taken in different times after the beginning of the experiment. The concentration of metamitron in those sub-samples was measured by HPLC analyses, performed in triplicate. The resulting dataset is available within the ‘aomisc’ package; we can load it and use the ‘lattice’ package to visualise the observed means over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(devtools)
# install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
library(aomisc)
library(lattice)
data(metamitron)
xyplot(Conc ~ Time|Herbicide, data = metamitron,
       xlab = &amp;quot;Time (d)&amp;quot;, ylab = &amp;quot;Concentration&amp;quot;,
       scales = list(alternating = F),
       panel = function(x, y, ...) { 
         panel.grid(h = -1, v = -1)
         fmy &amp;lt;- tapply(y, list(factor(x)), mean)
         fmx &amp;lt;- tapply(x, list(factor(x)), mean)
         panel.xyplot(fmx, fmy, col=&amp;quot;red&amp;quot;, type=&amp;quot;b&amp;quot;, cex = 1)
       })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nls_gnlht_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Considering this exemplary dataset, let’s see how we can answer the following research questions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the degradation rate for metamitron, in the four combinations?&lt;/li&gt;
&lt;li&gt;Is there a significant difference between the degradation rate of metamitron alone and with co-applied herbicides?&lt;/li&gt;
&lt;li&gt;What is the half-life for metamitron, in the four combinations?&lt;/li&gt;
&lt;li&gt;What are the times to reach 70 and 30% of the initial concentration, for metamitron in the four combinations?&lt;/li&gt;
&lt;li&gt;Is there a significant difference between the half-life of metamitron alone and with co-applied herbicides?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-degradation-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a degradation model&lt;/h1&gt;
&lt;p&gt;The figure above shows a visible difference in the degradation pattern of metamitron, which could be attributed to the presence of co-applied herbicides. The degradation kinetics can be described by the following (first-order) model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ C(t, h) = A_h \, \exp \left(-k_h  \, t \right) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(C(t, h)\)&lt;/span&gt; is the concentration of metamitron at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; in each of the four combinations &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(A_h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k_h\)&lt;/span&gt; are, respectively, the initial concentration and degradation rate for metamitron in each combination.&lt;/p&gt;
&lt;p&gt;The model is nonlinear and, therefore, we can use the ‘nls()’ function for nonlinear least squares regression. The code is given below: please, note that the two parameters are followed by the name of the factor variable in square brackets (i.e.: A[Herbicide] and k[Herbicide]). This is necessary to fit a different parameter value for each level of the ‘Herbicide’ factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Fit nls grouped model
modNlin &amp;lt;- nls(Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time), 
               start=list(A=rep(100, 4), k=rep(0.06, 4)), 
               data=metamitron)
summary(modNlin)
## 
## Formula: Conc ~ A[Herbicide] * exp(-k[Herbicide] * Time)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## A1 9.483e+01  4.796e+00   19.77   &amp;lt;2e-16 ***
## A2 1.021e+02  4.316e+00   23.65   &amp;lt;2e-16 ***
## A3 9.959e+01  4.463e+00   22.31   &amp;lt;2e-16 ***
## A4 1.116e+02  4.184e+00   26.68   &amp;lt;2e-16 ***
## k1 4.260e-02  4.128e-03   10.32   &amp;lt;2e-16 ***
## k2 2.574e-02  2.285e-03   11.26   &amp;lt;2e-16 ***
## k3 3.034e-02  2.733e-03   11.10   &amp;lt;2e-16 ***
## k4 2.186e-02  1.822e-03   12.00   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 9.701 on 88 degrees of freedom
## 
## Number of iterations to convergence: 5 
## Achieved convergence tolerance: 7.136e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of simplicity, I will neglige an accurate model check, although I need to point out that this is highly wrong. I’ll come back to this issue in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-model-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working with model parameters&lt;/h1&gt;
&lt;p&gt;Considering the research questions, it is clear that the above output answers the first one, as it gives the four degradation rates, &lt;span class=&#34;math inline&#34;&gt;\(k1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k4\)&lt;/span&gt;. All the other questions can be translated into sets of linear/nonlinear functions (combinations) of model parameters. If we use the naming of parameter estimates in the nonlinear regression object, for the second question we can write the following functions: &lt;span class=&#34;math inline&#34;&gt;\(k1 - k2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k1 - k3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k1 - k4\)&lt;/span&gt;. The third question requires some slightly more complex math: if we invert the equation above for one herbicide, we get to the following inverse:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ t = \frac{- log \left[\frac{C(t)}{A} \right] }{k} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I do not think this is complex enough to scare the biologists, is it? The half-life is the time required for C(t) to drop to half of the initial value, so that &lt;span class=&#34;math inline&#34;&gt;\(C(t)/A\)&lt;/span&gt; is equal to &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;. Thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ t_{1/2} = \frac{- \log \left[0.5 \right] }{k} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Analogously, we can answer the question 4, by replacing &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; respectively with &lt;span class=&#34;math inline&#34;&gt;\(0.7\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0.3\)&lt;/span&gt;. The difference between the half-lives of metamitron alone and combined with the second herbicide can be calculated by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{- \log \left[0.5 \right] }{k_1} - \frac{- \log \left[0.5 \right] }{k_2} = \frac{k_2 - k_1}{k_1 \, k_2} \, \log(0.5)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The other differences are obtained analogously.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inferences-and-hypotheses-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inferences and hypotheses testing&lt;/h1&gt;
&lt;p&gt;All parameter estimates are characterised by some uncertainty, which is summarised by way of the standard errors (see the code output above). Clearly, such an uncertainty propagates to their combinations. As for the first question, the combinations are linear, as only subtraction is involved. Therefore, the standard error for the difference can be easily calculated by the usual law of propagation of errors, which I have dealt with in &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_errorpropagation/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In R, linear combinations of model parameters can be built and tested by using the ‘glht()’ function in the ‘multcomp’ package. However, I wanted to find a general solution, that could handle both linear and nonlinear combinations of model parameters. Such a solution should be based on the ‘delta method’, which I have dealt with in &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_thedeltamethod/&#34;&gt;this post&lt;/a&gt;. Unfortunately, the function ‘deltaMethod()’ in the ‘car’ package is not flexible enough to the aims of my students and mine.&lt;/p&gt;
&lt;p&gt;Therefore, I wrote a wrapper for the ‘deltaMethod()’ function, which I named ‘gnlht()’, as it might play for nonlinear combinations the same role as ‘glht()’ for linear combinations. To use this function, apart from loading the ‘aomisc’ package, we need to prepare a list of formulas. Care needs to be taken to make sure that the element in the formulas correspond to the names of the estimated parameters in the model object, as returned by the ‘coef()’ method. In the box below, I show how we can calculate the differences between the degradation rates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~k1 - k2, ~k1 - k3, ~k1 - k4)
gnlht(modNlin, funList)
##      form   Estimate          SE  t-value      p-value
## 1 k1 - k2 0.01686533 0.004718465 3.574325 5.727311e-04
## 2 k1 - k3 0.01226241 0.004951372 2.476568 1.517801e-02
## 3 k1 - k4 0.02074109 0.004512710 4.596150 1.430392e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The very same code can be used for nonlinear combinations of model parameters. In order to calculate the half-lives, we can use the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ -log(0.5)/k1, ~ -log(0.5)/k2,
                ~ -log(0.5)/k3, ~ -log(0.5)/k4)
gnlht(modNlin, funList)
##           form Estimate       SE  t-value      p-value
## 1 -log(0.5)/k1 16.27089 1.576827 10.31876 7.987827e-17
## 2 -log(0.5)/k2 26.93390 2.391121 11.26413 9.552915e-19
## 3 -log(0.5)/k3 22.84747 2.058588 11.09861 2.064093e-18
## 4 -log(0.5)/k4 31.70942 2.643329 11.99601 3.257067e-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of writing ‘0.5’, we can introduce a new model term, e.g. ‘prop’, as a ‘constant’, in the sense that it is not an estimated parameter. We can pass a value for this constant in a data frame, by using the ‘const’ argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ -log(prop)/k1, ~ -log(prop)/k2,
                ~ -log(prop)/k3, ~ -log(prop)/k4)
gnlht(modNlin, funList, const = data.frame(prop = 0.5))
##            form prop Estimate       SE  t-value      p-value
## 1 -log(prop)/k1  0.5 16.27089 1.576827 10.31876 7.987827e-17
## 2 -log(prop)/k2  0.5 26.93390 2.391121 11.26413 9.552915e-19
## 3 -log(prop)/k3  0.5 22.84747 2.058588 11.09861 2.064093e-18
## 4 -log(prop)/k4  0.5 31.70942 2.643329 11.99601 3.257067e-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is very flexible, because it lets us to calculate, altogether, the half-life and the times required for the concentration to drop to 70 and 30% of the initial value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ -log(prop)/k1, ~ -log(prop)/k2,
                ~ -log(prop)/k3, ~ -log(prop)/k4)
gnlht(modNlin, funList, const = data.frame(prop = c(0.7, 0.5, 0.3)))
##             form prop  Estimate        SE  t-value      p-value
## 1  -log(prop)/k1  0.7  8.372564 0.8113927 10.31876 7.987827e-17
## 2  -log(prop)/k1  0.5 16.270892 1.5768267 10.31876 7.987827e-17
## 3  -log(prop)/k1  0.3 28.261979 2.7388937 10.31876 7.987827e-17
## 4  -log(prop)/k2  0.7 13.859465 1.2304069 11.26413 9.552915e-19
## 5  -log(prop)/k2  0.5 26.933905 2.3911214 11.26413 9.552915e-19
## 6  -log(prop)/k2  0.3 46.783265 4.1532956 11.26413 9.552915e-19
## 7  -log(prop)/k3  0.7 11.756694 1.0592942 11.09861 2.064093e-18
## 8  -log(prop)/k3  0.5 22.847468 2.0585881 11.09861 2.064093e-18
## 9  -log(prop)/k3  0.3 39.685266 3.5756966 11.09861 2.064093e-18
## 10 -log(prop)/k4  0.7 16.316814 1.3601864 11.99601 3.257067e-20
## 11 -log(prop)/k4  0.5 31.709415 2.6433295 11.99601 3.257067e-20
## 12 -log(prop)/k4  0.3 55.078163 4.5913724 11.99601 3.257067e-20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The differences between the half-lives (and other degradation times) can be calculated as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ (k2 - k1)/(k1 * k2) * log(prop),
                ~ (k3 - k1)/(k1 * k3) * log(prop), 
                ~ (k4 - k1)/(k1 * k4) * log(prop))
gnlht(modNlin, funList, const = data.frame(prop = c(0.7, 0.5, 0.3)))
##                              form prop  Estimate       SE  t-value      p-value
## 1 (k2 - k1)/(k1 * k2) * log(prop)  0.7  5.486900 1.473859 3.722813 3.468973e-04
## 2 (k2 - k1)/(k1 * k2) * log(prop)  0.5 10.663013 2.864235 3.722813 3.468973e-04
## 3 (k2 - k1)/(k1 * k2) * log(prop)  0.3 18.521287 4.975078 3.722813 3.468973e-04
## 4 (k3 - k1)/(k1 * k3) * log(prop)  0.7  3.384130 1.334340 2.536183 1.297111e-02
## 5 (k3 - k1)/(k1 * k3) * log(prop)  0.5  6.576577 2.593100 2.536183 1.297111e-02
## 6 (k3 - k1)/(k1 * k3) * log(prop)  0.3 11.423287 4.504125 2.536183 1.297111e-02
## 7 (k4 - k1)/(k1 * k4) * log(prop)  0.7  7.944250 1.583814 5.015900 2.718445e-06
## 8 (k4 - k1)/(k1 * k4) * log(prop)  0.5 15.438524 3.077917 5.015900 2.718445e-06
## 9 (k4 - k1)/(k1 * k4) * log(prop)  0.3 26.816185 5.346236 5.015900 2.718445e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The possibility of passing constants in a data.frame adds flexibility with respect to the ‘deltaMethod()’ function in the ‘car’ package. For example, we can use this method to make predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funList &amp;lt;- list(~ A1 * exp (- k1 * Time), ~ A2 * exp (- k2 * Time), 
                ~ A3 * exp (- k3 * Time), ~ A4 * exp (- k4 * Time))
pred &amp;lt;- gnlht(modNlin, funList, const = data.frame(Time = seq(0, 67, 1)))
head(pred)
##                   form Time Estimate       SE  t-value      p-value
## 1 A1 * exp(-k1 * Time)    0 94.83198 4.795948 19.77336 3.931107e-34
## 2 A1 * exp(-k1 * Time)    1 90.87694 4.381511 20.74101 1.223613e-35
## 3 A1 * exp(-k1 * Time)    2 87.08684 4.015039 21.69016 4.511113e-37
## 4 A1 * exp(-k1 * Time)    3 83.45482 3.695325 22.58389 2.205772e-38
## 5 A1 * exp(-k1 * Time)    4 79.97427 3.421034 23.37722 1.623774e-39
## 6 A1 * exp(-k1 * Time)    5 76.63888 3.190531 24.02072 2.050113e-40
tail(pred)
##                     form Time Estimate       SE  t-value      p-value
## 267 A4 * exp(-k4 * Time)   62 28.78518 2.657182 10.83297 7.138133e-18
## 268 A4 * exp(-k4 * Time)   63 28.16278 2.648687 10.63273 1.824651e-17
## 269 A4 * exp(-k4 * Time)   64 27.55384 2.639403 10.43942 4.525865e-17
## 270 A4 * exp(-k4 * Time)   65 26.95807 2.629361 10.25270 1.090502e-16
## 271 A4 * exp(-k4 * Time)   66 26.37517 2.618594 10.07227 2.555132e-16
## 272 A4 * exp(-k4 * Time)   67 25.80489 2.607131  9.89781 5.827812e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although this is not very fast, in contrast to the ‘predict()’ method for ‘nls’ objects, it has the advantage of reporting standard errors.&lt;/p&gt;
&lt;p&gt;Hope this is useful. Happy coding!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;John Fox and Sanford Weisberg (2019). An {R} Companion to Applied Regression, Third Edition. Thousand Oaks CA:Sage. URL: &lt;a href=&#34;https://socialsciences.mcmaster.ca/jfox/Books/Companion/&#34; class=&#34;uri&#34;&gt;https://socialsciences.mcmaster.ca/jfox/Books/Companion/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Torsten Hothorn, Frank Bretz and Peter Westfall (2008). Simultaneous Inference in General Parametric Models. Biometrical Journal 50(3), 346–363.&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J. C., Gerhard, D. (2015) Dose-Response Analysis Using R PLOS ONE, 10(12), e0146021&lt;/li&gt;
&lt;li&gt;Vischetti, C., Marini, M., Businelli, M., Onofri, A., 1996. The effect of temperature and co-applied herbicides on the degradation rate of phenmedipham, chloridazon and metamitron in a clay loam soil in the laboratory, in: Re, A.D., Capri, E., Evans, S.P., Trevisan, M. (Eds.), “The Environmental Phate of Xenobiotics”, Proceedings X Symposium on Pesticide Chemistry, Piacenza. La Goliardica Pavese, Piacenza, pp. 287–294.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #2</title>
      <link>/2019/stat_lmm_2-wayssplitrepeated/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_lmm_2-wayssplitrepeated/</guid>
      <description>


&lt;div id=&#34;a-repeated-split-plot-experiment-with-heteroscedastic-errors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A repeated split-plot experiment with heteroscedastic errors&lt;/h1&gt;
&lt;p&gt;Let’s imagine a field experiment, where different genotypes of khorasan wheat are to be compared under different nitrogen (N) fertilisation systems. Genotypes require bigger plots, with respect to fertilisation treatments and, therefore, the most convenient choice would be to lay-out the experiment as a split-plot, in a randomised complete block design. Genotypes would be randomly allocated to main plots, while fertilisation systems would be randomly allocated to sub-plots. As usual in agricultural research, the experiment should be repeated in different years, in order to explore the environmental variability of results.&lt;/p&gt;
&lt;p&gt;What could we expect from such an experiment?&lt;/p&gt;
&lt;p&gt;Please, look at the dataset ‘kamut.csv’, which is available on github. It provides the results for a split-plot experiment with 15 genotypes and 2 N fertilisation treatments, laid-out in three blocks and repeated in four years (360 observations, in all).&lt;/p&gt;
&lt;p&gt;The dataset has five columns, the ‘Year’, the ‘Genotype’, the fertilisation level (‘N’), the ‘Block’ and the response variable, i.e. ‘Yield’. The fifteen genotypes are coded by using the letters from A to O, while the levels of the other independent variables are coded by using numbers. The following snippets loads the file and recodes the numerical independent variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/kamut.csv&amp;quot;, header = T)
dataset$Block &amp;lt;- factor(dataset$Block)
dataset$Year &amp;lt;- factor(dataset$Year)
dataset$N &amp;lt;- factor(dataset$N)
head(dataset)
##   Year Genotype N Block Yield
## 1 2004        A 1     1 2.235
## 2 2004        A 1     2 2.605
## 3 2004        A 1     3 2.323
## 4 2004        A 2     1 3.766
## 5 2004        A 2     2 4.094
## 6 2004        A 2     3 3.902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, it may be useful to code some ‘helper’ factors, to represent the blocks (within years) and the main-plots. The first factors (‘YearBlock’) has 12 levels (4 years and 3 blocks per year) and the second factor (‘MainPlot’) has 180 levels (4 years, 3 blocks per year and 15 genotypes per block).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$YearBlock &amp;lt;- with(dataset, factor(Year:Block))
dataset$MainPlot &amp;lt;- with(dataset, factor(Year:Block:Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the analyses, we will make use of the ‘plyr’ (Wickham, 2011), ‘car’ (Fox and Weisberg, 2011) and ‘nlme’ (Pinheiro et al., 2018) packages, which we load now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(car)
library(nlme)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is always useful to start by separately considering the results for each year. This gives us a feel for what happened in all experiments. What model do we have to fit to single-year split-plot data? In order to avoid mathematical notation, I will follow the notation proposed by Piepho (2003), by using the names of variables, as reported in the dataset. The treatment model for this split-plot design is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Genotype * N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All treatment effects are fixed. The block model, referencing all grouping structures, is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Block + Block:MainPlot + Block:MainPlot:Subplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first element references the blocks, while the second element references the main-plots, to which the genotypes are randomly allocated (randomisation unit). The third element references the sub-plots, to which N treatments are randomly allocated (another randomisation unit); this latter element corresponds to the residual error and, therefore, it is fitted by default and needs not be explicitly included in the model. Main-plot and sub-plot effects need to be random, as they reference randomisation units (Piepho, 2003). The nature of the block effect is still under debate (Dixon, 2016), but I’ll take it as random (do not worry: I will also show how we can take it as fixed).&lt;/p&gt;
&lt;p&gt;Coding a split-plot model in ‘lme’ is rather simple:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lme(Yield ~ Genotype * N, random = ~1|Block/MainPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the notation ‘Block/MainPlot’ is totally equivalent to ‘Block + Block:MainPlot’. Instead of manually fitting this model four times (one per year), we can ask R to do so by using the ‘ddply()’ function in the ‘plyr’ package. In the code below, I used this technique to retrieve the residual variance for each experiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmmFits &amp;lt;- ddply(dataset, c(&amp;quot;Year&amp;quot;),
      function(df) summary( lme(Yield ~ Genotype * N,
                 random = ~1|Block/MainPlot,
                 data = df))$sigma^2 )
lmmFits
##   Year          V1
## 1 2004 0.052761644
## 2 2005 0.001423833
## 3 2006 0.776028791
## 4 2007 0.817594477&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see great differences! The residual variance in 2005 is more that 500 times smaller than that observed in 2007. Clearly, if we pool the data and make an ANOVA, when we pool the data, we violate the homoscedasticity assumption. In general, this problem has an obvious solution: we can model the variance-covariance matrix of observations, allowing a different variance per year. In R, this is only possible by using the ‘lme()’ function (unless we want to use the ‘asreml-R’ package, which is not freeware, unfortunately). The question is: how do we code such a model?&lt;/p&gt;
&lt;p&gt;First of all, let’s derive a correct mixed model. The treatment model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Yield ~ Genotype * N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have mentioned that the genotype and N effects are likely to be taken as fixed. The block model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year + Year/Block + Year:Block:MainPlot + Year:Block:MainPlot:Subplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second element in the block model references the blocks within years, the second element references the main-plots, while the third element references the sub-plots and, as before, it is not needed. The year effect is likely to interact with both the treatment effects, so we need to add the following effects:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year + Year:Genotype + Year:N + Year:Genotype:N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent to writing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; ~ Year*Genotype*N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The year effect can be taken as either as random or as fixed. In this post, we will show both approaches&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;year-effect-is-fixed&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Year effect is fixed&lt;/h1&gt;
&lt;p&gt;If we take the year effect as fixed and the block effect as random, we see that the random effects are nested (blocks within years and main-plots within blocks and within years). The function ‘lme()’ is specifically tailored to deal with nested random effects and, therefore, fitting the above model is rather easy. In the first snippet we fit a homoscedastic model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix1 &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = ~1|YearBlock/MainPlot,
                 data = dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also fit this model with the ‘lme4’ package and the ‘lmer()’; however, we are not happy with this, because we have seen clear signs of heteroscedastic within-year errors. Thus, let’s account for such an heteroscedasticity, by using the ‘weights()’ argument and the ‘varIdent()’ variance structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2 &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = ~1|YearBlock/MainPlot,
                 data = dataset,
               weights = varIdent(form = ~1|Year))
AIC(modMix1, modMix2)
##          df      AIC
## modMix1 123 856.6704
## modMix2 126 575.1967&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the Akaike Information Criterion, we see that the second model is better than the first one, which supports the idea of heteroscedastic residuals. From this moment on, the analyses proceeds as usual, e.g. by testing for fixed effects and comparing means, as necessary. Just a few words about testing for fixed effects: Wald F tests can be obtained by using the ‘anova()’ function, although I usually avoid this with ‘lme’ objects, as there is no reliable approximation to degrees of freedom. With ‘lme’ objects, I suggest using the ‘Anova()’ function in the ‘car’ package, which shows the results of Wald chi square tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Anova(modMix2)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##                    Chisq Df Pr(&amp;gt;Chisq)    
## Year              51.072  3  4.722e-11 ***
## Genotype         543.499 14  &amp;lt; 2.2e-16 ***
## N               2289.523  1  &amp;lt; 2.2e-16 ***
## Year:Genotype    123.847 42  5.281e-10 ***
## Year:N            21.695  3  7.549e-05 ***
## Genotype:N      1356.179 14  &amp;lt; 2.2e-16 ***
## Year:Genotype:N  224.477 42  &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One further aspect: do you prefer fixed blocks? Then you can fit the following model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix4 &amp;lt;- lme(Yield ~ Year * Genotype * N + Year:Block,
                 random = ~1|MainPlot,
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;year-effect-is-random&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Year effect is random&lt;/h1&gt;
&lt;p&gt;If we’d rather take the year effect as random, all the interactions therein are random as well (Year:Genotype, Year:N and Year:Genotype:N). Similarly, the block (within years) effect needs to be random. Therefore, we have several crossed random effects, which are not straightforward to code with ‘lme()’. First, I will show the code, second, I will comment it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix5 &amp;lt;- lme(Yield ~ Genotype * N,
                  random = list(Year = pdIdent(~1),
                                Year = pdIdent(~Block - 1),
                                Year = pdIdent(~MainPlot - 1),
                                Year = pdIdent(~Genotype - 1),
                                Year = pdIdent(~N - 1),
                                Genotype = pdIdent(~N - 1)),
                  data=dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that random effects are coded using a named list; each component of this list is a &lt;em&gt;pdMat&lt;/em&gt; object with name equal to a grouping factor. For example, the component ‘Year = pdIdent(~ 1)’ represents a random year effect, while ‘Year = pdIdent(~ Block - 1)’ represents a random year effect for each level of Block, i.e. a random ‘year x block’ interaction. This latter variance component is the same for all blocks (‘varIdent’), i.e. there is homoscedastic at this level.&lt;/p&gt;
&lt;p&gt;It is important to remember that the grouping factors in the list are treated as nested; however, the grouping factor is only one (‘Year’), so that the nesting is irrelevant. The only exception is the genotype, which is regarded as nested within the year. As the consequence, the component ‘Genotype = pdIdent(~N - 1)’, specifies a random year:genotype effect for each level of N treatment, i.e. a random year:genotype:N interaction.&lt;/p&gt;
&lt;p&gt;I agree, this is not straightforward to understand! If necessary, take a look at the good book of Gałecki and Burzykowski (2013). When fitting the above model, be patient; convergence may take a few seconds. I’d only like to reinforce the idea that, in case you need to test for fixed effects, you should not rely on the ‘anova()’ function, but you should prefer Wald chi square tests in the ‘Anova()’ function in the ‘car’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Anova(modMix5, type = 2)
## Analysis of Deviance Table (Type II tests)
## 
## Response: Yield
##              Chisq Df Pr(&amp;gt;Chisq)    
## Genotype   68.6430 14  3.395e-09 ***
## N           2.4682  1     0.1162    
## Genotype:N 14.1153 14     0.4412    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another note: coding random effects as a named list is always possible. For example ‘modMix2’ can also be coded as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2b &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = list(YearBlock = ~ 1, MainPlot = ~ 1),
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, also as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modMix2c &amp;lt;- lme(Yield ~ Year * Genotype * N,
                 random = list(YearBlock = pdIdent(~ 1), MainPlot = pdIdent(~ 1)),
                 data = dataset,
               weights = varIdent(form = ~1|Year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this is useful! Have fun with it.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
&lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34; class=&#34;email&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dixon, P., 2016. Should blocks be fixed or random? Conference on Applied Statistics in Agriculture. &lt;a href=&#34;https://doi.org/10.4148/2475-7772.1474&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4148/2475-7772.1474&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fox J. and Weisberg S. (2011). An {R} Companion to Applied Regression, Second Edition. Thousand Oaks CA: Sage. URL: &lt;a href=&#34;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion&#34; class=&#34;uri&#34;&gt;http://socserv.socsci.mcmaster.ca/jfox/Books/Companion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., Büchse, A., Emrich, K., 2003. A Hitchhiker’s Guide to Mixed Models for Randomized Experiments. Journal of Agronomy and Crop Science 189, 310–322.&lt;/li&gt;
&lt;li&gt;Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2018). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-137, &amp;lt;URL: &lt;a href=&#34;https://CRAN.R-project.org/package=nlme&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=nlme&lt;/a&gt;&amp;gt;.&lt;/li&gt;
&lt;li&gt;Hadley Wickham (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1-29. URL: &lt;a href=&#34;http://www.jstatsoft.org/v40/i01/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v40/i01/&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;: Example #4</title>
      <link>/2019/stat_nlmm_interaction/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_nlmm_interaction/</guid>
      <description>


&lt;div id=&#34;testing-for-interactions-in-nonlinear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing for interactions in nonlinear regression&lt;/h1&gt;
&lt;p&gt;Factorial experiments are very common in agriculture and they are usually laid down to test for the significance of interactions between experimental factors. For example, genotype assessments may be performed at two different nitrogen fertilisation levels (e.g. high and low) to understand whether the ranking of genotypes depends on nutrient availability. For those of you who are not very much into agriculture, I will only say that such an assessment is relevant, because we need to know whether we can recommend the same genotypes, e.g., both in conventional agriculture (high nitrogen availability) and in organic agriculture (relatively lower nitrogen availability).&lt;/p&gt;
&lt;p&gt;Let’s consider an experiment where we have tested three genotypes (let’s call them A, B and C, for brevity), at two nitrogen rates (‘high’ an ‘low’) in a complete block factorial design, with four replicates. Biomass subsamples were taken from each of the 24 plots in eight different times (Days After Sowing: DAS), to evaluate the growth of biomass over time.&lt;/p&gt;
&lt;p&gt;The dataset is available on gitHub and the following code loads it and transforms the ‘Block’ variable into a factor. For this post, we will use several packages, including ‘aomisc’, the accompanying package for this blog. Please refer to &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page for downloading and installing&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
library(lattice)
library(nlme)
library(aomisc)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/growthNGEN.csv&amp;quot;,
  header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)
##   Id DAS Block Plot GEN   N  Yield
## 1  1   0     1    1   A Low  2.786
## 2  2  15     1    1   A Low  5.871
## 3  3  30     1    1   A Low 13.265
## 4  4  45     1    1   A Low 16.926
## 5  5  60     1    1   A Low 22.812
## 6  6  75     1    1   A Low 25.346&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is composed by the following variables:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;‘Id’: a numerical code for observations&lt;/li&gt;
&lt;li&gt;‘DAS’: i.e., Days After Sowing. It is the moment when the sample was collected&lt;/li&gt;
&lt;li&gt;‘Block’, ‘Plot’, ‘GEN’ and ‘N’ represent respectively the block, plot, genotype and nitrogen level for each observation&lt;/li&gt;
&lt;li&gt;‘Yield’ represents the harvested biomass.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It may be useful to take a look at the observed growth data, as displayed on the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the growth is sygmoidal (presumably logistic) and that the variance of observations increases over time, i.e. the variance is proportional to the expected response.&lt;/p&gt;
&lt;p&gt;The question is: how do we analyse this data? Let’s build a model in a sequential fashion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The model&lt;/h1&gt;
&lt;p&gt;We could empirically assume that the relationship between biomass and time is logistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijkl} = \frac{d_{ijkl}}{1 + exp\left[b \left( X_{ijkl} - e_{ijkl}\right)\right]} + \varepsilon_{ijkl}\quad \quad (1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the observed biomass yield at time &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, for the i-th genotype, j-th nitrogen level, k-th block and l-th plot, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the maximum asymptotic biomass level when time goes to infinity, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope at inflection point, while &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the time when the biomass yield is equal to &lt;span class=&#34;math inline&#34;&gt;\(d/2\)&lt;/span&gt;. We are mostly interested in the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;: the first one describes the yield potential of a genotype, while the second one gives a measure of the speed of growth.&lt;/p&gt;
&lt;p&gt;There are repeated measures in each plot and, therefore, model parameters may show some variability, depending on the genotype, nitrogen level, block and plot. In particular, it may be acceptable to assume that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is pretty constant and independent on the above factors, while &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; may change according to the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left\{ {\begin{array}{*{20}{c}}
d_{ijkl} = \mu_d + g_{di} + N_{dj} + gN_{dij} + \theta_{dk} + \zeta_{dl}\\
e_{ijkl} = \mu_e + q_{ei} + N_{ej} + gN_{eij} + \theta_{ek} + \zeta_{el}
\end{array}} \right. \quad \quad (2) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, for each parameter, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the fixed effect of the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the fixed effect of j-th nitrogen level, &lt;span class=&#34;math inline&#34;&gt;\(gN\)&lt;/span&gt; is the fixed interaction effect, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the random effect of blocks, while &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; is the random effect of plots within blocks. These two equations are totally equivalent to those commonly used for linear mixed models, in the case of a two-factor factorial block design, wherein &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; would be the residual error term. Indeed, in principle, we could also think about a two-steps fitting procedure, where we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;fit the logistic model to the data for each plot and obtain estimates for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;use these estimates to fit a linear mixed model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will not pursue this two-steps technique here and we will concentrate on one-step fitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-wrong-method&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A wrong method&lt;/h1&gt;
&lt;p&gt;If the observations were independent (i.e. no blocks and no repeated measures), this model could be fit by using conventional nonlinear regression. My preference goes to the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2015).&lt;/p&gt;
&lt;p&gt;The coding is reported below: ‘Yield’ is a function of (&lt;span class=&#34;math inline&#34;&gt;\(\sim\)&lt;/span&gt;) DAS, by way of a three-parameters logistic function (‘fct = L.3()’). Different curves have to be fitted for different combinations of genotype and nitrogen levels (‘curveid = N:GEN’), although these curves should be partly based on common parameter values (‘pmodels = …). The ’pmodels’ argument requires a few additional comments. It must be a vector with as many element as there are parameters in the model (three, in this case: &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;). Each element represents a linear function of variables and refers to the parameters in alphabetical order, i.e. the first element refers to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the second refers to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the third to &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not dependent on any variable (‘~ 1’) and thus a constant value is fitted across curves; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; depend on a fully factorial combination of genotype and nitrogen level (~ N*GEN = ~N + GEN + N:GEN).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive1 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = GEN:N,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N*GEN))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model may be useful for other circumstances (no blocks and no repeated measures), but it is wrong in our example. Indeed, observations are clustered within blocks and plots; by neglecting this, we violate the assumption of independence of model residuals. Furthermore, a swift plot of residuals against fitted values shows clear signs of heteroscedasticity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Considering the above, we have to use a different model, here, although I will show that this naive fit may turn out useful.&lt;/p&gt;
&lt;div id=&#34;nonlinear-mixed-model-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nonlinear mixed model fitting&lt;/h2&gt;
&lt;p&gt;In order to account for the clustering of observations, we switch to a Nonlinear Mixed-Effect model (NLME). A good choice is the ‘nlme()’ function in the ‘nlme’ package (Pinheiro and Bates, 2000), although the syntax may be cumbersome, at times. I will try to help, listing and commenting the most important arguments for this function. We need to specify the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A deterministic function. In this case, we use the ‘nlsL.3()’ function in the ‘aomisc’ package, which provides a logistic growth model with the same parameterisation as the ‘L.3()’ function in the ‘drm’ package.&lt;/li&gt;
&lt;li&gt;Linear functions for model parameters. The ‘fixed’ argument in the ‘nlme’ function is very similar to the ‘pmodels’ argument in the ‘drm’ function above, in the sense that it requires a list, wherein each element is a linear function of variables. The only difference is that the parameter name needs to be specified on the left side of the function.&lt;/li&gt;
&lt;li&gt;Random effects for model parameters. These are specified by using the ‘random’ argument. In this case, the parameters &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; are expected to show random variability from block to block and from plot to plot, within a block. For the sake of simplicity, as the parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is not affected by the genotype and nitrogen level, we also expect that it does not show any random variability across blocks and plots.&lt;/li&gt;
&lt;li&gt;A variance function. The ‘weights’ argument is used to specify that the variance of residuals should be proportional to fitted values (‘varPower’ variance function)&lt;/li&gt;
&lt;li&gt;Starting values for model parameters. Self starting routines are not used by ‘nlme()’ and thus we need to specify a named vector, holding the initial values of model parameters. In this case, I decided to use the output from the ‘naive’ nonlinear regression above, which, therefore, turns out useful.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aomisc)
modnlme1 &amp;lt;- nlme(Yield ~ nlsL.3(DAS, b, d, e), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N*GEN),
                    weights = varPower(),
                    start = coef(modNaive1), control = list(msMaxIter = 200))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_nlmm_Interaction_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plots above, we see that the distribution of residuals has sensibly improved and we also see that the overall fit is good. Fixed effects and variance components for random effects are obtained as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(modnlme1)$tTable
##                      Value   Std.Error  DF     t-value       p-value
## b              -0.05776769 0.001366049 228 -42.2881409 6.822156e-110
## d.(Intercept)  34.03788426 1.727626178 228  19.7021119  3.982765e-51
## d.NLow         -3.44678488 1.841575492 228  -1.8716501  6.253555e-02
## d.GENB         18.93059593 2.166359447 228   8.7384372  5.195556e-16
## d.GENC         -1.33418290 1.906767185 228  -0.6997094  4.848221e-01
## d.NLow:GENB    -5.91014719 2.881736816 228  -2.0508976  4.142027e-02
## d.NLow:GENC    -5.52959860 2.551969245 228  -2.1667967  3.128677e-02
## e.(Intercept)  55.15777109 2.806817741 228  19.6513547  5.772338e-51
## e.NLow        -10.40726891 3.658304733 228  -2.8448338  4.847734e-03
## e.GENB         -4.39240147 3.588906465 228  -1.2238830  2.222596e-01
## e.GENC          2.66159912 3.628877450 228   0.7334497  4.640377e-01
## e.NLow:GENB    -3.86277932 5.132378567 228  -0.7526295  4.524490e-01
## e.NLow:GENC     3.99304928 5.182760610 228   0.7704483  4.418316e-01
VarCorr(modnlme1)
##               Variance                     StdDev    Corr  
## Block =       pdLogChol(list(d ~ 1,e ~ 1))                 
## d.(Intercept)  4.11920429                  2.0295823 d.(In)
## e.(Intercept)  2.49576526                  1.5797991 0.441 
## Plot =        pdLogChol(list(d ~ 1,e ~ 1))                 
## d.(Intercept)  2.74774204                  1.6576315 d.(In)
## e.(Intercept) 18.17957524                  4.2637513 0.187 
## Residual       0.07917494                  0.2813804&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s go back to our initial aim: testing the significance of the ‘genotype x nitrogen’ interaction. Indeed, we have two available tests: on for the parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and one for the parameter &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. First of all, we code two ‘reduced’ models, where the genotype and nitrogen effects are purely addictive. To do so, we change the specification of the fixed effects from ’~ N*GEN’ to ‘~ N + GEN’. Also in this case, we use a ‘naive’ nonlinear regression fit to get starting values for model parameters, to be used in the following NLME model fitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNaive2 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N + GEN,  ~ N * GEN))

modnlme2 &amp;lt;- nlme(Yield ~ nlsL.3(DAS, b, d, e), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N + GEN, e ~ N*GEN),
                    weights = varPower(),
                    start = coef(modNaive2), control = list(msMaxIter = 200))
modNaive3 &amp;lt;- drm(Yield ~ DAS, fct = L.3(), data = dataset,
            curveid = N:GEN,
            pmodels = c( ~ 1,  ~ N*GEN,  ~ N + GEN))

modnlme3 &amp;lt;- nlme(Yield ~ nlsL.3(DAS, b, d, e), data = dataset,
                    random = d + e ~ 1|Block/Plot,
                    fixed = list(b ~ 1, d ~ N*GEN, e ~ N + GEN),
                    weights = varPower(),
                    start = coef(modNaive3), control = list(msMaxIter = 200))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s consider the first reduced model ‘modnlme2’. In this model, the ‘genotype x nitrogen’ interaction has been removed for the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter. We can compare this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme2)
##          Model df      AIC      BIC    logLik   Test L.Ratio p-value
## modnlme1     1 21 1355.006 1430.101 -656.5032                       
## modnlme2     2 19 1356.277 1424.220 -659.1387 1 vs 2 5.27103  0.0717&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This test is on the borderline of significance, although the AIC value is slightly higher for the reduced model. It should be possible to conclude that the ‘genotype x nitrogen’ interaction is not significant and, therefore, the ranking of genotypes in terms of yield potential, as measured by the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; parameter should be independent on nitrogen level.&lt;/p&gt;
&lt;p&gt;Let’s now consider the second reduced model ‘modnlme3’. In this second model, the ‘genotype x nitrogen’ interaction has been removed for the ‘e’ parameter. We can compare also this reduced model with the full model ‘modnlme1’, by using a Likelihood Ratio Test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(modnlme1, modnlme3)
##          Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## modnlme1     1 21 1355.006 1430.101 -656.5032                        
## modnlme3     2 19 1353.721 1421.664 -657.8604 1 vs 2 2.714405  0.2574&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this second test, the lack of significance for the ‘genotype x nitrogen’ interaction seems to be less questionable than in the first one.&lt;/p&gt;
&lt;p&gt;I would like to conclude by drawing your attention to the ‘medrm’ function in the ‘medrc’ package, which can also be used to fit this type of nonlinear mixed-effects models.&lt;/p&gt;
&lt;p&gt;Happy coding with R!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Ritz, C., Baty, F., Streibig, J.C., Gerhard, D., 2015. Dose-Response Analysis Using R. PLOS ONE 10, e0146021. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0146021&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0146021&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting &#39;complex&#39; mixed models with &#39;nlme&#39;. Example #1</title>
      <link>/2019/stat_lmm_environmentalvariance/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_lmm_environmentalvariance/</guid>
      <description>


&lt;div id=&#34;the-environmental-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The environmental variance model&lt;/h1&gt;
&lt;p&gt;Fitting mixed models has become very common in biology and recent developments involve the manipulation of the variance-covariance matrix for random effects and residuals. To the best of my knowledge, within the frame of frequentist methods, the only freeware solution in R should be based on the ‘nlme’ package, as the ‘lmer’ package does not easily permit such manipulations. The ‘nlme’ package is fully described in Pinheiro and Bates (2000). Of course, the ‘asreml’ package can be used, but, unfortunately, this is not freeware.&lt;/p&gt;
&lt;p&gt;Coding mixed models in ‘nlme’ is not always easy, especially when we have crossed random effects, which is very common with agricultural experiments. I have been struggling with this issue very often in the last years and I thought it might be useful to publish a few examples in this blog, to save collegues from a few headaches. Please, note that I have already published other posts dealing with the use of the ‘lme()’ function in the ‘nlme’ package, for example &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence2/&#34;&gt;this post here&lt;/a&gt; about the correlation in designed experiments and &lt;a href=&#34;https://www.statforbiology.com/2019/stat_lmm_stabilityvariance/&#34;&gt;this other post here&lt;/a&gt;, about heteroscedastic multienvironment experiments.&lt;/p&gt;
&lt;p&gt;The first example in this series relates to a randomised complete block design with three replicates, comparing winter wheat genotypes. The experiment was repeated in seven years in the same location. The dataset (‘WinterWheat’) is available in the ‘aomisc’ package, which is the companion package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;. Please, note that this dataset shows the data for eight genotypes, but the model that we want to fit requires that the number of environments is higher than the number of genotypes. Therefore, we have to make a subset, at the beginning, removing a couple of genotypes.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset, subsets it and turns the ‘Genotype’, ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
data(WinterWheat)
WinterWheat &amp;lt;- WinterWheat[WinterWheat$Genotype != &amp;quot;SIMETO&amp;quot; &amp;amp; WinterWheat$Genotype != &amp;quot;SOLEX&amp;quot;,]
WinterWheat$Genotype &amp;lt;- factor(WinterWheat$Genotype)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996
## 11   91     2   DUILIO  5.57 1996
## 12  138     2   GRAZIA  6.09 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the so-called ‘environmental variance model’. This model is often used in stability analyses for multi-environment experiments and I will closely follow the coding proposed in Piepho (1999):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + g_i + r_{jk}  +  h_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(g_i\)&lt;/span&gt; is the effect for the i-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; is the effect for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(h_{ij}\)&lt;/span&gt; is a random deviation from the expected yield for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; is the residual variability of yield between plots, within each environment and block.&lt;/p&gt;
&lt;p&gt;We usually assume that &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ijk}\)&lt;/span&gt; are independent and normally distributed, with variances equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_r\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;, respectively. Such an assumption may be questioned, but we will not do it now, for the sake of simplicity.&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(h_{ij}\)&lt;/span&gt;, which we will assume as normally distributed with variance-covariance matrix equal to &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;. In particular, it is reasonable to expect that the genotypes will have different variances across environments (heteroscedasticity), which can be interpreted as static stability measures (‘environmental variances’; hence the name ‘environmental variance model’). Furthermore, it is reasonable that, if an environment is good for one genotype, it may also be good for other genotypes, so that yields in each environment are correlated, although the correlations can be different for each couple of genotypes. To reflect our expectations, the &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; matrix needs to be totally unstructured, with the only constraint that it is positive definite.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown how the above model can be coded by using SAS and I translated his code into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EnvVarMod &amp;lt;- lme(Yield ~ Genotype, 
  random = list(Year = pdSymm(~Genotype - 1), 
              Year = pdIdent(~Block - 1)),
  control = list(opt = &amp;quot;optim&amp;quot;, maxIter = 100),
  data=WinterWheat)
VarCorr(EnvVarMod)
##                  Variance             StdDev    Corr                
## Year =           pdSymm(Genotype - 1)                               
## GenotypeCOLOSSEO 0.48876512           0.6991174 GCOLOS GCRESO GDUILI
## GenotypeCRESO    0.70949309           0.8423141 0.969               
## GenotypeDUILIO   2.37438440           1.5409038 0.840  0.840        
## GenotypeGRAZIA   1.18078525           1.0866394 0.844  0.763  0.942 
## GenotypeIRIDE    1.23555204           1.1115539 0.857  0.885  0.970 
## GenotypeSANCARLO 0.93335518           0.9661031 0.928  0.941  0.962 
## Year =           pdIdent(Block - 1)                                 
## Block1           0.02748257           0.1657787                     
## Block2           0.02748257           0.1657787                     
## Block3           0.02748257           0.1657787                     
## Residual         0.12990355           0.3604214                     
##                               
## Year =                        
## GenotypeCOLOSSEO GGRAZI GIRIDE
## GenotypeCRESO                 
## GenotypeDUILIO                
## GenotypeGRAZIA                
## GenotypeIRIDE    0.896        
## GenotypeSANCARLO 0.884  0.942 
## Year =                        
## Block1                        
## Block2                        
## Block3                        
## Residual&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I coded the random effects as a list, by using the ‘Year’ as the nesting factor (Galecki and Burzykowski, 2013). In order to specify a totally unstructured variance-covariance matrix for the genotypes within years, I used the ‘pdMat’ construct ‘pdSymm()’. This model is rather complex and may take long to converge.&lt;/p&gt;
&lt;p&gt;The environmental variances are retrieved by the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;envVar &amp;lt;- as.numeric ( VarCorr(EnvVarMod)[2:7,1] )
envVar
## [1] 0.4887651 0.7094931 2.3743844 1.1807853 1.2355520 0.9333552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the correlations are given by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(EnvVarMod)[2:7,3:7]
##                  Corr                                        
## GenotypeCOLOSSEO &amp;quot;GCOLOS&amp;quot; &amp;quot;GCRESO&amp;quot; &amp;quot;GDUILI&amp;quot; &amp;quot;GGRAZI&amp;quot; &amp;quot;GIRIDE&amp;quot;
## GenotypeCRESO    &amp;quot;0.969&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeDUILIO   &amp;quot;0.840&amp;quot;  &amp;quot;0.840&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeGRAZIA   &amp;quot;0.844&amp;quot;  &amp;quot;0.763&amp;quot;  &amp;quot;0.942&amp;quot;  &amp;quot;&amp;quot;       &amp;quot;&amp;quot;      
## GenotypeIRIDE    &amp;quot;0.857&amp;quot;  &amp;quot;0.885&amp;quot;  &amp;quot;0.970&amp;quot;  &amp;quot;0.896&amp;quot;  &amp;quot;&amp;quot;      
## GenotypeSANCARLO &amp;quot;0.928&amp;quot;  &amp;quot;0.941&amp;quot;  &amp;quot;0.962&amp;quot;  &amp;quot;0.884&amp;quot;  &amp;quot;0.942&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unweighted-two-stage-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unweighted two-stage fitting&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit an environmental variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retrieved the means for genotypes in all years, we can fit the following model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \mu + g_i + a_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; is the mean yield for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment and &lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt; is the residual term, which includes the genotype x environment random interaction, the block x environment random interaction and the residual error term.&lt;/p&gt;
&lt;p&gt;In this model we have only one random effect (&lt;span class=&#34;math inline&#34;&gt;\(a_{ij}\)&lt;/span&gt;) and, therefore, this is a fixed linear model. However, we need to model the variance-covariance matrix of residuals (&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;), by adopting a totally unstructured form. Please, note that, when working with raw data, we have modelled &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;, i.e. the variance-covariance matrix for the random effects. I have used the ‘gls()’ function, together with the ‘weights’ and ‘correlation’ arguments. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
envVarModM &amp;lt;- gls(Yield ~ Genotype, 
  data = WinterWheatM,
  weights = varIdent(form=~1|Genotype),
  correlation = corSymm(form=~1|Year))
summary(envVarModM)
## Generalized least squares fit by REML
##   Model: Yield ~ Genotype 
##   Data: WinterWheatM 
##       AIC      BIC   logLik
##   80.6022 123.3572 -13.3011
## 
## Correlation Structure: General
##  Formula: ~1 | Year 
##  Parameter estimate(s):
##  Correlation: 
##   1     2     3     4     5    
## 2 0.947                        
## 3 0.809 0.815                  
## 4 0.816 0.736 0.921            
## 5 0.817 0.866 0.952 0.869      
## 6 0.888 0.925 0.949 0.856 0.907
## Variance function:
##  Structure: Different standard deviations per stratum
##  Formula: ~1 | Genotype 
##  Parameter estimates:
## COLOSSEO    CRESO   DUILIO   GRAZIA    IRIDE SANCARLO 
## 1.000000 1.189653 2.143713 1.528848 1.560620 1.356423 
## 
## Coefficients:
##                      Value Std.Error   t-value p-value
## (Intercept)       6.413333 0.2742314 23.386574  0.0000
## GenotypeCRESO    -0.439524 0.1107463 -3.968746  0.0003
## GenotypeDUILIO    0.178571 0.3999797  0.446451  0.6579
## GenotypeGRAZIA   -0.330952 0.2518270 -1.314205  0.1971
## GenotypeIRIDE     0.281905 0.2580726  1.092347  0.2819
## GenotypeSANCARLO -0.192857 0.1802547 -1.069915  0.2918
## 
##  Correlation: 
##                  (Intr) GCRESO GDUILI GGRAZI GIRIDE
## GenotypeCRESO     0.312                            
## GenotypeDUILIO    0.503  0.371                     
## GenotypeGRAZIA    0.269 -0.095  0.774              
## GenotypeIRIDE     0.292  0.545  0.857  0.638       
## GenotypeSANCARLO  0.310  0.612  0.856  0.537  0.713
## 
## Standardized residuals:
##        Min         Q1        Med         Q3        Max 
## -2.0949678 -0.5680656  0.1735444  0.7599596  1.3395000 
## 
## Residual standard error: 0.7255481 
## Degrees of freedom: 42 total; 36 residual&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance-covariance matrix for residuals can be obtained using the ‘getVarCov()’ function in the ‘nlme’ package, although I had to discover that there is a small buglet there, which causes problems in some instances (such as here). Please, &lt;a href=&#34;https://www.jepusto.com/bug-in-nlme-getvarcov/&#34;&gt;see this link&lt;/a&gt;; I have included the correct code in the ‘getVarCov.gls()’ function in the ‘aomisc’ package, that is the companion package for this blog.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- getVarCov.gls(envVarModM)
R
## Marginal variance covariance matrix
##         [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
## [1,] 0.52642 0.59280 0.91285 0.65647 0.67116 0.63376
## [2,] 0.59280 0.74503 1.09440 0.70422 0.84652 0.78560
## [3,] 0.91285 1.09440 2.41920 1.58850 1.67700 1.45230
## [4,] 0.65647 0.70422 1.58850 1.23040 1.09160 0.93442
## [5,] 0.67116 0.84652 1.67700 1.09160 1.28210 1.01070
## [6,] 0.63376 0.78560 1.45230 0.93442 1.01070 0.96855
##   Standard Deviations: 0.72555 0.86315 1.5554 1.1093 1.1323 0.98415&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the design is perfectly balanced, the diagonal elements of the above matrix correspond to the variances of genotypes across environments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(WinterWheatM$Yield, WinterWheatM$Genotype, var)
##  COLOSSEO     CRESO    DUILIO    GRAZIA     IRIDE  SANCARLO 
## 0.5264185 0.7450275 2.4191624 1.2304397 1.2821143 0.9685497&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can also be retreived by the ‘stability’ package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stability)
## Registered S3 methods overwritten by &amp;#39;lme4&amp;#39;:
##   method                          from
##   cooks.distance.influence.merMod car 
##   influence.merMod                car 
##   dfbeta.influence.merMod         car 
##   dfbetas.influence.merMod        car
envVarStab &amp;lt;-
  stab_measures(
    .data = WinterWheatM,
    .y = Yield,
    .gen = Genotype,
    .env = Year
  )

envVarStab$StabMeasures
## # A tibble: 6 x 7
##   Genotype  Mean GenSS   Var    CV  Ecov ShuklaVar
##   &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 COLOSSEO  6.41  3.16 0.526  11.3 1.25     0.258 
## 2 CRESO     5.97  4.47 0.745  14.4 1.01     0.198 
## 3 DUILIO    6.59 14.5  2.42   23.6 2.31     0.522 
## 4 GRAZIA    6.08  7.38 1.23   18.2 1.05     0.208 
## 5 IRIDE     6.70  7.69 1.28   16.9 0.614    0.0989
## 6 SANCARLO  6.22  5.81 0.969  15.8 0.320    0.0254&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Strictly speaking, those variances are not the environmental variances, as they also contain the within-experiment and within block random variability, which needs to be separately estimated during the first step.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Muhammad Yaseen, Kent M. Eskridge and Ghulam Murtaza (2018). stability: Stability Analysis of Genotype by Environment Interaction (GEI). R package version 0.5.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stability&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Germination data and time-to-event methods: comparing germination curves</title>
      <link>/2019/stat_survival_comparinglots/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_survival_comparinglots/</guid>
      <description>


&lt;p&gt;Very often, seed scientists need to compare the germination behaviour of different seed populations, e.g., different plant species, or one single plant species submitted to different temperatures, light conditions, priming treatments and so on. How should such a comparison be performed?&lt;/p&gt;
&lt;p&gt;Let’s take a practical approach and start from an appropriate example: a few years ago, some collegues studied the germination behaviour for seeds of a plant species (&lt;em&gt;Verbascum arcturus&lt;/em&gt;, BTW…), in different conditions. In detail, they considered the factorial combination of two storage periods (LONG and SHORT storage) and two temperature regimes (FIX: constant daily temperature of 20°C; ALT: alternating daily temperature regime, with 25°C during daytime and 15°C during night time, with a 12:12h photoperiod). If you are a seed scientist and are interested in this experiment, you’ll find detail in Catara &lt;em&gt;et al.&lt;/em&gt; (2016).&lt;/p&gt;
&lt;p&gt;If you are not a seed scientist you may wonder why my colleagues made such an assay; well, there is evidence that, for some plant species, the germination ability improves over time after seed maturation. Therefore, if we take seeds and store them for a different period of time, there might be an effect on their germination traits. Likewise, there is also evidence that some seeds may not germinate if they are not submitted to daily temperature fluctuations. These mechanisms are very interesting, as they permit to the seed to recognise that the environmental conditions are favourable for seedling survival.My colleagues wanted to discover whether this was the case for Verbascum.&lt;/p&gt;
&lt;p&gt;Let’s go back to our assay: the experimental design consisted of four combinations (LONG-FIX, LONG-ALT, SHORT-FIX and SHORT-ALT) and four replicates for each combination. One replicate consisted of a Petri dish, that is a small plastic box containing humid blotting paper, with 25 seeds of Verbascum. In all, there were 16 Petri dishes, put in climatic chambers with the appropriate conditions. During the assay, my collegues made daily inspections: germinated seeds were counted and removed from the dishes. Inspections were made for 15 days, until no more germinations could be observed.&lt;/p&gt;
&lt;p&gt;The dataset is available from a gitHub repository: let’s load it and have a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/TempStorage.csv&amp;quot;, header = T, check.names = F)
head(dataset)
##   Dish Storage Temp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
## 1    1     Low  Fix 0 0 0 0 0 0 0 0 3  4  6  0  1  0  3
## 2    2     Low  Fix 0 0 0 0 1 0 0 0 2  7  2  3  0  5  1
## 3    3     Low  Fix 0 0 0 0 1 0 0 1 3  5  2  4  0  1  3
## 4    4     Low  Fix 0 0 0 0 1 0 3 0 0  3  1  1  0  4  4
## 5    5    High  Fix 0 0 0 0 0 0 0 0 1  2  5  4  2  3  0
## 6    6    High  Fix 0 0 0 0 0 0 0 0 2  2  7  8  1  2  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have one row per Petri dish; the first three columns show, respectively, dish number, storage and temperature conditions. The next 15 columns represent the inspection times (from 1 to 15) and contain the counts of germinated seeds. The research question is:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is germination behaviour affected by storage and temperature conditions?&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;response-feature-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Response feature analyses&lt;/h1&gt;
&lt;p&gt;One possible line of attack is to take a summary measure for each dish, e.g. the total number of germinated seeds. Taking a single value for each dish brings us back to more common methods of data analysis: for example, we can fit some sort of GLM to test the significance of effects (storage, temperature and their interaction), within a fully factorial design.&lt;/p&gt;
&lt;p&gt;Although the above method is not wrong, undoubtedly, it may be sub-optimal. Indeed, dishes may contain the same total number of germinated seeds, but, nonetheless, they may differ for some other germination traits, such as velocity or uniformity. Indeed, we do not want to express a judgment about one specific characteristic of the seed lot, we would like to express a judgment about the whole seed lot. In other words, we are not specifically asking: “do the seed lots differ for their germination capability?”. We are, more generally, asking “are the seed lots different?”.&lt;/p&gt;
&lt;p&gt;In order to get a general assessment, a different method of analysis should be sought, which considers the entire time series (from 1 to 15 days) and not only one single summary measure. This method exists and it is available within the time-to-event platform, which has shown very useful and appropriate for seed germination studies (Onofri et al., 2011; Ritz et al., 2013; Onofri et al., 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-germination-time-course&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The germination time-course&lt;/h1&gt;
&lt;p&gt;It is necessary to re-organise the dataset in a more useful way. A good format can be obtained by using the ‘makeDrm()’ function in the ‘drcSeedGerm’ package, which can be installed from gitHub (see the code at: &lt;a href=&#34;https://www.statforbiology.com/seedgermination/index.html&#34;&gt;this link&lt;/a&gt;). The function needs to receive a dataframe storing the counts (dataset[,4:18]), a dataframe storing the factor variables (dataset[,2:3]), a vector with the number of seeds in each Petri dish (rep(25, 16)) and a vector of monitoring times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drcSeedGerm)
library(dplyr)
datasetR &amp;lt;- makeDrm(dataset[,4:18], dataset[,2:3], rep(25, 16), 1:15)
head(datasetR, 16)
##      Storage Temp Dish timeBef timeAf count nCum propCum
## 1        Low  Fix    1       0      1     0    0    0.00
## 1.1      Low  Fix    1       1      2     0    0    0.00
## 1.2      Low  Fix    1       2      3     0    0    0.00
## 1.3      Low  Fix    1       3      4     0    0    0.00
## 1.4      Low  Fix    1       4      5     0    0    0.00
## 1.5      Low  Fix    1       5      6     0    0    0.00
## 1.6      Low  Fix    1       6      7     0    0    0.00
## 1.7      Low  Fix    1       7      8     0    0    0.00
## 1.8      Low  Fix    1       8      9     3    3    0.12
## 1.9      Low  Fix    1       9     10     4    7    0.28
## 1.10     Low  Fix    1      10     11     6   13    0.52
## 1.11     Low  Fix    1      11     12     0   13    0.52
## 1.12     Low  Fix    1      12     13     1   14    0.56
## 1.13     Low  Fix    1      13     14     0   14    0.56
## 1.14     Low  Fix    1      14     15     3   17    0.68
## 1.15     Low  Fix    1      15    Inf     8   NA      NA
datasetR &amp;lt;- datasetR %&amp;gt;% 
  mutate(across(c(Storage, Temp, Dish), .fns = factor))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The snippet above shows the first dish. Roughly speaking, we have gone from a WIDE format to a LONG format. The column ‘timeAf’ contains the time when the inspection was made and the column ‘count’ contains the number of germinated seeds (e.g. 9 seeds were counted at day 9). These seeds did not germinate exactly at day 9; they germinated within the interval between two inspections, that is between day 8 and day 9. The beginning of the interval is given in the variable ‘timeBef’. Apart from these columns, we have additional columns, which we are not going to use for our analyses. The cumulative counts of germinated seeds are in the column ‘nCum’; these cumulative counts have been converted into cumulative proportions by dividing by 25 (i.e., the total number of seeds in a dish; see the column ‘propCum’).&lt;/p&gt;
&lt;p&gt;We can use a time-to-event model to parameterise the germination time-course for this dish. This is easily done by using the ‘drm()’ function in the ‘drc’ package (Ritz et al., 2013):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modPre &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(), 
              data = datasetR, 
              type = &amp;quot;event&amp;quot;, subset = c(Dish == 1))
plot(modPre, log = &amp;quot;&amp;quot;, xlab = &amp;quot;Time&amp;quot;, 
     ylab = &amp;quot;Proportion of germinated seeds&amp;quot;,
     xlim = c(0, 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_ComparingLots_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Please, note the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;we are using the counts (‘count’) as the dependent variable&lt;/li&gt;
&lt;li&gt;as the independent variable: we are using the extremes of the inspection interval, within which germinations were observed (count ~ timeBef + time Af)&lt;/li&gt;
&lt;li&gt;we have assumed a log-logistic distribution of germination times (fct = LL.3()). A three parameter model is necessary, because there is a final fraction of ungerminated seeds (truncated distribution)&lt;/li&gt;
&lt;li&gt;we have set the argument ‘type = “event”’. Indeed, we are fitting a time-to-event model, not a nonlinear regression model, which would be incorrect, in this setting (see &lt;a href=&#34;https://www.statforbiology.com/seedgermination/timetoevent&#34;&gt;this link here&lt;/a&gt; ).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As we have determined the germination time-course for dish 1, we can do the same for all dishes. However, we have to instruct ‘drm()’ to define a different curve for each combination of storage and temperature. It is necessary to make an appropriate use of the ‘curveid’ argument. Please, see below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(),
            data = datasetR, type = &amp;quot;event&amp;quot;, 
            curveid = Temp:Storage)
plot(mod1, log = &amp;quot;&amp;quot;, legendPos = c(2, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_ComparingLots_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears that there are visible differences between the curves (the legend considers the curves in alphabetical order, i.e. 1: Fix-Low, 2: Fix-High, 3: Alt-Low and 4: Alt-High). We can test that the curves are similar by coding a reduced model, where we have only one pooled curve for all treatment levels. It is enough to remove the ‘curveid’ argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;modNull &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(),
               data = datasetR, 
               type = &amp;quot;event&amp;quot;)
anova(mod1, modNull, test = &amp;quot;Chisq&amp;quot;)
## 
## 1st model
##  fct:      LL.3()
##  pmodels: Temp:Storage (for all parameters)
## 2nd model
##  fct:      LL.3()
##  pmodels: 1 (for all parameters)
## ANOVA-like table
## 
##           ModelDf  Loglik Df LR value p value
## 1st model     244 -753.54                    
## 2nd model     253 -854.93  9   202.77       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can compare the full model (four curves) with the reduced model (one common curve) by using a Likelihood Ratio Test, which is approximately distributed as a Chi-square. The test is highly significant. Of course, we can also test some other hypotheses. For example, we can code a model with different curves for storage times, assuming that the effect of temperature is irrelevant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(), 
            data = datasetR, type = &amp;quot;event&amp;quot;, 
            curveid = Storage)
anova(mod1, mod2, test = &amp;quot;Chisq&amp;quot;)
## 
## 1st model
##  fct:      LL.3()
##  pmodels: Temp:Storage (for all parameters)
## 2nd model
##  fct:      LL.3()
##  pmodels: Storage (for all parameters)
## ANOVA-like table
## 
##           ModelDf  Loglik Df LR value p value
## 1st model     244 -753.54                    
## 2nd model     250 -797.26  6   87.436       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that such an assumption (temperature effect is irrelevant) is not supported by the data: the temperature effect cannot be removed without causing a significant decrease in the likelihood of the model. Similarly, we can test the effect of storage:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod3 &amp;lt;- drm(count ~ timeBef + timeAf, fct = LL.3(), 
            data = datasetR, type = &amp;quot;event&amp;quot;, 
            curveid = Temp)
anova(mod1, mod3, test = &amp;quot;Chisq&amp;quot;)
## 
## 1st model
##  fct:      LL.3()
##  pmodels: Temp:Storage (for all parameters)
## 2nd model
##  fct:      LL.3()
##  pmodels: Temp (for all parameters)
## ANOVA-like table
## 
##           ModelDf  Loglik Df LR value p value
## 1st model     244 -753.54                    
## 2nd model     250 -849.48  6   191.87       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we get significant results. So, we need to conclude that temperature and storage time caused a significant influence on the germination behavior for the species under study.&lt;/p&gt;
&lt;p&gt;Before concluding, it is necessary to mention that, in general, the above LR tests should be taken with care: the results are only approximate and the observed data are not totally independent, as multiple observations are taken in each experimental unit (Petri dish). In order to restore independence, we would need to add to the model a random effect for the Petri dish, which is not an easy task in a time-to-event framework (Onofri et al., 2019). However, we got very low p-levels, which leave us rather confident about the significance of effects. It may be a good suggestion, in general, to avoid formal hypothesis testing and compare the models by using the Akaike Information Criterion (AIC: the lowest is the best), which confirms that the complete model with four curves is, indeed, the best one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AIC(mod1, mod2, mod3, modNull)
##          df      AIC
## mod1    244 1995.088
## mod2    250 2094.524
## mod3    250 2198.961
## modNull 253 2215.862&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For those who are familiar with linear model parameterisation, it is possible to reach even a higher degree of flexibility by using the ‘pmodels’ argument, within the ‘drm()’ function. However, this will require another post. Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Catara, S., Cristaudo, A., Gualtieri, A., Galesi, R., Impelluso, C., Onofri, A., 2016. Threshold temperatures for seed germination in nine species of Verbascum (Scrophulariaceae). Seed Science Research 26, 30–46.&lt;/li&gt;
&lt;li&gt;Onofri, A., Mesgaran, M.B., Tei, F., Cousens, R.D., 2011. The cure model: an improved way to describe seed germination? Weed Research 51, 516–524.&lt;/li&gt;
&lt;li&gt;Onofri, A., Piepho, H.-P., Kozak, M., 2019. Analysing censored data in agricultural research: A review with examples and software tips. Annals of Applied Biology 174, 3–13. &lt;a href=&#34;https://doi.org/10.1111/aab.12477&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/aab.12477&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ritz, C., Pipper, C.B., Streibig, J.C., 2013. Analysis of germination data from agricultural experiments. European Journal of Agronomy 45, 1–6.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Survival analysis and germination data: an overlooked connection</title>
      <link>/2019/stat_survival_germination/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_survival_germination/</guid>
      <description>


&lt;div id=&#34;the-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The background&lt;/h1&gt;
&lt;p&gt;Seed germination data describe the time until an event of interest occurs. In this sense, they are very similar to survival data, apart from the fact that we deal with a different (and less sad) event: germination instead of death. But, seed germination data are also similar to failure-time data, phenological data, time-to-remission data… the first point is: &lt;strong&gt;germination data are time-to-event data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You may wonder: what’s the matter with time-to-event data? Do they have anything special? With few exceptions, all time-to-event data are affected by a certain form of uncertainty, which takes the name of ‘censoring’. It relates to the fact that the exact time of event may not be precisely know. I think it is good to give an example.&lt;/p&gt;
&lt;p&gt;Let’s take a germination assay, where we put, say, 100 seeds in a Petri dish and make daily inspections. At each inspection, we count the number of germinated seeds. In the end, what have we learnt about the germination time of each seed? It is easy to note that we do not have a precise value, we only have an uncertainty interval. Let’s make three examples.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we found a germinated seed at the first inspection time, we only know that germination took place before the inspection (left-censoring).&lt;/li&gt;
&lt;li&gt;If we find a germinated seed at the second inspection time, we only know that germination took place somewhere between the first and the second inspection (interval-censoring).&lt;/li&gt;
&lt;li&gt;If we find an ungerminated seed at the end of the experiment, we only know that its germination time, if any, is higher than the duration of the experiment (right-censoring).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Censoring implies a lot of uncertainty, which is additional to other more common sources of uncertainty, such as the individual seed-to-seed variability or random errors in the manipulation process. Is censoring a problem? Yes, it is, although it is usually overlooked in seed science research. I made this point in a recent review (Onofri et al., 2019) and I would like to come back to this issue here. The second point is that &lt;strong&gt;the analyses of data from germination assays should always account for censoring&lt;/strong&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analyses-for-germination-assays&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data analyses for germination assays&lt;/h1&gt;
&lt;p&gt;A swift search of literature shows that seed scientists are often interested in describing the time-course of germinations, for different plant species, in different environmental conditions. In simple terms, if we take a population of seeds and give it enough water, the individual seeds will start germinating. Their germination times will be different, due to natural seed-to-seed variability and, therefore, the proportion of germinated seeds will progressively and monotonically increase over time. However, this proportion will almost never reach 1, because, there will often be a fraction of seeds that will not germinate in the given conditions, because it is either dormant or nonviable.&lt;/p&gt;
&lt;p&gt;In order to describe this progress to germination, a log-logistic function is often used:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
G(t) = \frac{d}{ 1 + exp \left\{ - b \right[ \log(t) - \log(e) \left] \right\}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt; is the fraction of germinated seeds at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the germinable fraction, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the median germination time for the germinable fraction and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The above model is sygmoidally shaped and it is symmetric on a log-time scale. The three parameters are biologically relevant, as they describe the three main features of seed germination, i.e. capability (&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;), speed (&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;) and uniformity (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;My third point in this post is that &lt;strong&gt;The process of data analysis for germination data is often based on fitting a log-logistic (or similar) model to the observed counts&lt;/strong&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-example-a-simulated-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivating example: a simulated dataset&lt;/h1&gt;
&lt;p&gt;Considering the above, we can simulate the results of a germination assay. Let’s take a 100-seed-sample from a population where we have 85% of germinable seeds (&lt;span class=&#34;math inline&#34;&gt;\(d = 0.85\)&lt;/span&gt;), with a median germination time &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt; days and &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt;. Obviously, this sample will not necessarily reflect the characteristics of the population. We can do this sampling in R, by using a three-steps approach.&lt;/p&gt;
&lt;div id=&#34;step-1-the-ungerminated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: the ungerminated fraction&lt;/h2&gt;
&lt;p&gt;First, let’s simulate the number of germinated seeds, assuming a binomial distribution with a proportion of successes equal to 0.85. We use the random number generator ‘rbinom()’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 1
d &amp;lt;- 0.85
set.seed(1234)
nGerm &amp;lt;- rbinom(1, 100, d)
nGerm
## [1] 89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, in this instance, 89 seeds germinated out of 100, which is not the expected 85%. This is a typical random fluctuation: indeed, we were lucky in selecting a good sample of seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-germination-times-for-the-germinated-fraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: germination times for the germinated fraction&lt;/h2&gt;
&lt;p&gt;Second, let’s simulate the germination times for these 89 germinable seeds, by drawing from a log-logistic distribution with &lt;span class=&#34;math inline&#34;&gt;\(b = 1.6\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e = 4.5\)&lt;/span&gt;. To this aim, we use the ‘rllogis()’ function in the ‘actuar’ package (Dutang et al., 2008):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Monte Carlo simulation - Step 2
library(actuar)
b &amp;lt;- 1.6; e &amp;lt;- 4.5 
Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))
Gtimes
##   [1]  3.2936708  3.4089762  3.2842199  1.4401630  3.1381457 82.1611955
##   [7]  9.4906364  2.9226745  4.3424551  2.7006042  4.0202158  8.0519663
##  [13]  0.9492013  7.8199588  1.6163588  7.9661485  8.4641154 11.2879041
##  [19]  9.5014360  7.2786264  7.5809838 12.7421713 32.7999661  9.9691944
##  [25]  1.8137333  4.2197542  1.0218849  1.6604417 30.0352308  5.0235265
##  [31]  8.5085067  7.5367739  4.4185382 11.5555259  2.1919263 10.6509339
##  [37]  8.6857151  0.2185902  1.8377033  3.9362727  3.0864702  7.3804164
##  [43]  3.2978782  7.0100360  4.4775843  2.8328842  4.6721090  9.1258796
##  [49]  2.1485568 21.8749808  7.4265984  2.5148724  4.4491466 13.1132301
##  [55]  4.4559642  4.5684584  2.2556488 11.8783556  1.5338755  1.4106592
##  [61] 31.8419420  7.2666641 65.0154287  9.2798476  2.5988399  7.4612907
##  [67]  4.4048509 27.7439121  3.8257187 15.4967751  1.1960785 62.5152642
##  [73]  2.0169970 19.1134899  4.2891084  6.0420938 22.6521417  7.1946293
##  [79]  2.9028993  0.9241876  4.8277336 13.8068124  4.0273655 10.8651761
##  [85]  1.1509735  5.9593534  7.4009589 12.6839405  1.1698335        Inf
##  [91]        Inf        Inf        Inf        Inf        Inf        Inf
##  [97]        Inf        Inf        Inf        Inf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we have a vector hosting 100 germination times (‘Gtimes’). Please, note that I added 11 infinite germination times, to represent non-germinable seeds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-counts-of-germinated-seeds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: counts of germinated seeds&lt;/h2&gt;
&lt;p&gt;Unfortunately, due to the monitoring schedule, we cannot observe the exact germination time for each single seed in the sample; we can only count the seeds which have germinated between two assessment times. Therefore, as the third step, we simulate the observed counts, by assuming daily monitoring for 40 days.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obsT &amp;lt;- seq(1, 40, by=1) #Observation schedule
count &amp;lt;- table( cut(Gtimes, breaks = c(0, obsT)) )
count
## 
##   (0,1]   (1,2]   (2,3]   (3,4]   (4,5]   (5,6]   (6,7]   (7,8]   (8,9] 
##       3      11      10       8      13       2       1      12       4 
##  (9,10] (10,11] (11,12] (12,13] (13,14] (14,15] (15,16] (16,17] (17,18] 
##       5       2       3       2       2       0       1       0       0 
## (18,19] (19,20] (20,21] (21,22] (22,23] (23,24] (24,25] (25,26] (26,27] 
##       0       1       0       1       1       0       0       0       0 
## (27,28] (28,29] (29,30] (30,31] (31,32] (32,33] (33,34] (34,35] (35,36] 
##       1       0       0       1       1       1       0       0       0 
## (36,37] (37,38] (38,39] (39,40] 
##       0       0       0       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that, e.g., 11 germinated seeds were counted at day 2; therefore they germinated between day 1 and day 2 and their real germination time is unknown, but included in the range between 1 and 2 (left-open and right-closed). This is a typical example of interval-censoring (see above).&lt;/p&gt;
&lt;p&gt;We can also see that, in total, we counted 86 germinated seeds and, therefore, 14 seeds were still ungerminated at the end of the assay. For this simulation exercise, we know that 11 seeds were non-germinable and three seeds were germinable, but were not allowed enough time to germinate (look at the table above: there are 3 seeds with germination times higher than 40). In real life, this is another source of uncertainty: we might be able to ascertain whether these 14 seeds are viable or not (e.g. by using a tetrazolium test), but, if they are viable, we would never be able to tell whether they are dormant or their germination time is simply longer than the duration of the assay. In real life, we can only reach an uncertain conclusion: the germination time of the 14 ungerminated seeds is comprised between 40 days to infinity; this is an example of right-censoring.&lt;/p&gt;
&lt;p&gt;The above uncertainty affects our capability of describing the germination time-course from the observed data. We can try to picture the situation in the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_Germination_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the real germination time-course? The red one? The blue one? Something in between? We cannot really say this from our dataset, we are uncertain. The grey areas represent the uncertainty due to censoring. Do you think that we can reasonably neglect it?&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model fitting&lt;/h1&gt;
&lt;p&gt;The question is: how do we fit a log-logistic function to this dataset? We can either neglect censoring or account for it. Let’s see the differences.&lt;/p&gt;
&lt;div id=&#34;ignoring-censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ignoring censoring&lt;/h2&gt;
&lt;p&gt;We have seen that the time-corse of germination can be described by using a log-logistic cumulative distribution function. Therefore, seed scientists are used to re-organising their data, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
propCum &amp;lt;- cumsum(counts)/100
df &amp;lt;- data.frame(time = obsT, counts = counts, propCum = propCum) 
df
##    time counts propCum
## 1     1      3    0.03
## 2     2     11    0.14
## 3     3     10    0.24
## 4     4      8    0.32
## 5     5     13    0.45
## 6     6      2    0.47
## 7     7      1    0.48
## 8     8     12    0.60
## 9     9      4    0.64
## 10   10      5    0.69
## 11   11      2    0.71
## 12   12      3    0.74
## 13   13      2    0.76
## 14   14      2    0.78
## 15   15      0    0.78
## 16   16      1    0.79
## 17   17      0    0.79
## 18   18      0    0.79
## 19   19      0    0.79
## 20   20      1    0.80
## 21   21      0    0.80
## 22   22      1    0.81
## 23   23      1    0.82
## 24   24      0    0.82
## 25   25      0    0.82
## 26   26      0    0.82
## 27   27      0    0.82
## 28   28      1    0.83
## 29   29      0    0.83
## 30   30      0    0.83
## 31   31      1    0.84
## 32   32      1    0.85
## 33   33      1    0.86
## 34   34      0    0.86
## 35   35      0    0.86
## 36   36      0    0.86
## 37   37      0    0.86
## 38   38      0    0.86
## 39   39      0    0.86
## 40   40      0    0.86&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In practice, seed scientists often use the observed counts to determine the cumulative proportion (or percentage) of germinated seeds. The cumulative proportion (‘propCum’) is used as the response variable, while the observation time (‘time’) is used as the independent variable and a log-logistic function is fitted by non-linear least squares regression. &lt;strong&gt;Hope that you can clearly see that, by doing so, we totally neglect the grey areas in the figure above, we only look at the observed points&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can fit a nonlinear regression model by using the ‘drm’ function, in the ‘drc’ package (Ritz et al., 2015). The argument ‘fct’ is used to set the fitted function to log-logistic with three parameters (the equation above).&lt;/p&gt;
&lt;p&gt;The ‘plot()’ and ‘summary()’ methods can be used to plot a graph and to retrieve the estimated parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
mod &amp;lt;- drm(propCum ~ time, data = df,
           fct = LL.3() )
plot(mod, log = &amp;quot;&amp;quot;,
      xlab = &amp;quot;Time (days)&amp;quot;,
      ylab = &amp;quot;Proportion of germinated seeds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_Survival_Germination_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.8497771  0.0702626 -26.327 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.8768793  0.0070126 125.044 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.2691575  0.1020457  51.635 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.01762168 (37 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that our estimates are very close to the real values (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; = 1.85 vs. 1.6; &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; = 5.27 vs. 4.5; &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; = 0.88 vs. 0.86) and we also see that standard errors are rather small (the coefficient of variability goes from 1 to 4%). There is a difference in sign for &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, which relates to the fact that the ‘LL.3()’ function in ‘drc’ removes the minus sign in the equation above. Please, disregard this aspect, which stems from the fact that the ‘drc’ package is rooted in pesticide bioassays.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting-for-censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accounting for censoring&lt;/h2&gt;
&lt;p&gt;So far, we have totally neglected censoring. But, how can we account for it? The answer is simple: as with survival studies, we should use time-to-event methods, which are specifically devised to incorporate the uncertainty due to censoring. In medicine, the body of time-to-event methods goes under the name of survival analysis, which explains the title of my post. My colleagues and I have extensively talked about these methods in two of our recent papers and related appendices (Onofri et al., 2019; Onofri et al., 2018). Therefore, I will not go into detail, now.&lt;/p&gt;
&lt;p&gt;I will just say that time-to-event methods directly consider the observed counts as the response variable. As the independent variable, they consider the extremes of each time interval (‘timeBef’ and ‘timeAf’; see below). In contrast to nonlinear regression, we do not need to transform the observed counts into cumulative proportions, as we did before. Furthermore, by using an interval as the independent variable, we inject into the model the uncertainty due to censoring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(timeBef = c(0, obsT), timeAf = c(obsT, Inf), counts = c(as.numeric(counts), 100 - sum(counts)) )
df
##    timeBef timeAf counts
## 1        0      1      3
## 2        1      2     11
## 3        2      3     10
## 4        3      4      8
## 5        4      5     13
## 6        5      6      2
## 7        6      7      1
## 8        7      8     12
## 9        8      9      4
## 10       9     10      5
## 11      10     11      2
## 12      11     12      3
## 13      12     13      2
## 14      13     14      2
## 15      14     15      0
## 16      15     16      1
## 17      16     17      0
## 18      17     18      0
## 19      18     19      0
## 20      19     20      1
## 21      20     21      0
## 22      21     22      1
## 23      22     23      1
## 24      23     24      0
## 25      24     25      0
## 26      25     26      0
## 27      26     27      0
## 28      27     28      1
## 29      28     29      0
## 30      29     30      0
## 31      30     31      1
## 32      31     32      1
## 33      32     33      1
## 34      33     34      0
## 35      34     35      0
## 36      35     36      0
## 37      36     37      0
## 38      37     38      0
## 39      38     39      0
## 40      39     40      0
## 41      40    Inf     14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time-to-event models can be easily fitted by using the same function as we used above (the ‘drm()’ function in the ‘drc’ package). However, there are some important differences in the model call. The first one relate to model definition: a nonlinear regression model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CumulativeProportion ~ timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the other hand, a time-to-event model is defined as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Count ~ timeBef + timeAf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A second difference is that we need to explicitly set the argument ‘type’, to ‘event’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Time-to-event model
modTE &amp;lt;- drm(counts ~ timeBef + timeAf, data = df, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
summary(modTE)
## 
## Model fitted: Log-logistic (ED50 as parameter) with lower limit at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -1.826006   0.194579 -9.3844 &amp;lt; 2.2e-16 ***
## d:(Intercept)  0.881476   0.036928 23.8701 &amp;lt; 2.2e-16 ***
## e:(Intercept)  5.302109   0.565273  9.3797 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With respect to the nonlinear regression fit, the estimates from a time-to-event fit are very similar, but the standard errors are much higher (the coefficient of variability now goes from 4 to 11%).&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neglecting-or-accounting-for-censoring&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neglecting or accounting for censoring?&lt;/h1&gt;
&lt;p&gt;You may wonder which of the two analysis is right and which is wrong. We cannot say this from just one dataset. However, we can repeat the Monte Carlo simulation above to extract 1000 samples, fit the model by using the two methods and retrieve parameter estimates and standard errors for each sample and method. We do this by using the code below (please, be patient… it may take some time).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GermSampling &amp;lt;- function(nSeeds, timeLast, stepOss, e, b, d){
    
    #Draw a sample as above
    nGerm &amp;lt;- rbinom(1, nSeeds, d)
    Gtimes &amp;lt;- rllogis(nGerm, shape = b, scale = e)
    Gtimes &amp;lt;- c(Gtimes, rep(Inf, 100 - nGerm))

    
    #Generate the observed data
    obsT &amp;lt;- seq(1, timeLast, by=stepOss) 
    counts &amp;lt;- as.numeric( table( cut(Gtimes, breaks = c(0, obsT)) ) )
    propCum &amp;lt;- cumsum(counts)/nSeeds
    timeBef &amp;lt;- c(0, obsT)
    timeAf &amp;lt;- c(obsT, Inf)
    counts &amp;lt;- c(counts, nSeeds - sum(counts))
    
    #Calculate the T50 with two methods
    mod &amp;lt;- drm(propCum ~ obsT, fct = LL.3() )
    modTE &amp;lt;- drm(counts ~ timeBef + timeAf, 
           fct = LL.3(), type = &amp;quot;event&amp;quot;)
    c(b1 = summary(mod)[[3]][1,1],
      ESb1 = summary(mod)[[3]][1,2],
      b2 = summary(modTE)[[3]][1,1],
      ESb2 = summary(modTE)[[3]][1,2],
      d1 = summary(mod)[[3]][2,1],
      ESd1 = summary(mod)[[3]][2,2],
      d2 = summary(modTE)[[3]][2,1],
      ESd2 = summary(modTE)[[3]][2,2],
      e1 = summary(mod)[[3]][3,1],
      ESe1 = summary(mod)[[3]][3,2],
      e2 = summary(modTE)[[3]][3,1],
      ESe2 = summary(modTE)[[3]][3,2] )
}

set.seed(1234)
result &amp;lt;- data.frame()
for (i in 1:1000) {
  res &amp;lt;- GermSampling(100, 40, 1, 4.5, 1.6, 0.85)
  result &amp;lt;- rbind(result, res)
} 
names(result) &amp;lt;- c(&amp;quot;b1&amp;quot;, &amp;quot;ESb1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;ESb2&amp;quot;,
                   &amp;quot;d1&amp;quot;, &amp;quot;ESd1&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;ESd2&amp;quot;,
                   &amp;quot;e1&amp;quot;, &amp;quot;ESe1&amp;quot;, &amp;quot;e2&amp;quot;, &amp;quot;ESe2&amp;quot;)
result &amp;lt;- result[result$d2 &amp;gt; 0,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have stored our results in the data frame ‘result’. The means of estimates obtained for both methods should be equal to the real values that we used for the simulation, which will ensure that estimators are unbiased. The means of standard errors (in brackets, below) should represent the real sample-to-sample variability, which may be obtained from the Monte Carlo standard deviation, i.e. from the standard deviation of the 1000 estimates for each parameter and method.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Nonlinear regression&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.63 (0.051)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.006)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.086)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Time-to-event method&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.62 (0.187)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.579)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Real values&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.60 (0.188)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.85 (0.041)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4.55 (0.593)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We clearly see that both nonlinear regression and the time-to-event method lead to unbiased estimates of model parameters. However, standard errors from nonlinear regression are severely biased and underestimated. On the contrary, standard errors from time-to-event method are unbiased.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Censoring is peculiar to germination assays and other time-to-event studies. It may have a strong impact on the reliability of our standard errors and, consequently, on hypotheses testing. Therefore, censoring should never be neglected and time-to-event methods should necessarily be used for data analyses with germination assays. The body of time-to-event methods often goes under the name of ‘survival analysis’, which creates a direct link between survival data and germination data.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;C. Dutang, V. Goulet and M. Pigeon (2008). actuar: An R Package for Actuarial Science. Journal of Statistical Software, vol. 25, no. 7, 1-37.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Paolo Benincasa, M B Mesgaran, and Christian Ritz. 2018. Hydrothermal-Time-to-Event Models for Seed Germination. European Journal of Agronomy 101: 129–39.&lt;/li&gt;
&lt;li&gt;Onofri, Andrea, Hans Peter Piepho, and Marcin Kozak. 2019. Analysing Censored Data in Agricultural Research: A Review with Examples and Software Tips. Annals of Applied Biology, 174, 3-13.&lt;/li&gt;
&lt;li&gt;Ritz, C., F. Baty, J. C. Streibig, and D. Gerhard. 2015. Dose-Response Analysis Using R. PLOS ONE, 10 (e0146021, 12).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stabilising transformations: how do I present my results?</title>
      <link>/2019/stat_general_reportingresults/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_reportingresults/</guid>
      <description>


&lt;p&gt;ANOVA is routinely used in applied biology for data analyses, although, in some instances, the basic assumptions of normality and homoscedasticity of residuals do not hold. In those instances, most biologists would be inclined to adopt some sort of stabilising transformations (logarithm, square root, arcsin square root…), prior to ANOVA. Yes, there might be more advanced and elegant solutions, but stabilising transformations are suggested in most traditional biometry books, they are very straightforward to apply and they do not require any specific statistical software. I do not think that this traditional technique should be underrated.&lt;/p&gt;
&lt;p&gt;However, the use of stabilising transformations has one remarkable drawback, it may hinder the clarity of results. I’d like to give a simple, but relevant example.&lt;/p&gt;
&lt;div id=&#34;an-example-with-counts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An example with counts&lt;/h1&gt;
&lt;p&gt;Consider the following dataset, that represents the counts of insects on 15 independent leaves, treated with the insecticides A, B and C (5 replicates):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- structure(data.frame(
  Insecticide = structure(c(1L, 1L, 1L, 1L, 1L, 
    2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L), 
    .Label = c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;), class = &amp;quot;factor&amp;quot;), 
  Count = c(448, 906, 484, 477, 634, 211, 276, 
    415, 587, 298, 50, 90, 73, 44, 26)), 
  .Names = c(&amp;quot;Insecticide&amp;quot;, &amp;quot;Count&amp;quot;))
dataset
##    Insecticide Count
## 1            A   448
## 2            A   906
## 3            A   484
## 4            A   477
## 5            A   634
## 6            B   211
## 7            B   276
## 8            B   415
## 9            B   587
## 10           B   298
## 11           C    50
## 12           C    90
## 13           C    73
## 14           C    44
## 15           C    26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We should not expect that a count variable is normally distributed with equal variances. Indeed, a graph of residuals against expected values shows clear signs of heteroscedasticity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lm(Count ~ Insecticide, data=dataset)
plot(mod, which = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_General_reportingResults_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this situation, a logarithmic transformation is often suggested to produce a new normal and homoscedastic dataset. Therefore we take the log-transformed variable and submit it to ANOVA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(log(Count) ~ Insecticide, data=dataset)
print(anova(model), digits=6)
## Analysis of Variance Table
## 
## Response: log(Count)
##             Df   Sum Sq Mean Sq F value     Pr(&amp;gt;F)    
## Insecticide  2 15.82001 7.91000 50.1224 1.4931e-06 ***
## Residuals   12  1.89376 0.15781                       
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
summary(model)
## 
## Call:
## lm(formula = log(Count) ~ Insecticide, data = dataset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.6908 -0.1849 -0.1174  0.2777  0.5605 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    6.3431     0.1777  35.704 1.49e-13 ***
## InsecticideB  -0.5286     0.2512  -2.104   0.0572 .  
## InsecticideC  -2.3942     0.2512  -9.529 6.02e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3973 on 12 degrees of freedom
## Multiple R-squared:  0.8931, Adjusted R-squared:  0.8753 
## F-statistic: 50.12 on 2 and 12 DF,  p-value: 1.493e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the standard error for each mean (SEM) corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{0.158/5}\)&lt;/span&gt;. In the end, we might show the following table of means for transformed data:&lt;/p&gt;
&lt;!-- table --&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Insecticide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Means (log n.)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6.343&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.815&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3.985&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;SEM&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.178&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- table --&gt;
&lt;p&gt;Unfortunately, we loose clarity: how many insects did we have on each leaf? If we present in our manuscript a table like this one we might be asked by our readers or by the reviewer to report the means on the original measurement unit. What should we do, then? Here are some suggestions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can present the means of the original data with standard deviations. This is clearly less than optimal, if we want to suggest more than the bare variability of the observed sample. Furthermore, &lt;strong&gt;please remember that the means of original data may not be a good measure of central tendency, if the original population is strongly ‘asymmetric’ (skewed)!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;We can show back-transformed means. Accordingly, if we have done, e.g., a logarithmic transformation, we can exponentiate the means of transformed data and report them back to the original measurement unit. Back-transformed means ‘estimate’ the medians of the original populations, which may be regarded as better measures of central tendency for skewed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We suggest that the use of the second method. However, this leaves us with the problem of adding a measure of uncertainty to back-transformed means. No worries, we can use the delta method to back-transform standard errors. It is straightforward:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the first derivative of the back-transform function [in this case the first derivative of exp(X)=exp(X)] and&lt;/li&gt;
&lt;li&gt;multiply it by the standard error of the transformed data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This may be simply done by hand, with e.g &lt;span class=&#34;math inline&#34;&gt;\(exp(6.343) \times 0.178 = 101.19\)&lt;/span&gt; (for insecticide A). This ‘manual’ solution is always available, regardless of the statistical software at hand. With R, we can use the ‘emmeans’ package (Lenth, 2016):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emmeans)
countM &amp;lt;- emmeans(model, ~Insecticide, transform = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is enough to set the argument ‘transform’ to ’response, although the transformation must be embedded in the model. It means: it is ok if we coded the model as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;log(Count) ~ Insecticide&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the contrary, it fails if we coded the model as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logCount ~ Insecticide&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the transformation was performed prior to fitting.&lt;/p&gt;
&lt;p&gt;Obviously, the back-transformed standard error is different for each mean (there is no homogeneity of variances on the original scale, but we knew this already). Back-transformed data might be presented as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Insecticide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;568.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;101.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;335.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51.88&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9.57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It would be appropriate to state it clearly (e.g. in a footnote), that means and SEs were obtained by back-transformation via the delta method. Far clearer, isn’t it? As I said, there are other solutions, such as fitting a GLM, but stabilising transformations are simple and they are easily acceptable in biological Journals.&lt;/p&gt;
&lt;p&gt;If you want to know something more about the delta-method you might start from &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_thedeltamethod/&#34;&gt;my post here&lt;/a&gt;. A few years ago, some collegues and I have also discussed these issues in a journal paper (Onofri et al., 2010).&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
University of Perugia (Italy)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Lenth, R.V., 2016. Least-Squares Means: The R Package lsmeans. Journal of Statistical Software 69. &lt;a href=&#34;https://doi.org/10.18637/jss.v069.i01&#34; class=&#34;uri&#34;&gt;https://doi.org/10.18637/jss.v069.i01&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Onofri, A., Carbonell, E.A., Piepho, H.-P., Mortimer, A.M., Cousens, R.D., 2010. Current statistical issues in Weed Research. Weed Research 50, 5–24.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genotype experiments: fitting a stability variance model with R</title>
      <link>/2019/stat_lmm_stabilityvariance/</link>
      <pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_lmm_stabilityvariance/</guid>
      <description>


&lt;p&gt;Yield stability is a fundamental aspect for the selection of crop genotypes. The definition of stability is rather complex (see, for example, Annichiarico, 2002); in simple terms, the yield is stable when it does not change much from one environment to the other. It is an important trait, that helps farmers to maintain a good income in most years.&lt;/p&gt;
&lt;p&gt;Agronomists and plant breeders are continuosly concerned with the assessment of genotype stability; this is accomplished by planning genotype experiments, where a number of genotypes is compared on randomised complete block designs, with three to five replicates. These experiments are repeated in several years and/or several locations, in order to measure how the environment influences yield level and the ranking of genotypes.&lt;/p&gt;
&lt;p&gt;I would like to show an exemplary dataset, referring to a multienvironment experiment with winter wheat. Eight genotypes were compared in seven years in central Italy, on randomised complete block designs with three replicates. The ‘WinterWheat’ dataset is available in the ‘aomisc’ package, which is the accompanying package for this blog and it is available on gitHub. Information on how to download and install the ‘aomisc’ package are given in &lt;a href=&#34;https://www.statforbiology.com/rpackages/&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first code snippet loads the ‘aomisc’ package and other necessary packages. Afterwards, it loads the ‘WinterWheat’ dataset and turns the ‘Year’ and ‘Block’ variables into factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plyr)
library(nlme)
library(aomisc)
## Loading required package: drc
## Loading required package: MASS
## Loading required package: drcData
## 
## &amp;#39;drc&amp;#39; has been loaded.
## Please cite R and &amp;#39;drc&amp;#39; if used for a publication,
## for references type &amp;#39;citation()&amp;#39; and &amp;#39;citation(&amp;#39;drc&amp;#39;)&amp;#39;.
## 
## Attaching package: &amp;#39;drc&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     gaussian, getInitial
data(WinterWheat)
WinterWheat$Year &amp;lt;- factor(WinterWheat$Year)
WinterWheat$Block &amp;lt;- factor(WinterWheat$Block)
head(WinterWheat, 10)
##    Plot Block Genotype Yield Year
## 1     2     1 COLOSSEO  6.73 1996
## 2     1     1    CRESO  6.02 1996
## 3    50     1   DUILIO  6.06 1996
## 4    49     1   GRAZIA  6.24 1996
## 5    63     1    IRIDE  6.23 1996
## 6    32     1 SANCARLO  5.45 1996
## 7    35     1   SIMETO  4.99 1996
## 8    33     1    SOLEX  6.08 1996
## 9   110     2 COLOSSEO  6.96 1996
## 10  137     2    CRESO  5.34 1996&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please, note that this is a multienvironment experiment as it is repeated in several years: each year is an ‘environment’ in itself. Furthermore, please note that the year effect (i.e. the environment effect) is of random nature: we select the years, but we cannot control the weather conditions.&lt;/p&gt;
&lt;div id=&#34;defining-a-linear-mixed-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defining a linear mixed model&lt;/h1&gt;
&lt;p&gt;Dealing with the above dataset, a good candidate model for data analyses is the following linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \mu + \gamma_{kj} + g_i + e_j +  ge_{ij} + \varepsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is yield (or other trait) for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th block in the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is the effect of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, &lt;span class=&#34;math inline&#34;&gt;\(ge\)&lt;/span&gt; is the interaction effect of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th genotype and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th environment, while &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the residual random term, which is assumed as normally distributed with variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As I said before, the block effect, the environment effect and the ‘genotype x environment’ interaction are usually regarded as random. Therefore, they are assumed as normally distributed, with means equal to 0 and variances respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{\gamma}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{e}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. Indeed, the above model is a Linear Mixed Model (LMM).&lt;/p&gt;
&lt;p&gt;Let’s concentrate on &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt;. It is clear that this value is a measure of instability: if it is high, genotypes may respond differently to different environments. In this way, each genotype can be favored in some specific environments and disfavored in some others. Shukla (1974) has suggested that we should allow &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ge}\)&lt;/span&gt; assume a different value for each genotype and use these components as a measure of stability (stability variances). According to Shukla, a genotype is considered stable when its stability variance is lower than &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Piepho (1999) has shown that stability variances can be obtained within the mixed model framework, by appropriately coding the variance-covariance matrix for random effects. He gave SAS code to accomplish this task and, to me, it was not straightforward to transport his code into R. I finally succeeded and I though I should better share my code, just in case it helps someone save a few headaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-stability-variance-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a stability variance model&lt;/h1&gt;
&lt;p&gt;As we have to model the variance-covariance of random effects, we need to use the ‘lme’ function in the ‘nlme’ package (Pinheiro and Bates, 2000). The problem is that random effects are crossed and they are not easily coded with this package. After an extensive literature search, I could find the solution in the aforementioned book (at pag. 162-163) and in Galecki and Burzykowski (2013). The trick is made by appropriately using the ‘pdMat’ construct (‘pdBlocked’ and ‘pdIdent’). In the code below, I have built a block-diagonal variance-covariance matrix for random effects, where blocks and genotypes are nested within years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdIdent(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix)
## Year = pdIdent(1), pdIdent(Block - 1), pdIdent(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      1.07314201 1.0359257
## Block1           0.01641744 0.1281306
## Block2           0.01641744 0.1281306
## Block3           0.01641744 0.1281306
## GenotypeCOLOSSEO 0.17034091 0.4127238
## GenotypeCRESO    0.17034091 0.4127238
## GenotypeDUILIO   0.17034091 0.4127238
## GenotypeGRAZIA   0.17034091 0.4127238
## GenotypeIRIDE    0.17034091 0.4127238
## GenotypeSANCARLO 0.17034091 0.4127238
## GenotypeSIMETO   0.17034091 0.4127238
## GenotypeSOLEX    0.17034091 0.4127238
## Residual         0.14880400 0.3857512&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wee see that the variance component for the ‘genotype x environment’ interaction is the same for all genotypes and equal to 0.170.&lt;/p&gt;
&lt;p&gt;Allowing for a different variance component per genotype is relatively easy, by replacing ‘pdIdent()’ with ‘pdDiag()’, as shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.mix2 &amp;lt;- lme(Yield ~ Genotype, 
  random=list(Year = pdBlocked(list(pdIdent(~1),
                                    pdIdent(~Block - 1),
                                    pdDiag(~Genotype - 1)))),
  data=WinterWheat)
VarCorr(model.mix2)
## Year = pdIdent(1), pdIdent(Block - 1), pdDiag(Genotype - 1) 
##                  Variance   StdDev   
## (Intercept)      0.86592829 0.9305527
## Block1           0.01641744 0.1281305
## Block2           0.01641744 0.1281305
## Block3           0.01641744 0.1281305
## GenotypeCOLOSSEO 0.10427267 0.3229128
## GenotypeCRESO    0.09588553 0.3096539
## GenotypeDUILIO   0.47612340 0.6900170
## GenotypeGRAZIA   0.15286445 0.3909788
## GenotypeIRIDE    0.11860160 0.3443858
## GenotypeSANCARLO 0.02575029 0.1604690
## GenotypeSIMETO   0.42998504 0.6557324
## GenotypeSOLEX    0.06713590 0.2591060
## Residual         0.14880439 0.3857517&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that we have now different variance components and we can classify some genotypes as stable (e.g. Sancarlo, Solex and Creso) and some others as unstable (e.g. Duilio and Simeto).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-the-means&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Working with the means&lt;/h1&gt;
&lt;p&gt;In his original paper, Piepho (1999) also gave SAS code to analyse the means of the ‘genotype x environment’ combinations. Indeed, agronomists and plant breeders often adopt a two-steps fitting procedure: in the first step, the means across blocks are calculated for all genotypes in all environments. In the second step, these means are used to fit a stability variance model. This two-step process is less demanding in terms of computer resources and it is correct whenever the experiments are equireplicated, with no missing ‘genotype x environment’ combinations. Furthermore, we need to be able to assume similar variances within all experiments.&lt;/p&gt;
&lt;p&gt;I would also like to give an example of this two-step analysis method. In the first step, we can use the ‘ddply()’ function in the package ‘plyr’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#First step
WinterWheatM &amp;lt;- ddply(WinterWheat, c(&amp;quot;Genotype&amp;quot;, &amp;quot;Year&amp;quot;), 
      function(df) c(Yield = mean(df$Yield)) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have retreived the means for genotypes in all years, we can fit a stability variance model, although we have to use a different approach, with respect to the one we used with the whole dataset. In this case, we need to model the variance of residuals, introducing within-group (within-year) heteroscedasticity. The ‘weights’ argument can be used, together with the ‘pdIdent()’ variance function, to allow for a different variance for each genotype. See the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Second step
model.mixM &amp;lt;- lme(Yield ~ Genotype, random = ~ 1|Year, data = WinterWheatM,
                 weights = varIdent(form=~1|Genotype))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code to retrieve the within-year variances is not obvious, unfortunately. However, you can use the folllowing snippet as a guidance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vF &amp;lt;- model.mixM$modelStruct$varStruct
sdRatios &amp;lt;- c(1, coef(vF, unconstrained = F))
names(sdRatios)[1] &amp;lt;- &amp;quot;COLOSSEO&amp;quot;
scalePar &amp;lt;- model.mixM$sigma
sigma2i &amp;lt;- (scalePar * sdRatios)^2
sigma2i
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.15387857 0.14548985 0.52571780 0.20246664 0.16820264 0.07535112 
##     SIMETO      SOLEX 
## 0.47958756 0.11673900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code outputs ‘sigma2i’, which does not contain the stability variances. Indeed, we should remove the within-experiment error (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;), which can only be estimated from the whole dataset. Indeed, if we take the estimate of 0.1488 (see code above), divide by three (the number of blocks) and subtract from ‘sigma2i’, we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2i - model.mix2$sigma^2/3
##   COLOSSEO      CRESO     DUILIO     GRAZIA      IRIDE   SANCARLO 
## 0.10427711 0.09588839 0.47611634 0.15286517 0.11860118 0.02574966 
##     SIMETO      SOLEX 
## 0.42998610 0.06713754&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which are the stability variances, as obtained with the analyses of the whole dataset.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Annichiarico, P., 2002. Genotype x Environment Interactions - Challenges and Opportunities for Plant Breeding and Cultivar Recommendations. Plant Production and protection paper, Food &amp;amp; Agriculture Organization of the United Nations (FAO), Roma.&lt;/li&gt;
&lt;li&gt;Gałecki, A., Burzykowski, T., 2013. Linear mixed-effects models using R: a step-by-step approach. Springer, Berlin.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 1999. Stability Analysis Using the SAS System. Agronomy Journal 91, 154–160.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-Effects Models in S and S-Plus, Springer-Verlag Inc. ed. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Shukla, G.K., 1972. Some statistical aspects of partitioning genotype-environmental components of variability. Heredity 29, 237–245.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Companion R Packages</title>
      <link>/rpackages/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/rpackages/</guid>
      <description>


&lt;p&gt;This blog is supported by a few R packages containing all functions, datasets and other utilities, which are necessary to work through posts, tutorials and books. The packages are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/aomisc&#34;&gt;aomisc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/agriCensData&#34;&gt;AgriCensData&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/drcSeedGerm&#34;&gt;drcSeedGerm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OnofriAndreaPG/lmDiallel&#34;&gt;lmDiallel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All packages are hosted on gitHub and can be installed from there. To do so, you need the ‘devtools’ package, so, if necessary, install this package first. Next, load this library and use the ‘install_github()’ function to install the three packages. For any problem, please, &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;email me&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)
library(devtools)
install_github(&amp;quot;OnofriAndreaPG/aomisc&amp;quot;)
install_github(&amp;quot;OnofriAndreaPG/agriCensData&amp;quot;)
install_github(&amp;quot;OnofriAndreaPG/drcSeedGerm&amp;quot;)
install_github(&amp;quot;OnofriAndreaPG/lmDiallel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How do we combine errors, in biology? The delta method</title>
      <link>/2019/stat_general_thedeltamethod/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_thedeltamethod/</guid>
      <description>


&lt;p&gt;In a recent post I have shown that we can build linear combinations of model parameters (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_errorpropagation/&#34;&gt;see here&lt;/a&gt; ). For example, if we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;, we can build a linear combination as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The standard error for this combination can be obtained as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma_Z = \sqrt{ A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In biology, nonlinear transformations are much more frequent than linear transformations. Nonlinear transformations are, e.g., &lt;span class=&#34;math inline&#34;&gt;\(Z = exp(Q + W)\)&lt;/span&gt;, or, &lt;span class=&#34;math inline&#34;&gt;\(Z = 1/(Q + W)\)&lt;/span&gt;. What is the standard error for these nonlinear transformations? This is not a complex problem, but the solution may be beyond biologists with an average level of statistical proficiency. It is named the ‘delta method’ and it provides the so called ‘delta standard errors’. I thought it might be useful to talk about it, by using a very simple language and a few examples.&lt;/p&gt;
&lt;div id=&#34;example-1-getting-the-half-life-of-a-herbicide&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1: getting the half-life of a herbicide&lt;/h1&gt;
&lt;p&gt;A herbicide has proven to follow a first order degradation kinetic in soil, with constant degradation rate &lt;span class=&#34;math inline&#34;&gt;\(k = -0.035\)&lt;/span&gt; and standard error equal to &lt;span class=&#34;math inline&#34;&gt;\(0.00195\)&lt;/span&gt;. What is the half-life (&lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;) of this herbicide and its standard error?&lt;/p&gt;
&lt;p&gt;Every pesticide chemist knows that the half-life (&lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;) is derived by the degradation rate, according to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T_{1/2} = \frac{\log(0.5)}{k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, the half-life for our herbicide is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Thalf &amp;lt;- log(0.5)/-0.035
Thalf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19.80421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But … what is the standard error of this half-life? There is some uncertainty around the estimate of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and it is clear that such an uncertainty should propagate to the estimate of &lt;span class=&#34;math inline&#34;&gt;\(T_{1/2}\)&lt;/span&gt;; unfortunately, the transformation is nonlinear and we cannot use the expression given above for linear transformations.&lt;/p&gt;
&lt;div id=&#34;the-basic-idea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The basic idea&lt;/h2&gt;
&lt;p&gt;The basic idea behind the delta method is that most of the simple nonlinear functions, which we use in biology, can be locally approximated by the tangent line through a point of interest. For example, our nonlinear half-life function is &lt;span class=&#34;math inline&#34;&gt;\(Y = \log(0.5)/X\)&lt;/span&gt; and, obviously, we are interested in the point where &lt;span class=&#34;math inline&#34;&gt;\(X = k = -0.035\)&lt;/span&gt;. In the graph below, we have represented our nonlinear function (in black) and its tangent line (in red) through the above point: we can see that the approximation is fairly good in the close vicinity of &lt;span class=&#34;math inline&#34;&gt;\(X = -0.035\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Stat_General_TheDeltaMethod_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the equation of the tangent line? In general, if the nonlinear function is &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, you may remember from high school that the slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; of the tangent line is equal to the first derivative of &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, that is &lt;span class=&#34;math inline&#34;&gt;\(G&amp;#39;(X)\)&lt;/span&gt;. You may also remember that the equation of a line with slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; through the point &lt;span class=&#34;math inline&#34;&gt;\(P(X_1, Y_1)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y - Y_1 = m(X - X_1)\)&lt;/span&gt;. As &lt;span class=&#34;math inline&#34;&gt;\(Y_1 = G(X_1)\)&lt;/span&gt;, the tangent line has equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = G(X_1) + G&amp;#39;(X_1)(X - X_1)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-need-the-derivative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We need the derivative!&lt;/h2&gt;
&lt;p&gt;In order to write the equation of the red line in the Figure above, we need to consider that &lt;span class=&#34;math inline&#34;&gt;\(X_1 = -0.035\)&lt;/span&gt; and we need to be able to calculate the first derivative of our nonlinear half-life fuction. I am not able to derive the expression of the first derivative for all nonlinear functions and it was a relief for me to discover that R can handle this task in simple ways, e.g. by using the function ‘D()’. For our case, it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(log(0.5)/X), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -(log(0.5)/X^2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, we can use this R function to calculate the slope &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; of the tangent line in the figure above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- -0.035
m &amp;lt;- eval( D(expression(log(0.5)/X), &amp;quot;X&amp;quot;) )
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 565.8344&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already know that &lt;span class=&#34;math inline&#34;&gt;\(G(-0.035) = 19.80421\)&lt;/span&gt;. Therefore, we can write the equation of the tangent line (red line in the graph above):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = 19.80421 + 565.8344 \, (X + 0.035)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = 19.80421 + 565.8344 \, X + 565.8344 \cdot 0.035 = 39.60841 + 565.8344 \, X\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;replacing-a-curve-with-a-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Replacing a curve with a line&lt;/h2&gt;
&lt;p&gt;Now, we have two functions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the original nonlinear half-life function &lt;span class=&#34;math inline&#34;&gt;\(Y = \log(0.5)/X\)&lt;/span&gt;$&lt;/li&gt;
&lt;li&gt;a new linear function (&lt;span class=&#34;math inline&#34;&gt;\(Y = 39.60841 + 565.8344 \, X\)&lt;/span&gt;), that is a very close approximation to the previous one, at least near to the point &lt;span class=&#34;math inline&#34;&gt;\(X = -0.035\)&lt;/span&gt;, which we are interested in.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, we can approximate the former with the latter! If we use the linear function, we see that the half-life is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;39.60841 + 565.8344 * -0.035&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19.80421&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is what we expected. The advantage is that we can now use the low of propagation of errors to estimate the standard error (see the first and second equation in this post):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sigma_{ \left[ 39.60841 + 565.8344 \, X \right]} = \sqrt{ 562.8344^2 \, \sigma^2_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt( m^2 * (0.00195 ^ 2) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.103377&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in-general&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In general…&lt;/h2&gt;
&lt;p&gt;If we have a nonlinear transformation &lt;span class=&#34;math inline&#34;&gt;\(G(X)\)&lt;/span&gt;, the standard error for this transformation is approximated by knowing the first derivative &lt;span class=&#34;math inline&#34;&gt;\(G&amp;#39;(X)\)&lt;/span&gt; and the standard error of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_{G(X)}  \simeq \sqrt{ [G&amp;#39;(X)]^2 \, \sigma^2_X }\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-a-back-transformed-count&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2: a back-transformed count&lt;/h1&gt;
&lt;p&gt;A paper reports that the mean number of microorganisms in a substrate, on a logarithmic scale, was &lt;span class=&#34;math inline&#34;&gt;\(X_1 = 5\)&lt;/span&gt; with standard error &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.84\)&lt;/span&gt;. It is easy to derive that the actual number of micro-organisms was &lt;span class=&#34;math inline&#34;&gt;\(\exp{5} = 148.4132\)&lt;/span&gt;; what is the standard error of the back-transformed mean?&lt;/p&gt;
&lt;p&gt;The first derivative of our nonlinear function is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(exp(X)), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## exp(X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and thus the slope of the tangent line is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 5
m &amp;lt;- eval( D(expression(exp(X)), &amp;quot;X&amp;quot;) )
m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 148.4132&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the function above, the standard error for the back-transformed mean is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma &amp;lt;- 0.84
sqrt( m^2 * sigma^2 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 124.6671&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3-selenium-concentration-in-olive-drupes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3: Selenium concentration in olive drupes&lt;/h1&gt;
&lt;p&gt;The concentration of selenium in olive drupes was found to be &lt;span class=&#34;math inline&#34;&gt;\(3.1 \, \mu g \,\, g^{-1}\)&lt;/span&gt; with standard error equal to 0.8. What is the intake of selenium when eating one drupe? Please, consider that one drupe weights, on average, 3.4 g (SE = 0.31) and that selenium concentration and drupe weight show a covariance of 0.55.&lt;/p&gt;
&lt;p&gt;The amount of selenium is easily calculated as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- 3.1; W = 3.4
X * W&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delta standard errors can be obtained by considering the partial derivatives for each of the two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mX &amp;lt;- eval( D(expression(X*W), &amp;quot;X&amp;quot;) )
mW &amp;lt;- eval( D(expression(X*W), &amp;quot;W&amp;quot;) )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and combining them as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- 0.8; sigmaW &amp;lt;- 0.31; sigmaXW &amp;lt;- 0.55
sqrt( (mX^2)*sigmaX^2 + (mW^2)*sigmaW^2 + 2*X*W*sigmaXW )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.462726&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For those of you who would like to get involved with matrix notation: we can reach the same result via matrix multiplication (see below). This might be easier when we have more than two variables to combine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;der &amp;lt;- matrix(c(mX, mW), 1, 2)
sigma &amp;lt;- matrix(c(sigmaX^2, sigmaXW, sigmaXW, sigmaW^2), 2, 2, byrow = T)
sqrt( der %*% sigma %*% t(der) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]
## [1,] 4.462726&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-delta-method-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The delta method with R&lt;/h1&gt;
&lt;p&gt;In R there is a shortcut function to calculate delta standard errors, that is available in the ‘car’ package. In order to use it, we need to have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a named vector for the variables that we have to combine&lt;/li&gt;
&lt;li&gt;an expression for the transformation&lt;/li&gt;
&lt;li&gt;a variance-covariance matrix&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the first example, we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;k&amp;quot; = -0.035)
sigma &amp;lt;- matrix(c(0.00195^2), 1, 1)

library(car)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: carData&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;deltaMethod(object = obj, g=&amp;quot;log(0.5)/k&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Estimate       SE    2.5 %   97.5 %
## log(0.5)/k 19.80421 1.103377 17.64163 21.96678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the second example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;X1&amp;quot; = 5)
sigma &amp;lt;- matrix(c(0.84^2), 1, 1)
deltaMethod(object = obj, g=&amp;quot;exp(X1)&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Estimate       SE     2.5 %   97.5 %
## exp(X1) 148.4132 124.6671 -95.92978 392.7561&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the third example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obj &amp;lt;- c(&amp;quot;X&amp;quot; = 3.1, &amp;quot;W&amp;quot; = 3.4)
sigma &amp;lt;- matrix(c(0.8^2, 0.55, 0.55, 0.31^2), 2, 2, byrow = T)
deltaMethod(object = obj, g=&amp;quot;X * W&amp;quot;, vcov = sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate       SE    2.5 %   97.5 %
## X * W    10.54 4.462726 1.793218 19.28678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function ‘deltaMethod()’ is very handy to be used in connection with model objects, as we do not need to provide anything, but the transformation function. But this is something that requires another post!&lt;/p&gt;
&lt;p&gt;However, two final notes relating to the delta method need to be pointed out here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the delta standard error is always approximate;&lt;/li&gt;
&lt;li&gt;if the original variables are gaussian, the transformed variable, usually, is not gaussian.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part II</title>
      <link>/2019/stat_general_correlationindependence2/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationindependence2/</guid>
      <description>


&lt;p&gt;With field experiments, studying the correlation between the observed traits may not be an easy task. Indeed, in these experiments, subjects are not independent, but they are grouped by treatment factors (e.g., genotypes or weed control methods) or by blocking factors (e.g., blocks, plots, main-plots). I have dealt with this problem &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in a previous post&lt;/a&gt; and I gave a solution based on traditional methods of data analyses.&lt;/p&gt;
&lt;p&gt;In a recent paper, Piepho (2018) proposed a more advanced solution based on mixed models. He presented four examplary datasets and gave SAS code to analyse them, based on PROC MIXED. I was very interested in those examples, but I do not use SAS. Therefore, I tried to ‘transport’ the models in R, which turned out to be a difficult task. After struggling for awhile with several mixed model packages, I came to an acceptable solution, which I would like to share.&lt;/p&gt;
&lt;div id=&#34;a-routine-experiment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A ‘routine’ experiment&lt;/h1&gt;
&lt;p&gt;I will use the same example as presented &lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;in my previous post&lt;/a&gt;, which should allow us to compare the results with those obtained by using more traditional methods of data analyses. It is a genotype experiment, laid out in randomised complete blocks, with 27 wheat genotypes and three replicates. For each plot, the collegue who made the experiment recorded several traits, including yield (Yield) and weight of thousand kernels (TKW). The dataset ‘WheatQuality.csv’ is available on ‘gitHub’; it consists of 81 records (plots) and just as many couples of measures in all. The code below loads the necessary packages, the data and transforms the numeric variable ‘Block’ into a factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
library(plyr)
library(nlme)
library(asreml)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
dataset$Block &amp;lt;- factor(dataset$Block)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlationindependence1/&#34;&gt;In my previous post&lt;/a&gt;, I used the above dataset to calculate the Pearson’s correlation coefficient between Yield and TKW for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;plot measurements,&lt;/li&gt;
&lt;li&gt;residuals,&lt;/li&gt;
&lt;li&gt;treatment/block means.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Piepho (2018) showed that, for an experiment like this one, the above correlations can be estimated by coding a multiresponse mixed model, as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_{ijk} = \mu_i + \beta_{ik} + \tau_{ij} + \epsilon_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_{ijk}\)&lt;/span&gt; is the response for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the rootstock &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the mean for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{ik}\)&lt;/span&gt; is the effect of the block &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{ij}\)&lt;/span&gt; is the effect of genotype &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for the trait &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; is the residual for each of 81 observations for two traits.&lt;/p&gt;
&lt;p&gt;In the above model, the residuals &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ijk}\)&lt;/span&gt; need to be normally distributed and heteroscedastic, with trait-specific variances. Furthermore, residuals belonging to the same plot (the two observed traits) need to be correlated (correlation of errors).&lt;/p&gt;
&lt;p&gt;Hans-Peter Piepho, in his paper, put forward the idea that the ‘genotype’ and ‘block’ effects for the two traits can be taken as random, even though they might be of fixed nature, especially the genotype effect. This idea makes sense, because, for this application, we are mainly interested in variances and covariances. Both random effects need to be heteroscedastic (trait-specific variance components) and there must be a correlation between the two traits.&lt;/p&gt;
&lt;p&gt;To the best of my knowledge, there is no way to fit such a complex model with the ‘nlme’ package and related ‘lme()’ function (I’ll gave a hint later on, for a simpler model). Therefore, I decided to use the package ‘asreml’ (Butler et al., 2018), although this is not freeware. With the function ‘asreml()’, we need to specify the following components.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The response variables. When we set a bivariate model with ‘asreml’, we can ‘cbind()’ Yield and TKW and use the name ‘trait’ to refer to them.&lt;/li&gt;
&lt;li&gt;The fixed model, that only contains the trait effect. The specification is, therefore, ‘cbind(Yield, TKW) ~ trait - 1’. Following Piepho (2018), I removed the intercept, to separately estimate the means for the two traits.&lt;/li&gt;
&lt;li&gt;The random model, that is composed by the interactions ‘genotype x trait’ and ‘block x trait’. For both, I specified a general unstructured variance covariance matrix, so that the traits are heteroscedastic and correlated. Therefore, the random model is ~ Genotype:us(trait) + Block:us(trait).&lt;/li&gt;
&lt;li&gt;The residual structure, where the observations in the same plot (the term ‘units’ is used in ‘asreml’ to represent the observational units, i.e. the plots) are heteroscedastic and correlated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait, init = c(3.7, -0.25, 1.7)) + 
          Block:us(trait, init = c(77, 38, 53)),
        residual = ~ units:us(trait, init = c(6, 0.16, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:22 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -641.556           1.0    160 18:20:22    0.0 (3 restrained)
##  2      -548.767           1.0    160 18:20:22    0.0
##  3      -448.970           1.0    160 18:20:22    0.0
##  4      -376.952           1.0    160 18:20:22    0.0
##  5      -334.100           1.0    160 18:20:22    0.0
##  6      -317.511           1.0    160 18:20:22    0.0
##  7      -312.242           1.0    160 18:20:22    0.0
##  8      -311.145           1.0    160 18:20:22    0.0
##  9      -311.057           1.0    160 18:20:22    0.0
## 10      -311.056           1.0    160 18:20:22    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   component  std.error    z.ratio
## Block:trait!trait_Yield:Yield     3.7104778  3.9364268  0.9426005
## Block:trait!trait_TKW:Yield      -0.2428390  1.9074544 -0.1273105
## Block:trait!trait_TKW:TKW         1.6684568  1.8343662  0.9095549
## Genotype:trait!trait_Yield:Yield 77.6346623 22.0956257  3.5135761
## Genotype:trait!trait_TKW:Yield   38.8322972 15.0909109  2.5732242
## Genotype:trait!trait_TKW:TKW     53.8616088 15.3520661  3.5084274
## units:trait!R                     1.0000000         NA         NA
## units:trait!trait_Yield:Yield     6.0939037  1.1951128  5.0990195
## units:trait!trait_TKW:Yield       0.1635551  0.7242690  0.2258209
## units:trait!trait_TKW:TKW         4.4717901  0.8769902  5.0990195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The box above shows the results about the variance-covariance parameters. In order to get the correlations, I specified the necessary combinations of variance-covariance parameters. It is necessary to remember that estimates, in ‘asreml’, are named as V1, V2, … Vn, according to their ordering in model output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parms &amp;lt;- mod.asreml$vparameters
vpredict(mod.asreml, rb ~ V2 / (sqrt(V1)*sqrt(V3) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Estimate        SE
## rb -0.09759916 0.7571335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, rt ~ V5 / (sqrt(V4)*sqrt(V6) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate       SE
## rt 0.6005174 0.130663&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vpredict(mod.asreml, re ~ V9 / (sqrt(V8)*sqrt(V10) ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Estimate        SE
## re 0.03133109 0.1385389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the estimates are very close to those obtained by using the Pearson’s correlation coefficients (see my previous post). The advantage of this mixed model solution is that we can also test hypotheses in a relatively reliable way. For example, I tested the hypothesis that residuals are not correlated by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;coding a reduced model where residuals are heteroscedastic and independent, and&lt;/li&gt;
&lt;li&gt;comparing this reduced model with the complete model by way of a REML-based Likelihood Ratio Test (LRT).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Removing the correlation of residuals is easily done, by changing the correlation structure from ‘us’ (unstructured variance-covariance matrix) to ‘idh’ (diagonal variance-covariance matrix).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:us(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:23 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.023           1.0    160 18:20:23    0.0 (2 restrained)
##  2      -383.859           1.0    160 18:20:23    0.0
##  3      -344.687           1.0    160 18:20:23    0.0
##  4      -321.489           1.0    160 18:20:23    0.0
##  5      -312.488           1.0    160 18:20:23    0.0
##  6      -311.167           1.0    160 18:20:23    0.0
##  7      -311.083           1.0    160 18:20:23    0.0
##  8      -311.082           1.0    160 18:20:23    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml2  1      0.05107    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, I tried to further reduce the model to test the significance of the correlation between block means and genotype means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:us(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:24 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -398.027           1.0    160 18:20:24    0.0 (2 restrained)
##  2      -383.866           1.0    160 18:20:24    0.0
##  3      -344.694           1.0    160 18:20:24    0.0
##  4      -321.497           1.0    160 18:20:24    0.0
##  5      -312.496           1.0    160 18:20:24    0.0
##  6      -311.175           1.0    160 18:20:24    0.0
##  7      -311.090           1.0    160 18:20:24    0.0
##  8      -311.090           1.0    160 18:20:24    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)
## mod.asreml/mod.asreml3  2     0.066663    0.6399&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml4 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:idh(trait) + Block:idh(trait),
        residual = ~ units:idh(trait), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:25 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -406.458           1.0    160 18:20:25    0.0 (2 restrained)
##  2      -394.578           1.0    160 18:20:25    0.0
##  3      -352.769           1.0    160 18:20:25    0.0
##  4      -327.804           1.0    160 18:20:25    0.0
##  5      -318.007           1.0    160 18:20:25    0.0
##  6      -316.616           1.0    160 18:20:25    0.0
##  7      -316.549           1.0    160 18:20:25    0.0
##  8      -316.549           1.0    160 18:20:25    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml, mod.asreml4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                        df LR-statistic Pr(Chisq)   
## mod.asreml/mod.asreml4  3       10.986  0.003364 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that only the genotype means are significantly correlated.&lt;/p&gt;
&lt;p&gt;An alternative (and more useful) way to code the same model is by using the ‘corgh’ structure, instead of ‘us’. These two structures are totally similar, apart from the fact that the first one uses the correlations, instead of the covariances. Another difference, which we should consider when giving starting values, is that correlations come before variances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:26 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1      -632.445           1.0    160 18:20:26    0.0 (3 restrained)
##  2      -539.383           1.0    160 18:20:26    0.0 (2 restrained)
##  3      -468.810           1.0    160 18:20:26    0.0 (1 restrained)
##  4      -422.408           1.0    160 18:20:26    0.0
##  5      -371.304           1.0    160 18:20:26    0.0
##  6      -336.191           1.0    160 18:20:26    0.0
##  7      -317.547           1.0    160 18:20:26    0.0
##  8      -312.105           1.0    160 18:20:26    0.0
##  9      -311.118           1.0    160 18:20:26    0.0
## 10      -311.057           1.0    160 18:20:26    0.0
## 11      -311.056           1.0    160 18:20:26    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09759916  0.7571335
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60051740  0.1306748
## Genotype:trait!trait_Yield                77.63466334 22.0981962
## Genotype:trait!trait_TKW                  53.86160957 15.3536792
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.03133109  0.1385389
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The advantage of this parameterisation is that we can test our hypotheses by easily setting up contraints on correlations. One way to do this is to run the model with the argument ‘start.values = T’. In this way I could derive a data frame (‘mod.init$parameters’), with the starting values for REML maximisation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting the starting values
mod.init &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait, init=c(-0.1, 3.8, 1.8))
        + Block:corgh(trait, init = c(0.6, 77, 53)),
        residual = ~ units:corgh(trait, init = c(0.03, 6, 4.5)), 
        data=dataset, start.values = T)
init &amp;lt;- mod.init$vparameters
init&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                    Component Value Constraint
## 1  Genotype:trait!trait!TKW:!trait!Yield.cor -0.10          U
## 2                 Genotype:trait!trait_Yield  3.80          P
## 3                   Genotype:trait!trait_TKW  1.80          P
## 4     Block:trait!trait!TKW:!trait!Yield.cor  0.60          U
## 5                    Block:trait!trait_Yield 77.00          P
## 6                      Block:trait!trait_TKW 53.00          P
## 7                              units:trait!R  1.00          F
## 8     units:trait!trait!TKW:!trait!Yield.cor  0.03          U
## 9                    units:trait!trait_Yield  6.00          P
## 10                     units:trait!trait_TKW  4.50          P&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the ‘init’ data frame has three columns: (i) names of parameters, (ii) initial values and (iii) type of constraint (U: unconstrained, P = positive, F = fixed). Now, if we take the seventh row (correlation of residuals), set the initial value to 0 and set the third column to ‘F’ (meaning: keep the initial value fixed), we are ready to fit a model without correlation of residuals (same as the ‘model.asreml2’ above). What I had to do was just to pass this data frame as the starting value matrix for a new model fit (see the argument ‘R.param’, below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init2 &amp;lt;- init
init2[8, 2] &amp;lt;- 0
init2[8, 3] &amp;lt;- &amp;quot;F&amp;quot;

mod.asreml.r2 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:28 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1136.066           1.0    160 18:20:28    0.0 (1 restrained)
##  2      -939.365           1.0    160 18:20:28    0.0 (1 restrained)
##  3      -719.371           1.0    160 18:20:28    0.0 (1 restrained)
##  4      -550.513           1.0    160 18:20:28    0.0
##  5      -427.355           1.0    160 18:20:28    0.0
##  6      -353.105           1.0    160 18:20:28    0.0
##  7      -323.421           1.0    160 18:20:28    0.0
##  8      -313.616           1.0    160 18:20:28    0.0
##  9      -311.338           1.0    160 18:20:28    0.0
## 10      -311.087           1.0    160 18:20:28    0.0
## 11      -311.082           1.0    160 18:20:28    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r2)$varcomp[,1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                             component  std.error
## Block:trait!trait!TKW:!trait!Yield.cor    -0.09516456  0.7572040
## Block:trait!trait_Yield                    3.71047783  3.9364268
## Block:trait!trait_TKW                      1.66845679  1.8343662
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.60136047  0.1305180
## Genotype:trait!trait_Yield                77.63463977 22.0809306
## Genotype:trait!trait_TKW                  53.86159936 15.3451466
## units:trait!R                              1.00000000         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.00000000         NA
## units:trait!trait_Yield                    6.09390366  1.1951128
## units:trait!trait_TKW                      4.47179012  0.8769902&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r2, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)
## mod.asreml.r/mod.asreml.r2  1     0.051075    0.4106&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is even more interesting is that ‘asreml’ permits to force the parameters to be linear combinations of one another. For instance, we can code a model where the residual correlation is contrained to be equal to the treatment correlation. To do so, we have to set up a two-column matrix (M), with row names matching the component names in the ‘asreml’ parameter vector. The matrix M should contain:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;in the first column, the equality relationships (same number, same value)&lt;/li&gt;
&lt;li&gt;in the second column, the coefficients for the multiplicative relationships&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, we would set the matrix M as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;firstCol &amp;lt;- c(1, 2, 3, 4, 5, 6, 7, 1, 8, 9)
secCol &amp;lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
M &amp;lt;- cbind(firstCol, secCol)
dimnames(M)[[1]] &amp;lt;- mod.init$vparameters$Component
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           firstCol secCol
## Genotype:trait!trait!TKW:!trait!Yield.cor        1      1
## Genotype:trait!trait_Yield                       2      1
## Genotype:trait!trait_TKW                         3      1
## Block:trait!trait!TKW:!trait!Yield.cor           4      1
## Block:trait!trait_Yield                          5      1
## Block:trait!trait_TKW                            6      1
## units:trait!R                                    7      1
## units:trait!trait!TKW:!trait!Yield.cor           1      1
## units:trait!trait_Yield                          8      1
## units:trait!trait_TKW                            9      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that in ‘firstCol’, the 1st and 8th element are both equal to 1, which contraints them to assume the same value. We can now pass the matrix M as the value of the ‘vcc’ argument in the model call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.asreml.r3 &amp;lt;- asreml(cbind(Yield, TKW) ~ trait - 1,
        random = ~ Genotype:corgh(trait)
        + Block:corgh(trait),
        residual = ~ units:corgh(trait), 
        data=dataset, R.param = init, vcc = M)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model fitted using the sigma parameterization.
## ASReml 4.1.0 Thu Jan  2 18:20:29 2020
##           LogLik        Sigma2     DF     wall    cpu
##  1     -1122.762           1.0    160 18:20:29    0.0 (1 restrained)
##  2      -900.308           1.0    160 18:20:29    0.0
##  3      -665.864           1.0    160 18:20:29    0.0
##  4      -492.020           1.0    160 18:20:29    0.0
##  5      -383.085           1.0    160 18:20:29    0.0
##  6      -336.519           1.0    160 18:20:29    0.0 (1 restrained)
##  7      -319.561           1.0    160 18:20:29    0.0
##  8      -315.115           1.0    160 18:20:29    0.0
##  9      -314.540           1.0    160 18:20:29    0.0
## 10      -314.523           1.0    160 18:20:29    0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.asreml.r3)$varcomp[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            component  std.error    z.ratio
## Block:trait!trait!TKW:!trait!Yield.cor    -0.1146908  0.7592678 -0.1510545
## Block:trait!trait_Yield                    3.6991785  3.9364622  0.9397216
## Block:trait!trait_TKW                      1.6601799  1.8344090  0.9050217
## Genotype:trait!trait!TKW:!trait!Yield.cor  0.2336876  0.1117699  2.0907919
## Genotype:trait!trait_Yield                70.5970531 19.9293722  3.5423621
## Genotype:trait!trait_TKW                  48.9763800 13.8464106  3.5371174
## units:trait!R                              1.0000000         NA         NA
## units:trait!trait!TKW:!trait!Yield.cor     0.2336876  0.1117699  2.0907919
## units:trait!trait_Yield                    6.3989855  1.2811965  4.9945387
## units:trait!trait_TKW                      4.6952670  0.9400807  4.9945358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lrt.asreml(mod.asreml.r3, mod.asreml.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Likelihood ratio test(s) assuming nested random models.
## (See Self &amp;amp; Liang, 1987)
## 
##                            df LR-statistic Pr(Chisq)   
## mod.asreml.r/mod.asreml.r3  1       6.9336   0.00423 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the residual and treatment correlations are equal in this latter model. We also see that this reduced model fits significantly worse than the complete model ‘mod.asreml.r’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-freeware&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going freeware&lt;/h1&gt;
&lt;p&gt;Considering that the block means are not correlated, if we were willing to take the ‘block’ effect as fixed, we could fit the resulting model also with the ‘nlme’ package and the function ‘lme()’ (Pinheiro and Bates, 2000). However, we should cast the model as a univariate model.&lt;/p&gt;
&lt;p&gt;To this aim, the two variables (Yield and TKW) need to be piled up and a new factor (‘Trait’) needs to be introduced to identify the observations for the two traits. Another factor is also necessary to identify the different plots, i.e. the observational units. To perform such a restructuring, I used the ‘melt()’ function in the ‘reshape2’ package Wickham, 2007) and assigned the name ‘Y’ to the response variable, that is now composed by the two original variables Yield and TKW, one on top of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset$Plot &amp;lt;- 1:81
mdataset &amp;lt;- melt(dataset[,c(-3,-6)], variable.name= &amp;quot;Trait&amp;quot;, value.name=&amp;quot;Y&amp;quot;, id=c(&amp;quot;Genotype&amp;quot;, &amp;quot;Block&amp;quot;, &amp;quot;Plot&amp;quot;))
mdataset$Block &amp;lt;- factor(mdataset$Block)
head(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait    Y
## 1 arcobaleno     1    1   TKW 44.5
## 2 arcobaleno     2    2   TKW 42.8
## 3 arcobaleno     3    3   TKW 42.7
## 4       baio     1    4   TKW 40.6
## 5       baio     2    5   TKW 42.7
## 6       baio     3    6   TKW 41.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Plot Trait     Y
## 157  vesuvio     1   76 Yield 54.37
## 158  vesuvio     2   77 Yield 55.02
## 159  vesuvio     3   78 Yield 53.41
## 160 vitromax     1   79 Yield 54.39
## 161 vitromax     2   80 Yield 50.97
## 162 vitromax     3   81 Yield 48.83&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fixed model is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y ~ Trait*Block&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to introduce a totally unstructured variance-covariance matrix for the random effect, I used the ‘pdMat’ construct:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;random = list(Genotype = pdSymm(~Trait - 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relating to the residuals, heteroscedasticity can be included by using the ‘weight()’ argument and the ‘varIdent’ variance function, which allows a different standard deviation per trait:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;weight = varIdent(form = ~1|Trait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also allowed the residuals to be correlated within each plot, by using the ‘correlation’ argument and specifying a general ‘corSymm()’ correlation form. Plots are nested within genotypes, thus I used a nesting operator, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;correlation = corSymm(form = ~1|Genotype/Plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final model call is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- lme(Y ~ Trait*Block, 
                 random = list(Genotype = pdSymm(~Trait - 1)),
                 weight = varIdent(form=~1|Trait), 
                 correlation = corCompSymm(form=~1|Genotype/Plot),
                 data = mdataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Retreiving the variance-covariance matrices needs some effort, as the function ‘getVarCov()’ does not appear to work properly with this multistratum model. First of all, we can retreive the variance-covariance matrix for the genotype random effect (G) and the corresponding correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Variance structure for random effects
obj &amp;lt;- mod
G &amp;lt;- matrix( as.numeric(getVarCov(obj)), 2, 2 )
G&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]
## [1,] 53.86081 38.83246
## [2,] 38.83246 77.63485&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.6005237
## [2,] 0.6005237 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we can retreive the ‘conditional’ variance-covariance matrix (R), that describes the correlation of errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Conditional variance-covariance matrix (residual error)
V &amp;lt;- corMatrix(obj$modelStruct$corStruct)[[1]] #Correlation for residuals
sds &amp;lt;- 1/varWeights(obj$modelStruct$varStruct)[1:2]
sds &amp;lt;- obj$sigma * sds #Standard deviations for residuals (one per trait)
R &amp;lt;- t(V * sds) * sds #Going from correlation to covariance
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 4.4718234 0.1635375
## [2,] 0.1635375 6.0939251&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]
## [1,] 1.00000000 0.03132756
## [2,] 0.03132756 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total correlation matrix is simply obtained as the sum of G and R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tr &amp;lt;- G + R
cov2cor(Tr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]      [,2]
## [1,] 1.0000000 0.5579906
## [2,] 0.5579906 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results are the same as those obtained by using ‘asreml’. Hope these snippets are useful!&lt;/p&gt;
&lt;p&gt;#References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Butler, D., Cullis, B.R., Gilmour, A., Gogel, B., Thomson, R., 2018. ASReml-r reference manual - version 4. UK.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;li&gt;Pinheiro, J.C., Bates, D.M., 2000. Mixed-effects models in s and s-plus. Springer-Verlag Inc., New York.&lt;/li&gt;
&lt;li&gt;Wickham, H., 2007. Reshaping data with the reshape package. Journal of Statistical Software 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with correlation in designed field experiments: part I</title>
      <link>/2019/stat_general_correlationindependence1/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationindependence1/</guid>
      <description>


&lt;div id=&#34;observations-are-grouped&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observations are grouped&lt;/h1&gt;
&lt;p&gt;When we have recorded two traits in different subjects, we can be interested in describing their joint variability, by using the Pearson’s correlation coefficient. That’s ok, altough we have to respect some basic assumptions (e.g. linearity) that have been detailed elsewhere (&lt;a href=&#34;https://www.statforbiology.com/2019/stat_general_correlation_alookat/&#34;&gt;see here&lt;/a&gt;). Problems may arise when we need to test the hypothesis that the correlation coefficient is equal to 0. In this case, we need to make sure that all the couples of observations are taken on independent subjects.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is most often false whenever measures are taken from designed field experiments. In this case, observations may be grouped by one or more treatment/blocking factors. This has been clearly described by Bland and Altman (1994); we would like to give an example that is more closely related to plant/crop science. Think about a genotype experiment, where we compare the behaviour of several genotypes in a randomised blocks design. Usually, we do not only measure yield. We also measure other traits, such as crop height. At the end of the experiment, we might be interested in reporting the correlation between yield and height. How should we proceed? It would seem an easy task, but it is not.&lt;/p&gt;
&lt;p&gt;Let’s assume that we have a randomised blocks design, with 27 genotypes and 3 replicates. For each plot, we recorded two traits, i.e. yield and the weight of thousand kernels (TKW). In the end, we have 81 plots and just as many couples of measures in all. We will use the dataset ‘WheatQuality.csv’, that is available on ‘gitHub’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Hmisc)
library(knitr)
library(plyr)
dataset &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/OnofriAndreaPG/aomisc/master/data/WheatQuality.csv&amp;quot;, header=T)
head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Genotype Block Height  TKW Yield Whectol
## 1 arcobaleno     1     90 44.5 64.40    83.2
## 2 arcobaleno     2     90 42.8 60.58    82.2
## 3 arcobaleno     3     88 42.7 59.42    83.1
## 4       baio     1     80 40.6 51.93    81.8
## 5       baio     2     75 42.7 51.34    81.3
## 6       baio     3     76 41.1 47.78    81.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-correlations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How many correlations?&lt;/h1&gt;
&lt;p&gt;It may be tempting to consider the whole lot of measures and calculate the correlation coefficient between yield and TKW. This is the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra &amp;lt;- with(dataset, rcorr(Yield, TKW) )
ra$r[1,2] #Correlation coefficient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.540957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ra$P[1,2] #P-level&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.850931e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We observe a positive correlation, and &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; seems to be significantly different from 0. Therefore, we would be encouraged to conclude that plots with a high value on yield tend to have a high value on TKW, as well. Unfortunately, such a conclusion is not supported by the data.&lt;/p&gt;
&lt;p&gt;Indeed, the test of significance is clearly invalid, as the 81 plots are not independent; they are grouped by block and genotype and we are totally neglecting these two effects. Are we sure that the same correlation holds for all genotypes/blocks? Let’s check this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor &amp;lt;- function(x) cor(x$Yield, x$TKW)
wgCors &amp;lt;- ddply(dataset, ~Genotype, wCor)
wgCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Genotype         V1
## 1   arcobaleno  0.9847228
## 2         baio  0.1611952
## 3      claudio -0.9993872
## 4     colorado  0.9837293
## 5     colosseo  0.4564855
## 6        creso -0.5910193
## 7       duilio -0.9882330
## 8       gianni -0.7603802
## 9       giotto  0.9520211
## 10      grazia  0.4980828
## 11       iride  0.7563338
## 12    meridano  0.1174342
## 13      neodur  0.4805871
## 14      orobel  0.8907754
## 15 pietrafitta -0.9633891
## 16  portobello  0.9210135
## 17   portofino -0.9900764
## 18   portorico  0.1394211
## 19       preco  0.9007067
## 20    quadrato -0.5840238
## 21    sancarlo -0.6460670
## 22      simeto -0.4051779
## 23       solex -0.6066363
## 24 terrabianca -0.4076416
## 25       verdi  0.5801404
## 26     vesuvio -0.7797493
## 27    vitromax -0.8056514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wbCors &amp;lt;- ddply(dataset, ~Block, wCor)
wbCors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Block        V1
## 1     1 0.5998137
## 2     2 0.5399990
## 3     3 0.5370398&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for the genotypes, we have 27 correlation coefficients, ranging from -0.999 to 0.985. We only have three couples of measurements per genotype and it is clear that this is not much, to reliably estimate a correlation coefficient. However, it is enough to be suspicious about the extent of correlation between yield and TKW, as it may depend on the genotype.&lt;/p&gt;
&lt;p&gt;On the other hand, the correlation within blocks is more constant, independent on the block and similar to the correlation between plots.&lt;/p&gt;
&lt;p&gt;It may be interesting to get an estimate of the average within-group correlation. To this aim, we can perform two separate ANOVAs (one per trait), including all relevant effects (blocks and genotypes) and calculate the correlation between the residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Yield ~ factor(Block) + Genotype, data = dataset)
mod2 &amp;lt;- lm(TKW ~ factor(Block) + Genotype, data = dataset)
wCor &amp;lt;- rcorr(residuals(mod1), residuals(mod2))
wCor$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x          y
## x 1.00000000 0.03133109
## y 0.03133109 1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wCor$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x         y
## x        NA 0.7812693
## y 0.7812693        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average within-group correlation is very small and unsignificant. Let’s think about this correlation: residuals represent yield and TKW values for all plots, once the effects of blocks and genotypes have been removed. A high correlation of residuals would mean that, letting aside the effects of the block and genotype to which it belongs, a plot with a high value on yield also shows a high value on TKW. The existence of such correlation is clearly unsopported by our dataset.&lt;/p&gt;
&lt;p&gt;As the next step, we could consider the means for genotypes/blocks and see whether they are correlated. Blocks and genotypes are independent and, in principle, significance testing is permitted. However, this is not recommended with block means, as three data are too few to make tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Genotype, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rgPear &amp;lt;- rcorr( as.matrix(means[,2:3]) )
rgPear$r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Mu1       Mu2
## Mu1 1.0000000 0.5855966
## Mu2 0.5855966 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgPear$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Mu1        Mu2
## Mu1         NA 0.00133149
## Mu2 0.00133149         NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means &amp;lt;- ddply(dataset, ~Block, summarise, Mu1=mean(Yield), Mu2 = mean(TKW))
rbPear &amp;lt;- cor( as.matrix(means[,2:3]) )
rbPear&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Mu1         Mu2
## Mu1  1.00000000 -0.08812544
## Mu2 -0.08812544  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We note that the correlation between genotype means is high and significant. On the contrary, the correlation between block means is near to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-so-what&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And so what?&lt;/h1&gt;
&lt;p&gt;At this stage, you may be confused… Let’s try to clear the fog.&lt;/p&gt;
&lt;p&gt;Obtaining a reliable measure of correlation from designed experiments is not obvious. Indeed, in every designed field experiment we have groups of subjects and there are several possible types of correlation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;correlation between plot measurements&lt;/li&gt;
&lt;li&gt;correlation between the residuals&lt;/li&gt;
&lt;li&gt;correlation between treatment/block means&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All these correlations should be investigated and used for interpretation.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The ‘naive’ correlation between the plot measurements is very easily calculated, but it is grossy misleading. Indeed, it disregards the treatment/block effects and it does not permit hypotheses testing, as the subjects are not independent. In this example, looking at the ‘naive’ correlation coefficient, we would wrongly conclude that plots with high yield also have high TKW, while further analyses show that this is not true, in general. We would reasonably suggest that the ‘naive’ correlation coefficient is never used for interpretation.&lt;/li&gt;
&lt;li&gt;The correlation between the residuals is a reliable measure of joint variation, because the experimental design is adequately referenced, by removing the effects of tratments/blocks. In this example, residuals are not correlated. Further analyses show that the correlation between yield and TKW, if any, may depend on the genotype, while it does not depend on the block.&lt;/li&gt;
&lt;li&gt;The correlation between treatment/block means permits to assess whether the treatment/block effects on the two traits are correlated. In this case, while we are not allowed to conclude that yield and TKW are, in general, correlated, we can conclude that the genotypes with a high level of yield also show a high level of TKW.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take-home message&lt;/h1&gt;
&lt;p&gt;Whenever we have data from designed field experiments, our correlation analyses should never be limited to the calculation of the ‘naive’ correlation coefficient between the observed values. This may not be meaningful! On the contrary, our interpretation should be mainly focused on the correlation between residuals and on the correlation between the effects of treatments/blocks.&lt;/p&gt;
&lt;p&gt;An elegant and advanced method to perform sound correlation analyses on data from designed field experiments has been put forward by Piepho (2018), within the frame of mixed models. Such an approach will be described in another post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bland, J.M., Altman, D.G., 1994. Statistics Notes: Correlation, regression, and repeated data. BMJ 308, 896–896.&lt;/li&gt;
&lt;li&gt;Piepho, H.-P., 2018. Allowing for the structure of a designed experiment when estimating and testing trait correlations. The Journal of Agricultural Science 156, 59–70.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Useful links</title>
      <link>/links/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/links/</guid>
      <description>&lt;p&gt;This site makes is written in markdown and Rmarkdown and makes use of a wide range of open source technologies. I&amp;rsquo;m adding a few links, for you to learn more.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;The R project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com&#34;&gt;R studio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com&#34;&gt;R markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown and bookdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gohugo.io&#34;&gt;Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would also like to add a few other links, which I keep an eye on, to have useful hints on R and statistics&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-bloggers.com&#34;&gt;R bloggers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-users.com&#34;&gt;R users: jobs for R-users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How do we combine errors? The linear case</title>
      <link>/2019/stat_general_errorpropagation/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_errorpropagation/</guid>
      <description>


&lt;p&gt;In our research work, we usually fit models to experimental data. Our aim is to estimate some biologically relevant parameters, together with their standard errors. Very often, these parameters are interesting in themselves, as they represent means, differences, rates or other important descriptors. In other cases, we use those estimates to derive further indices, by way of some appropriate calculations. For example, think that we have two parameter estimates, say Q and W, with standard errors respectively equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_W\)&lt;/span&gt;: it might be relevant to calculate the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = AQ + BW + C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where A, B and C are three coefficients. The above operation is named ‘linear combination’; it is a sort of a weighted sum of model parameters. The question is: what is the standard error of Z? I would like to show this by way of a simple biological example.&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;We have measured the germination rate for seeds of &lt;em&gt;Brassica rapa&lt;/em&gt; at six levels of water potential in the substrate (-1, -0.9, -0.8, -0.7, -0.6 and -0.5 MPa). We would like to predict the germination rate for a water potential level of -0.75 MPa.&lt;/p&gt;
&lt;p&gt;Literature references suggest that the relationship between germination rate and water potential in the substrate is linear. Therefore, as the first step, we fit a linear regression model to our observed data. If we are into R, the code to accomplish this task is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GR &amp;lt;- c(0.11, 0.20, 0.34, 0.46, 0.56, 0.68)
Psi &amp;lt;- c(-1, -0.9, -0.8, -0.7, -0.6, -0.5)
lMod &amp;lt;- lm(GR ~ Psi)
summary(lMod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GR ~ Psi)
## 
## Residuals:
##          1          2          3          4          5          6 
##  0.0076190 -0.0180952  0.0061905  0.0104762 -0.0052381 -0.0009524 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.25952    0.02179   57.79 5.37e-07 ***
## Psi          1.15714    0.02833   40.84 2.15e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.01185 on 4 degrees of freedom
## Multiple R-squared:  0.9976, Adjusted R-squared:  0.997 
## F-statistic:  1668 on 1 and 4 DF,  p-value: 2.148e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is clear that we can use the fitted model to calculate the GR-value at -0.75 MPa, as &lt;span class=&#34;math inline&#34;&gt;\(GR = 1.26 - 1.16 \times 0.75 = 0.39\)&lt;/span&gt;. This is indeed a linear combination of model parameters, as we have shown above. Q and W are the estimated model parameters, while &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(B = -0.75\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(C = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Of course, the derived response is also an estimate and we need to give a measure of uncertainty. For both model parameters we have standard errors; the question is: how does the uncertainty in parameter estimates propagates to their linear combination? In simpler words: it is easy to build a weightes sum of model parameters, but, how do we make a weighted sum of their standard errors?&lt;/p&gt;
&lt;p&gt;Sokal and Rohlf (1981) at pag. 818 of their book, show that, in general, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{var}(A \, Q + B \, W) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{QW} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{QW}\)&lt;/span&gt; is the covariance of Q and W. I attach the proof below; it is pretty simple to understand and it is worth the effort. However, several students in biology are rather reluctant when they have to delve into maths. Therefore, I would also like to give an empirical ‘proof’, by using some simple R code.&lt;/p&gt;
&lt;p&gt;Let’s take two samples (Q and W) and combine them by using two coefficients A and B. Let’s calculate the variance for the combination:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q &amp;lt;- c(12, 14, 11, 9)
W &amp;lt;- c(2, 4, 7, 8)
A &amp;lt;- 2
B &amp;lt;- 3
C &amp;lt;- 4
var(A * Q + B * W + C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.58333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A^2 * var(Q) + B^2 * var(W) + 2 * A * B * cov(Q, W)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.58333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the results match exactly! In our example, the variance and covariance of estimates are retrieved by using the ‘vcov()’ function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vcov(lMod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)          Psi
## (Intercept) 0.0004749433 0.0006020408
## Psi         0.0006020408 0.0008027211&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2Q &amp;lt;- vcov(lMod)[1,1]
sigma2W &amp;lt;- vcov(lMod)[2,2]
sigmaQW &amp;lt;- vcov(lMod)[1,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error for the prediction is simply obtained as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt( sigma2Q + 0.75^2 * sigma2W - 2 * 0.75 * sigmaQW )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004838667&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-functions-predict-and-glht&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The functions ‘predict()’ and ‘glht()’&lt;/h1&gt;
&lt;p&gt;Now that we have understood the concept, we can use R to make the calculations. For this example, the ‘predict()’ method represents the most logical approach. We only need to pass the model object and the X value which we have to make a prediction for. This latter value needs to be organised as a data frame, with column name(s) matching the name(s) of the X-variable(s) in the original dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(lMod, newdata = data.frame(Psi = -0.75), 
        se.fit = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $fit
##         1 
## 0.3916667 
## 
## $se.fit
## [1] 0.004838667
## 
## $df
## [1] 4
## 
## $residual.scale
## [1] 0.01185227&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from the predict method, there is another function of more general interest, which can be used to build linear combinations of model parameters. It is the ‘glht()’ function in the ‘multcomp’ package. To use this function, we need a model object and we need to organise the coefficients for the transformation in a matrix, with as many rows as there are combinations to calculate. When writing the coefficients, we disregard C: we have seen that every constant value does not affect the variance of the transformation.&lt;/p&gt;
&lt;p&gt;For example, just imagine that we want to predict the GR for two levels of water potential, i.e. -0.75 (as above) and -0.55 MPa. The coefficients (A, B) for the combinations are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Z1 &amp;lt;- c(1, -0.75)
Z2 &amp;lt;- c(1, -0.55)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We pile up the two vectors in one matrix with two rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M &amp;lt;- matrix(c(Z1, Z2), 2, 2, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we pass that matrix to the ‘glht()’ function as an argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(multcomp)
lcombs &amp;lt;- glht(lMod, linfct = M, adjust = &amp;quot;none&amp;quot;)
summary(lcombs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = GR ~ Psi)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## 1 == 0 0.391667   0.004839   80.94 2.30e-07 ***
## 2 == 0 0.623095   0.007451   83.62 2.02e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## (Adjusted p values reported -- single-step method)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above method can be easily extended to other types of linear models and linear combinations of model parameters. The ‘adjust’ argument is useful whenever we want to obtain familywise confidence intervals, which can account for the multiplicity problem. But this is another story…&lt;/p&gt;
&lt;p&gt;Happy work with these functions!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;p&gt;We know that the variance for a random variable is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ var(Z) = \frac{1}{n-1}\sum \left(Z - \hat{Z}\right)^2 = \\ = \frac{1}{n-1}\sum \left(Z - \frac{1}{n} \sum{Z}\right)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(Z = AQ + BW + C\)&lt;/span&gt;, where A, B and C are the coefficients and Q and W are two random variables, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ var(Z) = \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ \left(AQ + BW + C\right)}\right]^2 = \\ 
= \frac{1}{n-1}\sum \left[AQ + BW + C - \frac{1}{n} \sum{ AQ} - \frac{1}{n} \sum{ BW} - \frac{1}{n} \sum{ C}\right]^2 = \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= \frac{1}{n-1}\sum \left[AQ + BW + C - A \hat{Q} - B \hat{W} - C \right]^2 = \\
= \frac{1}{n-1}\sum \left[\left( AQ - A \hat{Q}\right) + \left( BW - B \hat{W}\right) \right]^2 = \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ =\frac{1}{n-1}\sum \left[A \left( Q - \hat{Q}\right) + B \left( W - \hat{W}\right) \right]^2 = \\
= \frac{1}{n-1}\sum \left[A^2 \left( Q - \hat{Q}\right)^2 + B^2 \left( W - \hat{W}\right)^2 + 2AB \left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right] =\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ = A^2 \frac{1}{n-1} \sum{\left( Q - \hat{Q}\right)^2} + B^2 \frac{1}{n-1}\sum \left( W - \hat{W}\right)^2 + 2AB \frac{1}{n-1}\sum{\left[\left( Q - \hat{Q}\right) \left( W - \hat{W}\right)\right]}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \textrm{var}(Z) = A^2 \sigma^2_Q + B^2 \sigma^2_W + 2AB \sigma_{Q,W}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some everyday data tasks: a few hints with R</title>
      <link>/2019/r_shapingdata/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/r_shapingdata/</guid>
      <description>


&lt;p&gt;We all work with data frames and it is important that we know how we can reshape them, as necessary to meet our needs. I think that there are, at least, four routine tasks that we need to be able to accomplish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;subsetting&lt;/li&gt;
&lt;li&gt;sorting&lt;/li&gt;
&lt;li&gt;casting&lt;/li&gt;
&lt;li&gt;melting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Obviously, there is a wide array of possibilities; I’ll just mention a few, which I regularly use.&lt;/p&gt;
&lt;div id=&#34;subsetting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subsetting the data&lt;/h1&gt;
&lt;p&gt;Subsetting means selecting the records (rows) or the variables (columns) which satisfy certain criteria. Let’s take the ‘students.csv’ dataset, which is available on one of my repositories. It is a database of student’s marks in a series of exams for different subjects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;students &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/students.csv&amp;quot;, header=T)
head(students)
##   Id  Subject       Date Mark Year  HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 2  2 AGRONOMY 08/07/2002   24 2001 AGRICULTURE
## 3  3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 4  4 AGRONOMY 24/06/2002   26 2001  HUMANITIES
## 5  5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6  6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say that we want a new dataset, which contains only the students with marks higher than 28.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;gt;= 28)
head(subData)
##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 3   3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 5   5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6   6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE
## 11 11 AGRONOMY 09/09/2002   28 2001  SCIENTIFIC
## 17 17 AGRONOMY 10/06/2002   30 2001  HUMANITIES&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s make it more difficult and extract the records were mark is ranging from 26 to 28 (margins included. Look at the AND clause):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26)
head(subData)
##    Id  Subject       Date Mark Year  HighSchool
## 4   4 AGRONOMY 24/06/2002   26 2001  HUMANITIES
## 6   6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE
## 7   7 AGRONOMY 24/02/2003   26 2001  HUMANITIES
## 8   8 AGRONOMY 09/09/2002   26 2001  SCIENTIFIC
## 10 10 AGRONOMY 08/07/2002   27 2001  HUMANITIES
## 11 11 AGRONOMY 09/09/2002   28 2001  SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are interested in those students who got a mark ranging from 26 to 28 in MATHS (please note the equality relationship written as ‘==’):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot;)
head(subData)
##      Id Subject       Date Mark Year  HighSchool
## 115 115   MATHS 15/07/2002   26 2001 AGRICULTURE
## 124 124   MATHS 16/09/2002   26 2001  SCIENTIFIC
## 138 138   MATHS 04/02/2002   27 2001  HUMANITIES
## 144 144   MATHS 10/02/2003   27 2001  HUMANITIES
## 145 145   MATHS 04/07/2003   27 2002  HUMANITIES
## 146 146   MATHS 28/02/2002   28 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets’ look for good students either in MATHS or in CHEMISTRY (OR clause; note the ‘|’ operator):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;)
head(subData)
##    Id   Subject       Date Mark Year   HighSchool
## 64 64 CHEMISTRY 18/06/2003   20 2002  AGRICULTURE
## 65 65 CHEMISTRY 06/06/2002   21 2001   HUMANITIES
## 66 66 CHEMISTRY 20/02/2003   21 2002   HUMANITIES
## 67 67 CHEMISTRY 20/02/2003   18 2002  AGRICULTURE
## 68 68 CHEMISTRY 04/06/2002   28 2001 OTHER SCHOOL
## 69 69 CHEMISTRY 26/06/2002   23 2001   ACCOUNTING&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also select columns; for example we may want to display only the ‘Subject’, ‘Mark’ and ‘HighSchool’ columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;,
                  select = c(Subject, Mark, HighSchool))
head(subData)
##      Subject Mark   HighSchool
## 64 CHEMISTRY   20  AGRICULTURE
## 65 CHEMISTRY   21   HUMANITIES
## 66 CHEMISTRY   21   HUMANITIES
## 67 CHEMISTRY   18  AGRICULTURE
## 68 CHEMISTRY   28 OTHER SCHOOL
## 69 CHEMISTRY   23   ACCOUNTING&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can as well drop the unwanted columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- subset(students, Mark &amp;lt;= 28 &amp;amp; Mark &amp;gt;=26 &amp;amp; 
                    Subject == &amp;quot;MATHS&amp;quot; | 
                    Subject == &amp;quot;CHEMISTRY&amp;quot;,
                  select = c(-Id, 
                             -Date,
                             -Year))
head(subData)
##      Subject Mark   HighSchool
## 64 CHEMISTRY   20  AGRICULTURE
## 65 CHEMISTRY   21   HUMANITIES
## 66 CHEMISTRY   21   HUMANITIES
## 67 CHEMISTRY   18  AGRICULTURE
## 68 CHEMISTRY   28 OTHER SCHOOL
## 69 CHEMISTRY   23   ACCOUNTING&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the function ‘subset()’ is very easy. However, we might have higher flexibility if we subset by using indices. We already know that the notation ‘dataframe[i,j]’ returns the element in the i-th row and j-th column in a data frame. We can of course replace i and j with some subsetting rules. For example, taking the exams where the mark is comprised between 25 and 29 is done as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students[(students$Mark &amp;lt;= 29 &amp;amp; students$Mark &amp;gt;=25),]
head(subData)
##    Id  Subject       Date Mark Year  HighSchool
## 4   4 AGRONOMY 24/06/2002   26 2001  HUMANITIES
## 6   6 AGRONOMY 09/09/2002   28 2001 AGRICULTURE
## 7   7 AGRONOMY 24/02/2003   26 2001  HUMANITIES
## 8   8 AGRONOMY 09/09/2002   26 2001  SCIENTIFIC
## 10 10 AGRONOMY 08/07/2002   27 2001  HUMANITIES
## 11 11 AGRONOMY 09/09/2002   28 2001  SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful to quickly edit the data. For example, if we want to replace all marks from 25 to 29 with NAs (Not Available), we can simply do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData &amp;lt;- students
subData[(subData$Mark &amp;lt;= 29 &amp;amp; subData$Mark &amp;gt;=25), &amp;quot;Mark&amp;quot;] &amp;lt;- NA
head(subData)
##   Id  Subject       Date Mark Year  HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 2  2 AGRONOMY 08/07/2002   24 2001 AGRICULTURE
## 3  3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 4  4 AGRONOMY 24/06/2002   NA 2001  HUMANITIES
## 5  5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6  6 AGRONOMY 09/09/2002   NA 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that I created a new dataset to make the replacement, in order not to modify the original dataset. Of course, I can use the ‘is.na()’ function to find the missing data and edit them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subData[is.na(subData$Mark) == T, &amp;quot;Mark&amp;quot;] &amp;lt;- 0 
head(subData)
##   Id  Subject       Date Mark Year  HighSchool
## 1  1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 2  2 AGRONOMY 08/07/2002   24 2001 AGRICULTURE
## 3  3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 4  4 AGRONOMY 24/06/2002    0 2001  HUMANITIES
## 5  5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 6  6 AGRONOMY 09/09/2002    0 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sorting the data&lt;/h1&gt;
&lt;p&gt;Sorting is very much like subsetting by indexing. I just need to use the ‘order’ function. For example, let’s sort the students dataset by mark:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(students$Mark), ]
head(sortedData)
##    Id   Subject       Date Mark Year   HighSchool
## 51 51   BIOLOGY 01/03/2002   18 2001   HUMANITIES
## 67 67 CHEMISTRY 20/02/2003   18 2002  AGRICULTURE
## 76 76 CHEMISTRY 24/02/2003   18 2002 OTHER SCHOOL
## 79 79 CHEMISTRY 18/06/2003   18 2002  AGRICULTURE
## 82 82 CHEMISTRY 18/07/2002   18 2001  AGRICULTURE
## 83 83 CHEMISTRY 23/01/2003   18 2001   SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also sort by decreasing order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark), ]
head(sortedData)
##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 3   3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 5   5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 17 17 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 18 18 AGRONOMY 10/06/2002   30 2001 AGRICULTURE
## 19 19 AGRONOMY 09/09/2002   30 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obviously use multiple keys. For example, let’s sort by subject within marks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark, students$Subject), ]
head(sortedData)
##    Id  Subject       Date Mark Year  HighSchool
## 1   1 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 3   3 AGRONOMY 24/06/2002   30 2001 AGRICULTURE
## 5   5 AGRONOMY 23/01/2003   30 2001  HUMANITIES
## 17 17 AGRONOMY 10/06/2002   30 2001  HUMANITIES
## 18 18 AGRONOMY 10/06/2002   30 2001 AGRICULTURE
## 19 19 AGRONOMY 09/09/2002   30 2001 AGRICULTURE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I want to sort in decreasing order on a character variable (such as Subject), I need to use the helper function ‘xtfrm()’:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sortedData &amp;lt;- students[order(-students$Mark, -xtfrm(students$Subject)), ]
head(sortedData)
##      Id Subject       Date Mark Year   HighSchool
## 116 116   MATHS 01/07/2002   30 2001 OTHER SCHOOL
## 117 117   MATHS 18/06/2002   30 2001   ACCOUNTING
## 118 118   MATHS 09/07/2002   30 2001  AGRICULTURE
## 121 121   MATHS 18/06/2002   30 2001   ACCOUNTING
## 123 123   MATHS 09/07/2002   30 2001   HUMANITIES
## 130 130   MATHS 07/02/2002   30 2001   SCIENTIFIC&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;casting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Casting the data&lt;/h1&gt;
&lt;p&gt;When we have a dataset in the LONG format, we might be interested in reshaping it in the WIDE format. This is the same as what the ‘pivot table’ function in Excel does. For example, take the ‘rimsulfuron.csv’ dataset in my repository. This contains the results of a randomised block experiment, where we have 16 herbicides in four blocks. The dataset is in the LONG format, with one row per plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rimsulfuron &amp;lt;- read.csv(&amp;quot;https://www.casaonofri.it/_datasets/rimsulfuron.csv&amp;quot;, header=T)
head(rimsulfuron)
##                      Herbicide Plot Code Block Box WeedCover Yield
## 1             Rimsulfuron (40)    1    1     1   1      27.8 85.91
## 2             Rimsulfuron (45)    2    2     1   1      27.8 93.03
## 3             Rimsulfuron (50)    3    3     1   1      23.0 86.93
## 4             Rimsulfuron (60)    4    4     1   1      42.8 52.99
## 5    Rimsulfuron (50+30 split)    5    5     1   2      15.1 71.36
## 6 Rimsulfuron + thyfensulfuron    6    6     1   2      22.9 75.28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets’put this data frame in the WIDE format, with one row per herbicide and one column per block. To do so, I usually use the ‘cast()’ function in the ‘reshape’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape)
castData &amp;lt;- cast(Herbicide ~ Block, data = rimsulfuron,
     value = &amp;quot;Yield&amp;quot;)
head(castData)
##                                    Herbicide     1     2      3      4
## 1                  Alachlor + terbuthylazine 12.06 49.58  41.34  16.37
## 2                                Hand-Weeded 77.58 92.08  86.59  99.63
## 3         Metolachlor + terbuthylazine (pre) 51.77 52.10  49.46  34.67
## 4 Pendimethalin (post) + rimsuulfuron (post) 94.82 87.72 102.05 101.94
## 5   Pendimethalin (pre) + rimsulfuron (post) 65.51 88.72  95.52  82.39
## 6                           Rimsulfuron (40) 85.91 91.09 111.42  93.15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other similar functions are available within the ‘reshape2’ and ‘tidyr’ packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;melting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Melting the data&lt;/h1&gt;
&lt;p&gt;In this case we do the reverse: we transform the dataset from WIDE to LONG format. For this task, I like the ‘melt()’ function in the ‘reshape2’ package, which requires a data frame as input. I would like to use the ‘castData’ object which we have just created by using the ‘cast()’ function above. Unfortunately, this object has a ‘cast_df’ class. Therefore, in order to avoid weird results, I need to change ‘castData’ into a data frame, by using the ‘as.data.frame()’ function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
castData &amp;lt;- as.data.frame(castData)
mdati &amp;lt;- melt(castData,
              variable.name = &amp;quot;Block&amp;quot;,
              value.name = &amp;quot;Yield&amp;quot;,
              id=c(&amp;quot;Herbicide&amp;quot;))

head(mdati)
##                                    Herbicide Block Yield
## 1                  Alachlor + terbuthylazine     1 12.06
## 2                                Hand-Weeded     1 77.58
## 3         Metolachlor + terbuthylazine (pre)     1 51.77
## 4 Pendimethalin (post) + rimsuulfuron (post)     1 94.82
## 5   Pendimethalin (pre) + rimsulfuron (post)     1 65.51
## 6                           Rimsulfuron (40)     1 85.91&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have a nice work with these functions!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Drowning in a glass of water: variance-covariance and correlation matrices</title>
      <link>/2019/stat_general_correlationcovariance/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlationcovariance/</guid>
      <description>


&lt;p&gt;One of the easiest tasks in R is to get correlations between each pair of variables in a dataset. As an example, let’s take the first four columns in the ‘mtcars’ dataset, that is available within R. Getting the variances-covariances and the correlations is straightforward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
matr &amp;lt;- mtcars[,1:4]

#Covariances
cov(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Correlations
cor(matr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s really a piece of cake! Unfortunately, a few days ago I had a covariance matrix without the original dataset and I wanted the corresponding correlation matrix. Although this is an easy task as well, at first I was stuck, because I could not find an immediate solution… So I started wondering how I could make it.&lt;/p&gt;
&lt;p&gt;Indeed, having the two variables X and Y, their covariance is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cov(X, Y) = \sum\limits_{i=1}^{n} {(X_i - \hat{X})(Y_i - \hat{Y})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{X}\)&lt;/span&gt; are the means for each variable. The correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cor(X, Y) = \frac{cov(X, Y)}{\sigma_x \sigma_y} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y\)&lt;/span&gt; are the standard deviations for X and Y.&lt;/p&gt;
&lt;p&gt;The opposite relationship is clear:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ cov(X, Y) = cor(X, Y) \sigma_x \sigma_y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, converting from covariance to correlation is pretty easy. For example, take the covariance between ‘cyl’ and ‘mpg’ above (-9.172379), the correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-633.097208 / (sqrt(36.324103) * sqrt(15360.7998))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8475514&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the reverse, if we have the correlation (-0.8521620), the covariance is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-0.8475514 * sqrt(36.324103) * sqrt(15360.7998)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -633.0972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My covariance matrix was pretty large, so I started wondering how I could perform this task altogether. What I had to do was to take each element in the covariance matrix and divide it by the square root of the diagonal elements in the same column and in the same row (see below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://lu4yxa.db.files.1drv.com/y4mZ-7ZQc0LCMyDG3kqC_0_bzMZYyEpb37ug_I616tXoPNL_DbILLSOa8HujEZCekvRNeeYsfwtrYP-0T_PfzlOUqUNliHdKU3sDLHwBnr5C4jF-U-u1QkOlWg3ZbQXKJw4TM2VrQIQqjh-Pb-5cOEY49q-3pfnt4ZYJUAYZIBhW4GgJ0svrEEAnKQZfNTs2LW5iZhGyYFYVKFT2Y1O7SjKjA?width=637&amp;amp;height=156&amp;amp;cropmode=none&#34; style=&#34;width:95.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This is easily done by matrix multiplication. I need a square matrix where the standard deviations for each variable are repeated along the rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V &amp;lt;- cov(matr)
SM1 &amp;lt;- matrix(rep(sqrt(diag(V)), 4), 4, 4)
SM1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]       [,3]       [,4]
## [1,]   6.026948   6.026948   6.026948   6.026948
## [2,]   1.785922   1.785922   1.785922   1.785922
## [3,] 123.938694 123.938694 123.938694 123.938694
## [4,]  68.562868  68.562868  68.562868  68.562868&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and another one where they are repeated along the columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SM2 &amp;lt;- matrix(rep(sqrt(diag(V)), each = 4), 4, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can take my covariance matrix (V) and simply multiply these three matrices as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V * 1/SM1 * 1/SM2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, there is not even the need to use ‘rep’ when we create SM1, as R will recycle the elements as needed.&lt;/p&gt;
&lt;p&gt;Going from correlation to covariance can be done similarly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- cor(matr)
R / (1/SM1 * 1/SM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl       disp        hp
## mpg    36.324103  -9.172379  -633.0972 -320.7321
## cyl    -9.172379   3.189516   199.6603  101.9315
## disp -633.097208 199.660282 15360.7998 6721.1587
## hp   -320.732056 101.931452  6721.1587 4700.8669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an easy task, but it got me stuck for a few minutes…&lt;/p&gt;
&lt;p&gt;Lately, I finally discovered that there is (at least) one function in R taking care of the above task; it is the ‘cov2cor()’ function in the ‘nlme’ package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme)
cov2cor(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             mpg        cyl       disp         hp
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475
## disp -0.8475514  0.9020329  1.0000000  0.7909486
## hp   -0.7761684  0.8324475  0.7909486  1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is really easy to get drown in a glass of water!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>/2019/stat_general_correlation_alookat/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/stat_general_correlation_alookat/</guid>
      <description>


&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-for-the-pearson-correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions for the Pearson Correlation Coefficient&lt;/h1&gt;
&lt;p&gt;The Pearson correlation coefficients makes a few assumptions, which should be carefully checked.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interval-level measurement. Both variables should be measured on a quantitative scale.&lt;/li&gt;
&lt;li&gt;Random sampling. Each subject in the sample should contribute one value on X, and one value on Y. The values for both variables should represent a random sample drawn from the population of interest.&lt;/li&gt;
&lt;li&gt;Linearity. The relationship between X and Y should be linear.&lt;/li&gt;
&lt;li&gt;Bivarlate normal distribution. This means that (i) values of X should form a normal distribution at each value of Y and (ii) values of Y should form a normal distribution at each value of X.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;It is possible to test whether &lt;span class=&#34;math inline&#34;&gt;\(r = 0\)&lt;/span&gt; against the alternative $ r 0$. The test is based on the idea that the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}\]&lt;/span&gt;
is distributed as a Student’s t variable.&lt;/p&gt;
&lt;p&gt;Let’s take the two variables ‘cyl’ and ‘mpg’ from the ‘mtcars’ data frame. The correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- cor(mtcars$cyl, mtcars$gear)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.4926866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The T statistic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- r * sqrt(32 - 2) / sqrt(1 - r^2)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.101051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value for the null is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * pt(T, 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004173297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is clearly highly significant. The null can be rejected.&lt;/p&gt;
&lt;p&gt;As for hypothesis testing, it should be considered that the individuals where couple of measurements were taken should be independent. If they are not, the t test is invalid. I am dealing with this aspect somewhere else in my blog.&lt;/p&gt;
&lt;p&gt;#Correlation in R&lt;/p&gt;
&lt;p&gt;We have already seen that we can use the usual function ‘cor(matrix, method=)’. In order to obtain the significance, we can use the ‘rcorr()’ function in the Hmisc package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlations with significance levels
library(Hmisc)
corr2 &amp;lt;- rcorr(as.matrix(mtcars), type=&amp;quot;pearson&amp;quot;)
print(corr2$r, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg   cyl  disp    hp   drat    wt   qsec    vs     am  gear   carb
## mpg   1.00 -0.85 -0.85 -0.78  0.681 -0.87  0.419  0.66  0.600  0.48 -0.551
## cyl  -0.85  1.00  0.90  0.83 -0.700  0.78 -0.591 -0.81 -0.523 -0.49  0.527
## disp -0.85  0.90  1.00  0.79 -0.710  0.89 -0.434 -0.71 -0.591 -0.56  0.395
## hp   -0.78  0.83  0.79  1.00 -0.449  0.66 -0.708 -0.72 -0.243 -0.13  0.750
## drat  0.68 -0.70 -0.71 -0.45  1.000 -0.71  0.091  0.44  0.713  0.70 -0.091
## wt   -0.87  0.78  0.89  0.66 -0.712  1.00 -0.175 -0.55 -0.692 -0.58  0.428
## qsec  0.42 -0.59 -0.43 -0.71  0.091 -0.17  1.000  0.74 -0.230 -0.21 -0.656
## vs    0.66 -0.81 -0.71 -0.72  0.440 -0.55  0.745  1.00  0.168  0.21 -0.570
## am    0.60 -0.52 -0.59 -0.24  0.713 -0.69 -0.230  0.17  1.000  0.79  0.058
## gear  0.48 -0.49 -0.56 -0.13  0.700 -0.58 -0.213  0.21  0.794  1.00  0.274
## carb -0.55  0.53  0.39  0.75 -0.091  0.43 -0.656 -0.57  0.058  0.27  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(corr2$P, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mpg     cyl    disp      hp    drat      wt    qsec      vs
## mpg       NA 6.1e-10 9.4e-10 1.8e-07 1.8e-05 1.3e-10 1.7e-02 3.4e-05
## cyl  6.1e-10      NA 1.8e-12 3.5e-09 8.2e-06 1.2e-07 3.7e-04 1.8e-08
## disp 9.4e-10 1.8e-12      NA 7.1e-08 5.3e-06 1.2e-11 1.3e-02 5.2e-06
## hp   1.8e-07 3.5e-09 7.1e-08      NA 1.0e-02 4.1e-05 5.8e-06 2.9e-06
## drat 1.8e-05 8.2e-06 5.3e-06 1.0e-02      NA 4.8e-06 6.2e-01 1.2e-02
## wt   1.3e-10 1.2e-07 1.2e-11 4.1e-05 4.8e-06      NA 3.4e-01 9.8e-04
## qsec 1.7e-02 3.7e-04 1.3e-02 5.8e-06 6.2e-01 3.4e-01      NA 1.0e-06
## vs   3.4e-05 1.8e-08 5.2e-06 2.9e-06 1.2e-02 9.8e-04 1.0e-06      NA
## am   2.9e-04 2.2e-03 3.7e-04 1.8e-01 4.7e-06 1.1e-05 2.1e-01 3.6e-01
## gear 5.4e-03 4.2e-03 9.6e-04 4.9e-01 8.4e-06 4.6e-04 2.4e-01 2.6e-01
## carb 1.1e-03 1.9e-03 2.5e-02 7.8e-07 6.2e-01 1.5e-02 4.5e-05 6.7e-04
##           am    gear    carb
## mpg  2.9e-04 5.4e-03 1.1e-03
## cyl  2.2e-03 4.2e-03 1.9e-03
## disp 3.7e-04 9.6e-04 2.5e-02
## hp   1.8e-01 4.9e-01 7.8e-07
## drat 4.7e-06 8.4e-06 6.2e-01
## wt   1.1e-05 4.6e-04 1.5e-02
## qsec 2.1e-01 2.4e-01 4.5e-05
## vs   3.6e-01 2.6e-01 6.7e-04
## am        NA 5.8e-08 7.5e-01
## gear 5.8e-08      NA 1.3e-01
## carb 7.5e-01 1.3e-01      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also use these functions with two matrices, to obtain the correlations of each column in one matrix with each column in the other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation matrix from mtcars
x &amp;lt;- mtcars[1:3]
y &amp;lt;- mtcars[4:6]
cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              hp       drat         wt
## mpg  -0.7761684  0.6811719 -0.8676594
## cyl   0.8324475 -0.6999381  0.7824958
## disp  0.7909486 -0.7102139  0.8879799&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Relationship to slope in linear regression&lt;/p&gt;
&lt;p&gt;The correlation coefficient and slope in linear regression bear some similarities, as both describe how Y changes when X is changed. However, in correlation, we have two random variables, while in regression we have Y random, X fixed and Y is regarded as a function of X (not the other way round).&lt;/p&gt;
&lt;p&gt;Without neglecting their different meaning, it may be useful to show the algebraic relationship between the correlation coefficient and the slope in regression. Let’s simulate a dataset with two variables, coming from a multivariate normal distribution, with means respectively equal to 10 and 2, and variance-covariance matrix of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
cov &amp;lt;- matrix(c(2.20, 0.48, 0.48, 0.20), 2, 2)
cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] 2.20 0.48
## [2,] 0.48 0.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the ‘mvrnomr()’ function to generate the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
dataset &amp;lt;- data.frame( mvrnorm(n=10, mu = c(10, 2), Sigma = cov) )
names(dataset) &amp;lt;- c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;)
dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X        Y
## 1  11.756647 2.547203
## 2   9.522180 2.199740
## 3   8.341254 1.862362
## 4  13.480005 2.772031
## 5   9.428296 1.573435
## 6   9.242788 1.861756
## 7  10.817449 2.343918
## 8  10.749047 2.451999
## 9  10.780400 2.436263
## 10 11.480301 1.590436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficient and slope are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- with(dataset, cor(X, Y))
b1 &amp;lt;- coef( lm(Y ~ X, data=dataset) )[2]
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equation for the slope is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 = \frac{ \sum_{i = 1}^n \left[ ( X-\mu_X )( Y-\mu_Y )\right] }{ \sigma^2_X } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From there, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r = b_1 \frac{\sigma_X}{ \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ b_1 = r \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- with(dataset, sd(X) )
sigmaY &amp;lt;- with(dataset, sd(Y) )
b1 * sigmaX / sigmaY &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r * sigmaY / sigmaX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also easy to see that the correlation coefficient is the slope of regression of standardised Y against standardised X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yst &amp;lt;- with(dataset, scale(Y, scale=T) )
summary( lm(Yst ~ I(scale(X, scale = T) ), data = dataset) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Yst ~ I(scale(X, scale = T)), data = dataset)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082006 -0.067143 -0.036850  0.009214  0.237923 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)            -5.633e-18  3.478e-02   0.000   1.0000  
## I(scale(X, scale = T))  1.785e-01  7.633e-02   2.339   0.0475 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.11 on 8 degrees of freedom
## Multiple R-squared:  0.4061, Adjusted R-squared:  0.3319 
## F-statistic: 5.471 on 1 and 8 DF,  p-value: 0.04748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;#Intra-class correlation (ICC)&lt;/p&gt;
&lt;p&gt;It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intra-class correlation coefficient is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IC = {\displaystyle {\frac {\sigma _{\alpha }^{2}}{\sigma _{\alpha }^{2}+\sigma _{\varepsilon }^{2}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\alpha }^{2}\)&lt;/span&gt; is the variance between groups and &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\varepsilon }^{2}\)&lt;/span&gt; is the variance within a group (better, the variance of one observation within a group). The sum of those two variances is the total variance of observations. In words, the intra-class correlation coefficient measures the joint variability of subjects in the same group (that relates on how groups are different from one another), with respect to the total variability of observations. If subjects in one group are very similar to one another (small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\varepsilon}\)&lt;/span&gt;) but groups are very different (high &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha}\)&lt;/span&gt;), the ICC is very high.&lt;/p&gt;
&lt;p&gt;The existence of grouping of residuals is very important in ANOVA, as it means that independence is violated, which calls for the use of mixed models.&lt;/p&gt;
&lt;p&gt;But … this is a totally different story …&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some useful equations for nonlinear regression in R</title>
      <link>/articles/usefulequations/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/articles/usefulequations/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Very rarely, biological processes follow linear trends. Just think about how a crop grows, or responds to increasing doses of fertilisers/xenobiotics. Or think about how an herbicide degrades in the soil, or about the germination pattern of a seed population. It is very easy to realise that curvilinear trends are far more common than linear trends. Furthermore, asymptotes and/or inflection points are very common in nature. We can be sure: linear equations in biology are just a way to approximate a response over a very narrow range for the independent variable.&lt;/p&gt;
&lt;p&gt;Therefore, as biologists, we need to be able to describe our experimental data by using a wide range of curvilinear equations. We need to be able to ‘read’ those equations and use their parameters to interpret and understand biological processes. I thought that it would be useful to list the most commonly used curvilinear functions and show examples of how they can be fit by using R.&lt;/p&gt;
&lt;p&gt;When it comes to nonlinear regression, I have a strong personal preference for the ‘drc’ package and the ‘drm()’ function therein. However, it is also worth mentioning the traditional ‘nls()’ function in the ‘stats’ package. You may know that nonlinear least squares work iteratively: we need to provide initial guesses for model parameters and the algorithm adjusts them step by step, finally converging on the approximate least squares solution. To my experience, providing initial guesses may be troublesome. Therefore, it is very convenient to use R functions together with the appropriate self-starting routines, which can greatly semplify the fitting process. These self-starters can be found in the ‘drc’, ‘nlme’ and ‘aomisc’ packages.&lt;/p&gt;
&lt;p&gt;Let’s load the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
library(nlme)
library(aomisc)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;curve-shapes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Curve shapes&lt;/h1&gt;
&lt;p&gt;Functions can be easily classified by the shape they show when they are plotted in a graph. This is helpful to select the correct one, according to the trend of the process under study. We have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomials
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Straight line&lt;/li&gt;
&lt;li&gt;Quadratic polynomial&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Concave/Convex curves (no inflection)
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exponential function&lt;/li&gt;
&lt;li&gt;Asymptotic function&lt;/li&gt;
&lt;li&gt;Negative exponential function&lt;/li&gt;
&lt;li&gt;Power curve function&lt;/li&gt;
&lt;li&gt;Logarithmic function&lt;/li&gt;
&lt;li&gt;Rectangular hyperbola&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Sigmoidal curves
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Logistic function&lt;/li&gt;
&lt;li&gt;Gompertz function&lt;/li&gt;
&lt;li&gt;Modified Gompertz function&lt;/li&gt;
&lt;li&gt;Log-logistic function&lt;/li&gt;
&lt;li&gt;Weibull (type 1) function&lt;/li&gt;
&lt;li&gt;Weibull (type 2) function&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Curves with maxima/minima
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bragg function&lt;/li&gt;
&lt;li&gt;Lorentz function&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomials&lt;/h1&gt;
&lt;p&gt;Polynomials are the most flexible tool to describe biological processes. They are simple and, although curvilinear, they are linear in the parameters and can be fitted by using linear regression. One disadvantage is that they cannot describe asymptotic processes, which are very common in biology. Furthermore, they are prone to overfitting, as we may be tempted to add terms to improve the fit, with little care for biological realism.&lt;/p&gt;
&lt;p&gt;Nowadays, thanks to the wide availability of nonlinear regression algorithms, the use of polynomials has sensibly decreased; linear or quadratic polynomials are mainly used when we want to approximate the observed response within a narrow range of a quantitative predictor. On the other hand, higher order polynomials are very rarely seen, in practice.&lt;/p&gt;
&lt;div id=&#34;straight-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Straight line&lt;/h2&gt;
&lt;p&gt;Obviously, this is not a curve, although it deserves to be mentioned here. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1 \, X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is the slope, i.e. the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The Y increases as X increases when &lt;span class=&#34;math inline&#34;&gt;\(b_1 &amp;gt; 0\)&lt;/span&gt;, otherwise it decreases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic function&lt;/h2&gt;
&lt;p&gt;The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1\, X + b_2 \, X^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt;, taken separately, lack a clear biological meaning. However, it is interesting to consider that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a + b*X + c*X^2), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## b + c * (2 * X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which measures the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. We see that such an increase/decrease is not constant, but it changes according to the level of X. The stationary point is &lt;span class=&#34;math inline&#34;&gt;\(X_m = - b_1 / 2 b_2\)&lt;/span&gt;; it is a maximum when &lt;span class=&#34;math inline&#34;&gt;\(b_2 &amp;gt; 0\)&lt;/span&gt;, otherwise it is a minimum.&lt;/p&gt;
&lt;p&gt;At the maximum/minimum, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_m = \frac{4\,b_0\,b_2 - b_1^2}{4\,b_2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-fitting-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polynomial fitting in R&lt;/h2&gt;
&lt;p&gt;Polynomials in R are fit by using the linear model function ‘lm()’. Although this is not efficient, in a couple of cases I found myself in the need of fitting a polynomial by using the ‘nls()’ o ‘drm()’ functions. For these unusual cases, one can use the ‘NLS.Linear()’, NLS.poly2(), ‘DRC.Linear()’ and DRC.Poly2() self-starting function, as available in the ‘aomisc’ package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concaveconvex-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concave/Convex curves&lt;/h1&gt;
&lt;div id=&#34;exponential-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential curve&lt;/h2&gt;
&lt;p&gt;The exponential function describes an increasing/decreasing &lt;em&gt;trend&lt;/em&gt;, with constant relative rate. The most common equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  e^{k X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other possible parameterisations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  b^X  =  e^{d + k X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above parameterisations are equivalent, as proved by setting &lt;span class=&#34;math inline&#34;&gt;\(b = e^k\)&lt;/span&gt; e &lt;span class=&#34;math inline&#34;&gt;\(a = e^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  b^X  = a  (e^k)^{X} =  a  e^{kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  e^{kX} = e^d \cdot e^{kX} =  e^{d + kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The meaning of parameters is clear: &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; represents the relative increase/decrease of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit increase of X. &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases if &lt;span class=&#34;math inline&#34;&gt;\(k &amp;gt; 0\)&lt;/span&gt; (exponential growth), while it decreases when &lt;span class=&#34;math inline&#34;&gt;\(k &amp;lt; 0\)&lt;/span&gt; (exponential decay). This curve is used to describe the growth of populations in unlimiting environmental conditions, or to describe the degradation of xenobiotics in the environment (first-order degradation kinetic).&lt;/p&gt;
&lt;p&gt;The exponential function is nonlinear in &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and needs to be fitted by using ‘nls()’ or ‘drm()’. It is possible to make profit of the self-starting routines in ‘NLS.expoGrowth()’, ‘NLS.expoDecay()’, ‘DRC.expoGrowth()’ and ‘DRC.expoDecay()’. All these functions are available in the ‘aomisc’ package. The ‘drc’ package also contains the function ‘EXD.2()’, that fits an exponential decay model, with a slightly different parameterisation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \exp(-x/e) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the same as &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in the model above and &lt;span class=&#34;math inline&#34;&gt;\(e = 1/k\)&lt;/span&gt;. For all the forementioned exponential decay equations &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The function ‘EXD.3()’ in the ‘drc’ package also includes a lower asymptote &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d -c) \exp(-x/e) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(degradation)
degradation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Time    Conc
## 1     0  96.400
## 2    10  46.300
## 3    20  21.200
## 4    30  17.890
## 5    40  10.100
## 6    50   6.900
## 7    60   3.500
## 8    70   1.900
## 9     0 102.300
## 10   10  49.200
## 11   20  26.310
## 12   30  14.220
## 13   40   5.400
## 14   50   3.400
## 15   60   0.500
## 16   70   0.200
## 17    0 101.330
## 18   10  54.890
## 19   20  28.120
## 20   30  13.330
## 21   40   6.110
## 22   50   0.350
## 23   60   2.100
## 24   70   0.922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Conc ~ Time, fct = DRC.expoDecay(),
             data = degradation)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Exponential Decay Model (2 parms)
## 
## Parameter estimates:
## 
##                    Estimate Std. Error t-value   p-value    
## init:(Intercept) 99.6349312  1.4646680  68.026 &amp;lt; 2.2e-16 ***
## k:(Intercept)     0.0670391  0.0019089  35.120 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.621386 (22 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotic regression model&lt;/h2&gt;
&lt;p&gt;The asymptotic regression model describes a limited growth, where Y approaches an horizontal asymptote as X tends to infinity. The rate of growth is maximum at the beginning and approaches 0 as Y approaches the plateau. This equation is used in several different parameterisations and it is also known as Monomolecular Growth, Mitscherlich law or von Bertalanffy law.&lt;/p&gt;
&lt;p&gt;Due to its biological meaning, the most widespread parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a - (a - b) \, \exp (- c  X)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum attainable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is proportional to the relative rate of Y increase while X increases. Indeed, we can see that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a - (a - b) * exp (- c * X)), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (a - b) * (exp(-c * X) * c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;that is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y’ = c \, (a - Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model can be fit with R by using the self starter functions ‘NLS.asymReg()’ and DRC.asymReg(), in the ‘aomisc’ package. The ‘drc’ package contains the function AR.3(), that is a similar parameterisation where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation named ‘SSasymp()’, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We simulate an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X &amp;lt;- c(1, 3, 5, 7, 9, 11, 13, 20)
a &amp;lt;- 20; b &amp;lt;- 5; c &amp;lt;- 0.3
Ye &amp;lt;- asymReg.fun(X, a, b, c)
epsilon &amp;lt;- rnorm(8, 0, 0.5)
Y &amp;lt;- Ye + epsilon
model &amp;lt;- drm(Y ~ X, fct = DRC.asymReg())
plot(model, log = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we take the above equation and add the constraint that &lt;span class=&#34;math inline&#34;&gt;\(b = 0\)&lt;/span&gt;, we get the following equation, that is often known as negative exponential equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a [1 -  \exp (- c  X) ]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This equation has a similar shape to the asymptotic regression, but Y is 0 when X is 0 (the curve passes through the origin). It is often used to model the absorbed Photosintetically Active Radiation (&lt;span class=&#34;math inline&#34;&gt;\(Y = PAR_a\)&lt;/span&gt;) as a function of incident PAR (&lt;span class=&#34;math inline&#34;&gt;\(a = PAR_i\)&lt;/span&gt;), Leaf Area Index (X = LAI) and the extinction coefficient (c = k).&lt;/p&gt;
&lt;p&gt;This model can be fit with R by using the self starter functions ‘NLS.negExp()’ and DRC.negExp(), in the ‘aomisc’ package. The ‘drc’ package contains the function AR.2(), where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named ‘SSasympOrig()’, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power curve&lt;/h2&gt;
&lt;p&gt;The power curve is also known as Freundlich equation or allometric function and the most common parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a \, X^b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is perfectly equivalent to an exponential curve on the
logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a\,X^b  = a\, e^{\log( X^b )}  = a\,e^{b \, \log(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve does not have an asymptote for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The slope (first derivative) of the curve is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * X^b), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a * (X^(b - 1) * b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both paraeters relate to the slope of the curve and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates its shape. If &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt;- b &amp;lt; 1\)&lt;/span&gt;, Y increases as X increases and the curve is convex up. This is used, e.g., to model the number of plant species as a function of sampling area (Muller-Dumbois method).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(speciesArea)
speciesArea&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Area numSpecies
## 1    1          4
## 2    2          5
## 3    4          7
## 4    8          8
## 5   16         10
## 6   32         14
## 7   64         19
## 8  128         22
## 9  256         26&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(numSpecies ~ Area, fct = DRC.powerCurve(),
             data = speciesArea)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Power curve (Freundlich equation) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 4.348404   0.337197  12.896 3.917e-06 ***
## b:(Intercept) 0.329770   0.016723  19.719 2.155e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.9588598 (7 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curve(powerCurve.fun(x, coef(model)[1], -coef(model)[2]),
      xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 1\)&lt;/span&gt; is negative, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curve(powerCurve.fun(x, coef(model)[1], 2),
      xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logarithmic-equation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logarithmic equation&lt;/h2&gt;
&lt;p&gt;This is indeed a linear model on log-transformed X:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = a + b \, \log(X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to the logarithmic function, X must be $ &amp;gt; 0$. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates the shape: if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 0\)&lt;/span&gt;, the curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The logarithmic equation can be fit by using ‘lm()’. If necessary, it can also be fit by using ‘nls()’ and ‘drm()’; the self-starting functions ‘NLS.logCurve()’ and ‘DRC.logCurve()’ are available within the ‘aomisc’ package. We show some simulated data as examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#b is positive
set.seed(5678)
X &amp;lt;- c(1,2,4,5,7,12)
a&amp;lt;-2; b&amp;lt;- 0.5
Ye &amp;lt;-  a + b*log(X)
res &amp;lt;- rnorm(6, 0, 0.1)
Y &amp;lt;- Ye + res
model &amp;lt;- lm(Y ~ log(X) )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ log(X))
## 
## Residuals:
##         1         2         3         4         5         6 
## -0.025947  0.013207  0.050827 -0.011989 -0.008408 -0.017690 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.99653    0.02494   80.06 1.46e-07 ***
## log(X)       0.45088    0.01580   28.54 8.97e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.03146 on 4 degrees of freedom
## Multiple R-squared:  0.9951, Adjusted R-squared:  0.9939 
## F-statistic: 814.7 on 1 and 4 DF,  p-value: 8.967e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Linear regression on log-transformed x (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 1.996534   0.024939  80.058 1.459e-07 ***
## b:(Intercept) 0.450883   0.015797  28.543 8.967e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.03145785 (4 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#b is negative
X &amp;lt;- c(1,2,4,5,7,12)
a &amp;lt;- 2; b &amp;lt;- -0.5
Ye &amp;lt;-  a + b*log(X)
res &amp;lt;- rnorm(6, 0, 0.1)
Y &amp;lt;- Ye + res
model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Linear regression on log-transformed x (2 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## a:(Intercept)  2.115759   0.103920 20.3595 3.437e-05 ***
## b:(Intercept) -0.569125   0.065826 -8.6459 0.0009843 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.1310851 (4 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;michaelis-menten-equation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Michaelis-Menten equation&lt;/h2&gt;
&lt;p&gt;This is a rectangular hyperbola, often parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, up to a plateau level. The parameter &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the higher asymptote (for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;), while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the X value giving a response equal to &lt;span class=&#34;math inline&#34;&gt;\(a/2\)&lt;/span&gt;. Indeed, it is easily shown that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a}{2} = \frac{a\,X_{50} } {b + X_{50} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which leads to &lt;span class=&#34;math inline&#34;&gt;\(b = x_{50}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The slope (first derivative) is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression( (a*X) / (b + X) ), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a/(b + X) - (a * X)/(b + X)^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we can see that the initial slope (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) is $i = a/b $.&lt;/p&gt;
&lt;p&gt;In R, this model can be fit by using ‘nls()’ and the self starting functions ‘SSmicmen()’, within the package ‘nlme’. If we prefer a ‘drm()’ fit, we can use the ‘MM.2()’ function in the package ‘drc’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X &amp;lt;- c(3, 5, 7, 22, 28, 39, 46, 200)
a &amp;lt;- 15; b &amp;lt;- 0.5
Ye &amp;lt;- as.numeric( SSmicmen(X, a, b) )
res &amp;lt;- rnorm(8, 0, 0.1)
Y &amp;lt;- Ye + res

#nls fit
model &amp;lt;- nls(Y ~ SSmicmen(X, a, b))
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: Y ~ SSmicmen(X, a, b)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## a 14.97166    0.06057  247.17 2.96e-13 ***
## b  0.50207    0.03345   15.01 5.51e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1196 on 6 degrees of freedom
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 3.043e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#drm fit
model &amp;lt;- drm(Y ~ X, fct = MM.2())
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Michaelis-Menten (2 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## d:(Intercept) 14.971733   0.060492 247.499 2.936e-13 ***
## e:(Intercept)  0.502126   0.033359  15.052 5.418e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.1195748 (6 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The ‘drc’ package also contains the self starting function ‘MM.3()’, where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is allowed to be equal to &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt;, when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-loss-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield-loss curve&lt;/h2&gt;
&lt;p&gt;Weed-crop competition studies make use of a reparameterised Michaelis-Menten model. Indeed, th initial slope of a Michaelis-Menten can be assumed as a measure of competition, that is the reduction in yield (Y) when the first weed is added to the system. Therefore, the Michaelis-Methen model has been reparameterised to include &lt;span class=&#34;math inline&#34;&gt;\(i = a/b\)&lt;/span&gt; as an explicit parameter. The reparameterised equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{i \, X}{1 + \frac{i \, X}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model can be used to describe yield losses as a function of weed density. It can be fit by using the self starting functions ‘NLS.YL()’ or ‘DRC.YL()’ in the ‘aomisc’ package. Usually, competion studies produce yield data and, therefore, yield lossed need to be calculated by using the weed-free yield and the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_L  = \frac{Y_{WF}  - Y_w }{Y_{WF} } \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_W\)&lt;/span&gt; is the observed yield and &lt;span class=&#34;math inline&#34;&gt;\(Y_{WF}\)&lt;/span&gt; is the weed-free yield. We show an example relating to sunflower grown at increasing densities of the weed &lt;em&gt;Sinapis arvensis&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(competition)
Ywf &amp;lt;- mean( competition$Yield[competition$Dens == 0] )
competition$YL &amp;lt;- ( Ywf - competition$Yield ) / Ywf * 100 

#nls fit
model &amp;lt;- nls(YL ~ NLS.YL(Dens, a, i), data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: YL ~ NLS.YL(Dens, a, i)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## a    8.207      1.187   6.914 1.93e-08 ***
## i   75.048      2.353  31.894  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 6.061 on 42 degrees of freedom
## 
## Number of iterations to convergence: 2 
## Achieved convergence tolerance: 2.685e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#drc fit
model &amp;lt;- drm(YL ~ Dens, fct = DRC.YL(), data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Yield-Loss function (Cousens, 1985) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## i:(Intercept)   8.2068     1.1715  7.0056 1.427e-08 ***
## A:(Intercept)  75.0492     2.3298 32.2133 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  6.060578 (42 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above fit constrains the yield loss to be 0 when weed density is 0. This is logical, but, it has the important consequence that the weed-free yield is constrained to be equal to the observed weed-free yield, which is not realistic. Therefore, we can reparameterise the yield-loss function, in order to use the observed yield as the dependent variable.&lt;/p&gt;
&lt;p&gt;Indeed, from the above equation we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_W  = Y_{WF}  - \frac{Y_L }{100}Y_{WF}  = Y_{WF} \left( {1 - \frac{Y_L }{100}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_W  = Y_{WF} \left( 1 - \frac{i\, X}{100 \left( 1 + \frac{i \, X}{a} \right) } \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function can be fit with ‘drm()’, by using the ‘DRC.cousens85()’ self starting function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Yield ~ Dens, fct = DRC.cousens85(), 
             data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Yield-Weed Density function (Cousens, 1985) (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## YWF:(Intercept) 30.47211    0.92763 32.8493 &amp;lt; 2.2e-16 ***
## i:(Intercept)    8.24038    1.36541  6.0351 3.857e-07 ***
## a:(Intercept)   75.07312    2.40366 31.2328 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  1.866311 (41 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sygmoidal-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sygmoidal curves&lt;/h1&gt;
&lt;p&gt;Sygmoidal curves are S-shaped and they may be increasing, decreasing, symmetric or non-simmetric around the inflection point. They are parameterised in countless ways, which may be often confusing. Therefore, we will show a common parameterisation, that is very useful in biological terms.&lt;/p&gt;
&lt;div id=&#34;logistic-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic curve&lt;/h2&gt;
&lt;p&gt;The logistic curve derives from the cumulative logistic distribution function; the curve is symmetric around the inflection point and it it may be parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + exp(b (X - e))}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the higher asymptote, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the lower asymptote, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value producing a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be positive or negative and, consequently, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may increase or decrease as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The above function is known as four-parameter logistic. If necessary, contraints can be put on parameter values, i.e. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; can be constrained to 0 (three-parameter logistic). Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be also contrained to 1 (two-parameter logistic).&lt;/p&gt;
&lt;p&gt;The four- and three-parameter logistic curves can be fit by ‘nls()’, respectively with the self-starting functions ‘SSfpl()’ and ‘SSlogis’ (‘nlme’ package). In these functions, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(scal = -1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With ‘drm()’, we can use the self-starting functions ‘L.4()’ and ‘L.3()’, The ‘L.2()’ function has been included in the ‘aomisc’ package.&lt;/p&gt;
&lt;p&gt;Logistic functions are very useful, e.g., for plant growth studies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(beetGrowth)
beetGrowth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    DAE weightInf weightFree
## 1   21   0.06000  0.0715091
## 2   21   0.06000  0.0662547
## 3   21   0.11000  0.0747931
## 4   27   0.20000  0.3368074
## 5   27   0.20000  0.3952256
## 6   27   0.21000  0.2520960
## 7   38   2.13000  2.3225072
## 8   38   3.03000  1.7163224
## 9   38   1.27000  1.2189231
## 10  49   6.13000 11.7761096
## 11  49   5.76000 13.6191507
## 12  49   7.78000 12.1462931
## 13  65  17.05000 33.1067720
## 14  65  22.48000 24.9648226
## 15  65  12.66000 34.6577561
## 16 186  21.51010 38.8329912
## 17 186  26.25887 27.8375016
## 18 186  27.67733 37.7165427&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(weightFree ~ DAE, fct = L.3(), data = beetGrowth)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Logistic (ED50 as parameter) with lower limit fixed at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.179393   0.039059 -4.5929 0.0003519 ***
## d:(Intercept) 34.532001   1.676430 20.5985 2.057e-12 ***
## e:(Intercept) 52.384788   1.580269 33.1493 1.838e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.970191 (15 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gompertz-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gompertz Curve&lt;/h2&gt;
&lt;p&gt;The Gompertz curve is parameterised in very many ways. We favour a parameterisation that resambles the one used for the logistic function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;were the parameters have the same meaning as those in the logistic function. The difference is that this curve is not symmetric around the inflection point. As for the logistic, we can have a four-, three- and two-parameter Gompertz functions, which can be fit by using ‘drm()’ and, respectively the ‘G.4()’, ‘G.3()’ and ‘G.2()’ sef-starters. The three-parameter Gompertz can also be fit with ‘nls()’, by using the ‘SSGompertz()’ self-starter in the ‘nlme’ package, although this is a different parameterisation.&lt;/p&gt;
&lt;p&gt;We give an example of the different shapes for the logistic (red) and Gompertz (black) functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-type-of-asimmetry&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another type of asimmetry&lt;/h2&gt;
&lt;p&gt;We have seen that, with respect to the logistic, the Gompertz shows a longer lag at the beginning, but raises steadily afterwards. We could describe a different pattern by changing the Gompertz function as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We add to the previous graph this function (in blue), to show how it differs from the logistic and Gompertz.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- 10; c &amp;lt;- 2; e &amp;lt;- 7; b &amp;lt;- - 0.5
curve( G.4.fun(x, b, c, d, e), xlim = c(0, 20) , xlab=&amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)
curve( L.4.fun(x, b, c, d, e), add = T, col = &amp;quot;red&amp;quot; )
curve( E.4.fun(x, b, c, d, e), add = T, col = &amp;quot;blue&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The self-starters for this function are not yet available, at least to the best of my knowledge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-logistic-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log-logistic curve&lt;/h2&gt;
&lt;p&gt;In many applications, the sigmoidal response curve is symmetric on the logarithm of x, which requires a log-logistic curve (a log-normal curve would be practically equivalent, but it is used far less often). For example, in biologic assays (but also in germination assays), the log-logistic curve is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \exp \left\{ b \left[ \log(X) - \log(e) \right] \right\} } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the parameters have the very same meanng as the logistic equationn given above. It is easy to see that the above equation is equivalent to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \left( \frac{X}{e} \right)^b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another possible parameterisation is the so-called Hill function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = \frac{a \, X^b}{ X^b + e^b} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{a \, X^b}{ X^b + e^b} =  \frac{a}{ \frac{X^b}{X^b} + \frac{c^b}{X^b}} = \frac{a}{ 1 + \left( \frac{c}{X} \right)^b} = \frac{a}{ 1 + \left( \frac{c}{X} \right)^b} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Log-logistic functions are used for crop growth, seed germination and bioassay work and they can have the same constraints as the logistic function. The four-parameter logistic is available as ‘LL.4()’ in the ‘drc’ package and as ‘SSfpl()’ in the ‘nlme’ package. This latter function replaces &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(scal = 1/b\)&lt;/span&gt;. Also in ‘drc’, we have ‘LL.3()’ (three-parameter logistic, with &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;) and ‘LL.2()’ (two-parameter logistic, with &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;). In ‘nlme’ we have ‘SSlogis()’, that is a three-parameter logistic with &lt;span class=&#34;math inline&#34;&gt;\(scal = 1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We show an example of a log-logistic fit, relating to a bioassay with &lt;em&gt;Brassica rapa&lt;/em&gt; treated at increasing dosages of an herbicide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)
model &amp;lt;- drm(FW ~ Dose, fct = LL.4(), data = brassica)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Log-logistic (ED50 as parameter) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.45113    0.24113  6.0181 1.743e-06 ***
## c:(Intercept)  0.34948    0.18580  1.8810   0.07041 .  
## d:(Intercept)  4.53636    0.20514 22.1140 &amp;lt; 2.2e-16 ***
## e:(Intercept)  2.46557    0.35111  7.0221 1.228e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4067837 (28 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-curve-type-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull curve (type 2)&lt;/h2&gt;
&lt;p&gt;The type 2 Weibull curve is for the Gompertz curve what the log-logistic curve is for the logistic curve. The equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ b \, (log(X) - log(e)) \right] \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the parameters have the very same meaning as the other sygmoidal curves given above.&lt;/p&gt;
&lt;p&gt;As for fitting, the ‘drc’ package contains the self-starting functions ‘W2.2()’, ‘W2.3()’ and ‘W2.4()’ that can be used to fit respectively the two-, three- and four-parameter type 2 Weibull functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-curve-type-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull curve (type 1)&lt;/h2&gt;
&lt;p&gt;The type 1 Weibull is similar to the type 2 Weibull, but describes a different type of asymmetry (see above):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (log(X) - log(e)) \right] \right\} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(FW ~ Dose, fct = W2.4(), data = brassica)
model2 &amp;lt;- drm(FW ~ Dose, fct = W1.4(), data = brassica)
plot(model)
plot(model2, add=T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Articles/usefulEquations_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About this site</title>
      <link>/aboutthis/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aboutthis/</guid>
      <description>


&lt;div id=&#34;the-broken-bridge-between-biologists-and-statisticians&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The broken bridge between biologists and statisticians&lt;/h1&gt;
&lt;p&gt;Hi, all!&lt;/p&gt;
&lt;p&gt;This website deals with statistic. “What’s new then?”, you might say. “There’s plenty of them, out there!”. Yes, that’s right. However, the new thing is that I am not a statistician. I am an applied biologist who happened to be fascinated by the idea that natural phenomena might be described, interpreted and understood by using the universal language of math. Since then, I’ve been mainly involved with experimental design and data analyses. Indeed, I turned myself into an applied statistician…&lt;/p&gt;
&lt;p&gt;My collegue Marcin Kozak, in one of his papers, noted that, although efficient communication between biologists and statisticians is fundamental for the development of knowledge, there is indeed a problem in this respect (the broken bridge)&lt;span class=&#34;citation&#34;&gt;(Kozak 2016)&lt;/span&gt;. I would like to help build such bridge.&lt;/p&gt;
&lt;p&gt;Indeed, I developed the awareness that biostatistic is a fundamental component of science: it helps reach more reliable conclusions. Now I’m going grey… I would like to pass some of my experience to my younger collegues and help them save a few headaches… Though my language might be rough and overly simplistic, I do hope it is useful to reach the above aim: spread a better awareness of the potential usefulness, beauty and limitations of biostatistic.&lt;/p&gt;
&lt;p&gt;This is an ongoing project. All blog posts are listed under the ‘Home’ directory of this website and they are searchable by date and tag. You will also find a series of tutorials and a web book (in italian), under the list pages of the single sections, i.e., &lt;a href=&#34;/tutorials/&#34;&gt;tutorials&lt;/a&gt; and &lt;a href=&#34;/_statbook/&#34;&gt;book&lt;/a&gt;. If you would like to contribute or comment, please, drop me a note to &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-KozakCommunicationagriculturalscientists2016&#34;&gt;
&lt;p&gt;&lt;span class=&#34;smallcaps&#34;&gt;Kozak, M&lt;/span&gt; (2016) Communication between agricultural scientists and statisticians: A broken bridge? &lt;em&gt;Scientia Agricola&lt;/em&gt; &lt;strong&gt;73&lt;/strong&gt;, 505–511&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tutorials and articles</title>
      <link>/tutorials/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/tutorials/</guid>
      <description>

&lt;h1 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/nonLinearRegression/&#34;&gt;Non-linear regression analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onofriandreapg.github.io/agriCensData/&#34;&gt;Interval-censored data in agriculture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/seedGermination/&#34;&gt;Analysing seed germination data with R: a tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;articles&#34;&gt;Articles&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Blogdow: &lt;a href=&#34;/articles/BlogdownSteps/&#34;&gt;My learning path with blogdown&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;other-articles-italian-language&#34;&gt;Other articles (Italian language)&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/PCA/&#34;&gt;L&amp;rsquo;analisi delle componenti principali&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/CVA/&#34;&gt;L&amp;rsquo;analisi delle variabili canoniche&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>My first experience with blogdown</title>
      <link>/2018/2018-11_15-first-day-with-blogdown/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-11_15-first-day-with-blogdown/</guid>
      <description>&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;

&lt;p&gt;I thought that it might be useful to write down a few notes, to summarise my steps ahead, during the learning process. I do not work with blogdown everyday and I tend to forget things quite easily. Therefore, these notes may help me recap how far I have come. And they might also help other beginners, to speed up their initial steps with such a powerful blogging platform.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll find my notes &lt;a href=&#34;/articles/blogdownSteps/&#34;&gt;here&lt;/a&gt;; I&amp;rsquo;ll try to keep them updated.&lt;/p&gt;

&lt;p&gt;Happy reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My learning path with blogdown</title>
      <link>/articles/blogdownsteps/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/articles/blogdownsteps/</guid>
      <description>

&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;

&lt;p&gt;I worked with R Studio and installed the library &amp;lsquo;blogdown&amp;rsquo;. I followed the instructions and used the &amp;lsquo;NEW PROJECT&amp;rsquo; menu and added a &amp;lsquo;NEW BLOGDOWN SITE&amp;rsquo; to my file system. I selected the &amp;lsquo;hugo-blog-jeffprod&amp;rsquo; theme. This created a bunch of directories in my working directory. If I use the &amp;lsquo;serve_site()&amp;rsquo; function in console, the Hugo server is called and the site is continuously updated, as soon as I make changes and save them. When the server is started, a message appears, saying: &amp;ldquo;Serving the directory PROJECTDIR at &lt;a href=&#34;http://127.0.0.1:7958&amp;quot;&#34;&gt;http://127.0.0.1:7958&amp;quot;&lt;/a&gt;. I copied the http address on Safari (I am working on Mac OS&amp;hellip;), so that I can check my changes there.&lt;/p&gt;

&lt;p&gt;Here are my steps to the learning the process.&lt;/p&gt;

&lt;h1 id=&#34;step-1-the-mathjax-problem&#34;&gt;STEP 1 - The MathJax problem&lt;/h1&gt;

&lt;p&gt;I added a post to the &amp;lsquo;content&amp;rsquo; directory. This was a trial post, with an equation in latex code. I immediately noted that there was no support to MathJax. Surfing the net, I found a solution for this. These were my steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The folder &amp;lsquo;themes/hugo-blog-jeffprod/layouts/partials&amp;rsquo; contains the template file &amp;lsquo;header.html&amp;rsquo;. The blogdown documentation suggests not to directly modify this file. Thus I created a copy on the &amp;lsquo;layouts/partials/&amp;rsquo; folder in my root directory. This folder did not exist at the beginning, so I created it.&lt;/li&gt;
&lt;li&gt;I opened the newly created file &amp;lsquo;layouts/partials/header.html&amp;rsquo; and added the following lines within the &lt;head&gt; &amp;hellip; &lt;/head&gt; section:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Equations seem to render ok, now.&lt;/p&gt;

&lt;h1 id=&#34;step-2-change-the-blog-name-and-author&#34;&gt;STEP 2 - Change the blog name and author&lt;/h1&gt;

&lt;p&gt;I modified the &amp;lsquo;config.toml&amp;rsquo; file in the root directory and added name and author of the site. An easy step&amp;hellip;&lt;/p&gt;

&lt;h1 id=&#34;step-3-clean-the-example-files&#34;&gt;STEP 3 - Clean the example files&lt;/h1&gt;

&lt;p&gt;There were a lot of example posts in the &amp;lsquo;content&amp;rsquo; directory. I removed them all, with no problems. I also had to remove the content of the &amp;lsquo;static&amp;rsquo; folder. These files were not needed, because they belonged to the example posts.&lt;/p&gt;

&lt;h1 id=&#34;step-4-where-do-i-put-my-post&#34;&gt;Step 4 - Where do I put my post?&lt;/h1&gt;

&lt;p&gt;New posts need to be put in the &amp;lsquo;content/post&amp;rsquo; directory. In order to add a post, I am using the function &amp;lsquo;blogdown:::new_post_addin()&amp;rsquo; (I like working in console&amp;hellip;), which creates a correct skeleton. Blog posts are listed under the home page of my blog, sorted by date and by tags.&lt;/p&gt;

&lt;h1 id=&#34;step-5-where-do-i-put-my-non-post-pages&#34;&gt;Step 5 - Where do I put my non-post pages?&lt;/h1&gt;

&lt;p&gt;I did not intend to create a pure blog. I wanted also to add static pages, such as articles, tutorials, a project page, links and other stuff. I noted that, when .md or .Rmd files are left in the &amp;lsquo;content&amp;rsquo; folder, a folder with the same name is created in &amp;lsquo;public&amp;rsquo; and the post is rendered within that folder. Therefore, I put a &amp;lsquo;links.md&amp;rsquo; file in the &amp;lsquo;content&amp;rsquo; folder, containing a few links to other web pages of interest. This file is not rendered as a post and it is not visible in my home page. However, a folder named &amp;lsquo;link&amp;rsquo; is created in the public directory, containing the &amp;lsquo;links.md&amp;rsquo; file, rendered as &amp;lsquo;index.html&amp;rsquo; file. Therefore, I can link this file by using the &amp;lsquo;/link/&amp;rsquo; address.&lt;/p&gt;

&lt;h1 id=&#34;step-6-how-do-i-create-further-entries-for-the-navigation-bar&#34;&gt;Step 6 - How do I create further entries for the navigation bar?&lt;/h1&gt;

&lt;p&gt;I just created the &amp;lsquo;/link/index.html&amp;rsquo; file in the &amp;lsquo;public&amp;rsquo; directory. I would like the reader to access this web page by clicking on an appropriate link at the top of the home-page, next to the &amp;lsquo;Home&amp;rsquo; and &amp;lsquo;About me&amp;rsquo; links. In order to add this link, I had to edit again the &amp;lsquo;layouts/partials/header.html&amp;rsquo; file. In the &amp;lsquo;nav&amp;rsquo; section, there was the line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;navbar-item&amp;quot; href=&amp;quot;{{ &amp;quot;about&amp;quot; | absURL }}&amp;quot;&amp;gt;About me&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line creates the &amp;lsquo;About me&amp;rsquo; link on the navigation bar. Below this line, I added another line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;navbar-item&amp;quot; href=&amp;quot;{{ &amp;quot;links&amp;quot; | absURL }}&amp;quot;&amp;gt;Links&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I guess I can use this approach to redirect readers to other web pages of interest.&lt;/p&gt;

&lt;h1 id=&#34;step-7-i-need-to-put-some-order-in-the-content-folder-4-1-2019&#34;&gt;Step 7 - I need to put some order in the &amp;lsquo;content&amp;rsquo; folder&amp;rsquo; (4/1/2019)!&lt;/h1&gt;

&lt;p&gt;After adding a few non-post pages to my content folder I wanted to avoid too much confusion. I discovered that I can add folders to the &amp;lsquo;content&amp;rsquo; folder. They are rendered as such in the public directory. For example, I created an &amp;lsquo;article&amp;rsquo; folder and put a &amp;lsquo;page1.md&amp;rsquo; file inside it. After rendering, a folder &amp;lsquo;article/page1/&amp;rsquo; is created in the &amp;lsquo;public&amp;rsquo; folder, containing the &amp;lsquo;index.html&amp;rsquo; page, which is the rendered version of the &amp;lsquo;page1.md&amp;rsquo; file.&lt;/p&gt;

&lt;p&gt;I think I start feeling slightly more confident with blogdown!&lt;/p&gt;

&lt;h1 id=&#34;step-8-literature-support-7-1-2019&#34;&gt;Step 8 - Literature support (7/1/2019)!&lt;/h1&gt;

&lt;p&gt;This problem took me a while to solve. Lately, I discovered that, in order to be able to insert citations, the post file needs to be &amp;lsquo;.Rmd&amp;rsquo; and not &amp;lsquo;.md.&amp;rsquo; Furthermore, the literature file (&amp;lsquo;.bib&amp;rsquo;) and the &amp;lsquo;.csl&amp;rsquo; file need to be in the &amp;lsquo;/content/&amp;rsquo; folder. A reference to both files need to be put in the YAML header, e.g., :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bibliography: myFile.bib
csl: myStyle.csl
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-9-google-analytics-6-2-2019&#34;&gt;Step 9 - Google Analytics (6/2/2019)&lt;/h1&gt;

&lt;p&gt;I would like to track the visits I receive. To do so I enabled Google Analytics and got my tracking id (&lt;a href=&#34;https://support.google.com/analytics/answer/1008080&#34;&gt;see here&lt;/a&gt;). I also worked on my &amp;lsquo;config.toml&amp;rsquo; file and added the following statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GoogleAnalytics = &amp;quot;UA-my_ID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In contrast to what I expected, Google Analytics did not work and no hits were recorded for my blog. Lately, I worked on the &amp;lsquo;layouts/partials/header.html&amp;rsquo; and added the following code, right after the &amp;lsquo;&lt;head&gt;&amp;rsquo; tag (just replaced &amp;lsquo;UA-my_ID&amp;rsquo; with my tracking id).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Global Site Tag (gtag.js) - Google Analytics --&amp;gt;
&amp;lt;script async src=&amp;quot;https://www.googletagmanager.com/gtag/js?id=UA-my_ID&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag(&#39;js&#39;, new Date());

  gtag(&#39;config&#39;, &#39;UA-my_ID&#39;);
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now Google Analytics works ok!&lt;/p&gt;

&lt;h1 id=&#34;step-10-add-an-rss-template&#34;&gt;Step 10 - Add an RSS Template&lt;/h1&gt;

&lt;p&gt;I wanted my RSS feed to contain the whole post and not only a description. I did the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Took the default template from  &lt;a href=&#34;https://gohugo.io/templates/rss/#the-embedded-rss-xml&#34;&gt;this site&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Replaced &amp;lsquo;{{ .Summary | html }}&amp;rsquo; with &amp;lsquo;{{ .Content | html }}&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Saved the file as &amp;lsquo;RSS.xml&amp;rsquo;under the &amp;lsquo;layouts&amp;rsquo; directory.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;step-11-further-info-on-google-analytics-10-12-2020&#34;&gt;Step 11 - Further info on Google Analytics (10/12/2020)&lt;/h1&gt;

&lt;p&gt;A very kind reader (Emma) emailed me that there have been some relevant updates for Google analytics (v4). She pointed me to the following guide: &lt;a href=&#34;https://www.websiteplanet.com/blog/ultimate-beginners-guide-google-analytics/&#34;&gt;https://www.websiteplanet.com/blog/ultimate-beginners-guide-google-analytics/&lt;/a&gt;. This is useful, to have a better comprehension of the whole Google Analytics platform, just in case someone is interested.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/about/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;my name is Andrea Onofri. I have been always fond of science. I got my degree in Agricultural Sciences in 1987 and I like to think of  myself as an applied biologist. When I started doing research (in 1990) I was very much fascinated by the idea that natural phenomena might be described, interpreted and understood by using the universal language of math. My PhD studies revolved around nonlinear regression analyses and, afterwards, I&amp;rsquo;ve been mainly involved with biostatistics, data analyses and modelling. I do think that biostatistics is fundamental in developing scientific thinking.&lt;/p&gt;

&lt;p&gt;Today, I am a Professor and I teach Experimental Methodology at the University of Perugia. My students are biologists and they do not have a very deep background in maths and statistic. Teaching them requires great care and a very simple language.&lt;/p&gt;

&lt;p&gt;If you want to make comments or contact me for any reasons, here are my whereabouts:&lt;/p&gt;

&lt;p&gt;Prof. Andrea Onofri&lt;br /&gt;
Department of Agricultural, Food and Environmental Sciences&lt;br /&gt;
University of Perugia (Italy)&lt;br /&gt;
Borgo XX Giugno 74&lt;br /&gt;
I-06121 - PERUGIA&lt;br /&gt;
Email: &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;&lt;br /&gt;
My ORCID: &lt;a href=&#34;https://orcid.org/0000-0002-6603-329X&#34;&gt;https://orcid.org/0000-0002-6603-329X&lt;/a&gt;&lt;br /&gt;
SCOPUS preview: &lt;a href=&#34;https://www.scopus.com/authid/detail.uri?authorId=57193769604&#34;&gt;https://www.scopus.com/authid/detail.uri?authorId=57193769604&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample variance and population variance: which of the two?</title>
      <link>/2018/stat_general_variancesamplepop/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/stat_general_variancesamplepop/</guid>
      <description>


&lt;p&gt;Teaching experimental methodology in agriculture related master courses poses some peculiar problems. One of these is to explain the difference between sample variance and population variance. For the students it is usually easy to grasp the idea that, being the mean the ‘center’ of the dataset, it is relevant to measure the average distance to the mean for all individuals in the dataset. Of course, we need to take the sum of squared distances, otherwise negative and positive residuals cancel each other out.&lt;/p&gt;
&lt;p&gt;It is also very intuitive that the average of squared residuals (mean square or variance) is calculated by using the following expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - \mu )^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, this is not what R (and other software) does. R divides by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; and, at this stage, the students usually ask: “&lt;em&gt;why are you telling me that I have to divide the sum of squares by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;? I have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals, not &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;!&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;You may think that the answer is pretty obvious. I don’t think so. The concept of variance is introduced at the beginning of the course, within the frame of descriptive statistics. At this stage, the students know nothing about probability distributions, sampling, inference and all the related concepts. They do not even know anything about degrees of freedom, yet!&lt;/p&gt;
&lt;p&gt;If I were a mathematician and were facing students in math/stat, I could show the class how clever I am by giving a formal proof that &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; is the correct denominator in most circumstances. In particular, this is true when we have a sample taken from a population and we want to infere the variance of the population by using the sample. The formal proof can be found in most stat books and I am attaching it at the end of this post. However, I will never show this formal proof to my students at this stage. Indeed, such a proof assumes that the students know what a sampling distribution is, what the standard error is and how it is calculated. Furthermore, students in agriculture are usually very reluctant, when it comes to dealing with maths!&lt;/p&gt;
&lt;p&gt;Therefore, I usually refrain from trying to appear more clever than I am. My question: “is it possible to teach the difference between the population variance and the sample variance, without talking about degrees of freedom or other ‘difficult’ concepts?”. Here is how I try to put it.&lt;/p&gt;
&lt;p&gt;Let’s take a big, but finite population, i.e. an hectar of maize plants (roughly 70,000 plants). Let’s imagine that we know the individual yields for all plants in the population. With R we can get those yields by taking random values in the interval from 150 to 250 grams (this is reasonable… students in agriculture know these things pretty well! And, I am not referring to any density distribution… it would be too much at this stage).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
pop &amp;lt;- runif(70000, min = 150, max = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our population (I will not show it… lack of space!). It is a finite population, so we can easily calculate the mean and the variance, i.e. the mean squared distance of all 70,000 individuals to the mean. You see that I am using the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; underneath).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0893&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2 &amp;lt;- sum( ( pop - mean(pop) )^2 )/70000
sigma2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 835.184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now forget about the population. In the field, we would never be able to measure the yield for all the plants in one hectar, due to lack of resources (time and money). Therefore, let’s measure 10 randomly selected plants (one in 7000; samples are often small in agriculture!). We can do this easily in R by sampling the original population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(pop, 10)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 237.7442 242.4557 243.2478 152.7499 246.9411 154.4087 226.4784
##  [8] 236.2380 216.2226 187.3896&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now calculate the mean and the variance for the sample, by using the equation above (i.e. using &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; as the denominator).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 214.3876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2_s &amp;lt;- sum( ( x - mean(x) )^2 )/10
sigma2_s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1197.856&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not know the population but we know our sample. As usual, in lack of other information, we conclude that the population should have the same characteristics of our sample. This is usually seen as a reasonable guess; at least the most reasonable one. The procedure we use to make a guess is called ‘estimator’ and the guess in itself is an ‘estimate’. Have we used a good estimator?&lt;/p&gt;
&lt;p&gt;We can’t answer. Unless we repeat the sampling process for a lot of times and see how our estimates behave in the long run. Our estimator is good if, in the long run, it converges on the real value for the population. Let’s check this: we repeatedly take samples of 10 plants from our population and calculate the mean and variance as above. We repeat this process 10,000 times, storing the 10,000 means and variances in two vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanS &amp;lt;- c(); varS &amp;lt;- c()
for(i in 1:10000){
  x &amp;lt;- sample(pop, 10)
  meanS[i] &amp;lt;- mean(x)
  varS[i] &amp;lt;- sum( ( x - mean(x) )^2 )/10
}
mean(meanS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0309&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 753.4311&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the mean of the means is 200.03. We got really very close to the real mean of the population. Indeed, the mean of the sample is a good (unbiased) estimator of the mean of the population.&lt;/p&gt;
&lt;p&gt;On the other hand, the mean of the variances is 753.43. Please note that this is much smaller than the real variance of the population. It means that if we use the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; at the denominator) to calculate the variance for the sample, we do not have a good estimator of the variance for the whole population. We can now look at the ratio between the variance of the population and our guess in the long run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(pop)/mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.108523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that this is roughly equal to &lt;span class=&#34;math inline&#34;&gt;\(10/9\)&lt;/span&gt;. In other words, if we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the sample and calculate the variance by using the equation above,&lt;/li&gt;
&lt;li&gt;multiply by &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;divide by &lt;span class=&#34;math inline&#34;&gt;\(n - 1 = 9\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we can get a good estimator. This leads us to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - m )^2}{n - 1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is exactly the variance for the sample. This one we can easily calculate with R.&lt;/p&gt;
&lt;div id=&#34;which-of-the-two-then&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which of the two, then?&lt;/h1&gt;
&lt;p&gt;Just rememeber:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the variance (mean square) is calculated by dividing the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. This makes sense, because we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals.&lt;/li&gt;
&lt;li&gt;However, if we have a random sample taken from a population, and if our aim is to estimate the variance for the whole population, we need to divide the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;. Otherwise, we get an understimation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-formal-proof-look-how-clever-i-am&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The formal proof (look how clever I am!)&lt;/h1&gt;
&lt;p&gt;We have a population with mean equal to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Let’s take a sample with its mean &lt;span class=&#34;math inline&#34;&gt;\(m \neq \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m - \mu = \varepsilon\)&lt;/span&gt;. For the population, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu )^2  = \sum\limits_{i = 1}^n (X_ i - m + \varepsilon)^2 = \sum\limits_{i = 1}^n ({X_i} - m)^2 + n \varepsilon^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We already see that the sum of squared residuals is higher if we take &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, unless &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon = 0\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m = \mu\)&lt;/span&gt;. From the above, we derive&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  \sum\limits_{i = 1}^n (X_i - \mu )^2  - n ( {m - \mu } )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the first equation above (variance for the population) we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  n \sigma^2 - n( m - \mu  )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that the rightmost term of this equation (&lt;span class=&#34;math inline&#34;&gt;\(n ( m - \mu )^2\)&lt;/span&gt; is the sum of squared distances for the sampling distribution of the mean. That is &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times the squared standard error. Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } n{\sigma ^2} - n\left( {\frac{{{\sigma ^2}}}{n}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } (n - 1){\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go! Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{\sum\limits_{i = 1}^n (X_i - m)^2 }{n - 1} = {\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is R dangerous? Side effects of free software for biologists</title>
      <link>/2014/2014-06-08-the-danger-of-r/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/2014-06-08-the-danger-of-r/</guid>
      <description>


&lt;p&gt;When I started my career in the biological field (it’s already 25 years ago), only the luckiest of us had access to very advanced statistical software. Licenses were very expensive and it was not easy to convince the boss that they were really necessary: “Why do you need to spend so much money to perform an ANOVA?”. Indeed, simple one-way or two-ways ANOVAs were quite easy to perform and one of the people in my group had already built the appropriate routines for several designs, by using the GW-BASIC language. But I wanted more!&lt;/p&gt;
&lt;p&gt;My agriculture experiments often showed complex designs and split-plot, strip-plot, subsampling and repeated measures/experiments were much more than an exception. I decided to start writing several Quick-BASIC routines to implement those types of ANOVAs on my PC. At the beginning of the ’90s, nonlinear response models became in fashion and I had to programme my first Gauss-Newton optimiser, also with Quick-BASIC. GLMs were not yet widespread among biologists and I mainly relied on stabilising transformations for those cases where the basic assumptions for linear/nonlinear models were not met.&lt;/p&gt;
&lt;p&gt;This ‘poor and humble’ statistical life gave me an undeniable advantage: it forced me into thoroughly studying and understanding the principles of each new technique and algorithm, before I could be able to programme it. I had to become acquainted with all those strange mathematical objects, such as matrices, eigenvalues and determinants, which are not usually a part of the mathematical background of biologists (at least in Italy). And my BASIC routines, once completed, could only do what they were programmed to do; only one specific solution to one specific task, no further options and no error management. In one sentence: no general solutions.&lt;/p&gt;
&lt;p&gt;Nowadays I have R: it is free and everything is possible and smooth. A few lines of code and I can fit whatever model comes to my mind in a few minutes. I can try several options: which is the best one? Which is the one that makes my data tell the story I would like to tell? Biometry books have changed as well; they have taken a more ‘algorithmic’ approach and math is confined within boxes that may be easily skipped. I have to admit that I frequently skip them: code snippets are more than enough to do the trick and I can also find thousands of them in the Internet. In other words, why should I bother studying such an abstract thing called statistics when I have R?&lt;/p&gt;
&lt;p&gt;Obviously this is just an exaggeration. However, I have the feeling that there might be some drawbacks relating to the availability of such a powerful free software. Biologists (especially students) may mistake studying R for studying stats. I am very much surprised to see how many complex models are fit on these days, with hardly any biological and statistical justifications and with very little care about the basic assumptions that these models make. A few days ago a PhD student at my Department showed me the results of fitting a reduced rank regression to a biological dataset. He was very proud of how he mastered the R coding process: by using the correct option (found after a thorough search over the Internet). He had even managed to avoid a ‘pretty strange’ warning message. Unfortunately, that warning message had been misinterpreted and therefore the analysis was wrong from the very beginning.&lt;/p&gt;
&lt;p&gt;A warning message for all biologists (including myself): R is really wonderful, but it will not necessarily bring to sound data analyses. Let’s use R, but let’s study stats first!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>