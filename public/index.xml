<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Models are wrong on The broken bridge between biologists and statisticians</title>
    <link>/</link>
    <description>Recent content in Models are wrong on The broken bridge between biologists and statisticians</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2018, @AndreaOnofri</copyright>
    <lastBuildDate>Thu, 14 Feb 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>prova.Rmd</title>
      <link>/2019/2019-02-14-prova/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-02-14-prova/</guid>
      <description>


&lt;p&gt;Il passaggio standard del Lorem Ipsum, utilizzato sin dal sedicesimo secolo&lt;/p&gt;
&lt;p&gt;“Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.”&lt;/p&gt;
&lt;p&gt;La sezione 1.10.32 del “de Finibus Bonorum et Malorum”, scritto da Cicerone nel 45 AC&lt;/p&gt;
&lt;p&gt;“Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?”&lt;/p&gt;
&lt;p&gt;Traduzione del 1914 di H. Rackham&lt;/p&gt;
&lt;p&gt;“But I must explain to you how all this mistaken idea of denouncing pleasure and praising pain was born and I will give you a complete account of the system, and expound the actual teachings of the great explorer of the truth, the master-builder of human happiness. No one rejects, dislikes, or avoids pleasure itself, because it is pleasure, but because those who do not know how to pursue pleasure rationally encounter consequences that are extremely painful. Nor again is there anyone who loves or pursues or desires to obtain pain of itself, because it is pain, but because occasionally circumstances occur in which toil and pain can procure him some great pleasure. To take a trivial example, which of us ever undertakes laborious physical exercise, except to obtain some advantage from it? But who has any right to find fault with a man who chooses to enjoy a pleasure that has no annoying consequences, or one who avoids a pain that produces no resultant pleasure?”&lt;/p&gt;
&lt;p&gt;La sezione 1.10.33 del “de Finibus Bonorum et Malorum”, scritto da Cicerone nel 45 AC&lt;/p&gt;
&lt;p&gt;“At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.”&lt;/p&gt;
&lt;p&gt;Traduzione del 1914 di H. Rackham&lt;/p&gt;
&lt;p&gt;“On the other hand, we denounce with righteous indignation and dislike men who are so beguiled and demoralized by the charms of pleasure of the moment, so blinded by desire, that they cannot foresee the pain and trouble that are bound to ensue; and equal blame belongs to those who fail in their duty through weakness of will, which is the same as saying through shrinking from toil and pain. These cases are perfectly simple and easy to distinguish. In a free hour, when our power of choice is untrammelled and when nothing prevents our being able to do what we like best, every pleasure is to be welcomed and every pain avoided. But in certain circumstances and owing to the claims of duty or the obligations of business it will frequently occur that pleasures have to be repudiated and annoyances accepted. The wise man therefore always holds in these matters to this principle of selection: he rejects pleasures to secure other greater pleasures, or else he endures pains to avoid worse pains.”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going back to the basics: the correlation coefficient</title>
      <link>/2019/alookatcorrelation/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/alookatcorrelation/</guid>
      <description>


&lt;div id=&#34;a-measure-of-joint-variability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A measure of joint variability&lt;/h1&gt;
&lt;p&gt;In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. It is often measured by the Pearson correlation coefficient:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho _{X,Y} =\textrm{corr} (X,Y) = \frac {\textrm{cov}(X,Y) }{ \sigma_X \sigma_Y } = \frac{ \sum_{1 = 1}^n [(X - \mu_X)(Y - \mu_Y)] }{ \sigma_X \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other measures of correlation can be thought of, such as the Spearman &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; rank correlation coefficient or Kendall &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; rank correlation coefficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-for-the-pearson-correlation-coefficient&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions for the Pearson Correlation Coefficient&lt;/h1&gt;
&lt;p&gt;The Pearson correlation coefficients makes a few assumptions, which should be carefully checked.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interval-level measurement. Both variables should be measured on a quantitative scale.&lt;/li&gt;
&lt;li&gt;Random sampling. Each subject in the sample should contribute one value on X, and one value on Y. The values for both variables should represent a random sample drawn from the population of interest.&lt;/li&gt;
&lt;li&gt;Linearity. The relationship between X and Y should be linear.&lt;/li&gt;
&lt;li&gt;Bivarlate normal distribution. This means that (i) values of X should form a normal distribution at each value of Y and (ii) values of Y should form a normal distribution at each value of X.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;It is possible to test whether &lt;span class=&#34;math inline&#34;&gt;\(r = 0\)&lt;/span&gt; against the alternative $ r 0$. The test is based on the idea that the amount:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ T = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}\]&lt;/span&gt; is distributed as a Student’s t variable.&lt;/p&gt;
&lt;p&gt;Let’s take the two variables ‘cyl’ and ‘mpg’ from the ‘mtcars’ data frame. The correlation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- cor(mtcars$cyl, mtcars$gear)
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.4926866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The T statistic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;T &amp;lt;- r * sqrt(32 - 2) / sqrt(1 - r^2)
T&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -3.101051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value for the null is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * pt(T, 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004173297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is clearly highly significant. The null can be rejected.&lt;/p&gt;
&lt;p&gt;As for hypothesis testing, it should be considered that the individuals where couple of measurements were taken should be independent. If they are not, the t test is invalid. I am dealing with this aspect somewhere else in my blog.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation in R&lt;/h1&gt;
&lt;p&gt;We have already seen that we can use the usual function ‘cor(matrix, method=)’. In order to obtain the significance, we can use the ‘rcorr()’ function in the Hmisc package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlations with significance levels
library(Hmisc)
corr2 &amp;lt;- rcorr(as.matrix(mtcars), type=&amp;quot;pearson&amp;quot;)
print(corr2$r, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg   cyl  disp    hp   drat    wt   qsec    vs     am  gear   carb
## mpg   1.00 -0.85 -0.85 -0.78  0.681 -0.87  0.419  0.66  0.600  0.48 -0.551
## cyl  -0.85  1.00  0.90  0.83 -0.700  0.78 -0.591 -0.81 -0.523 -0.49  0.527
## disp -0.85  0.90  1.00  0.79 -0.710  0.89 -0.434 -0.71 -0.591 -0.56  0.395
## hp   -0.78  0.83  0.79  1.00 -0.449  0.66 -0.708 -0.72 -0.243 -0.13  0.750
## drat  0.68 -0.70 -0.71 -0.45  1.000 -0.71  0.091  0.44  0.713  0.70 -0.091
## wt   -0.87  0.78  0.89  0.66 -0.712  1.00 -0.175 -0.55 -0.692 -0.58  0.428
## qsec  0.42 -0.59 -0.43 -0.71  0.091 -0.17  1.000  0.74 -0.230 -0.21 -0.656
## vs    0.66 -0.81 -0.71 -0.72  0.440 -0.55  0.745  1.00  0.168  0.21 -0.570
## am    0.60 -0.52 -0.59 -0.24  0.713 -0.69 -0.230  0.17  1.000  0.79  0.058
## gear  0.48 -0.49 -0.56 -0.13  0.700 -0.58 -0.213  0.21  0.794  1.00  0.274
## carb -0.55  0.53  0.39  0.75 -0.091  0.43 -0.656 -0.57  0.058  0.27  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(corr2$P, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          mpg     cyl    disp      hp    drat      wt    qsec      vs
## mpg       NA 6.1e-10 9.4e-10 1.8e-07 1.8e-05 1.3e-10 1.7e-02 3.4e-05
## cyl  6.1e-10      NA 1.8e-12 3.5e-09 8.2e-06 1.2e-07 3.7e-04 1.8e-08
## disp 9.4e-10 1.8e-12      NA 7.1e-08 5.3e-06 1.2e-11 1.3e-02 5.2e-06
## hp   1.8e-07 3.5e-09 7.1e-08      NA 1.0e-02 4.1e-05 5.8e-06 2.9e-06
## drat 1.8e-05 8.2e-06 5.3e-06 1.0e-02      NA 4.8e-06 6.2e-01 1.2e-02
## wt   1.3e-10 1.2e-07 1.2e-11 4.1e-05 4.8e-06      NA 3.4e-01 9.8e-04
## qsec 1.7e-02 3.7e-04 1.3e-02 5.8e-06 6.2e-01 3.4e-01      NA 1.0e-06
## vs   3.4e-05 1.8e-08 5.2e-06 2.9e-06 1.2e-02 9.8e-04 1.0e-06      NA
## am   2.9e-04 2.2e-03 3.7e-04 1.8e-01 4.7e-06 1.1e-05 2.1e-01 3.6e-01
## gear 5.4e-03 4.2e-03 9.6e-04 4.9e-01 8.4e-06 4.6e-04 2.4e-01 2.6e-01
## carb 1.1e-03 1.9e-03 2.5e-02 7.8e-07 6.2e-01 1.5e-02 4.5e-05 6.7e-04
##           am    gear    carb
## mpg  2.9e-04 5.4e-03 1.1e-03
## cyl  2.2e-03 4.2e-03 1.9e-03
## disp 3.7e-04 9.6e-04 2.5e-02
## hp   1.8e-01 4.9e-01 7.8e-07
## drat 4.7e-06 8.4e-06 6.2e-01
## wt   1.1e-05 4.6e-04 1.5e-02
## qsec 2.1e-01 2.4e-01 4.5e-05
## vs   3.6e-01 2.6e-01 6.7e-04
## am        NA 5.8e-08 7.5e-01
## gear 5.8e-08      NA 1.3e-01
## carb 7.5e-01 1.3e-01      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also use these functions with two matrices, to obtain the correlations of each column in one matrix with each column in the other&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation matrix from mtcars
x &amp;lt;- mtcars[1:3]
y &amp;lt;- mtcars[4:6]
cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              hp       drat         wt
## mpg  -0.7761684  0.6811719 -0.8676594
## cyl   0.8324475 -0.6999381  0.7824958
## disp  0.7909486 -0.7102139  0.8879799&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-to-slope-in-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Relationship to slope in linear regression&lt;/h1&gt;
&lt;p&gt;The correlation coefficient and slope in linear regression bear some similarities, as both describe how Y changes when X is changed. However, in correlation, we have two random variables, while in regression we have Y random, X fixed and Y is regarded as a function of X (not the other way round).&lt;/p&gt;
&lt;p&gt;Without neglecting their different meaning, it may be useful to show the algebraic relationship between the correlation coefficient and the slope in regression. Let’s simulate a dataset with two variables, coming from a multivariate normal distribution, with means respectively equal to 10 and 2, and variance-covariance matrix of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
cov &amp;lt;- matrix(c(2.20, 0.48, 0.48, 0.20), 2, 2)
cov&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,] 2.20 0.48
## [2,] 0.48 0.20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the ‘mvrnomr()’ function to generate the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
dataset &amp;lt;- data.frame( mvrnorm(n=10, mu = c(10, 2), Sigma = cov) )
names(dataset) &amp;lt;- c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;)
dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            X        Y
## 1  11.756647 2.547203
## 2   9.522180 2.199740
## 3   8.341254 1.862362
## 4  13.480005 2.772031
## 5   9.428296 1.573435
## 6   9.242788 1.861756
## 7  10.817449 2.343918
## 8  10.749047 2.451999
## 9  10.780400 2.436263
## 10 11.480301 1.590436&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation coefficient and slope are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- with(dataset, cor(X, Y))
b1 &amp;lt;- coef( lm(Y ~ X, data=dataset) )[2]
r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equation for the slope is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 = \frac{ \sum_{i = 1}^n \left[ ( X-\mu_X )( Y-\mu_Y )\right] }{ \sigma^2_X } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From there, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r = b_1 \frac{\sigma_X}{ \sigma_Y }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ b_1 = r \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmaX &amp;lt;- with(dataset, sd(X) )
sigmaY &amp;lt;- with(dataset, sd(Y) )
b1 * sigmaX / sigmaY &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X 
## 0.6372927&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r * sigmaY / sigmaX&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1785312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also easy to see that the correlation coefficient is the slope of regression of standardised Y against standardised X:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yst &amp;lt;- with(dataset, scale(Y, scale=T) )
summary( lm(Yst ~ I(scale(X, scale = T) ), data = dataset) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Yst ~ I(scale(X, scale = T)), data = dataset)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.082006 -0.067143 -0.036850  0.009214  0.237923 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)            -5.633e-18  3.478e-02   0.000   1.0000  
## I(scale(X, scale = T))  1.785e-01  7.633e-02   2.339   0.0475 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.11 on 8 degrees of freedom
## Multiple R-squared:  0.4061, Adjusted R-squared:  0.3319 
## F-statistic: 5.471 on 1 and 8 DF,  p-value: 0.04748&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intra-class-correlation-icc&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intra-class correlation (ICC)&lt;/h1&gt;
&lt;p&gt;It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intra-class correlation coefficient is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IC = {\displaystyle {\frac {\sigma _{\alpha }^{2}}{\sigma _{\alpha }^{2}+\sigma _{\varepsilon }^{2}}}.}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\alpha }^{2}\)&lt;/span&gt; is the variance between groups and &lt;span class=&#34;math inline&#34;&gt;\(\sigma _{\varepsilon }^{2}\)&lt;/span&gt; is the variance within a group (better, the variance of one observation within a group). The sum of those two variances is the total variance of observations. In words, the intra-class correlation coefficient measures the joint variability of subjects in the same group (that relates on how groups are different from one another), with respect to the total variability of observations. If subjects in one group are very similar to one another (small &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\varepsilon}\)&lt;/span&gt;) but groups are very different (high &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\alpha}\)&lt;/span&gt;), the ICC is very high.&lt;/p&gt;
&lt;p&gt;The existence of grouping of residuals is very important in ANOVA, as it means that independence is violated, which calls for the use of mixed model.&lt;/p&gt;
&lt;p&gt;But … this is a totally different story …&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some useful equations for nonlinear regression in R</title>
      <link>/articles/usefulequations/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/articles/usefulequations/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Very rarely, biological processes follow linear trends. Just think about how a crop grows, or responds to increasing doses of fertilisers/xenobiotics. Or think about how an herbicide degrades in the soil, or about the germination pattern of a seed population. It is very easy to realise that curvilinear trends are far more common than linear trends. Furthermore, asymptotes and/or inflection points are very common in nature. We can be sure: linear equations in biology are just a way to approximate a response over a very narrow range for the independent variable.&lt;/p&gt;
&lt;p&gt;Therefore, as biologists, we need to be able to describe our experimental data by using a wide range of curvilinear equations. We need to be able to ‘read’ those equations and use their parameters to interpret and understand biological processes. I thought that it would be useful to list the most commonly used curvilinear functions and show examples of how they can be fit by using R.&lt;/p&gt;
&lt;p&gt;When it comes to nonlinear regression, I have a strong personal preference for the ‘drc’ package and the ‘drm()’ function therein. However, it is also worth mentioning the traditional ‘nls()’ function in the ‘stats’ package. You may know that nonlinear least squares work iteratively: we need to provide initial guesses for model parameters and the algorithm adjusts them step by step, finally converging on the approximate least squares solution. To my experience, providing initial guesses may be troublesome. Therefore, it is very convenient to use R functions together with the appropriate self-starting routines, which can greatly semplify the fitting process. These self-starters can be found in the ‘drc’, ‘nlme’ and ‘aomisc’ packages.&lt;/p&gt;
&lt;p&gt;Let’s load the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(drc)
library(nlme)
library(aomisc)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;curve-shapes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Curve shapes&lt;/h1&gt;
&lt;p&gt;Curves can be easily classified by their shape, which is very helpful to select the correct one, according to the trend of the process under study. We have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Polynomials
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Straight line&lt;/li&gt;
&lt;li&gt;Quadratic polynomial&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Concave/Convex curves (no inflection)
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exponential curve&lt;/li&gt;
&lt;li&gt;Asymptotic regression&lt;/li&gt;
&lt;li&gt;Negative exponential function&lt;/li&gt;
&lt;li&gt;Power curve&lt;/li&gt;
&lt;li&gt;Logarithmic equation&lt;/li&gt;
&lt;li&gt;Rectangular hyperbola&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Sygmoidal curves
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Logistic curve&lt;/li&gt;
&lt;li&gt;Gompertz curve&lt;/li&gt;
&lt;li&gt;Extreme value equation&lt;/li&gt;
&lt;li&gt;Log-logistic curve (Hill equation)&lt;/li&gt;
&lt;li&gt;Weibull-type 1&lt;/li&gt;
&lt;li&gt;Weibull-type 2&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomials&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomials&lt;/h1&gt;
&lt;p&gt;Polynomials are the most flexible tool to describe biological processes. They are simple and, although curvilinear, they are linear in the parameters and can be fitted by using linear regression. One disadvantage is that they cannot describe asymptotic processes, which are very common in biology. Furthermore, they are prone to overfitting, as we may be tempted to add terms to improve the fit, with little care for biological realism.&lt;/p&gt;
&lt;p&gt;Nowadays, thanks to the wide availability of nonlinear regression algorithms, the use of polynomials has sensibly decreased; linear or quadratic polynomials are mainly used when we want to approximate the observed response within a narrow range of a quantitative predictor. On the other hand, higher order polynomials are very rarely seen, in practice.&lt;/p&gt;
&lt;div id=&#34;straight-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Straight line&lt;/h2&gt;
&lt;p&gt;Obviously, this is not a curve, although it deserves to be mentioned here. The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1 \, X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is the slope, i.e. the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The Y increases as X increases when &lt;span class=&#34;math inline&#34;&gt;\(b_1 &amp;gt; 0\)&lt;/span&gt;, otherwise it decreases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic function&lt;/h2&gt;
&lt;p&gt;The equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = b_0 + b_1\, X + b_2 \, X^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt;, taken separately, lack a clear biological meaning. However, it is interesting to consider that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a + b*X + c*X^2), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## b + c * (2 * X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which measures the increase/decrease in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit-increase in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. We see that such an increase/decrease is not constant, but it changes according to the level of X. The stationary point is &lt;span class=&#34;math inline&#34;&gt;\(X_m = - b_1 / 2 b_2\)&lt;/span&gt;; it is a maximum when &lt;span class=&#34;math inline&#34;&gt;\(b_2 &amp;gt; 0\)&lt;/span&gt;, otherwise it is a minimum.&lt;/p&gt;
&lt;p&gt;At the maximum/minimum, it is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_m = \frac{4\,b_0\,b_2 - b_1^2}{4\,b_2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-fitting-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polynomial fitting in R&lt;/h2&gt;
&lt;p&gt;Polynomials in R are fit by using the linear model function ‘lm()’. Although this is not efficient, in a couple of cases I found myself in the need of fitting a polynomial by using the ‘nls()’ o ‘drm()’ functions. For these unusual cases, one can use the ‘NLS.Linear()’, NLS.poly2(), ‘DRC.Linear()’ and DRC.Poly2() self-starting function, as available in the ‘aomisc’ package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concaveconvex-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concave/Convex curves&lt;/h1&gt;
&lt;div id=&#34;exponential-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exponential curve&lt;/h2&gt;
&lt;p&gt;The exponential function describes an increasing/decreasing &lt;em&gt;trend&lt;/em&gt;, with constant relative rate. The most common equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  e^{k X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other possible parameterisations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = a  b^X  =  e^{d + k X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above parameterisations are equivalent, as proved by setting &lt;span class=&#34;math inline&#34;&gt;\(b = e^k\)&lt;/span&gt; e &lt;span class=&#34;math inline&#34;&gt;\(a = e^d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  b^X  = a  (e^k)^{X} =  a  e^{kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a  e^{kX} = e^d \cdot e^{kX} =  e^{d + kX}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The meaning of parameters is clear: &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; represents the relative increase/decrease of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for a unit increase of X. &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases if &lt;span class=&#34;math inline&#34;&gt;\(k &amp;gt; 0\)&lt;/span&gt; (exponential growth), while it decreases when &lt;span class=&#34;math inline&#34;&gt;\(k &amp;lt; 0\)&lt;/span&gt; (exponential decay). This curve is used to describe the growth of populations in unlimiting environmental conditions, or to describe the degradation of xenobiotics in the environment (first-order degradation kinetic).&lt;/p&gt;
&lt;p&gt;The exponential function is nonlinear in &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and needs to be fitted by using ‘nls()’ or ‘drm()’. It is possible to make profit of the self-starting routines in ‘NLS.expoGrowth()’, ‘NLS.expoDecay()’, ‘DRC.expoGrowth()’ and ‘DRC.expoDecay()’. All these functions are available in the ‘aomisc’ package. The ‘drc’ package also contains the function ‘EXD.2()’, that fits an exponential decay model, with a slightly different parameterisation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = d \exp(-x/e) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the same as &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; in the model above and &lt;span class=&#34;math inline&#34;&gt;\(e = 1/k\)&lt;/span&gt;. For all the forementioned exponential decay equations &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The function ‘EXD.3()’ in the ‘drc’ package also includes a lower asymptote &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d -c) \exp(-x/e) \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(degradation)
degradation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Time    Conc
## 1     0  96.400
## 2    10  46.300
## 3    20  21.200
## 4    30  17.890
## 5    40  10.100
## 6    50   6.900
## 7    60   3.500
## 8    70   1.900
## 9     0 102.300
## 10   10  49.200
## 11   20  26.310
## 12   30  14.220
## 13   40   5.400
## 14   50   3.400
## 15   60   0.500
## 16   70   0.200
## 17    0 101.330
## 18   10  54.890
## 19   20  28.120
## 20   30  13.330
## 21   40   6.110
## 22   50   0.350
## 23   60   2.100
## 24   70   0.922&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Conc ~ Time, fct = DRC.expoDecay(),
             data = degradation)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Exponential Decay Model (2 parms)
## 
## Parameter estimates:
## 
##                    Estimate Std. Error t-value   p-value    
## init:(Intercept) 99.6349312  1.4646680  68.026 &amp;lt; 2.2e-16 ***
## k:(Intercept)     0.0670391  0.0019089  35.120 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.621386 (22 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotic regression model&lt;/h2&gt;
&lt;p&gt;The asymptotic regression model describes a limited growth, where Y approaches an horizontal asymptote as X tends to infinity. The rate of growth is maximum at the beginning and approaches 0 as Y approaches the plateau. This equation is used in several different parameterisations and it is also known as Monomolecular Growth, Mitscherlich law or von Bertalanffy law.&lt;/p&gt;
&lt;p&gt;Due to its biological meaning, the most widespread parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a - (a - b) \, \exp (- c  X)\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the maximum attainable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is proportional to the relative rate of Y increase while X increases. Indeed, we can see that the first derivative is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a - (a - b) * exp (- c * X)), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (a - b) * (exp(-c * X) * c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;that is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y&amp;#39; = c \, (a - Y)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model can be fit with R by using the self starter functions ‘NLS.asymReg()’ and DRC.asymReg(), in the ‘aomisc’ package. The ‘drc’ package contains the function AR.3(), that is a similar parameterisation where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation named ‘SSasymp()’, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We simulate an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X &amp;lt;- c(1, 3, 5, 7, 9, 11, 13, 20)
a &amp;lt;- 20; b &amp;lt;- 5; c &amp;lt;- 0.3
Ye &amp;lt;- asymReg.fun(X, a, b, c)
epsilon &amp;lt;- rnorm(8, 0, 0.5)
Y &amp;lt;- Ye + epsilon
model &amp;lt;- drm(Y ~ X, fct = DRC.asymReg())
plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we take the above equation and add the constraint that &lt;span class=&#34;math inline&#34;&gt;\(b = 0\)&lt;/span&gt;, we get the following equation, that is often known as negative exponential equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a [1 -  \exp (- c  X) ]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This equation has a similar shape to the asymptotic regression, but Y is 0 when X is 0 (the curve passes through the origin). It is often used to model the absorbed Photosintetically Active Radiation (&lt;span class=&#34;math inline&#34;&gt;\(Y = PAR_a\)&lt;/span&gt;) as a function of incident PAR (&lt;span class=&#34;math inline&#34;&gt;\(a = PAR_i\)&lt;/span&gt;), Leaf Area Index (X = LAI) and the extinction coefficient (c = k).&lt;/p&gt;
&lt;p&gt;This model can be fit with R by using the self starter functions ‘NLS.negExp()’ and DRC.negExp(), in the ‘aomisc’ package. The ‘drc’ package contains the function AR.2(), where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(e = 1/c\)&lt;/span&gt;. The ‘nlme’ package also contains an alternative parameterisation, named ‘SSasympOrig()’, where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(\phi_3 = \log(c)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;power-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power curve&lt;/h2&gt;
&lt;p&gt;The power curve is also known as Freundlich equation or allometric function and the most common parameterisation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = a \, X^b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is perfectly equivalent to an exponential curve on the logarithm of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[a\,X^b  = a\, e^{\log( X^b )}  = a\,e^{b \, \log(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve does not have an asymptote for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;. The slope (first derivative) of the curve is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression(a * X^b), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a * (X^(b - 1) * b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both paraeters relate to the slope of the curve and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates its shape. If &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt;- b &amp;lt; 1\)&lt;/span&gt;, Y increases as X increases and the curve is convex up. This is used, e.g., to model the number of plant species as a function of sampling area (Muller-Dumbois method).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(speciesArea)
speciesArea&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Area numSpecies
## 1    1          4
## 2    2          5
## 3    4          7
## 4    8          8
## 5   16         10
## 6   32         14
## 7   64         19
## 8  128         22
## 9  256         26&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(numSpecies ~ Area, fct = DRC.powerCurve(),
             data = speciesArea)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Power curve (Freundlich equation) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 4.348404   0.337197  12.896 3.917e-06 ***
## b:(Intercept) 0.329770   0.016723  19.719 2.155e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.9588598 (7 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curve(powerCurve.fun(x, coef(model)[1], -coef(model)[2]),
      xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 1\)&lt;/span&gt; is negative, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curve(powerCurve.fun(x, coef(model)[1], 2),
      xlab = &amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logarithmic-equation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logarithmic equation&lt;/h2&gt;
&lt;p&gt;This is indeed a linear model on log-transformed X:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = a + b \, \log(X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to the logarithmic function, X must be $ &amp;gt; 0$. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; dictates the shape: if &lt;span class=&#34;math inline&#34;&gt;\(b &amp;gt; 0\)&lt;/span&gt;, the curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases. If &lt;span class=&#34;math inline&#34;&gt;\(b &amp;lt; 0\)&lt;/span&gt;, the curve is concave up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; decreases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The logarithmic equation can be fit by using ‘lm()’. If necessary, it can also be fit by using ‘nls()’ and ‘drm()’; the self-starting functions ‘NLS.logCurve()’ and ‘DRC.logCurve()’ are available within the ‘aomisc’ package. We show some simulated data as examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#b is positive
set.seed(5678)
X &amp;lt;- c(1,2,4,5,7,12)
a&amp;lt;-2; b&amp;lt;- 0.5
Ye &amp;lt;-  a + b*log(X)
res &amp;lt;- rnorm(6, 0, 0.1)
Y &amp;lt;- Ye + res
model &amp;lt;- lm(Y ~ log(X) )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ log(X))
## 
## Residuals:
##         1         2         3         4         5         6 
## -0.025947  0.013207  0.050827 -0.011989 -0.008408 -0.017690 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.99653    0.02494   80.06 1.46e-07 ***
## log(X)       0.45088    0.01580   28.54 8.97e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.03146 on 4 degrees of freedom
## Multiple R-squared:  0.9951, Adjusted R-squared:  0.9939 
## F-statistic: 814.7 on 1 and 4 DF,  p-value: 8.967e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Linear regression on log-transformed x (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## a:(Intercept) 1.996534   0.024939  80.058 1.459e-07 ***
## b:(Intercept) 0.450883   0.015797  28.543 8.967e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.03145785 (4 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#b is negative
X &amp;lt;- c(1,2,4,5,7,12)
a &amp;lt;- 2; b &amp;lt;- -0.5
Ye &amp;lt;-  a + b*log(X)
res &amp;lt;- rnorm(6, 0, 0.1)
Y &amp;lt;- Ye + res
model &amp;lt;- drm(Y ~ X, fct = DRC.logCurve() )
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Linear regression on log-transformed x (2 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## a:(Intercept)  2.115759   0.103920 20.3595 3.437e-05 ***
## b:(Intercept) -0.569125   0.065826 -8.6459 0.0009843 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.1310851 (4 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;michaelis-menten-equation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Michaelis-Menten equation&lt;/h2&gt;
&lt;p&gt;This is a rectangular hyperbola, often parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{a \, X} {b + X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This curve is convex up and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increases as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases, up to a plateau level. The parameter &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; represents the higher asymptote (for &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \infty\)&lt;/span&gt;), while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the X value giving a response equal to &lt;span class=&#34;math inline&#34;&gt;\(a/2\)&lt;/span&gt;. Indeed, it is easily shown that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a}{2} = \frac{a\,X_{50} } {b + X_{50} }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which leads to &lt;span class=&#34;math inline&#34;&gt;\(b = x_{50}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The slope (first derivative) is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D(expression( (a*X) / (b + X) ), &amp;quot;X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a/(b + X) - (a * X)/(b + X)^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there, we can see that the initial slope (at &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;) is $i = a/b $.&lt;/p&gt;
&lt;p&gt;In R, this model can be fit by using ‘nls()’ and the self starting functions ‘SSmicmen()’, within the package ‘nlme’. If we prefer a ‘drm()’ fit, we can use the ‘MM.2()’ function in the package ‘drc’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
X &amp;lt;- c(3, 5, 7, 22, 28, 39, 46, 200)
a &amp;lt;- 15; b &amp;lt;- 0.5
Ye &amp;lt;- as.numeric( SSmicmen(X, a, b) )
res &amp;lt;- rnorm(8, 0, 0.1)
Y &amp;lt;- Ye + res

#nls fit
model &amp;lt;- nls(Y ~ SSmicmen(X, a, b))
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: Y ~ SSmicmen(X, a, b)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## a 14.97166    0.06057  247.17 2.96e-13 ***
## b  0.50207    0.03345   15.01 5.51e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1196 on 6 degrees of freedom
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 3.043e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#drm fit
model &amp;lt;- drm(Y ~ X, fct = MM.2())
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Michaelis-Menten (2 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## d:(Intercept) 14.971733   0.060492 247.499 2.936e-13 ***
## e:(Intercept)  0.502126   0.033359  15.052 5.418e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.1195748 (6 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The ‘drc’ package also contains the self starting function ‘MM.3()’, where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is allowed to be equal to &lt;span class=&#34;math inline&#34;&gt;\(c \neq 0\)&lt;/span&gt;, when &lt;span class=&#34;math inline&#34;&gt;\(X = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-loss-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield-loss curve&lt;/h2&gt;
&lt;p&gt;Weed-crop competition studies make use of a reparameterised Michaelis-Menten model. Indeed, th initial slope of a Michaelis-Menten can be assumed as a measure of competition, that is the reduction in yield (Y) when the first weed is added to the system. Therefore, the Michaelis-Methen model has been reparameterised to include &lt;span class=&#34;math inline&#34;&gt;\(i = a/b\)&lt;/span&gt; as an explicit parameter. The reparameterised equation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \frac{i \, X}{1 + \frac{i \, X}{a}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model can be used to describe yield losses as a function of weed density. It can be fit by using the self starting functions ‘NLS.YL()’ or ‘DRC.YL()’ in the ‘aomisc’ package. Usually, competion studies produce yield data and, therefore, yield lossed need to be calculated by using the weed-free yield and the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_L  = \frac{Y_{WF}  - Y_w }{Y_{WF} } \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y_W\)&lt;/span&gt; is the observed yield and &lt;span class=&#34;math inline&#34;&gt;\(Y_{WF}\)&lt;/span&gt; is the weed-free yield. We show an example relating to sunflower grown at increasing densities of the weed &lt;em&gt;Sinapis arvensis&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(competition)
Ywf &amp;lt;- mean( competition$Yield[competition$Dens == 0] )
competition$YL &amp;lt;- ( Ywf - competition$Yield ) / Ywf * 100 

#nls fit
model &amp;lt;- nls(YL ~ NLS.YL(Dens, a, i), data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Formula: YL ~ NLS.YL(Dens, a, i)
## 
## Parameters:
##   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## a    8.207      1.187   6.914 1.93e-08 ***
## i   75.048      2.353  31.894  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 6.061 on 42 degrees of freedom
## 
## Number of iterations to convergence: 2 
## Achieved convergence tolerance: 2.685e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#drc fit
model &amp;lt;- drm(YL ~ Dens, fct = DRC.YL(), data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Yield-Loss function (Cousens, 1985) (2 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## i:(Intercept)   8.2068     1.1715  7.0056 1.427e-08 ***
## A:(Intercept)  75.0492     2.3298 32.2133 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  6.060578 (42 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above fit constrains the yield loss to be 0 when weed density is 0. This is logical, but, it has the important consequence that the weed-free yield is constrained to be equal to the observed weed-free yield, which is not realistic. Therefore, we can reparameterise the yield-loss function, in order to use the observed yield as the dependent variable.&lt;/p&gt;
&lt;p&gt;Indeed, from the above equation we derive:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_W  = Y_{WF}  - \frac{Y_L }{100}Y_{WF}  = Y_{WF} \left( {1 - \frac{Y_L }{100}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_W  = Y_{WF} \left( 1 - \frac{i\, X}{100 \left( 1 + \frac{i \, X}{a} \right) } \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This function can be fit with ‘drm()’, by using the ‘DRC.cousens85()’ self starting function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(Yield ~ Dens, fct = DRC.cousens85(), 
             data = competition)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Yield-Weed Density function (Cousens, 1985) (3 parms)
## 
## Parameter estimates:
## 
##                 Estimate Std. Error t-value   p-value    
## YWF:(Intercept) 30.47211    0.92763 32.8493 &amp;lt; 2.2e-16 ***
## i:(Intercept)    8.24038    1.36541  6.0351 3.857e-07 ***
## a:(Intercept)   75.07312    2.40366 31.2328 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  1.866311 (41 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sygmoidal-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sygmoidal curves&lt;/h1&gt;
&lt;p&gt;Sygmoidal curves are S-shaped and they may be increasing, decreasing, symmetric or non-simmetric around the inflection point. They are parameterised in countless ways, which may be often confusing. Therefore, we will show a common parameterisation, that is very useful in biological terms.&lt;/p&gt;
&lt;div id=&#34;logistic-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic curve&lt;/h2&gt;
&lt;p&gt;The logistic curve derives from the cumulative logistic distribution function; the curve is symmetric around the inflection point and it it may be parameterised as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + exp(b (X - e))}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the higher asymptote, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is the lower asymptote, &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; value producing a response half-way between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the slope around the inflection point. The parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can be positive or negative and, consequently, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; may increase or decrease as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;The above function is known as four-parameter logistic. If necessary, contraints can be put on parameter values, i.e. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; can be constrained to 0 (three-parameter logistic). Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be also contrained to 1 (two-parameter logistic).&lt;/p&gt;
&lt;p&gt;The four- and three-parameter logistic curves can be fit by ‘nls()’, respectively with the self-starting functions ‘SSfpl()’ and ‘SSlogis’ (‘nlme’ package). In these functions, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is replaced by &lt;span class=&#34;math inline&#34;&gt;\(scal = -1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With ‘drm()’, we can use the self-starting functions ‘L.4()’ and ‘L.3()’, The ‘L.2()’ function has been included in the ‘aomisc’ package.&lt;/p&gt;
&lt;p&gt;Logistic functions are very useful, e.g., for plant growth studies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(beetGrowth)
beetGrowth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    DAE weightInf weightFree
## 1   21   0.06000  0.0715091
## 2   21   0.06000  0.0662547
## 3   21   0.11000  0.0747931
## 4   27   0.20000  0.3368074
## 5   27   0.20000  0.3952256
## 6   27   0.21000  0.2520960
## 7   38   2.13000  2.3225072
## 8   38   3.03000  1.7163224
## 9   38   1.27000  1.2189231
## 10  49   6.13000 11.7761096
## 11  49   5.76000 13.6191507
## 12  49   7.78000 12.1462931
## 13  65  17.05000 33.1067720
## 14  65  22.48000 24.9648226
## 15  65  12.66000 34.6577561
## 16 186  21.51010 38.8329912
## 17 186  26.25887 27.8375016
## 18 186  27.67733 37.7165427&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(weightFree ~ DAE, fct = L.3(), data = beetGrowth)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Logistic (ED50 as parameter) with lower limit fixed at 0 (3 parms)
## 
## Parameter estimates:
## 
##                Estimate Std. Error t-value   p-value    
## b:(Intercept) -0.179393   0.039059 -4.5929 0.0003519 ***
## d:(Intercept) 34.532001   1.676430 20.5985 2.057e-12 ***
## e:(Intercept) 52.384788   1.580269 33.1493 1.838e-15 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  2.970191 (15 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model, log=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gompertz-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gompertz Curve&lt;/h2&gt;
&lt;p&gt;The Gompertz curve is parameterised in very many ways. We favour a parameterisation that resambles the one used for the logistic function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;were the parameters have the same meaning as those in the logistic function. The difference is that this curve is not symmetric around the inflection point. As for the logistic, we can have a four-, three- and two-parameter Gompertz functions, which can be fit by using ‘drm()’ and, respectively the ‘G.4()’, ‘G.3()’ and ‘G.2()’ sef-starters. The three-parameter Gompertz can also be fit with ‘nls()’, by using the ‘SSGompertz()’ self-starter in the ‘nlme’ package, although this is a different parameterisation.&lt;/p&gt;
&lt;p&gt;We give an example of the different shapes for the logistic (red) and Gompertz (black) functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-type-of-asimmetry&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another type of asimmetry&lt;/h2&gt;
&lt;p&gt;We have seen that, with respect to the logistic, the Gompertz shows a longer lag at the beginning, but raises steadily afterwards. We could describe a different pattern by changing the Gompertz function as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (X - e) \right] \right\} \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We add to the previous graph this function (in blue), to show how it differs from the logistic and Gompertz.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- 10; c &amp;lt;- 2; e &amp;lt;- 7; b &amp;lt;- - 0.5
curve( G.fun(x, b, c, d, e), xlim = c(0, 20) , xlab=&amp;quot;X&amp;quot;, ylab = &amp;quot;Y&amp;quot;)
curve( L.fun(x, b, c, d, e), add = T, col = &amp;quot;red&amp;quot; )
curve( E.fun(x, b, c, d, e), add = T, col = &amp;quot;blue&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The self-starters for this function are not yet available, at least to the best of my knowledge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-logistic-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log-logistic curve&lt;/h2&gt;
&lt;p&gt;In many applications, the sigmoidal response curve is symmetric on the logarithm of x, which requires a log-logistic curve (a log-normal curve would be practically equivalent, but it is used far less often). For example, in biologic assays (but also in germination assays), the log-logistic curve is defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \exp \left\{ b \left[ \log(X) - \log(e) \right] \right\} } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the parameters have the very same meanng as the logistic equationn given above. It is easy to see that the above equation is equivalent to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = c + \frac{d - c}{1 + \left( \frac{X}{e} \right)^b}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another possible parameterisation is the so-called Hill function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = \frac{a \, X^b}{ X^b + e^b} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{a \, X^b}{ X^b + e^b} =  \frac{a}{ \frac{X^b}{X^b} + \frac{c^b}{X^b}} = \frac{a}{ 1 + \left( \frac{c}{X} \right)^b} = \frac{a}{ 1 + \left( \frac{c}{X} \right)^b} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Log-logistic functions are used for crop growth, seed germination and bioassay work and they can have the same constraints as the logistic function. The four-parameter logistic is available as ‘LL.4()’ in the ‘drc’ package and as ‘SSfpl()’ in the ‘nlme’ package. This latter function replaces &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(scal = 1/b\)&lt;/span&gt;. Also in ‘drc’, we have ‘LL.3()’ (three-parameter logistic, with &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;) and ‘LL.2()’ (two-parameter logistic, with &lt;span class=&#34;math inline&#34;&gt;\(d = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = 0\)&lt;/span&gt;). In ‘nlme’ we have ‘SSlogis()’, that is a three-parameter logistic with &lt;span class=&#34;math inline&#34;&gt;\(scal = 1/b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We show an example of a log-logistic fit, relating to a bioassay with &lt;em&gt;Brassica rapa&lt;/em&gt; treated at increasing dosages of an herbicide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(brassica)
model &amp;lt;- drm(FW ~ Dose, fct = LL.4(), data = brassica)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model fitted: Log-logistic (ED50 as parameter) (4 parms)
## 
## Parameter estimates:
## 
##               Estimate Std. Error t-value   p-value    
## b:(Intercept)  1.45113    0.24113  6.0181 1.743e-06 ***
## c:(Intercept)  0.34948    0.18580  1.8810   0.07041 .  
## d:(Intercept)  4.53636    0.20514 22.1140 &amp;lt; 2.2e-16 ***
## e:(Intercept)  2.46557    0.35111  7.0221 1.228e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error:
## 
##  0.4067837 (28 degrees of freedom)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-curve-type-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull curve (type 2)&lt;/h2&gt;
&lt;p&gt;The type 2 Weibull curve is for the Gompertz curve what the log-logistic curve is for the logistic curve. The equation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \exp \left\{- \exp \left[ b \, (log(X) - log(e)) \right] \right\} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the parameters have the very same meaning as the other sygmoidal curves given above.&lt;/p&gt;
&lt;p&gt;As for fitting, the ‘drc’ package contains the self-starting functions ‘W2.2()’, ‘W2.3()’ and ‘W2.4()’ that can be used to fit respectively the two-, three- and four-parameter type 2 Weibull functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weibull-curve-type-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weibull curve (type 1)&lt;/h2&gt;
&lt;p&gt;The type 1 Weibull is similar to the type 2 Weibull, but describes a different type of asymmetry (see above):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y = c + (d - c) \left\{ 1 - \exp \left\{- \exp \left[ b \, (log(X) - log(e)) \right] \right\} \right\}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- drm(FW ~ Dose, fct = W2.4(), data = brassica)
model2 &amp;lt;- drm(FW ~ Dose, fct = W1.4(), data = brassica)
plot(model)
plot(model2, add=T, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/articles/usefulEquations_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About this site</title>
      <link>/aboutthis/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aboutthis/</guid>
      <description>


&lt;div id=&#34;the-broken-bridge-between-biologists-and-statisticians&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The broken bridge between biologists and statisticians&lt;/h1&gt;
&lt;p&gt;Hi, all!&lt;/p&gt;
&lt;p&gt;This website deals with statistic. “What’s new then?”, you might say. “There’s plenty of them, out there!”. Yes, that’s right. However, the new thing is that I am not a statistician. I am an applied biologist who happened to be fascinated by the idea that natural phenomena might be described, interpreted and understood by using the universal language of math. Since then, I’ve been mainly involved with experimental design and data analyses. Indeed, I turned myself into an applied statistician…&lt;/p&gt;
&lt;p&gt;My collegue Marcin Kozak, in one of his papers, noted that, although efficient communication between biologists and statisticians is fundamental for the development of knowledge, there is indeed a problem in this respect (the broken bridge)&lt;span class=&#34;citation&#34;&gt;(Kozak 2016)&lt;/span&gt;. I would like to help build such bridge.&lt;/p&gt;
&lt;p&gt;Indeed, I developed the awareness that biostatistic is a fundamental component of science: it helps reach more reliable conclusions. Now I’m going grey… I would like to pass some of my experience to my younger collegues and help them save a few headaches… Though my language might be rough and overly simplistic, I do hope it is useful to reach the above aim: spread a better awareness of the potential usefulness, beauty and limitations of biostatistic.&lt;/p&gt;
&lt;p&gt;This is an ongoing project. All blog posts are listed under the ‘Home’ directory of this website and they are searchable by date and tag. You will also find a series of tutorials and a web book (in italian), under the list pages of the single sections, i.e., &lt;a href=&#34;/tutorials/&#34;&gt;tutorials&lt;/a&gt; and &lt;a href=&#34;/_statbook/&#34;&gt;book&lt;/a&gt;. If you would like to contribute or comment, please, drop me a note to &lt;a href=&#34;mailto:andrea.onofri@unipg.it&#34;&gt;andrea.onofri@unipg.it&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-KozakCommunicationagriculturalscientists2016&#34;&gt;
&lt;p&gt;&lt;span style=&#34;font-variant: small-caps;&#34;&gt;Kozak, M&lt;/span&gt; (2016) Communication between agricultural scientists and statisticians: A broken bridge? &lt;em&gt;Scientia Agricola&lt;/em&gt; &lt;strong&gt;73&lt;/strong&gt;, 505–511&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tutorials and articles</title>
      <link>/tutorials/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/tutorials/</guid>
      <description>

&lt;h1 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/nonLinearRegression/&#34;&gt;Non-linear regression analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onofriandreapg.github.io/agriCensData/&#34;&gt;Interval-censored data in agriculture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/seedGermination/&#34;&gt;Analysing seed germination data with R: a tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;articles&#34;&gt;Articles&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Blogdow: &lt;a href=&#34;/articles/BlogdownSteps/&#34;&gt;My learning path with blogdown&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>My first experience with blogdown</title>
      <link>/2018/2018-11_15-first-day-with-blogdown/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-11_15-first-day-with-blogdown/</guid>
      <description>&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;

&lt;p&gt;I thought that it might be useful to write down a few notes, to summarise my steps ahead, during the learning process. I do not work with blogdown everyday and I tend to forget things quite easily. Therefore, these notes may help me recap how far I have come. And they might also help other beginners, to speed up their initial steps with such a powerful blogging platform.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll find my notes &lt;a href=&#34;/articles/blogdownSteps/&#34;&gt;here&lt;/a&gt;; I&amp;rsquo;ll try to keep them updated.&lt;/p&gt;

&lt;p&gt;Happy reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My learning path with blogdown</title>
      <link>/articles/blogdownsteps/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/articles/blogdownsteps/</guid>
      <description>

&lt;p&gt;This is my first day at work with blogdown. I must admit it is pretty overwhelming at the beginning &amp;hellip;&lt;/p&gt;

&lt;p&gt;I worked with R Studio and installed the library &amp;lsquo;blogdown&amp;rsquo;. I followed the instructions and used the &amp;lsquo;NEW PROJECT&amp;rsquo; menu and added a &amp;lsquo;NEW BLOGDOWN SITE&amp;rsquo; to my file system. I selected the &amp;lsquo;hugo-blog-jeffprod&amp;rsquo; theme. This created a bunch of directories in my working directory. If I use the &amp;lsquo;serve_site()&amp;rsquo; function in console, the Hugo server is called and the site is continuously updated, as soon as I make changes and save them. When the server is started, a message appears, saying: &amp;ldquo;Serving the directory PROJECTDIR at &lt;a href=&#34;http://127.0.0.1:7958&amp;quot;&#34;&gt;http://127.0.0.1:7958&amp;quot;&lt;/a&gt;. I copied the http address on Safari (I am working on Mac OS&amp;hellip;), so that I can check my changes there.&lt;/p&gt;

&lt;p&gt;Here are my steps to the learning the process.&lt;/p&gt;

&lt;h1 id=&#34;step-1-the-mathjax-problem&#34;&gt;STEP 1 - The MathJax problem&lt;/h1&gt;

&lt;p&gt;I added a post to the &amp;lsquo;content&amp;rsquo; directory. This was a trial post, with an equation in latex code. I immediately noted that there was no support to MathJax. Surfing the net, I found a solution for this. These were my steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The folder &amp;lsquo;themes/hugo-blog-jeffprod/layouts/partials&amp;rsquo; contains the template file &amp;lsquo;header.html&amp;rsquo;. The blogdown documentation suggests not to directly modify this file. Thus I created a copy on the &amp;lsquo;layouts/partials/&amp;rsquo; folder in my root directory. This folder did not exist at the beginning, so I created it.&lt;/li&gt;
&lt;li&gt;I opened the newly created file &amp;lsquo;layouts/partials/header.html&amp;rsquo; and added the following lines within the &lt;head&gt; &amp;hellip; &lt;/head&gt; section:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Equations seem to render ok, now.&lt;/p&gt;

&lt;h1 id=&#34;step-2-change-the-blog-name-and-author&#34;&gt;STEP 2 - Change the blog name and author&lt;/h1&gt;

&lt;p&gt;I modified the &amp;lsquo;config.toml&amp;rsquo; file in the root directory and added name and author of the site. An easy step&amp;hellip;&lt;/p&gt;

&lt;h1 id=&#34;step-3-clean-the-example-files&#34;&gt;STEP 3 - Clean the example files&lt;/h1&gt;

&lt;p&gt;There were a lot of example posts in the &amp;lsquo;content&amp;rsquo; directory. I removed them all, with no problems. I also had to remove the content of the &amp;lsquo;static&amp;rsquo; folder. These files were not needed, because they belonged to the example posts.&lt;/p&gt;

&lt;h1 id=&#34;step-4-where-do-i-put-my-post&#34;&gt;Step 4 - Where do I put my post?&lt;/h1&gt;

&lt;p&gt;New posts need to be put in the &amp;lsquo;content/post&amp;rsquo; directory. In order to add a post, I am using the function &amp;lsquo;blogdown:::new_post_addin()&amp;rsquo; (I like working in console&amp;hellip;), which creates a correct skeleton. Blog posts are listed under the home page of my blog, sorted by date and by tags.&lt;/p&gt;

&lt;h1 id=&#34;step-5-where-do-i-put-my-non-post-pages&#34;&gt;Step 5 - Where do I put my non-post pages?&lt;/h1&gt;

&lt;p&gt;I did not intend to create a pure blog. I wanted also to add static pages, such as articles, tutorials, a project page, links and other stuff. I noted that, when .md or .Rmd files are left in the &amp;lsquo;content&amp;rsquo; folder, a folder with the same name is created in &amp;lsquo;public&amp;rsquo; and the post is rendered within that folder. Therefore, I put a &amp;lsquo;links.md&amp;rsquo; file in the &amp;lsquo;content&amp;rsquo; folder, containing a few links to other web pages of interest. This file is not rendered as a post and it is not visible in my home page. However, a folder named &amp;lsquo;link&amp;rsquo; is created in the public directory, containing the &amp;lsquo;links.md&amp;rsquo; file, rendered as &amp;lsquo;index.html&amp;rsquo; file. Therefore, I can link this file by using the &amp;lsquo;/link/&amp;rsquo; address.&lt;/p&gt;

&lt;h1 id=&#34;step-6-how-do-i-create-further-entries-for-the-navigation-bar&#34;&gt;Step 6 - How do I create further entries for the navigation bar?&lt;/h1&gt;

&lt;p&gt;I just created the &amp;lsquo;/link/index.html&amp;rsquo; file in the &amp;lsquo;public&amp;rsquo; directory. I would like the reader to access this web page by clicking on an appropriate link at the top of the home-page, next to the &amp;lsquo;Home&amp;rsquo; and &amp;lsquo;About me&amp;rsquo; links. In order to add this link, I had to edit again the &amp;lsquo;layouts/partials/header.html&amp;rsquo; file. In the &amp;lsquo;nav&amp;rsquo; section, there was the line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;navbar-item&amp;quot; href=&amp;quot;{{ &amp;quot;about&amp;quot; | absURL }}&amp;quot;&amp;gt;About me&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line creates the &amp;lsquo;About me&amp;rsquo; link on the navigation bar. Below this line, I added another line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;navbar-item&amp;quot; href=&amp;quot;{{ &amp;quot;links&amp;quot; | absURL }}&amp;quot;&amp;gt;Links&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I guess I can use this approach to redirect readers to other web pages of interest.&lt;/p&gt;

&lt;h1 id=&#34;step-7-i-need-to-put-some-order-in-the-content-folder-4-1-2019&#34;&gt;Step 7 - I need to put some order in the &amp;lsquo;content&amp;rsquo; folder&amp;rsquo; (4/1/2019)!&lt;/h1&gt;

&lt;p&gt;After adding a few non-post pages to my content folder I wanted to avoid too much confusion. I discovered that I can add folders to the &amp;lsquo;content&amp;rsquo; folder. They are rendered as such in the public directory. For example, I created an &amp;lsquo;article&amp;rsquo; folder and put a &amp;lsquo;page1.md&amp;rsquo; file inside it. After rendering, a folder &amp;lsquo;article/page1/&amp;rsquo; is created in the &amp;lsquo;public&amp;rsquo; folder, containing the &amp;lsquo;index.html&amp;rsquo; page, which is the rendered version of the &amp;lsquo;page1.md&amp;rsquo; file.&lt;/p&gt;

&lt;p&gt;I think I start feeling slightly more confident with blogdown!&lt;/p&gt;

&lt;h1 id=&#34;step-8-literature-support-7-1-2019&#34;&gt;Step 8 - Literature support (7/1/2019)!&lt;/h1&gt;

&lt;p&gt;This problem took me a while to solve. Lately, I discovered that, in order to be able to insert citations, the post file needs to be &amp;lsquo;.Rmd&amp;rsquo; and not &amp;lsquo;.md.&amp;rsquo; Furthermore, the literature file (&amp;lsquo;.bib&amp;rsquo;) and the &amp;lsquo;.csl&amp;rsquo; file need to be in the &amp;lsquo;/content/&amp;rsquo; folder. A reference to both files need to be put in the YAML header, e.g., :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bibliography: myFile.bib
csl: myStyle.csl
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-9-google-analytics-6-2-2019&#34;&gt;Step 9 - Google Analytics (6/2/2019)&lt;/h1&gt;

&lt;p&gt;I would like to track the visits I receive. To do so I enabled Google Analytics and got my tracking id (&lt;a href=&#34;https://support.google.com/analytics/answer/1008080&#34;&gt;see here&lt;/a&gt;). I also worked on my &amp;lsquo;config.toml&amp;rsquo; file and added the following statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GoogleAnalytics = &amp;quot;UA-my_ID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In contrast to what I expected, Google Analytics did not work and no hits were recorded for my blog. Lately, I worked on the &amp;lsquo;layouts/partials/header.html&amp;rsquo; and added the following code, right after the &amp;lsquo;&lt;head&gt;&amp;rsquo; tag (just replaced &amp;lsquo;UA-my_ID&amp;rsquo; with my tracking id).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Global Site Tag (gtag.js) - Google Analytics --&amp;gt;
&amp;lt;script async src=&amp;quot;https://www.googletagmanager.com/gtag/js?id=UA-my_ID&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag(&#39;js&#39;, new Date());

  gtag(&#39;config&#39;, &#39;UA-my_ID&#39;);
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now Google Analytics works ok!&lt;/p&gt;

&lt;h1 id=&#34;step-10-add-an-rss-template&#34;&gt;Step 10 - Add an RSS Template&lt;/h1&gt;

&lt;p&gt;I wanted my RSS feed to contain the whole post and not only a description. I did the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Took the default template from  &lt;a href=&#34;https://gohugo.io/templates/rss/#the-embedded-rss-xml&#34;&gt;this site&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Replaced &amp;lsquo;{{ .Summary | html }}&amp;rsquo; with &amp;lsquo;{{ .Content | html }}&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Saved the file as &amp;lsquo;RSS.xml&amp;rsquo;under the &amp;lsquo;layouts&amp;rsquo; directory.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/about/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;I have been always fond of science. I got my degree in Agricultural Sciences in 1987 and I like to think of  myself as an applied biologist. When I started doing research (in 1990) I was very much fascinated by the idea that natural phenomena might be described, interpreted and understood by using the universal language of math. My PhD studies revolved around nonlinear regression analyses and, afterwards, I&amp;rsquo;ve been mainly involved with biostatistics, data analyses and modelling. I do think that biostatistics is fundamental in developing scientific thinking.&lt;/p&gt;

&lt;p&gt;Today, I am a Professor and I teach Experimental Methodology at the University of Perugia. My students are biologists and they do not have a very deep background in maths and statistic. Teaching them requires great care and a very simple language.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sample variance and population variance: which of the two?</title>
      <link>/2018/2018-11-09-sample-variance/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-11-09-sample-variance/</guid>
      <description>


&lt;p&gt;Teaching experimental methodology in agriculture related master courses poses some peculiar problems. One of these is to explain the difference between sample variance and population variance. For the students it is usually easy to grasp the idea that, being the mean the ‘center’ of the dataset, it is relevant to measure the average distance to the mean for all individuals in the dataset. Of course, we need to take the sum of squared distances, otherwise negative and positive residuals cancel each other out.&lt;/p&gt;
&lt;p&gt;It is also very intuitive that the average of squared residuals (mean square or variance) is calculated by using the following expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - \mu )^2}{n}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, this is not what R (and other software) does. R divides by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; and, at this stage, the students usually ask: “&lt;em&gt;why are you telling me that I have to divide the sum of squares by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;, instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;? I have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals, not &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;!&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;You may think that the answer is pretty obvious. I don’t think so. The concept of variance is introduced at the beginning of the course, within the frame of descriptive statistics. At this stage, the students know nothing about probability distributions, sampling, inference and all the related concepts. They do not even know anything about degrees of freedom, yet!&lt;/p&gt;
&lt;p&gt;If I were a mathematician and were facing students in math/stat, I could show the class how clever I am by giving a formal proof that &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; is the correct denominator in most circumstances. In particular, this is true when we have a sample taken from a population and we want to infere the variance of the population by using the sample. The formal proof can be found in most stat books and I am attaching it at the end of this post. However, I will never show this formal proof to my students at this stage. Indeed, such a proof assumes that the students know what a sampling distribution is, what the standard error is and how it is calculated. Furthermore, students in agriculture are usually very reluctant, when it comes to dealing with maths!&lt;/p&gt;
&lt;p&gt;Therefore, I usually refrain from trying to appear more clever than I am. My question: “is it possible to teach the difference between the population variance and the sample variance, without talking about degrees of freedom or other ‘difficult’ concepts?”. Here is how I try to put it.&lt;/p&gt;
&lt;p&gt;Let’s take a big, but finite population, i.e. an hectar of maize plants (roughly 70,000 plants). Let’s imagine that we know the individual yields for all plants in the population. With R we can get those yields by taking random values in the interval from 150 to 250 grams (this is reasonable… students in agriculture know these things pretty well! And, I am not referring to any density distribution… it would be too much at this stage).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
pop &amp;lt;- runif(70000, min = 150, max = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our population (I will not show it… lack of space!). It is a finite population, so we can easily calculate the mean and the variance, i.e. the mean squared distance of all 70,000 individuals to the mean. You see that I am using the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; underneath).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0893&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2 &amp;lt;- sum( ( pop - mean(pop) )^2 )/70000
sigma2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 835.184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now forget about the population. In the field, we would never be able to measure the yield for all the plants in one hectar, due to lack of resources (time and money). Therefore, let’s measure 10 randomly selected plants (one in 7000; samples are often small in agriculture!). We can do this easily in R by sampling the original population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(pop, 10)
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 237.7442 242.4557 243.2478 152.7499 246.9411 154.4087 226.4784
##  [8] 236.2380 216.2226 187.3896&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now calculate the mean and the variance for the sample, by using the equation above (i.e. using &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; as the denominator).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 214.3876&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma2_s &amp;lt;- sum( ( x - mean(x) )^2 )/10
sigma2_s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1197.856&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not know the population but we know our sample. As usual, in lack of other information, we conclude that the population should have the same characteristics of our sample. This is usually seen as a reasonable guess; at least the most reasonable one. The procedure we use to make a guess is called ‘estimator’ and the guess in itself is an ‘estimate’. Have we used a good estimator?&lt;/p&gt;
&lt;p&gt;We can’t answer. Unless we repeat the sampling process for a lot of times and see how our estimates behave in the long run. Our estimator is good if, in the long run, it converges on the real value for the population. Let’s check this: we repeatedly take samples of 10 plants from our population and calculate the mean and variance as above. We repeat this process 10,000 times, storing the 10,000 means and variances in two vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanS &amp;lt;- c(); varS &amp;lt;- c()
for(i in 1:10000){
  x &amp;lt;- sample(pop, 10)
  meanS[i] &amp;lt;- mean(x)
  varS[i] &amp;lt;- sum( ( x - mean(x) )^2 )/10
}
mean(meanS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200.0309&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 753.4311&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the mean of the means is 200.03. We got really very close to the real mean of the population. Indeed, the mean of the sample is a good (unbiased) estimator of the mean of the population.&lt;/p&gt;
&lt;p&gt;On the other hand, the mean of the variances is 753.43. Please note that this is much smaller than the real variance of the population. It means that if we use the equation above (with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; at the denominator) to calculate the variance for the sample, we do not have a good estimator of the variance for the whole population. We can now look at the ratio between the variance of the population and our guess in the long run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(pop)/mean(varS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.108523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that this is roughly equal to &lt;span class=&#34;math inline&#34;&gt;\(10/9\)&lt;/span&gt;. In other words, if we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;take the sample and calculate the variance by using the equation above,&lt;/li&gt;
&lt;li&gt;multiply by &lt;span class=&#34;math inline&#34;&gt;\(n = 10\)&lt;/span&gt; and&lt;/li&gt;
&lt;li&gt;divide by &lt;span class=&#34;math inline&#34;&gt;\(n - 1 = 9\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we can get a good estimator. This leads us to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\sigma ^2} = \frac{\sum\limits_{i = 1}^n ({X_i} - m )^2}{n - 1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is exactly the variance for the sample. This one we can easily calculate with R.&lt;/p&gt;
&lt;div id=&#34;which-of-the-two-then&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which of the two, then?&lt;/h1&gt;
&lt;p&gt;Just rememeber:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the variance (mean square) is calculated by dividing the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. This makes sense, because we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; squared residuals.&lt;/li&gt;
&lt;li&gt;However, if we have a random sample taken from a population, and if our aim is to estimate the variance for the whole population, we need to divide the sum of squared residuals by &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;. Otherwise, we get an understimation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-formal-proof-look-how-clever-i-am&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The formal proof (look how clever I am!)&lt;/h1&gt;
&lt;p&gt;We have a population with mean equal to &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance equal to &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Let’s take a sample with its mean &lt;span class=&#34;math inline&#34;&gt;\(m \neq \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m - \mu = \varepsilon\)&lt;/span&gt;. For the population, we can write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu )^2  = \sum\limits_{i = 1}^n (X_ i - m + \varepsilon)^2 = \sum\limits_{i = 1}^n ({X_i} - m)^2 + n \varepsilon^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We already see that the sum of squared residuals is higher if we take &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, unless &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon = 0\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(m = \mu\)&lt;/span&gt;. From the above, we derive&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  \sum\limits_{i = 1}^n (X_i - \mu )^2  - n ( {m - \mu } )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we consider the first equation above (variance for the population) we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n (X_i - \mu)^2 =  n \sigma^2 - n( m - \mu  )^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We see that the rightmost term of this equation (&lt;span class=&#34;math inline&#34;&gt;\(n ( m - \mu )^2\)&lt;/span&gt; is the sum of squared distances for the sampling distribution of the mean. That is &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times the squared standard error. Therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } n{\sigma ^2} - n\left( {\frac{{{\sigma ^2}}}{n}} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum\limits_{i = 1}^n {{{({X_i} - \bar X)}^2} = } (n - 1){\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we go! Indeed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{\sum\limits_{i = 1}^n (X_i - m)^2 }{n - 1} = {\sigma ^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is R dangerous? Side effects of free software for biologists</title>
      <link>/2014/2014-06-08-the-danger-of-r/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/2014/2014-06-08-the-danger-of-r/</guid>
      <description>


&lt;p&gt;When I started my career in the biological field (it’s already 25 years ago), only the luckiest of us had access to very advanced statistical software. Licenses were very expensive and it was not easy to convince the boss that they were really necessary: “Why do you need to spend so much money to perform an ANOVA?”. Indeed, simple one-way or two-ways ANOVAs were quite easy to perform and one of the people in my group had already built the appropriate routines for several designs, by using the GW-BASIC language. But I wanted more!&lt;/p&gt;
&lt;p&gt;My agriculture experiments often showed complex designs and split-plot, strip-plot, subsampling and repeated measures/experiments were much more than an exception. I decided to start writing several Quick-BASIC routines to implement those types of ANOVAs on my PC. At the beginning of the ’90s, nonlinear response models became in fashion and I had to programme my first Gauss-Newton optimiser, also with Quick-BASIC. GLMs were not yet widespread among biologists and I mainly relied on stabilising transformations for those cases where the basic assumptions for linear/nonlinear models were not met.&lt;/p&gt;
&lt;p&gt;This ‘poor and humble’ statistical life gave me an undeniable advantage: it forced me into thoroughly studying and understanding the principles of each new technique and algorithm, before I could be able to programme it. I had to become acquainted with all those strange mathematical objects, such as matrices, eigenvalues and determinants, which are not usually a part of the mathematical background of biologists (at least in Italy). And my BASIC routines, once completed, could only do what they were programmed to do; only one specific solution to one specific task, no further options and no error management. In one sentence: no general solutions.&lt;/p&gt;
&lt;p&gt;Nowadays I have R: it is free and everything is possible and smooth. A few lines of code and I can fit whatever model comes to my mind in a few minutes. I can try several options: which is the best one? Which is the one that makes my data tell the story I would like to tell? Biometry books have changed as well; they have taken a more ‘algorithmic’ approach and math is confined within boxes that may be easily skipped. I have to admit that I frequently skip them: code snippets are more than enough to do the trick and I can also find thousands of them in the Internet. In other words, why should I bother studying such an abstract thing called statistics when I have R?&lt;/p&gt;
&lt;p&gt;Obviously this is just an exaggeration. However, I have the feeling that there might be some drawbacks relating to the availability of such a powerful free software. Biologists (especially students) may mistake studying R for studying stats. I am very much surprised to see how many complex models are fit on these days, with hardly any biological and statistical justifications and with very little care about the basic assumptions that these models make. A few days ago a PhD student at my Department showed me the results of fitting a reduced rank regression to a biological dataset. He was very proud of how he mastered the R coding process: by using the correct option (found after a thorough search over the Internet). He had even managed to avoid a ‘pretty strange’ warning message. Unfortunately, that warning message had been misinterpreted and therefore the analysis was wrong from the very beginning.&lt;/p&gt;
&lt;p&gt;A warning message for all biologists (including myself): R is really wonderful, but it will not necessarily bring to sound data analyses. Let’s use R, but let’s study stats first!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Useful links</title>
      <link>/links/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/links/</guid>
      <description>&lt;p&gt;This site makes is written in markdown and Rmarkdown and makes use of a wide range of open source technologies. I&amp;rsquo;m adding a few links, for you to learn more.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;The R project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com&#34;&gt;R studio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com&#34;&gt;R markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown and bookdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gohugo.io&#34;&gt;Hugo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>